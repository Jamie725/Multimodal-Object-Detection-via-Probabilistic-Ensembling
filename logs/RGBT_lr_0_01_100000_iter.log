../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
4 channel input
[32m[04/15 05:24:36 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 05:24:37 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.06 seconds.
[5m[31mWARNING[0m [32m[04/15 05:24:37 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 05:24:38 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 05:24:38 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 05:24:38 d2.data.build]: [0mDistribution of instances among all 79 categories:
[36m|   category    | #instances   |   category    | #instances   |   category    | #instances   |
|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|
|    person     | 21744        |    bicycle    | 3806         |      car      | 39372        |
|  motorcycle   | 0            |   airplane    | 0            |      bus      | 0            |
|     train     | 0            |     truck     | 0            |     boat      | 0            |
| traffic light | 0            | fire hydrant  | 0            |   stop sign   | 0            |
| parking meter | 0            |     bench     | 0            |     bird      | 0            |
|      cat      | 0            |      dog      | 219          |     horse     | 0            |
|     sheep     | 0            |      cow      | 0            |   elephant    | 0            |
|     bear      | 0            |     zebra     | 0            |    giraffe    | 0            |
|   backpack    | 0            |   umbrella    | 0            |    handbag    | 0            |
|      tie      | 0            |   suitcase    | 0            |    frisbee    | 0            |
|     skis      | 0            |   snowboard   | 0            |  sports ball  | 0            |
|     kite      | 0            | baseball bat  | 0            | baseball gl.. | 0            |
|  skateboard   | 0            |   surfboard   | 0            | tennis racket | 0            |
|    bottle     | 0            |  wine glass   | 0            |      cup      | 0            |
|     fork      | 0            |     knife     | 0            |     spoon     | 0            |
|     bowl      | 0            |    banana     | 0            |     apple     | 0            |
|   sandwich    | 0            |    orange     | 0            |   broccoli    | 0            |
|    carrot     | 0            |    hot dog    | 0            |     pizza     | 0            |
|     donut     | 0            |     cake      | 0            |     chair     | 0            |
|     couch     | 0            | potted plant  | 0            |      bed      | 0            |
| dining table  | 0            |    toilet     | 0            |      tv       | 0            |
|    laptop     | 0            |     mouse     | 0            |    remote     | 0            |
|   keyboard    | 0            |  cell phone   | 0            |   microwave   | 0            |
|     oven      | 0            |     bike      | 0            |    hydrant    | 0            |
|     motor     | 0            |     rider     | 0            |     light     | 0            |
|     sign      | 0            | motor vehicle | 0            |  human face   | 0            |
|  hair drier   | 0            |               |              |               |              |
|     total     | 65141        |               |              |               |              |[0m
[32m[04/15 05:24:38 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 05:24:39 d2.data.build]: [0mUsing training sampler TrainingSampler
============== The  0  *  2000  iterations ============
[32m[04/15 05:24:50 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 05:25:20 d2.utils.events]: [0m eta: 0:18:21  iter: 19  total_loss: 1.312  loss_cls: 0.262  loss_box_reg: 0.075  loss_rpn_cls: 0.788  loss_rpn_loc: 0.141  time: 0.5532  data_time: 0.0675  lr: 0.000200  max_mem: 3062M
[32m[04/15 05:25:31 d2.utils.events]: [0m eta: 0:18:04  iter: 39  total_loss: 0.751  loss_cls: 0.222  loss_box_reg: 0.112  loss_rpn_cls: 0.316  loss_rpn_loc: 0.113  time: 0.5489  data_time: 0.0079  lr: 0.000400  max_mem: 3062M
[32m[04/15 05:25:41 d2.utils.events]: [0m eta: 0:17:38  iter: 59  total_loss: 0.612  loss_cls: 0.176  loss_box_reg: 0.110  loss_rpn_cls: 0.192  loss_rpn_loc: 0.081  time: 0.5432  data_time: 0.0076  lr: 0.000599  max_mem: 3062M
[32m[04/15 05:25:52 d2.utils.events]: [0m eta: 0:17:21  iter: 79  total_loss: 0.754  loss_cls: 0.245  loss_box_reg: 0.177  loss_rpn_cls: 0.154  loss_rpn_loc: 0.111  time: 0.5411  data_time: 0.0101  lr: 0.000799  max_mem: 3062M
[32m[04/15 05:26:03 d2.utils.events]: [0m eta: 0:17:10  iter: 99  total_loss: 0.896  loss_cls: 0.316  loss_box_reg: 0.277  loss_rpn_cls: 0.161  loss_rpn_loc: 0.104  time: 0.5390  data_time: 0.0069  lr: 0.000999  max_mem: 3062M
[32m[04/15 05:26:14 d2.utils.events]: [0m eta: 0:16:57  iter: 119  total_loss: 0.709  loss_cls: 0.263  loss_box_reg: 0.165  loss_rpn_cls: 0.145  loss_rpn_loc: 0.096  time: 0.5371  data_time: 0.0076  lr: 0.001199  max_mem: 3062M
[32m[04/15 05:26:25 d2.utils.events]: [0m eta: 0:16:51  iter: 139  total_loss: 0.619  loss_cls: 0.227  loss_box_reg: 0.210  loss_rpn_cls: 0.116  loss_rpn_loc: 0.076  time: 0.5388  data_time: 0.0110  lr: 0.001399  max_mem: 3062M
[32m[04/15 05:26:35 d2.utils.events]: [0m eta: 0:16:37  iter: 159  total_loss: 0.797  loss_cls: 0.279  loss_box_reg: 0.262  loss_rpn_cls: 0.135  loss_rpn_loc: 0.093  time: 0.5379  data_time: 0.0093  lr: 0.001598  max_mem: 3062M
[32m[04/15 05:26:46 d2.utils.events]: [0m eta: 0:16:27  iter: 179  total_loss: 0.604  loss_cls: 0.213  loss_box_reg: 0.189  loss_rpn_cls: 0.124  loss_rpn_loc: 0.082  time: 0.5368  data_time: 0.0059  lr: 0.001798  max_mem: 3062M
[32m[04/15 05:26:57 d2.utils.events]: [0m eta: 0:16:17  iter: 199  total_loss: 0.662  loss_cls: 0.227  loss_box_reg: 0.266  loss_rpn_cls: 0.088  loss_rpn_loc: 0.095  time: 0.5375  data_time: 0.0070  lr: 0.001998  max_mem: 3062M
[32m[04/15 05:27:08 d2.utils.events]: [0m eta: 0:16:05  iter: 219  total_loss: 0.885  loss_cls: 0.309  loss_box_reg: 0.305  loss_rpn_cls: 0.124  loss_rpn_loc: 0.078  time: 0.5379  data_time: 0.0071  lr: 0.002198  max_mem: 3062M
[32m[04/15 05:27:18 d2.utils.events]: [0m eta: 0:15:53  iter: 239  total_loss: 0.775  loss_cls: 0.305  loss_box_reg: 0.271  loss_rpn_cls: 0.080  loss_rpn_loc: 0.099  time: 0.5366  data_time: 0.0082  lr: 0.002398  max_mem: 3062M
[32m[04/15 05:27:29 d2.utils.events]: [0m eta: 0:15:43  iter: 259  total_loss: 0.976  loss_cls: 0.310  loss_box_reg: 0.393  loss_rpn_cls: 0.096  loss_rpn_loc: 0.105  time: 0.5364  data_time: 0.0110  lr: 0.002597  max_mem: 3062M
[32m[04/15 05:27:40 d2.utils.events]: [0m eta: 0:15:30  iter: 279  total_loss: 0.852  loss_cls: 0.267  loss_box_reg: 0.293  loss_rpn_cls: 0.077  loss_rpn_loc: 0.082  time: 0.5357  data_time: 0.0082  lr: 0.002797  max_mem: 3062M
[32m[04/15 05:27:51 d2.utils.events]: [0m eta: 0:15:20  iter: 299  total_loss: 0.962  loss_cls: 0.324  loss_box_reg: 0.418  loss_rpn_cls: 0.078  loss_rpn_loc: 0.072  time: 0.5361  data_time: 0.0079  lr: 0.002997  max_mem: 3062M
[32m[04/15 05:28:02 d2.utils.events]: [0m eta: 0:15:10  iter: 319  total_loss: 0.907  loss_cls: 0.301  loss_box_reg: 0.348  loss_rpn_cls: 0.095  loss_rpn_loc: 0.099  time: 0.5365  data_time: 0.0062  lr: 0.003197  max_mem: 3062M
[32m[04/15 05:28:13 d2.utils.events]: [0m eta: 0:15:00  iter: 339  total_loss: 0.744  loss_cls: 0.259  loss_box_reg: 0.317  loss_rpn_cls: 0.095  loss_rpn_loc: 0.104  time: 0.5373  data_time: 0.0081  lr: 0.003397  max_mem: 3062M
[32m[04/15 05:28:24 d2.utils.events]: [0m eta: 0:14:50  iter: 359  total_loss: 0.774  loss_cls: 0.266  loss_box_reg: 0.354  loss_rpn_cls: 0.064  loss_rpn_loc: 0.083  time: 0.5377  data_time: 0.0068  lr: 0.003596  max_mem: 3062M
[32m[04/15 05:28:35 d2.utils.events]: [0m eta: 0:14:41  iter: 379  total_loss: 0.845  loss_cls: 0.320  loss_box_reg: 0.387  loss_rpn_cls: 0.070  loss_rpn_loc: 0.095  time: 0.5396  data_time: 0.0064  lr: 0.003796  max_mem: 3062M
[32m[04/15 05:28:47 d2.utils.events]: [0m eta: 0:14:31  iter: 399  total_loss: 0.939  loss_cls: 0.342  loss_box_reg: 0.434  loss_rpn_cls: 0.070  loss_rpn_loc: 0.078  time: 0.5406  data_time: 0.0071  lr: 0.003996  max_mem: 3062M
[32m[04/15 05:28:57 d2.utils.events]: [0m eta: 0:14:20  iter: 419  total_loss: 0.834  loss_cls: 0.291  loss_box_reg: 0.352  loss_rpn_cls: 0.064  loss_rpn_loc: 0.109  time: 0.5406  data_time: 0.0065  lr: 0.004196  max_mem: 3062M
[32m[04/15 05:29:09 d2.utils.events]: [0m eta: 0:14:11  iter: 439  total_loss: 0.870  loss_cls: 0.284  loss_box_reg: 0.349  loss_rpn_cls: 0.080  loss_rpn_loc: 0.086  time: 0.5422  data_time: 0.0087  lr: 0.004396  max_mem: 3062M
[32m[04/15 05:29:20 d2.utils.events]: [0m eta: 0:14:01  iter: 459  total_loss: 1.043  loss_cls: 0.359  loss_box_reg: 0.418  loss_rpn_cls: 0.073  loss_rpn_loc: 0.118  time: 0.5431  data_time: 0.0062  lr: 0.004595  max_mem: 3062M
[32m[04/15 05:29:32 d2.utils.events]: [0m eta: 0:13:51  iter: 479  total_loss: 0.886  loss_cls: 0.327  loss_box_reg: 0.409  loss_rpn_cls: 0.063  loss_rpn_loc: 0.064  time: 0.5438  data_time: 0.0070  lr: 0.004795  max_mem: 3062M
[32m[04/15 05:29:43 d2.utils.events]: [0m eta: 0:13:41  iter: 499  total_loss: 0.848  loss_cls: 0.311  loss_box_reg: 0.362  loss_rpn_cls: 0.081  loss_rpn_loc: 0.074  time: 0.5445  data_time: 0.0103  lr: 0.004995  max_mem: 3062M
[32m[04/15 05:29:54 d2.utils.events]: [0m eta: 0:13:29  iter: 519  total_loss: 0.713  loss_cls: 0.266  loss_box_reg: 0.332  loss_rpn_cls: 0.084  loss_rpn_loc: 0.066  time: 0.5441  data_time: 0.0085  lr: 0.005195  max_mem: 3062M
[32m[04/15 05:30:05 d2.utils.events]: [0m eta: 0:13:19  iter: 539  total_loss: 0.739  loss_cls: 0.264  loss_box_reg: 0.334  loss_rpn_cls: 0.066  loss_rpn_loc: 0.092  time: 0.5446  data_time: 0.0065  lr: 0.005395  max_mem: 3062M
[32m[04/15 05:30:16 d2.utils.events]: [0m eta: 0:13:09  iter: 559  total_loss: 0.949  loss_cls: 0.343  loss_box_reg: 0.393  loss_rpn_cls: 0.088  loss_rpn_loc: 0.090  time: 0.5452  data_time: 0.0102  lr: 0.005594  max_mem: 3062M
[32m[04/15 05:30:27 d2.utils.events]: [0m eta: 0:12:58  iter: 579  total_loss: 0.655  loss_cls: 0.244  loss_box_reg: 0.300  loss_rpn_cls: 0.059  loss_rpn_loc: 0.067  time: 0.5454  data_time: 0.0080  lr: 0.005794  max_mem: 3062M
[32m[04/15 05:30:39 d2.utils.events]: [0m eta: 0:12:48  iter: 599  total_loss: 0.920  loss_cls: 0.304  loss_box_reg: 0.388  loss_rpn_cls: 0.061  loss_rpn_loc: 0.095  time: 0.5462  data_time: 0.0069  lr: 0.005994  max_mem: 3062M
[32m[04/15 05:30:51 d2.utils.events]: [0m eta: 0:12:39  iter: 619  total_loss: 0.732  loss_cls: 0.276  loss_box_reg: 0.294  loss_rpn_cls: 0.078  loss_rpn_loc: 0.086  time: 0.5470  data_time: 0.0088  lr: 0.006194  max_mem: 3062M
[32m[04/15 05:31:02 d2.utils.events]: [0m eta: 0:12:28  iter: 639  total_loss: 0.765  loss_cls: 0.259  loss_box_reg: 0.327  loss_rpn_cls: 0.071  loss_rpn_loc: 0.078  time: 0.5472  data_time: 0.0064  lr: 0.006394  max_mem: 3062M
[32m[04/15 05:31:13 d2.utils.events]: [0m eta: 0:12:17  iter: 659  total_loss: 0.787  loss_cls: 0.295  loss_box_reg: 0.340  loss_rpn_cls: 0.062  loss_rpn_loc: 0.081  time: 0.5476  data_time: 0.0090  lr: 0.006593  max_mem: 3062M
[32m[04/15 05:31:24 d2.utils.events]: [0m eta: 0:12:08  iter: 679  total_loss: 0.885  loss_cls: 0.301  loss_box_reg: 0.364  loss_rpn_cls: 0.070  loss_rpn_loc: 0.105  time: 0.5482  data_time: 0.0087  lr: 0.006793  max_mem: 3062M
[32m[04/15 05:31:36 d2.utils.events]: [0m eta: 0:11:57  iter: 699  total_loss: 0.808  loss_cls: 0.274  loss_box_reg: 0.336  loss_rpn_cls: 0.063  loss_rpn_loc: 0.090  time: 0.5483  data_time: 0.0084  lr: 0.006993  max_mem: 3062M
[32m[04/15 05:31:47 d2.utils.events]: [0m eta: 0:11:45  iter: 719  total_loss: 0.696  loss_cls: 0.266  loss_box_reg: 0.291  loss_rpn_cls: 0.077  loss_rpn_loc: 0.069  time: 0.5483  data_time: 0.0072  lr: 0.007193  max_mem: 3062M
[32m[04/15 05:31:58 d2.utils.events]: [0m eta: 0:11:35  iter: 739  total_loss: 0.798  loss_cls: 0.256  loss_box_reg: 0.358  loss_rpn_cls: 0.069  loss_rpn_loc: 0.093  time: 0.5483  data_time: 0.0113  lr: 0.007393  max_mem: 3062M
[32m[04/15 05:32:09 d2.utils.events]: [0m eta: 0:11:25  iter: 759  total_loss: 0.960  loss_cls: 0.334  loss_box_reg: 0.430  loss_rpn_cls: 0.099  loss_rpn_loc: 0.106  time: 0.5487  data_time: 0.0086  lr: 0.007592  max_mem: 3062M
[32m[04/15 05:32:23 d2.utils.events]: [0m eta: 0:11:15  iter: 779  total_loss: 0.670  loss_cls: 0.242  loss_box_reg: 0.274  loss_rpn_cls: 0.087  loss_rpn_loc: 0.075  time: 0.5524  data_time: 0.1664  lr: 0.007792  max_mem: 3062M
[32m[04/15 05:32:35 d2.utils.events]: [0m eta: 0:11:05  iter: 799  total_loss: 0.624  loss_cls: 0.189  loss_box_reg: 0.262  loss_rpn_cls: 0.058  loss_rpn_loc: 0.071  time: 0.5529  data_time: 0.0074  lr: 0.007992  max_mem: 3062M
[32m[04/15 05:32:46 d2.utils.events]: [0m eta: 0:10:53  iter: 819  total_loss: 0.748  loss_cls: 0.264  loss_box_reg: 0.316  loss_rpn_cls: 0.060  loss_rpn_loc: 0.074  time: 0.5527  data_time: 0.0079  lr: 0.008192  max_mem: 3062M
[32m[04/15 05:32:57 d2.utils.events]: [0m eta: 0:10:42  iter: 839  total_loss: 0.804  loss_cls: 0.285  loss_box_reg: 0.369  loss_rpn_cls: 0.058  loss_rpn_loc: 0.090  time: 0.5530  data_time: 0.0079  lr: 0.008392  max_mem: 3062M
[32m[04/15 05:33:08 d2.utils.events]: [0m eta: 0:10:31  iter: 859  total_loss: 0.689  loss_cls: 0.226  loss_box_reg: 0.261  loss_rpn_cls: 0.065  loss_rpn_loc: 0.079  time: 0.5527  data_time: 0.0062  lr: 0.008591  max_mem: 3062M
[32m[04/15 05:33:19 d2.utils.events]: [0m eta: 0:10:20  iter: 879  total_loss: 0.902  loss_cls: 0.321  loss_box_reg: 0.382  loss_rpn_cls: 0.076  loss_rpn_loc: 0.093  time: 0.5528  data_time: 0.0070  lr: 0.008791  max_mem: 3062M
[32m[04/15 05:33:30 d2.utils.events]: [0m eta: 0:10:09  iter: 899  total_loss: 0.832  loss_cls: 0.286  loss_box_reg: 0.322  loss_rpn_cls: 0.071  loss_rpn_loc: 0.071  time: 0.5529  data_time: 0.0079  lr: 0.008991  max_mem: 3062M
[32m[04/15 05:33:41 d2.utils.events]: [0m eta: 0:09:58  iter: 919  total_loss: 0.785  loss_cls: 0.267  loss_box_reg: 0.359  loss_rpn_cls: 0.072  loss_rpn_loc: 0.083  time: 0.5527  data_time: 0.0079  lr: 0.009191  max_mem: 3062M
[32m[04/15 05:33:52 d2.utils.events]: [0m eta: 0:09:47  iter: 939  total_loss: 0.628  loss_cls: 0.208  loss_box_reg: 0.255  loss_rpn_cls: 0.081  loss_rpn_loc: 0.078  time: 0.5525  data_time: 0.0069  lr: 0.009391  max_mem: 3062M
[32m[04/15 05:34:03 d2.utils.events]: [0m eta: 0:09:36  iter: 959  total_loss: 0.629  loss_cls: 0.239  loss_box_reg: 0.246  loss_rpn_cls: 0.070  loss_rpn_loc: 0.058  time: 0.5525  data_time: 0.0089  lr: 0.009590  max_mem: 3062M
[32m[04/15 05:34:15 d2.utils.events]: [0m eta: 0:09:25  iter: 979  total_loss: 0.825  loss_cls: 0.316  loss_box_reg: 0.363  loss_rpn_cls: 0.070  loss_rpn_loc: 0.078  time: 0.5524  data_time: 0.0068  lr: 0.009790  max_mem: 3062M
[32m[04/15 05:34:26 d2.utils.events]: [0m eta: 0:09:14  iter: 999  total_loss: 0.853  loss_cls: 0.280  loss_box_reg: 0.389  loss_rpn_cls: 0.058  loss_rpn_loc: 0.110  time: 0.5529  data_time: 0.0067  lr: 0.009990  max_mem: 3062M
[32m[04/15 05:34:37 d2.utils.events]: [0m eta: 0:09:02  iter: 1019  total_loss: 0.727  loss_cls: 0.262  loss_box_reg: 0.338  loss_rpn_cls: 0.066  loss_rpn_loc: 0.077  time: 0.5526  data_time: 0.0075  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:34:49 d2.utils.events]: [0m eta: 0:08:52  iter: 1039  total_loss: 0.837  loss_cls: 0.289  loss_box_reg: 0.389  loss_rpn_cls: 0.062  loss_rpn_loc: 0.085  time: 0.5532  data_time: 0.0069  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:35:00 d2.utils.events]: [0m eta: 0:08:42  iter: 1059  total_loss: 0.876  loss_cls: 0.293  loss_box_reg: 0.345  loss_rpn_cls: 0.110  loss_rpn_loc: 0.105  time: 0.5533  data_time: 0.0069  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:35:11 d2.utils.events]: [0m eta: 0:08:30  iter: 1079  total_loss: 0.869  loss_cls: 0.297  loss_box_reg: 0.382  loss_rpn_cls: 0.081  loss_rpn_loc: 0.103  time: 0.5532  data_time: 0.0082  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:35:22 d2.utils.events]: [0m eta: 0:08:20  iter: 1099  total_loss: 0.698  loss_cls: 0.236  loss_box_reg: 0.306  loss_rpn_cls: 0.087  loss_rpn_loc: 0.063  time: 0.5534  data_time: 0.0084  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:35:34 d2.utils.events]: [0m eta: 0:08:10  iter: 1119  total_loss: 0.737  loss_cls: 0.240  loss_box_reg: 0.287  loss_rpn_cls: 0.104  loss_rpn_loc: 0.080  time: 0.5535  data_time: 0.0069  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:35:45 d2.utils.events]: [0m eta: 0:07:59  iter: 1139  total_loss: 0.821  loss_cls: 0.307  loss_box_reg: 0.355  loss_rpn_cls: 0.081  loss_rpn_loc: 0.097  time: 0.5536  data_time: 0.0079  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:35:56 d2.utils.events]: [0m eta: 0:07:48  iter: 1159  total_loss: 0.768  loss_cls: 0.261  loss_box_reg: 0.328  loss_rpn_cls: 0.074  loss_rpn_loc: 0.083  time: 0.5537  data_time: 0.0119  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:36:07 d2.utils.events]: [0m eta: 0:07:37  iter: 1179  total_loss: 0.747  loss_cls: 0.304  loss_box_reg: 0.339  loss_rpn_cls: 0.077  loss_rpn_loc: 0.067  time: 0.5537  data_time: 0.0069  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:36:19 d2.utils.events]: [0m eta: 0:07:26  iter: 1199  total_loss: 0.847  loss_cls: 0.288  loss_box_reg: 0.361  loss_rpn_cls: 0.074  loss_rpn_loc: 0.074  time: 0.5536  data_time: 0.0071  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:36:30 d2.utils.events]: [0m eta: 0:07:16  iter: 1219  total_loss: 0.709  loss_cls: 0.238  loss_box_reg: 0.327  loss_rpn_cls: 0.079  loss_rpn_loc: 0.096  time: 0.5539  data_time: 0.0073  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:36:41 d2.utils.events]: [0m eta: 0:07:05  iter: 1239  total_loss: 0.864  loss_cls: 0.298  loss_box_reg: 0.369  loss_rpn_cls: 0.075  loss_rpn_loc: 0.095  time: 0.5539  data_time: 0.0081  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:36:52 d2.utils.events]: [0m eta: 0:06:54  iter: 1259  total_loss: 0.751  loss_cls: 0.298  loss_box_reg: 0.321  loss_rpn_cls: 0.065  loss_rpn_loc: 0.064  time: 0.5536  data_time: 0.0059  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:37:03 d2.utils.events]: [0m eta: 0:06:43  iter: 1279  total_loss: 0.882  loss_cls: 0.316  loss_box_reg: 0.390  loss_rpn_cls: 0.064  loss_rpn_loc: 0.079  time: 0.5538  data_time: 0.0078  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:37:15 d2.utils.events]: [0m eta: 0:06:32  iter: 1299  total_loss: 0.825  loss_cls: 0.266  loss_box_reg: 0.360  loss_rpn_cls: 0.058  loss_rpn_loc: 0.070  time: 0.5539  data_time: 0.0099  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:37:26 d2.utils.events]: [0m eta: 0:06:21  iter: 1319  total_loss: 0.654  loss_cls: 0.246  loss_box_reg: 0.287  loss_rpn_cls: 0.045  loss_rpn_loc: 0.069  time: 0.5538  data_time: 0.0106  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:37:37 d2.utils.events]: [0m eta: 0:06:10  iter: 1339  total_loss: 0.820  loss_cls: 0.298  loss_box_reg: 0.407  loss_rpn_cls: 0.048  loss_rpn_loc: 0.097  time: 0.5541  data_time: 0.0078  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:37:49 d2.utils.events]: [0m eta: 0:05:58  iter: 1359  total_loss: 0.812  loss_cls: 0.273  loss_box_reg: 0.410  loss_rpn_cls: 0.054  loss_rpn_loc: 0.086  time: 0.5542  data_time: 0.0077  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:38:00 d2.utils.events]: [0m eta: 0:05:47  iter: 1379  total_loss: 0.893  loss_cls: 0.310  loss_box_reg: 0.392  loss_rpn_cls: 0.071  loss_rpn_loc: 0.102  time: 0.5541  data_time: 0.0077  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:38:10 d2.utils.events]: [0m eta: 0:05:35  iter: 1399  total_loss: 0.852  loss_cls: 0.311  loss_box_reg: 0.391  loss_rpn_cls: 0.045  loss_rpn_loc: 0.089  time: 0.5538  data_time: 0.0066  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:38:21 d2.utils.events]: [0m eta: 0:05:24  iter: 1419  total_loss: 0.770  loss_cls: 0.282  loss_box_reg: 0.361  loss_rpn_cls: 0.060  loss_rpn_loc: 0.072  time: 0.5536  data_time: 0.0066  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:38:33 d2.utils.events]: [0m eta: 0:05:13  iter: 1439  total_loss: 0.809  loss_cls: 0.260  loss_box_reg: 0.346  loss_rpn_cls: 0.070  loss_rpn_loc: 0.094  time: 0.5541  data_time: 0.0075  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:38:45 d2.utils.events]: [0m eta: 0:05:02  iter: 1459  total_loss: 0.864  loss_cls: 0.280  loss_box_reg: 0.383  loss_rpn_cls: 0.084  loss_rpn_loc: 0.095  time: 0.5543  data_time: 0.0066  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:38:56 d2.utils.events]: [0m eta: 0:04:51  iter: 1479  total_loss: 0.693  loss_cls: 0.220  loss_box_reg: 0.304  loss_rpn_cls: 0.055  loss_rpn_loc: 0.082  time: 0.5543  data_time: 0.0085  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:39:07 d2.utils.events]: [0m eta: 0:04:40  iter: 1499  total_loss: 0.871  loss_cls: 0.293  loss_box_reg: 0.434  loss_rpn_cls: 0.058  loss_rpn_loc: 0.093  time: 0.5544  data_time: 0.0097  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:39:19 d2.utils.events]: [0m eta: 0:04:29  iter: 1519  total_loss: 0.831  loss_cls: 0.297  loss_box_reg: 0.420  loss_rpn_cls: 0.063  loss_rpn_loc: 0.079  time: 0.5545  data_time: 0.0068  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:39:33 d2.utils.events]: [0m eta: 0:04:18  iter: 1539  total_loss: 0.801  loss_cls: 0.265  loss_box_reg: 0.364  loss_rpn_cls: 0.074  loss_rpn_loc: 0.082  time: 0.5544  data_time: 0.0071  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:39:44 d2.utils.events]: [0m eta: 0:04:06  iter: 1559  total_loss: 0.654  loss_cls: 0.183  loss_box_reg: 0.287  loss_rpn_cls: 0.079  loss_rpn_loc: 0.069  time: 0.5542  data_time: 0.0125  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:39:55 d2.utils.events]: [0m eta: 0:03:55  iter: 1579  total_loss: 0.777  loss_cls: 0.294  loss_box_reg: 0.363  loss_rpn_cls: 0.068  loss_rpn_loc: 0.070  time: 0.5544  data_time: 0.0099  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:40:07 d2.utils.events]: [0m eta: 0:03:44  iter: 1599  total_loss: 0.762  loss_cls: 0.265  loss_box_reg: 0.338  loss_rpn_cls: 0.075  loss_rpn_loc: 0.071  time: 0.5543  data_time: 0.0074  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:40:22 d2.utils.events]: [0m eta: 0:03:32  iter: 1619  total_loss: 0.802  loss_cls: 0.239  loss_box_reg: 0.297  loss_rpn_cls: 0.054  loss_rpn_loc: 0.089  time: 0.5539  data_time: 0.0084  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:40:33 d2.utils.events]: [0m eta: 0:03:21  iter: 1639  total_loss: 0.802  loss_cls: 0.282  loss_box_reg: 0.370  loss_rpn_cls: 0.066  loss_rpn_loc: 0.079  time: 0.5536  data_time: 0.0055  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:40:44 d2.utils.events]: [0m eta: 0:03:09  iter: 1659  total_loss: 0.743  loss_cls: 0.247  loss_box_reg: 0.340  loss_rpn_cls: 0.048  loss_rpn_loc: 0.075  time: 0.5534  data_time: 0.0045  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:40:55 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.786  loss_cls: 0.262  loss_box_reg: 0.378  loss_rpn_cls: 0.064  loss_rpn_loc: 0.077  time: 0.5535  data_time: 0.0068  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:41:06 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.822  loss_cls: 0.288  loss_box_reg: 0.391  loss_rpn_cls: 0.050  loss_rpn_loc: 0.094  time: 0.5535  data_time: 0.0068  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:41:18 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.827  loss_cls: 0.289  loss_box_reg: 0.313  loss_rpn_cls: 0.061  loss_rpn_loc: 0.096  time: 0.5536  data_time: 0.0063  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:41:29 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.748  loss_cls: 0.255  loss_box_reg: 0.323  loss_rpn_cls: 0.079  loss_rpn_loc: 0.084  time: 0.5535  data_time: 0.0081  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:41:40 d2.utils.events]: [0m eta: 0:02:14  iter: 1759  total_loss: 0.771  loss_cls: 0.305  loss_box_reg: 0.322  loss_rpn_cls: 0.069  loss_rpn_loc: 0.100  time: 0.5535  data_time: 0.0082  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:41:51 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.653  loss_cls: 0.259  loss_box_reg: 0.309  loss_rpn_cls: 0.044  loss_rpn_loc: 0.078  time: 0.5536  data_time: 0.0093  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:42:02 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.752  loss_cls: 0.256  loss_box_reg: 0.345  loss_rpn_cls: 0.055  loss_rpn_loc: 0.081  time: 0.5536  data_time: 0.0074  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:42:14 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.664  loss_cls: 0.240  loss_box_reg: 0.317  loss_rpn_cls: 0.034  loss_rpn_loc: 0.074  time: 0.5537  data_time: 0.0082  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:42:25 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.929  loss_cls: 0.309  loss_box_reg: 0.454  loss_rpn_cls: 0.050  loss_rpn_loc: 0.086  time: 0.5537  data_time: 0.0067  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:42:36 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.774  loss_cls: 0.273  loss_box_reg: 0.352  loss_rpn_cls: 0.047  loss_rpn_loc: 0.087  time: 0.5538  data_time: 0.0091  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:42:47 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.771  loss_cls: 0.254  loss_box_reg: 0.364  loss_rpn_cls: 0.057  loss_rpn_loc: 0.075  time: 0.5537  data_time: 0.0096  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:42:58 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.777  loss_cls: 0.264  loss_box_reg: 0.357  loss_rpn_cls: 0.051  loss_rpn_loc: 0.087  time: 0.5535  data_time: 0.0076  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:43:09 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.776  loss_cls: 0.246  loss_box_reg: 0.356  loss_rpn_cls: 0.051  loss_rpn_loc: 0.095  time: 0.5536  data_time: 0.0084  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:43:21 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.820  loss_cls: 0.286  loss_box_reg: 0.433  loss_rpn_cls: 0.053  loss_rpn_loc: 0.098  time: 0.5536  data_time: 0.0084  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:43:32 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.767  loss_cls: 0.240  loss_box_reg: 0.369  loss_rpn_cls: 0.046  loss_rpn_loc: 0.082  time: 0.5537  data_time: 0.0071  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:43:43 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.780  loss_cls: 0.249  loss_box_reg: 0.367  loss_rpn_cls: 0.066  loss_rpn_loc: 0.077  time: 0.5537  data_time: 0.0126  lr: 0.010000  max_mem: 3062M
[5m[31mWARNING[0m [32m[04/15 05:44:04 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 05:44:04 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 05:44:04 d2.data.build]: [0mDistribution of instances among all 80 categories:
[36m|   category    | #instances   |   category    | #instances   |   category    | #instances   |
|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|
|    person     | 5406         |    bicycle    | 420          |      car      | 5008         |
|  motorcycle   | 0            |   airplane    | 0            |      bus      | 0            |
|     train     | 0            |     truck     | 0            |     boat      | 0            |
| traffic light | 0            | fire hydrant  | 0            |   stop sign   | 0            |
| parking meter | 0            |     bench     | 0            |     bird      | 0            |
|      cat      | 0            |      dog      | 13           |     horse     | 0            |
|     sheep     | 0            |      cow      | 0            |   elephant    | 0            |
|     bear      | 0            |     zebra     | 0            |    giraffe    | 0            |
|   backpack    | 0            |   umbrella    | 0            |    handbag    | 0            |
|      tie      | 0            |   suitcase    | 0            |    frisbee    | 0            |
|     skis      | 0            |   snowboard   | 0            |  sports ball  | 0            |
|     kite      | 0            | baseball bat  | 0            | baseball gl.. | 0            |
|  skateboard   | 0            |   surfboard   | 0            | tennis racket | 0            |
|    bottle     | 0            |  wine glass   | 0            |      cup      | 0            |
|     fork      | 0            |     knife     | 0            |     spoon     | 0            |
|     bowl      | 0            |    banana     | 0            |     apple     | 0            |
|   sandwich    | 0            |    orange     | 0            |   broccoli    | 0            |
|    carrot     | 0            |    hot dog    | 0            |     pizza     | 0            |
|     donut     | 0            |     cake      | 0            |     chair     | 0            |
|     couch     | 0            | potted plant  | 0            |      bed      | 0            |
| dining table  | 0            |    toilet     | 0            |      tv       | 0            |
|    laptop     | 0            |     mouse     | 0            |    remote     | 0            |
|   keyboard    | 0            |  cell phone   | 0            |   microwave   | 0            |
|     oven      | 0            |     bike      | 0            |    hydrant    | 0            |
|     motor     | 0            |     rider     | 0            |     light     | 0            |
|     sign      | 0            | motor vehicle | 0            |  human face   | 0            |
|  hair drier   | 0            | license plate | 0            |               |              |
|     total     | 10847        |               |              |               |              |[0m
[5m[31mWARNING[0m [32m[04/15 05:44:04 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 05:44:04 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.818  loss_cls: 0.245  loss_box_reg: 0.324  loss_rpn_cls: 0.068  loss_rpn_loc: 0.117  time: 0.5536  data_time: 0.0069  lr: 0.010000  max_mem: 3062M
[32m[04/15 05:44:07 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:26 (0.5539 s / it)
[32m[04/15 05:44:07 d2.engine.hooks]: [0mTotal training time: 0:18:56 (0:00:30 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 05:44:15 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 05:44:15 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 05:44:15 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 05:44:17 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1211 s / img. ETA=0:02:33
[32m[04/15 05:44:22 d2.evaluation.evaluator]: [0mInference done 51/1257. 0.1232 s / img. ETA=0:02:32
[32m[04/15 05:44:27 d2.evaluation.evaluator]: [0mInference done 90/1257. 0.1243 s / img. ETA=0:02:28
[32m[04/15 05:44:32 d2.evaluation.evaluator]: [0mInference done 129/1257. 0.1245 s / img. ETA=0:02:23
[32m[04/15 05:44:37 d2.evaluation.evaluator]: [0mInference done 165/1257. 0.1247 s / img. ETA=0:02:22
[32m[04/15 05:44:42 d2.evaluation.evaluator]: [0mInference done 204/1257. 0.1247 s / img. ETA=0:02:17
[32m[04/15 05:44:47 d2.evaluation.evaluator]: [0mInference done 244/1257. 0.1245 s / img. ETA=0:02:11
[32m[04/15 05:44:52 d2.evaluation.evaluator]: [0mInference done 284/1257. 0.1242 s / img. ETA=0:02:05
[32m[04/15 05:44:57 d2.evaluation.evaluator]: [0mInference done 324/1257. 0.1242 s / img. ETA=0:02:00
[32m[04/15 05:45:02 d2.evaluation.evaluator]: [0mInference done 364/1257. 0.1240 s / img. ETA=0:01:54
[32m[04/15 05:45:07 d2.evaluation.evaluator]: [0mInference done 403/1257. 0.1242 s / img. ETA=0:01:49
[32m[04/15 05:45:12 d2.evaluation.evaluator]: [0mInference done 441/1257. 0.1246 s / img. ETA=0:01:45
[32m[04/15 05:45:17 d2.evaluation.evaluator]: [0mInference done 477/1257. 0.1246 s / img. ETA=0:01:41
[32m[04/15 05:45:23 d2.evaluation.evaluator]: [0mInference done 515/1257. 0.1248 s / img. ETA=0:01:36
[32m[04/15 05:45:28 d2.evaluation.evaluator]: [0mInference done 550/1257. 0.1249 s / img. ETA=0:01:32
[32m[04/15 05:45:33 d2.evaluation.evaluator]: [0mInference done 584/1257. 0.1249 s / img. ETA=0:01:28
[32m[04/15 05:45:38 d2.evaluation.evaluator]: [0mInference done 622/1257. 0.1250 s / img. ETA=0:01:23
[32m[04/15 05:45:43 d2.evaluation.evaluator]: [0mInference done 661/1257. 0.1251 s / img. ETA=0:01:18
[32m[04/15 05:45:48 d2.evaluation.evaluator]: [0mInference done 700/1257. 0.1251 s / img. ETA=0:01:13
[32m[04/15 05:45:53 d2.evaluation.evaluator]: [0mInference done 739/1257. 0.1251 s / img. ETA=0:01:08
[32m[04/15 05:45:58 d2.evaluation.evaluator]: [0mInference done 776/1257. 0.1254 s / img. ETA=0:01:03
[32m[04/15 05:46:03 d2.evaluation.evaluator]: [0mInference done 814/1257. 0.1256 s / img. ETA=0:00:58
[32m[04/15 05:46:08 d2.evaluation.evaluator]: [0mInference done 853/1257. 0.1256 s / img. ETA=0:00:53
[32m[04/15 05:46:13 d2.evaluation.evaluator]: [0mInference done 891/1257. 0.1257 s / img. ETA=0:00:48
[32m[04/15 05:46:18 d2.evaluation.evaluator]: [0mInference done 930/1257. 0.1257 s / img. ETA=0:00:43
[32m[04/15 05:46:23 d2.evaluation.evaluator]: [0mInference done 969/1257. 0.1258 s / img. ETA=0:00:37
[32m[04/15 05:46:28 d2.evaluation.evaluator]: [0mInference done 1008/1257. 0.1258 s / img. ETA=0:00:32
[32m[04/15 05:46:33 d2.evaluation.evaluator]: [0mInference done 1046/1257. 0.1259 s / img. ETA=0:00:27
[32m[04/15 05:46:38 d2.evaluation.evaluator]: [0mInference done 1084/1257. 0.1259 s / img. ETA=0:00:22
[32m[04/15 05:46:43 d2.evaluation.evaluator]: [0mInference done 1122/1257. 0.1260 s / img. ETA=0:00:17
[32m[04/15 05:46:48 d2.evaluation.evaluator]: [0mInference done 1160/1257. 0.1261 s / img. ETA=0:00:12
[32m[04/15 05:46:54 d2.evaluation.evaluator]: [0mInference done 1199/1257. 0.1261 s / img. ETA=0:00:07
[32m[04/15 05:46:59 d2.evaluation.evaluator]: [0mInference done 1238/1257. 0.1261 s / img. ETA=0:00:02
[32m[04/15 05:47:01 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:45.107509 (0.131875 s / img per device, on 1 devices)
[32m[04/15 05:47:01 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:37 (0.126120 s / img per device, on 1 devices)
[32m[04/15 05:47:01 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 05:47:01 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 05:47:01 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
[5m[31mWARNING[0m [32m[04/15 05:47:01 d2.evaluation.FLIR_evaluation]: [0mNo predictions from the model! Set scores to -1
============== The  1  *  2000  iterations ============
4 channel input
[32m[04/15 05:47:03 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 05:47:04 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.42 seconds.
[5m[31mWARNING[0m [32m[04/15 05:47:04 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 05:47:04 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 05:47:05 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 05:47:05 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 05:47:05 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 05:47:06 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 05:47:17 d2.utils.events]: [0m eta: 0:18:03  iter: 19  total_loss: 0.755  loss_cls: 0.234  loss_box_reg: 0.346  loss_rpn_cls: 0.080  loss_rpn_loc: 0.088  time: 0.5486  data_time: 0.0361  lr: 0.000200  max_mem: 3068M
[32m[04/15 05:47:28 d2.utils.events]: [0m eta: 0:17:40  iter: 39  total_loss: 0.703  loss_cls: 0.237  loss_box_reg: 0.311  loss_rpn_cls: 0.065  loss_rpn_loc: 0.095  time: 0.5447  data_time: 0.0076  lr: 0.000400  max_mem: 3068M
[32m[04/15 05:47:39 d2.utils.events]: [0m eta: 0:17:47  iter: 59  total_loss: 0.737  loss_cls: 0.245  loss_box_reg: 0.292  loss_rpn_cls: 0.064  loss_rpn_loc: 0.087  time: 0.5500  data_time: 0.0091  lr: 0.000599  max_mem: 3068M
[32m[04/15 05:47:50 d2.utils.events]: [0m eta: 0:17:45  iter: 79  total_loss: 0.618  loss_cls: 0.227  loss_box_reg: 0.307  loss_rpn_cls: 0.051  loss_rpn_loc: 0.059  time: 0.5494  data_time: 0.0076  lr: 0.000799  max_mem: 3068M
[32m[04/15 05:48:02 d2.utils.events]: [0m eta: 0:17:30  iter: 99  total_loss: 0.684  loss_cls: 0.230  loss_box_reg: 0.309  loss_rpn_cls: 0.057  loss_rpn_loc: 0.076  time: 0.5508  data_time: 0.0094  lr: 0.000999  max_mem: 3068M
[32m[04/15 05:48:13 d2.utils.events]: [0m eta: 0:17:21  iter: 119  total_loss: 0.663  loss_cls: 0.243  loss_box_reg: 0.311  loss_rpn_cls: 0.050  loss_rpn_loc: 0.065  time: 0.5505  data_time: 0.0074  lr: 0.001199  max_mem: 3068M
[32m[04/15 05:48:24 d2.utils.events]: [0m eta: 0:17:04  iter: 139  total_loss: 0.792  loss_cls: 0.260  loss_box_reg: 0.378  loss_rpn_cls: 0.051  loss_rpn_loc: 0.083  time: 0.5492  data_time: 0.0072  lr: 0.001399  max_mem: 3068M
[32m[04/15 05:48:39 d2.utils.events]: [0m eta: 0:17:00  iter: 159  total_loss: 0.766  loss_cls: 0.270  loss_box_reg: 0.363  loss_rpn_cls: 0.056  loss_rpn_loc: 0.079  time: 0.5754  data_time: 0.2523  lr: 0.001598  max_mem: 3068M
[32m[04/15 05:48:50 d2.utils.events]: [0m eta: 0:16:50  iter: 179  total_loss: 0.777  loss_cls: 0.286  loss_box_reg: 0.333  loss_rpn_cls: 0.041  loss_rpn_loc: 0.090  time: 0.5738  data_time: 0.0123  lr: 0.001798  max_mem: 3068M
[32m[04/15 05:49:02 d2.utils.events]: [0m eta: 0:16:40  iter: 199  total_loss: 0.750  loss_cls: 0.264  loss_box_reg: 0.367  loss_rpn_cls: 0.042  loss_rpn_loc: 0.089  time: 0.5723  data_time: 0.0081  lr: 0.001998  max_mem: 3068M
[32m[04/15 05:49:13 d2.utils.events]: [0m eta: 0:16:30  iter: 219  total_loss: 0.705  loss_cls: 0.235  loss_box_reg: 0.332  loss_rpn_cls: 0.033  loss_rpn_loc: 0.063  time: 0.5709  data_time: 0.0077  lr: 0.002198  max_mem: 3068M
[32m[04/15 05:49:24 d2.utils.events]: [0m eta: 0:16:17  iter: 239  total_loss: 0.602  loss_cls: 0.193  loss_box_reg: 0.328  loss_rpn_cls: 0.039  loss_rpn_loc: 0.054  time: 0.5690  data_time: 0.0079  lr: 0.002398  max_mem: 3068M
[32m[04/15 05:49:35 d2.utils.events]: [0m eta: 0:16:11  iter: 259  total_loss: 0.813  loss_cls: 0.287  loss_box_reg: 0.358  loss_rpn_cls: 0.040  loss_rpn_loc: 0.062  time: 0.5691  data_time: 0.0065  lr: 0.002597  max_mem: 3068M
[32m[04/15 05:49:46 d2.utils.events]: [0m eta: 0:15:58  iter: 279  total_loss: 0.630  loss_cls: 0.221  loss_box_reg: 0.323  loss_rpn_cls: 0.034  loss_rpn_loc: 0.050  time: 0.5678  data_time: 0.0066  lr: 0.002797  max_mem: 3068M
[32m[04/15 05:49:58 d2.utils.events]: [0m eta: 0:15:46  iter: 299  total_loss: 0.689  loss_cls: 0.220  loss_box_reg: 0.299  loss_rpn_cls: 0.038  loss_rpn_loc: 0.061  time: 0.5664  data_time: 0.0079  lr: 0.002997  max_mem: 3068M
[32m[04/15 05:50:09 d2.utils.events]: [0m eta: 0:15:37  iter: 319  total_loss: 0.762  loss_cls: 0.276  loss_box_reg: 0.374  loss_rpn_cls: 0.052  loss_rpn_loc: 0.063  time: 0.5674  data_time: 0.0107  lr: 0.003197  max_mem: 3068M
[32m[04/15 05:50:21 d2.utils.events]: [0m eta: 0:15:27  iter: 339  total_loss: 0.907  loss_cls: 0.296  loss_box_reg: 0.423  loss_rpn_cls: 0.044  loss_rpn_loc: 0.075  time: 0.5670  data_time: 0.0070  lr: 0.003397  max_mem: 3068M
[32m[04/15 05:50:32 d2.utils.events]: [0m eta: 0:15:15  iter: 359  total_loss: 0.705  loss_cls: 0.239  loss_box_reg: 0.308  loss_rpn_cls: 0.043  loss_rpn_loc: 0.074  time: 0.5659  data_time: 0.0071  lr: 0.003596  max_mem: 3068M
[32m[04/15 05:50:43 d2.utils.events]: [0m eta: 0:15:03  iter: 379  total_loss: 0.820  loss_cls: 0.247  loss_box_reg: 0.403  loss_rpn_cls: 0.041  loss_rpn_loc: 0.089  time: 0.5646  data_time: 0.0073  lr: 0.003796  max_mem: 3068M
[32m[04/15 05:50:54 d2.utils.events]: [0m eta: 0:14:51  iter: 399  total_loss: 0.720  loss_cls: 0.241  loss_box_reg: 0.379  loss_rpn_cls: 0.034  loss_rpn_loc: 0.068  time: 0.5638  data_time: 0.0124  lr: 0.003996  max_mem: 3068M
[32m[04/15 05:51:05 d2.utils.events]: [0m eta: 0:14:41  iter: 419  total_loss: 0.831  loss_cls: 0.274  loss_box_reg: 0.391  loss_rpn_cls: 0.054  loss_rpn_loc: 0.103  time: 0.5642  data_time: 0.0094  lr: 0.004196  max_mem: 3068M
[32m[04/15 05:51:16 d2.utils.events]: [0m eta: 0:14:28  iter: 439  total_loss: 0.760  loss_cls: 0.248  loss_box_reg: 0.370  loss_rpn_cls: 0.042  loss_rpn_loc: 0.095  time: 0.5633  data_time: 0.0088  lr: 0.004396  max_mem: 3068M
[32m[04/15 05:51:27 d2.utils.events]: [0m eta: 0:14:17  iter: 459  total_loss: 0.804  loss_cls: 0.265  loss_box_reg: 0.389  loss_rpn_cls: 0.047  loss_rpn_loc: 0.065  time: 0.5628  data_time: 0.0078  lr: 0.004595  max_mem: 3068M
[32m[04/15 05:51:38 d2.utils.events]: [0m eta: 0:14:05  iter: 479  total_loss: 0.727  loss_cls: 0.242  loss_box_reg: 0.373  loss_rpn_cls: 0.038  loss_rpn_loc: 0.093  time: 0.5620  data_time: 0.0074  lr: 0.004795  max_mem: 3068M
[32m[04/15 05:51:50 d2.utils.events]: [0m eta: 0:13:55  iter: 499  total_loss: 0.878  loss_cls: 0.274  loss_box_reg: 0.413  loss_rpn_cls: 0.041  loss_rpn_loc: 0.079  time: 0.5621  data_time: 0.0061  lr: 0.004995  max_mem: 3068M
[32m[04/15 05:52:01 d2.utils.events]: [0m eta: 0:13:45  iter: 519  total_loss: 0.636  loss_cls: 0.234  loss_box_reg: 0.328  loss_rpn_cls: 0.034  loss_rpn_loc: 0.067  time: 0.5623  data_time: 0.0070  lr: 0.005195  max_mem: 3068M
[32m[04/15 05:52:12 d2.utils.events]: [0m eta: 0:13:33  iter: 539  total_loss: 0.721  loss_cls: 0.245  loss_box_reg: 0.338  loss_rpn_cls: 0.032  loss_rpn_loc: 0.062  time: 0.5622  data_time: 0.0071  lr: 0.005395  max_mem: 3068M
[32m[04/15 05:52:24 d2.utils.events]: [0m eta: 0:13:22  iter: 559  total_loss: 0.659  loss_cls: 0.225  loss_box_reg: 0.319  loss_rpn_cls: 0.049  loss_rpn_loc: 0.070  time: 0.5618  data_time: 0.0064  lr: 0.005594  max_mem: 3068M
[32m[04/15 05:52:35 d2.utils.events]: [0m eta: 0:13:11  iter: 579  total_loss: 0.684  loss_cls: 0.242  loss_box_reg: 0.336  loss_rpn_cls: 0.039  loss_rpn_loc: 0.080  time: 0.5617  data_time: 0.0083  lr: 0.005794  max_mem: 3068M
[32m[04/15 05:52:46 d2.utils.events]: [0m eta: 0:12:59  iter: 599  total_loss: 0.658  loss_cls: 0.210  loss_box_reg: 0.293  loss_rpn_cls: 0.045  loss_rpn_loc: 0.074  time: 0.5611  data_time: 0.0064  lr: 0.005994  max_mem: 3068M
[32m[04/15 05:52:57 d2.utils.events]: [0m eta: 0:12:48  iter: 619  total_loss: 0.788  loss_cls: 0.262  loss_box_reg: 0.406  loss_rpn_cls: 0.043  loss_rpn_loc: 0.077  time: 0.5610  data_time: 0.0087  lr: 0.006194  max_mem: 3068M
[32m[04/15 05:53:08 d2.utils.events]: [0m eta: 0:12:37  iter: 639  total_loss: 0.824  loss_cls: 0.279  loss_box_reg: 0.436  loss_rpn_cls: 0.047  loss_rpn_loc: 0.084  time: 0.5611  data_time: 0.0098  lr: 0.006394  max_mem: 3068M
[32m[04/15 05:53:20 d2.utils.events]: [0m eta: 0:12:27  iter: 659  total_loss: 0.619  loss_cls: 0.220  loss_box_reg: 0.287  loss_rpn_cls: 0.046  loss_rpn_loc: 0.075  time: 0.5613  data_time: 0.0097  lr: 0.006593  max_mem: 3068M
[32m[04/15 05:53:31 d2.utils.events]: [0m eta: 0:12:15  iter: 679  total_loss: 0.716  loss_cls: 0.225  loss_box_reg: 0.278  loss_rpn_cls: 0.046  loss_rpn_loc: 0.051  time: 0.5602  data_time: 0.0069  lr: 0.006793  max_mem: 3068M
[32m[04/15 05:53:42 d2.utils.events]: [0m eta: 0:12:03  iter: 699  total_loss: 0.765  loss_cls: 0.282  loss_box_reg: 0.341  loss_rpn_cls: 0.053  loss_rpn_loc: 0.053  time: 0.5598  data_time: 0.0073  lr: 0.006993  max_mem: 3068M
[32m[04/15 05:53:53 d2.utils.events]: [0m eta: 0:11:52  iter: 719  total_loss: 0.803  loss_cls: 0.293  loss_box_reg: 0.359  loss_rpn_cls: 0.035  loss_rpn_loc: 0.068  time: 0.5602  data_time: 0.0074  lr: 0.007193  max_mem: 3068M
[32m[04/15 05:54:04 d2.utils.events]: [0m eta: 0:11:41  iter: 739  total_loss: 0.756  loss_cls: 0.260  loss_box_reg: 0.359  loss_rpn_cls: 0.046  loss_rpn_loc: 0.085  time: 0.5598  data_time: 0.0072  lr: 0.007393  max_mem: 3068M
[32m[04/15 05:54:16 d2.utils.events]: [0m eta: 0:11:31  iter: 759  total_loss: 0.920  loss_cls: 0.320  loss_box_reg: 0.414  loss_rpn_cls: 0.048  loss_rpn_loc: 0.078  time: 0.5602  data_time: 0.0071  lr: 0.007592  max_mem: 3068M
[32m[04/15 05:54:27 d2.utils.events]: [0m eta: 0:11:20  iter: 779  total_loss: 0.721  loss_cls: 0.250  loss_box_reg: 0.381  loss_rpn_cls: 0.042  loss_rpn_loc: 0.070  time: 0.5607  data_time: 0.0077  lr: 0.007792  max_mem: 3068M
[32m[04/15 05:54:39 d2.utils.events]: [0m eta: 0:11:09  iter: 799  total_loss: 0.710  loss_cls: 0.261  loss_box_reg: 0.371  loss_rpn_cls: 0.033  loss_rpn_loc: 0.057  time: 0.5609  data_time: 0.0085  lr: 0.007992  max_mem: 3068M
[32m[04/15 05:54:51 d2.utils.events]: [0m eta: 0:10:59  iter: 819  total_loss: 0.775  loss_cls: 0.273  loss_box_reg: 0.349  loss_rpn_cls: 0.042  loss_rpn_loc: 0.095  time: 0.5613  data_time: 0.0078  lr: 0.008192  max_mem: 3068M
[32m[04/15 05:55:02 d2.utils.events]: [0m eta: 0:10:48  iter: 839  total_loss: 0.735  loss_cls: 0.243  loss_box_reg: 0.348  loss_rpn_cls: 0.059  loss_rpn_loc: 0.052  time: 0.5614  data_time: 0.0066  lr: 0.008392  max_mem: 3068M
[32m[04/15 05:55:13 d2.utils.events]: [0m eta: 0:10:37  iter: 859  total_loss: 0.758  loss_cls: 0.259  loss_box_reg: 0.332  loss_rpn_cls: 0.043  loss_rpn_loc: 0.076  time: 0.5613  data_time: 0.0089  lr: 0.008591  max_mem: 3068M
[32m[04/15 05:55:24 d2.utils.events]: [0m eta: 0:10:25  iter: 879  total_loss: 0.718  loss_cls: 0.228  loss_box_reg: 0.324  loss_rpn_cls: 0.056  loss_rpn_loc: 0.069  time: 0.5611  data_time: 0.0089  lr: 0.008791  max_mem: 3068M
[32m[04/15 05:55:36 d2.utils.events]: [0m eta: 0:10:14  iter: 899  total_loss: 0.752  loss_cls: 0.261  loss_box_reg: 0.345  loss_rpn_cls: 0.048  loss_rpn_loc: 0.087  time: 0.5609  data_time: 0.0060  lr: 0.008991  max_mem: 3068M
[32m[04/15 05:55:47 d2.utils.events]: [0m eta: 0:10:04  iter: 919  total_loss: 0.849  loss_cls: 0.290  loss_box_reg: 0.398  loss_rpn_cls: 0.043  loss_rpn_loc: 0.092  time: 0.5611  data_time: 0.0072  lr: 0.009191  max_mem: 3068M
[32m[04/15 05:55:58 d2.utils.events]: [0m eta: 0:09:52  iter: 939  total_loss: 0.777  loss_cls: 0.250  loss_box_reg: 0.375  loss_rpn_cls: 0.041  loss_rpn_loc: 0.086  time: 0.5608  data_time: 0.0081  lr: 0.009391  max_mem: 3068M
[32m[04/15 05:56:09 d2.utils.events]: [0m eta: 0:09:41  iter: 959  total_loss: 0.796  loss_cls: 0.288  loss_box_reg: 0.388  loss_rpn_cls: 0.050  loss_rpn_loc: 0.065  time: 0.5604  data_time: 0.0084  lr: 0.009590  max_mem: 3068M
[32m[04/15 05:56:21 d2.utils.events]: [0m eta: 0:09:30  iter: 979  total_loss: 0.711  loss_cls: 0.258  loss_box_reg: 0.347  loss_rpn_cls: 0.045  loss_rpn_loc: 0.061  time: 0.5605  data_time: 0.0082  lr: 0.009790  max_mem: 3068M
[32m[04/15 05:56:32 d2.utils.events]: [0m eta: 0:09:18  iter: 999  total_loss: 0.916  loss_cls: 0.278  loss_box_reg: 0.403  loss_rpn_cls: 0.082  loss_rpn_loc: 0.090  time: 0.5602  data_time: 0.0066  lr: 0.009990  max_mem: 3068M
[32m[04/15 05:56:43 d2.utils.events]: [0m eta: 0:09:08  iter: 1019  total_loss: 0.740  loss_cls: 0.270  loss_box_reg: 0.376  loss_rpn_cls: 0.060  loss_rpn_loc: 0.068  time: 0.5605  data_time: 0.0065  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:56:55 d2.utils.events]: [0m eta: 0:08:58  iter: 1039  total_loss: 0.770  loss_cls: 0.259  loss_box_reg: 0.345  loss_rpn_cls: 0.053  loss_rpn_loc: 0.079  time: 0.5603  data_time: 0.0061  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:57:06 d2.utils.events]: [0m eta: 0:08:47  iter: 1059  total_loss: 0.703  loss_cls: 0.273  loss_box_reg: 0.358  loss_rpn_cls: 0.037  loss_rpn_loc: 0.070  time: 0.5605  data_time: 0.0072  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:57:17 d2.utils.events]: [0m eta: 0:08:36  iter: 1079  total_loss: 0.781  loss_cls: 0.261  loss_box_reg: 0.380  loss_rpn_cls: 0.043  loss_rpn_loc: 0.080  time: 0.5605  data_time: 0.0070  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:57:29 d2.utils.events]: [0m eta: 0:08:24  iter: 1099  total_loss: 0.713  loss_cls: 0.230  loss_box_reg: 0.291  loss_rpn_cls: 0.062  loss_rpn_loc: 0.101  time: 0.5605  data_time: 0.0075  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:57:41 d2.utils.events]: [0m eta: 0:08:14  iter: 1119  total_loss: 0.711  loss_cls: 0.272  loss_box_reg: 0.377  loss_rpn_cls: 0.043  loss_rpn_loc: 0.074  time: 0.5610  data_time: 0.0070  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:57:52 d2.utils.events]: [0m eta: 0:08:03  iter: 1139  total_loss: 0.648  loss_cls: 0.223  loss_box_reg: 0.295  loss_rpn_cls: 0.084  loss_rpn_loc: 0.068  time: 0.5608  data_time: 0.0076  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:58:04 d2.utils.events]: [0m eta: 0:07:52  iter: 1159  total_loss: 0.772  loss_cls: 0.249  loss_box_reg: 0.401  loss_rpn_cls: 0.038  loss_rpn_loc: 0.070  time: 0.5612  data_time: 0.0072  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:58:15 d2.utils.events]: [0m eta: 0:07:41  iter: 1179  total_loss: 0.986  loss_cls: 0.309  loss_box_reg: 0.430  loss_rpn_cls: 0.081  loss_rpn_loc: 0.118  time: 0.5614  data_time: 0.0067  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:58:26 d2.utils.events]: [0m eta: 0:07:29  iter: 1199  total_loss: 0.821  loss_cls: 0.282  loss_box_reg: 0.384  loss_rpn_cls: 0.067  loss_rpn_loc: 0.101  time: 0.5612  data_time: 0.0068  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:58:37 d2.utils.events]: [0m eta: 0:07:18  iter: 1219  total_loss: 0.728  loss_cls: 0.236  loss_box_reg: 0.317  loss_rpn_cls: 0.052  loss_rpn_loc: 0.084  time: 0.5608  data_time: 0.0070  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:58:48 d2.utils.events]: [0m eta: 0:07:07  iter: 1239  total_loss: 0.759  loss_cls: 0.265  loss_box_reg: 0.357  loss_rpn_cls: 0.042  loss_rpn_loc: 0.078  time: 0.5606  data_time: 0.0111  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:59:00 d2.utils.events]: [0m eta: 0:06:56  iter: 1259  total_loss: 0.740  loss_cls: 0.259  loss_box_reg: 0.360  loss_rpn_cls: 0.051  loss_rpn_loc: 0.078  time: 0.5605  data_time: 0.0085  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:59:11 d2.utils.events]: [0m eta: 0:06:45  iter: 1279  total_loss: 0.660  loss_cls: 0.199  loss_box_reg: 0.329  loss_rpn_cls: 0.058  loss_rpn_loc: 0.072  time: 0.5607  data_time: 0.0111  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:59:22 d2.utils.events]: [0m eta: 0:06:33  iter: 1299  total_loss: 0.769  loss_cls: 0.256  loss_box_reg: 0.374  loss_rpn_cls: 0.049  loss_rpn_loc: 0.074  time: 0.5605  data_time: 0.0063  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:59:33 d2.utils.events]: [0m eta: 0:06:22  iter: 1319  total_loss: 0.735  loss_cls: 0.245  loss_box_reg: 0.392  loss_rpn_cls: 0.048  loss_rpn_loc: 0.067  time: 0.5604  data_time: 0.0068  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:59:45 d2.utils.events]: [0m eta: 0:06:11  iter: 1339  total_loss: 0.686  loss_cls: 0.238  loss_box_reg: 0.357  loss_rpn_cls: 0.047  loss_rpn_loc: 0.072  time: 0.5604  data_time: 0.0069  lr: 0.010000  max_mem: 3068M
[32m[04/15 05:59:56 d2.utils.events]: [0m eta: 0:06:00  iter: 1359  total_loss: 0.826  loss_cls: 0.280  loss_box_reg: 0.388  loss_rpn_cls: 0.055  loss_rpn_loc: 0.073  time: 0.5605  data_time: 0.0078  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:00:07 d2.utils.events]: [0m eta: 0:05:49  iter: 1379  total_loss: 0.770  loss_cls: 0.247  loss_box_reg: 0.347  loss_rpn_cls: 0.042  loss_rpn_loc: 0.087  time: 0.5602  data_time: 0.0074  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:00:18 d2.utils.events]: [0m eta: 0:05:38  iter: 1399  total_loss: 0.630  loss_cls: 0.202  loss_box_reg: 0.295  loss_rpn_cls: 0.053  loss_rpn_loc: 0.083  time: 0.5602  data_time: 0.0071  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:00:29 d2.utils.events]: [0m eta: 0:05:26  iter: 1419  total_loss: 0.776  loss_cls: 0.299  loss_box_reg: 0.360  loss_rpn_cls: 0.060  loss_rpn_loc: 0.086  time: 0.5600  data_time: 0.0068  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:00:41 d2.utils.events]: [0m eta: 0:05:15  iter: 1439  total_loss: 0.821  loss_cls: 0.266  loss_box_reg: 0.361  loss_rpn_cls: 0.043  loss_rpn_loc: 0.076  time: 0.5600  data_time: 0.0116  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:00:52 d2.utils.events]: [0m eta: 0:05:04  iter: 1459  total_loss: 0.766  loss_cls: 0.232  loss_box_reg: 0.306  loss_rpn_cls: 0.047  loss_rpn_loc: 0.071  time: 0.5603  data_time: 0.0090  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:01:03 d2.utils.events]: [0m eta: 0:04:53  iter: 1479  total_loss: 0.801  loss_cls: 0.263  loss_box_reg: 0.397  loss_rpn_cls: 0.061  loss_rpn_loc: 0.087  time: 0.5599  data_time: 0.0077  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:01:15 d2.utils.events]: [0m eta: 0:04:42  iter: 1499  total_loss: 0.642  loss_cls: 0.213  loss_box_reg: 0.289  loss_rpn_cls: 0.049  loss_rpn_loc: 0.084  time: 0.5600  data_time: 0.0075  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:01:26 d2.utils.events]: [0m eta: 0:04:31  iter: 1519  total_loss: 0.802  loss_cls: 0.277  loss_box_reg: 0.393  loss_rpn_cls: 0.047  loss_rpn_loc: 0.084  time: 0.5602  data_time: 0.0086  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:01:38 d2.utils.events]: [0m eta: 0:04:19  iter: 1539  total_loss: 0.730  loss_cls: 0.221  loss_box_reg: 0.316  loss_rpn_cls: 0.049  loss_rpn_loc: 0.083  time: 0.5603  data_time: 0.0082  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:01:48 d2.utils.events]: [0m eta: 0:04:08  iter: 1559  total_loss: 0.703  loss_cls: 0.236  loss_box_reg: 0.368  loss_rpn_cls: 0.042  loss_rpn_loc: 0.076  time: 0.5599  data_time: 0.0063  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:02:00 d2.utils.events]: [0m eta: 0:03:57  iter: 1579  total_loss: 0.658  loss_cls: 0.210  loss_box_reg: 0.293  loss_rpn_cls: 0.045  loss_rpn_loc: 0.059  time: 0.5600  data_time: 0.0086  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:02:12 d2.utils.events]: [0m eta: 0:03:46  iter: 1599  total_loss: 0.787  loss_cls: 0.244  loss_box_reg: 0.390  loss_rpn_cls: 0.045  loss_rpn_loc: 0.085  time: 0.5598  data_time: 0.0061  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:02:23 d2.utils.events]: [0m eta: 0:03:34  iter: 1619  total_loss: 0.585  loss_cls: 0.203  loss_box_reg: 0.261  loss_rpn_cls: 0.042  loss_rpn_loc: 0.058  time: 0.5597  data_time: 0.0252  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:02:34 d2.utils.events]: [0m eta: 0:03:23  iter: 1639  total_loss: 0.682  loss_cls: 0.223  loss_box_reg: 0.340  loss_rpn_cls: 0.052  loss_rpn_loc: 0.074  time: 0.5595  data_time: 0.0099  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:02:45 d2.utils.events]: [0m eta: 0:03:11  iter: 1659  total_loss: 1.035  loss_cls: 0.332  loss_box_reg: 0.467  loss_rpn_cls: 0.053  loss_rpn_loc: 0.122  time: 0.5593  data_time: 0.0069  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:03:06 d2.utils.events]: [0m eta: 0:03:00  iter: 1679  total_loss: 0.837  loss_cls: 0.263  loss_box_reg: 0.383  loss_rpn_cls: 0.043  loss_rpn_loc: 0.095  time: 0.5591  data_time: 0.0088  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:03:17 d2.utils.events]: [0m eta: 0:02:49  iter: 1699  total_loss: 0.811  loss_cls: 0.252  loss_box_reg: 0.420  loss_rpn_cls: 0.037  loss_rpn_loc: 0.078  time: 0.5592  data_time: 0.0088  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:03:28 d2.utils.events]: [0m eta: 0:02:38  iter: 1719  total_loss: 0.681  loss_cls: 0.215  loss_box_reg: 0.332  loss_rpn_cls: 0.057  loss_rpn_loc: 0.076  time: 0.5590  data_time: 0.0078  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:03:39 d2.utils.events]: [0m eta: 0:02:26  iter: 1739  total_loss: 0.786  loss_cls: 0.272  loss_box_reg: 0.363  loss_rpn_cls: 0.036  loss_rpn_loc: 0.088  time: 0.5587  data_time: 0.0063  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:03:51 d2.utils.events]: [0m eta: 0:02:15  iter: 1759  total_loss: 0.657  loss_cls: 0.233  loss_box_reg: 0.310  loss_rpn_cls: 0.058  loss_rpn_loc: 0.090  time: 0.5587  data_time: 0.0071  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:04:02 d2.utils.events]: [0m eta: 0:02:04  iter: 1779  total_loss: 0.821  loss_cls: 0.279  loss_box_reg: 0.366  loss_rpn_cls: 0.055  loss_rpn_loc: 0.087  time: 0.5588  data_time: 0.0074  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:04:12 d2.utils.events]: [0m eta: 0:01:52  iter: 1799  total_loss: 0.826  loss_cls: 0.281  loss_box_reg: 0.409  loss_rpn_cls: 0.048  loss_rpn_loc: 0.102  time: 0.5584  data_time: 0.0068  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:04:27 d2.utils.events]: [0m eta: 0:01:41  iter: 1819  total_loss: 0.744  loss_cls: 0.260  loss_box_reg: 0.347  loss_rpn_cls: 0.036  loss_rpn_loc: 0.068  time: 0.5602  data_time: 0.2237  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:04:38 d2.utils.events]: [0m eta: 0:01:30  iter: 1839  total_loss: 0.821  loss_cls: 0.284  loss_box_reg: 0.380  loss_rpn_cls: 0.042  loss_rpn_loc: 0.081  time: 0.5601  data_time: 0.0081  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:04:49 d2.utils.events]: [0m eta: 0:01:19  iter: 1859  total_loss: 0.912  loss_cls: 0.273  loss_box_reg: 0.458  loss_rpn_cls: 0.049  loss_rpn_loc: 0.088  time: 0.5599  data_time: 0.0069  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:05:00 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.713  loss_cls: 0.245  loss_box_reg: 0.348  loss_rpn_cls: 0.046  loss_rpn_loc: 0.079  time: 0.5598  data_time: 0.0073  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:05:11 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.827  loss_cls: 0.238  loss_box_reg: 0.364  loss_rpn_cls: 0.060  loss_rpn_loc: 0.093  time: 0.5597  data_time: 0.0079  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:05:22 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.769  loss_cls: 0.260  loss_box_reg: 0.350  loss_rpn_cls: 0.053  loss_rpn_loc: 0.095  time: 0.5595  data_time: 0.0071  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:05:33 d2.utils.events]: [0m eta: 0:00:34  iter: 1939  total_loss: 0.665  loss_cls: 0.207  loss_box_reg: 0.328  loss_rpn_cls: 0.055  loss_rpn_loc: 0.091  time: 0.5594  data_time: 0.0077  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:05:45 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.745  loss_cls: 0.263  loss_box_reg: 0.348  loss_rpn_cls: 0.060  loss_rpn_loc: 0.076  time: 0.5593  data_time: 0.0080  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:05:56 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.766  loss_cls: 0.248  loss_box_reg: 0.357  loss_rpn_cls: 0.047  loss_rpn_loc: 0.092  time: 0.5595  data_time: 0.0556  lr: 0.010000  max_mem: 3068M
[5m[31mWARNING[0m [32m[04/15 06:06:22 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:06:22 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 06:06:22 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 06:06:22 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.727  loss_cls: 0.250  loss_box_reg: 0.389  loss_rpn_cls: 0.041  loss_rpn_loc: 0.060  time: 0.5595  data_time: 0.0078  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:06:25 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:37 (0.5598 s / it)
[32m[04/15 06:06:25 d2.engine.hooks]: [0mTotal training time: 0:19:17 (0:00:39 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 06:06:40 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:06:40 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 06:06:40 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 06:06:42 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1212 s / img. ETA=0:02:34
[32m[04/15 06:06:48 d2.evaluation.evaluator]: [0mInference done 38/1257. 0.1210 s / img. ETA=0:03:45
[32m[04/15 06:06:53 d2.evaluation.evaluator]: [0mInference done 68/1257. 0.1218 s / img. ETA=0:03:32
[32m[04/15 06:06:58 d2.evaluation.evaluator]: [0mInference done 99/1257. 0.1222 s / img. ETA=0:03:21
[32m[04/15 06:07:03 d2.evaluation.evaluator]: [0mInference done 134/1257. 0.1231 s / img. ETA=0:03:06
[32m[04/15 06:07:08 d2.evaluation.evaluator]: [0mInference done 167/1257. 0.1227 s / img. ETA=0:02:58
[32m[04/15 06:07:13 d2.evaluation.evaluator]: [0mInference done 207/1257. 0.1227 s / img. ETA=0:02:43
[32m[04/15 06:07:18 d2.evaluation.evaluator]: [0mInference done 246/1257. 0.1229 s / img. ETA=0:02:33
[32m[04/15 06:07:23 d2.evaluation.evaluator]: [0mInference done 283/1257. 0.1228 s / img. ETA=0:02:26
[32m[04/15 06:07:29 d2.evaluation.evaluator]: [0mInference done 324/1257. 0.1226 s / img. ETA=0:02:16
[32m[04/15 06:07:34 d2.evaluation.evaluator]: [0mInference done 363/1257. 0.1226 s / img. ETA=0:02:09
[32m[04/15 06:07:39 d2.evaluation.evaluator]: [0mInference done 402/1257. 0.1225 s / img. ETA=0:02:02
[32m[04/15 06:07:44 d2.evaluation.evaluator]: [0mInference done 435/1257. 0.1225 s / img. ETA=0:01:58
[32m[04/15 06:07:49 d2.evaluation.evaluator]: [0mInference done 460/1257. 0.1224 s / img. ETA=0:01:57
[32m[04/15 06:07:54 d2.evaluation.evaluator]: [0mInference done 493/1257. 0.1225 s / img. ETA=0:01:52
[32m[04/15 06:07:59 d2.evaluation.evaluator]: [0mInference done 525/1257. 0.1224 s / img. ETA=0:01:48
[32m[04/15 06:08:04 d2.evaluation.evaluator]: [0mInference done 560/1257. 0.1226 s / img. ETA=0:01:43
[32m[04/15 06:08:09 d2.evaluation.evaluator]: [0mInference done 593/1257. 0.1227 s / img. ETA=0:01:38
[32m[04/15 06:08:14 d2.evaluation.evaluator]: [0mInference done 626/1257. 0.1226 s / img. ETA=0:01:33
[32m[04/15 06:08:19 d2.evaluation.evaluator]: [0mInference done 654/1257. 0.1226 s / img. ETA=0:01:30
[32m[04/15 06:08:24 d2.evaluation.evaluator]: [0mInference done 684/1257. 0.1227 s / img. ETA=0:01:26
[32m[04/15 06:08:30 d2.evaluation.evaluator]: [0mInference done 717/1257. 0.1227 s / img. ETA=0:01:21
[32m[04/15 06:08:35 d2.evaluation.evaluator]: [0mInference done 752/1257. 0.1227 s / img. ETA=0:01:16
[32m[04/15 06:08:40 d2.evaluation.evaluator]: [0mInference done 788/1257. 0.1228 s / img. ETA=0:01:10
[32m[04/15 06:08:45 d2.evaluation.evaluator]: [0mInference done 827/1257. 0.1230 s / img. ETA=0:01:04
[32m[04/15 06:08:50 d2.evaluation.evaluator]: [0mInference done 866/1257. 0.1231 s / img. ETA=0:00:58
[32m[04/15 06:08:55 d2.evaluation.evaluator]: [0mInference done 906/1257. 0.1231 s / img. ETA=0:00:51
[32m[04/15 06:09:00 d2.evaluation.evaluator]: [0mInference done 945/1257. 0.1232 s / img. ETA=0:00:45
[32m[04/15 06:09:05 d2.evaluation.evaluator]: [0mInference done 985/1257. 0.1232 s / img. ETA=0:00:39
[32m[04/15 06:09:10 d2.evaluation.evaluator]: [0mInference done 1025/1257. 0.1233 s / img. ETA=0:00:33
[32m[04/15 06:09:15 d2.evaluation.evaluator]: [0mInference done 1065/1257. 0.1233 s / img. ETA=0:00:27
[32m[04/15 06:09:20 d2.evaluation.evaluator]: [0mInference done 1103/1257. 0.1234 s / img. ETA=0:00:22
[32m[04/15 06:09:25 d2.evaluation.evaluator]: [0mInference done 1142/1257. 0.1235 s / img. ETA=0:00:16
[32m[04/15 06:09:30 d2.evaluation.evaluator]: [0mInference done 1181/1257. 0.1237 s / img. ETA=0:00:10
[32m[04/15 06:09:35 d2.evaluation.evaluator]: [0mInference done 1220/1257. 0.1237 s / img. ETA=0:00:05
[32m[04/15 06:09:40 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:58.471222 (0.142549 s / img per device, on 1 devices)
[32m[04/15 06:09:40 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:34 (0.123769 s / img per device, on 1 devices)
[32m[04/15 06:09:40 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 06:09:40 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 06:09:40 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.24s).
Accumulating evaluation results...
DONE (t=0.75s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.099
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.204
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.082
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.067
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.116
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.230
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.130
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.131
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.154
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.267
[32m[04/15 06:09:51 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:-----:|:------:|:------:|:-----:|:------:|:------:|
| 9.915 | 20.414 | 8.220  | 6.689 | 11.613 | 22.985 |
[32m[04/15 06:09:51 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 12.054 | bicycle       | 1.685 | car            | 25.920 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  2  *  2000  iterations ============
4 channel input
[32m[04/15 06:09:52 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[5m[31mWARNING[0m [32m[04/15 06:09:53 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:09:53 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 06:09:54 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 06:09:54 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 06:09:54 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 06:09:54 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 06:10:06 d2.utils.events]: [0m eta: 0:18:36  iter: 19  total_loss: 0.684  loss_cls: 0.227  loss_box_reg: 0.329  loss_rpn_cls: 0.034  loss_rpn_loc: 0.072  time: 0.5574  data_time: 0.0453  lr: 0.000200  max_mem: 3068M
[32m[04/15 06:10:17 d2.utils.events]: [0m eta: 0:17:40  iter: 39  total_loss: 0.710  loss_cls: 0.240  loss_box_reg: 0.331  loss_rpn_cls: 0.042  loss_rpn_loc: 0.060  time: 0.5417  data_time: 0.0072  lr: 0.000400  max_mem: 3068M
[32m[04/15 06:10:28 d2.utils.events]: [0m eta: 0:17:31  iter: 59  total_loss: 0.736  loss_cls: 0.253  loss_box_reg: 0.363  loss_rpn_cls: 0.035  loss_rpn_loc: 0.067  time: 0.5401  data_time: 0.0072  lr: 0.000599  max_mem: 3068M
[32m[04/15 06:10:39 d2.utils.events]: [0m eta: 0:17:27  iter: 79  total_loss: 0.593  loss_cls: 0.215  loss_box_reg: 0.317  loss_rpn_cls: 0.033  loss_rpn_loc: 0.062  time: 0.5442  data_time: 0.0081  lr: 0.000799  max_mem: 3068M
[32m[04/15 06:10:49 d2.utils.events]: [0m eta: 0:17:07  iter: 99  total_loss: 0.779  loss_cls: 0.265  loss_box_reg: 0.419  loss_rpn_cls: 0.036  loss_rpn_loc: 0.068  time: 0.5415  data_time: 0.0077  lr: 0.000999  max_mem: 3068M
[32m[04/15 06:11:01 d2.utils.events]: [0m eta: 0:16:59  iter: 119  total_loss: 0.565  loss_cls: 0.197  loss_box_reg: 0.297  loss_rpn_cls: 0.035  loss_rpn_loc: 0.059  time: 0.5441  data_time: 0.0064  lr: 0.001199  max_mem: 3068M
[32m[04/15 06:11:12 d2.utils.events]: [0m eta: 0:16:58  iter: 139  total_loss: 0.744  loss_cls: 0.233  loss_box_reg: 0.390  loss_rpn_cls: 0.035  loss_rpn_loc: 0.086  time: 0.5471  data_time: 0.0091  lr: 0.001399  max_mem: 3068M
[32m[04/15 06:11:23 d2.utils.events]: [0m eta: 0:16:46  iter: 159  total_loss: 0.765  loss_cls: 0.259  loss_box_reg: 0.416  loss_rpn_cls: 0.035  loss_rpn_loc: 0.067  time: 0.5463  data_time: 0.0096  lr: 0.001598  max_mem: 3068M
[32m[04/15 06:11:35 d2.utils.events]: [0m eta: 0:16:40  iter: 179  total_loss: 0.665  loss_cls: 0.248  loss_box_reg: 0.343  loss_rpn_cls: 0.027  loss_rpn_loc: 0.054  time: 0.5496  data_time: 0.0082  lr: 0.001798  max_mem: 3068M
[32m[04/15 06:11:46 d2.utils.events]: [0m eta: 0:16:32  iter: 199  total_loss: 0.699  loss_cls: 0.259  loss_box_reg: 0.332  loss_rpn_cls: 0.029  loss_rpn_loc: 0.066  time: 0.5506  data_time: 0.0071  lr: 0.001998  max_mem: 3068M
[32m[04/15 06:11:57 d2.utils.events]: [0m eta: 0:16:23  iter: 219  total_loss: 0.669  loss_cls: 0.227  loss_box_reg: 0.342  loss_rpn_cls: 0.043  loss_rpn_loc: 0.059  time: 0.5506  data_time: 0.0076  lr: 0.002198  max_mem: 3068M
[32m[04/15 06:12:08 d2.utils.events]: [0m eta: 0:16:20  iter: 239  total_loss: 0.722  loss_cls: 0.248  loss_box_reg: 0.390  loss_rpn_cls: 0.028  loss_rpn_loc: 0.075  time: 0.5518  data_time: 0.0062  lr: 0.002398  max_mem: 3068M
[32m[04/15 06:12:20 d2.utils.events]: [0m eta: 0:16:10  iter: 259  total_loss: 0.652  loss_cls: 0.208  loss_box_reg: 0.326  loss_rpn_cls: 0.036  loss_rpn_loc: 0.065  time: 0.5523  data_time: 0.0077  lr: 0.002597  max_mem: 3068M
[32m[04/15 06:12:31 d2.utils.events]: [0m eta: 0:15:56  iter: 279  total_loss: 0.713  loss_cls: 0.262  loss_box_reg: 0.357  loss_rpn_cls: 0.033  loss_rpn_loc: 0.066  time: 0.5516  data_time: 0.0066  lr: 0.002797  max_mem: 3068M
[32m[04/15 06:12:42 d2.utils.events]: [0m eta: 0:15:44  iter: 299  total_loss: 0.689  loss_cls: 0.252  loss_box_reg: 0.370  loss_rpn_cls: 0.029  loss_rpn_loc: 0.077  time: 0.5515  data_time: 0.0073  lr: 0.002997  max_mem: 3068M
[32m[04/15 06:12:53 d2.utils.events]: [0m eta: 0:15:32  iter: 319  total_loss: 0.793  loss_cls: 0.243  loss_box_reg: 0.441  loss_rpn_cls: 0.031  loss_rpn_loc: 0.073  time: 0.5514  data_time: 0.0068  lr: 0.003197  max_mem: 3068M
[32m[04/15 06:13:04 d2.utils.events]: [0m eta: 0:15:21  iter: 339  total_loss: 0.732  loss_cls: 0.236  loss_box_reg: 0.398  loss_rpn_cls: 0.036  loss_rpn_loc: 0.052  time: 0.5517  data_time: 0.0104  lr: 0.003397  max_mem: 3068M
[32m[04/15 06:13:15 d2.utils.events]: [0m eta: 0:15:09  iter: 359  total_loss: 0.670  loss_cls: 0.231  loss_box_reg: 0.331  loss_rpn_cls: 0.031  loss_rpn_loc: 0.049  time: 0.5518  data_time: 0.0074  lr: 0.003596  max_mem: 3068M
[32m[04/15 06:13:27 d2.utils.events]: [0m eta: 0:15:02  iter: 379  total_loss: 0.803  loss_cls: 0.254  loss_box_reg: 0.411  loss_rpn_cls: 0.032  loss_rpn_loc: 0.074  time: 0.5529  data_time: 0.0069  lr: 0.003796  max_mem: 3068M
[32m[04/15 06:13:38 d2.utils.events]: [0m eta: 0:14:52  iter: 399  total_loss: 0.659  loss_cls: 0.226  loss_box_reg: 0.342  loss_rpn_cls: 0.037  loss_rpn_loc: 0.064  time: 0.5537  data_time: 0.0095  lr: 0.003996  max_mem: 3068M
[32m[04/15 06:13:49 d2.utils.events]: [0m eta: 0:14:39  iter: 419  total_loss: 0.745  loss_cls: 0.257  loss_box_reg: 0.361  loss_rpn_cls: 0.037  loss_rpn_loc: 0.085  time: 0.5532  data_time: 0.0072  lr: 0.004196  max_mem: 3068M
[32m[04/15 06:14:01 d2.utils.events]: [0m eta: 0:14:28  iter: 439  total_loss: 0.778  loss_cls: 0.269  loss_box_reg: 0.399  loss_rpn_cls: 0.027  loss_rpn_loc: 0.063  time: 0.5534  data_time: 0.0067  lr: 0.004396  max_mem: 3068M
[32m[04/15 06:14:12 d2.utils.events]: [0m eta: 0:14:16  iter: 459  total_loss: 0.749  loss_cls: 0.235  loss_box_reg: 0.416  loss_rpn_cls: 0.030  loss_rpn_loc: 0.076  time: 0.5534  data_time: 0.0075  lr: 0.004595  max_mem: 3068M
[32m[04/15 06:14:23 d2.utils.events]: [0m eta: 0:14:06  iter: 479  total_loss: 0.667  loss_cls: 0.210  loss_box_reg: 0.333  loss_rpn_cls: 0.036  loss_rpn_loc: 0.055  time: 0.5544  data_time: 0.0070  lr: 0.004795  max_mem: 3068M
[32m[04/15 06:14:34 d2.utils.events]: [0m eta: 0:13:54  iter: 499  total_loss: 0.779  loss_cls: 0.246  loss_box_reg: 0.386  loss_rpn_cls: 0.042  loss_rpn_loc: 0.074  time: 0.5538  data_time: 0.0059  lr: 0.004995  max_mem: 3068M
[32m[04/15 06:14:45 d2.utils.events]: [0m eta: 0:13:41  iter: 519  total_loss: 0.876  loss_cls: 0.288  loss_box_reg: 0.351  loss_rpn_cls: 0.041  loss_rpn_loc: 0.095  time: 0.5531  data_time: 0.0074  lr: 0.005195  max_mem: 3068M
[32m[04/15 06:14:56 d2.utils.events]: [0m eta: 0:13:30  iter: 539  total_loss: 0.611  loss_cls: 0.206  loss_box_reg: 0.308  loss_rpn_cls: 0.028  loss_rpn_loc: 0.065  time: 0.5531  data_time: 0.0074  lr: 0.005395  max_mem: 3068M
[32m[04/15 06:15:08 d2.utils.events]: [0m eta: 0:13:20  iter: 559  total_loss: 0.780  loss_cls: 0.250  loss_box_reg: 0.407  loss_rpn_cls: 0.042  loss_rpn_loc: 0.083  time: 0.5534  data_time: 0.0067  lr: 0.005594  max_mem: 3068M
[32m[04/15 06:15:19 d2.utils.events]: [0m eta: 0:13:09  iter: 579  total_loss: 0.616  loss_cls: 0.210  loss_box_reg: 0.311  loss_rpn_cls: 0.037  loss_rpn_loc: 0.057  time: 0.5532  data_time: 0.0083  lr: 0.005794  max_mem: 3068M
[32m[04/15 06:15:29 d2.utils.events]: [0m eta: 0:12:57  iter: 599  total_loss: 0.694  loss_cls: 0.253  loss_box_reg: 0.335  loss_rpn_cls: 0.042  loss_rpn_loc: 0.071  time: 0.5528  data_time: 0.0073  lr: 0.005994  max_mem: 3068M
[32m[04/15 06:15:41 d2.utils.events]: [0m eta: 0:12:45  iter: 619  total_loss: 0.638  loss_cls: 0.208  loss_box_reg: 0.314  loss_rpn_cls: 0.043  loss_rpn_loc: 0.074  time: 0.5527  data_time: 0.0073  lr: 0.006194  max_mem: 3068M
[32m[04/15 06:15:52 d2.utils.events]: [0m eta: 0:12:35  iter: 639  total_loss: 0.725  loss_cls: 0.237  loss_box_reg: 0.372  loss_rpn_cls: 0.032  loss_rpn_loc: 0.070  time: 0.5535  data_time: 0.0068  lr: 0.006394  max_mem: 3068M
[32m[04/15 06:16:03 d2.utils.events]: [0m eta: 0:12:24  iter: 659  total_loss: 0.686  loss_cls: 0.241  loss_box_reg: 0.362  loss_rpn_cls: 0.044  loss_rpn_loc: 0.064  time: 0.5533  data_time: 0.0072  lr: 0.006593  max_mem: 3068M
[32m[04/15 06:16:14 d2.utils.events]: [0m eta: 0:12:13  iter: 679  total_loss: 0.765  loss_cls: 0.245  loss_box_reg: 0.409  loss_rpn_cls: 0.037  loss_rpn_loc: 0.079  time: 0.5529  data_time: 0.0077  lr: 0.006793  max_mem: 3068M
[32m[04/15 06:16:25 d2.utils.events]: [0m eta: 0:12:02  iter: 699  total_loss: 0.760  loss_cls: 0.269  loss_box_reg: 0.402  loss_rpn_cls: 0.037  loss_rpn_loc: 0.064  time: 0.5532  data_time: 0.0069  lr: 0.006993  max_mem: 3068M
[32m[04/15 06:16:37 d2.utils.events]: [0m eta: 0:11:51  iter: 719  total_loss: 0.684  loss_cls: 0.233  loss_box_reg: 0.339  loss_rpn_cls: 0.050  loss_rpn_loc: 0.060  time: 0.5531  data_time: 0.0089  lr: 0.007193  max_mem: 3068M
[32m[04/15 06:16:48 d2.utils.events]: [0m eta: 0:11:39  iter: 739  total_loss: 0.723  loss_cls: 0.253  loss_box_reg: 0.362  loss_rpn_cls: 0.040  loss_rpn_loc: 0.073  time: 0.5529  data_time: 0.0102  lr: 0.007393  max_mem: 3068M
[32m[04/15 06:16:59 d2.utils.events]: [0m eta: 0:11:29  iter: 759  total_loss: 0.725  loss_cls: 0.254  loss_box_reg: 0.385  loss_rpn_cls: 0.037  loss_rpn_loc: 0.047  time: 0.5535  data_time: 0.0079  lr: 0.007592  max_mem: 3068M
[32m[04/15 06:17:10 d2.utils.events]: [0m eta: 0:11:18  iter: 779  total_loss: 0.834  loss_cls: 0.275  loss_box_reg: 0.445  loss_rpn_cls: 0.033  loss_rpn_loc: 0.082  time: 0.5537  data_time: 0.0074  lr: 0.007792  max_mem: 3068M
[32m[04/15 06:17:22 d2.utils.events]: [0m eta: 0:11:07  iter: 799  total_loss: 0.894  loss_cls: 0.300  loss_box_reg: 0.428  loss_rpn_cls: 0.040  loss_rpn_loc: 0.062  time: 0.5542  data_time: 0.0066  lr: 0.007992  max_mem: 3068M
[32m[04/15 06:17:33 d2.utils.events]: [0m eta: 0:10:56  iter: 819  total_loss: 0.614  loss_cls: 0.224  loss_box_reg: 0.300  loss_rpn_cls: 0.039  loss_rpn_loc: 0.058  time: 0.5545  data_time: 0.0075  lr: 0.008192  max_mem: 3068M
[32m[04/15 06:17:45 d2.utils.events]: [0m eta: 0:10:45  iter: 839  total_loss: 0.787  loss_cls: 0.273  loss_box_reg: 0.389  loss_rpn_cls: 0.034  loss_rpn_loc: 0.070  time: 0.5546  data_time: 0.0082  lr: 0.008392  max_mem: 3068M
[32m[04/15 06:17:56 d2.utils.events]: [0m eta: 0:10:34  iter: 859  total_loss: 0.775  loss_cls: 0.260  loss_box_reg: 0.398  loss_rpn_cls: 0.041  loss_rpn_loc: 0.064  time: 0.5549  data_time: 0.0104  lr: 0.008591  max_mem: 3068M
[32m[04/15 06:18:07 d2.utils.events]: [0m eta: 0:10:23  iter: 879  total_loss: 0.676  loss_cls: 0.226  loss_box_reg: 0.337  loss_rpn_cls: 0.031  loss_rpn_loc: 0.053  time: 0.5549  data_time: 0.0080  lr: 0.008791  max_mem: 3068M
[32m[04/15 06:18:19 d2.utils.events]: [0m eta: 0:10:12  iter: 899  total_loss: 0.787  loss_cls: 0.267  loss_box_reg: 0.413  loss_rpn_cls: 0.043  loss_rpn_loc: 0.071  time: 0.5552  data_time: 0.0080  lr: 0.008991  max_mem: 3068M
[32m[04/15 06:18:30 d2.utils.events]: [0m eta: 0:10:01  iter: 919  total_loss: 0.771  loss_cls: 0.241  loss_box_reg: 0.371  loss_rpn_cls: 0.044  loss_rpn_loc: 0.105  time: 0.5552  data_time: 0.0078  lr: 0.009191  max_mem: 3068M
[32m[04/15 06:18:41 d2.utils.events]: [0m eta: 0:09:50  iter: 939  total_loss: 0.682  loss_cls: 0.230  loss_box_reg: 0.322  loss_rpn_cls: 0.026  loss_rpn_loc: 0.064  time: 0.5551  data_time: 0.0067  lr: 0.009391  max_mem: 3068M
[32m[04/15 06:18:53 d2.utils.events]: [0m eta: 0:09:39  iter: 959  total_loss: 0.742  loss_cls: 0.257  loss_box_reg: 0.378  loss_rpn_cls: 0.044  loss_rpn_loc: 0.071  time: 0.5554  data_time: 0.0062  lr: 0.009590  max_mem: 3068M
[32m[04/15 06:19:04 d2.utils.events]: [0m eta: 0:09:28  iter: 979  total_loss: 0.776  loss_cls: 0.253  loss_box_reg: 0.416  loss_rpn_cls: 0.042  loss_rpn_loc: 0.077  time: 0.5560  data_time: 0.0070  lr: 0.009790  max_mem: 3068M
[32m[04/15 06:19:16 d2.utils.events]: [0m eta: 0:09:18  iter: 999  total_loss: 0.889  loss_cls: 0.270  loss_box_reg: 0.396  loss_rpn_cls: 0.037  loss_rpn_loc: 0.088  time: 0.5564  data_time: 0.0096  lr: 0.009990  max_mem: 3068M
[32m[04/15 06:19:27 d2.utils.events]: [0m eta: 0:09:06  iter: 1019  total_loss: 0.782  loss_cls: 0.286  loss_box_reg: 0.386  loss_rpn_cls: 0.044  loss_rpn_loc: 0.083  time: 0.5564  data_time: 0.0079  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:19:39 d2.utils.events]: [0m eta: 0:08:57  iter: 1039  total_loss: 0.720  loss_cls: 0.247  loss_box_reg: 0.378  loss_rpn_cls: 0.038  loss_rpn_loc: 0.074  time: 0.5565  data_time: 0.0074  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:19:50 d2.utils.events]: [0m eta: 0:08:45  iter: 1059  total_loss: 0.698  loss_cls: 0.268  loss_box_reg: 0.315  loss_rpn_cls: 0.035  loss_rpn_loc: 0.073  time: 0.5563  data_time: 0.0068  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:20:01 d2.utils.events]: [0m eta: 0:08:33  iter: 1079  total_loss: 0.797  loss_cls: 0.284  loss_box_reg: 0.390  loss_rpn_cls: 0.038  loss_rpn_loc: 0.092  time: 0.5560  data_time: 0.0093  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:20:12 d2.utils.events]: [0m eta: 0:08:22  iter: 1099  total_loss: 0.823  loss_cls: 0.272  loss_box_reg: 0.401  loss_rpn_cls: 0.043  loss_rpn_loc: 0.094  time: 0.5560  data_time: 0.0064  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:20:28 d2.utils.events]: [0m eta: 0:08:12  iter: 1119  total_loss: 0.783  loss_cls: 0.261  loss_box_reg: 0.406  loss_rpn_cls: 0.037  loss_rpn_loc: 0.073  time: 0.5602  data_time: 0.2694  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:20:39 d2.utils.events]: [0m eta: 0:08:01  iter: 1139  total_loss: 0.697  loss_cls: 0.238  loss_box_reg: 0.346  loss_rpn_cls: 0.042  loss_rpn_loc: 0.067  time: 0.5601  data_time: 0.0119  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:20:50 d2.utils.events]: [0m eta: 0:07:50  iter: 1159  total_loss: 0.735  loss_cls: 0.259  loss_box_reg: 0.367  loss_rpn_cls: 0.037  loss_rpn_loc: 0.069  time: 0.5600  data_time: 0.0101  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:21:01 d2.utils.events]: [0m eta: 0:07:38  iter: 1179  total_loss: 0.778  loss_cls: 0.267  loss_box_reg: 0.371  loss_rpn_cls: 0.042  loss_rpn_loc: 0.083  time: 0.5598  data_time: 0.0087  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:21:13 d2.utils.events]: [0m eta: 0:07:27  iter: 1199  total_loss: 0.811  loss_cls: 0.249  loss_box_reg: 0.396  loss_rpn_cls: 0.036  loss_rpn_loc: 0.081  time: 0.5599  data_time: 0.0090  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:21:24 d2.utils.events]: [0m eta: 0:07:16  iter: 1219  total_loss: 0.733  loss_cls: 0.261  loss_box_reg: 0.337  loss_rpn_cls: 0.042  loss_rpn_loc: 0.071  time: 0.5599  data_time: 0.0070  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:21:35 d2.utils.events]: [0m eta: 0:07:04  iter: 1239  total_loss: 0.907  loss_cls: 0.289  loss_box_reg: 0.399  loss_rpn_cls: 0.048  loss_rpn_loc: 0.089  time: 0.5599  data_time: 0.0077  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:21:47 d2.utils.events]: [0m eta: 0:06:53  iter: 1259  total_loss: 0.791  loss_cls: 0.278  loss_box_reg: 0.371  loss_rpn_cls: 0.044  loss_rpn_loc: 0.071  time: 0.5599  data_time: 0.0068  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:21:58 d2.utils.events]: [0m eta: 0:06:43  iter: 1279  total_loss: 0.712  loss_cls: 0.251  loss_box_reg: 0.329  loss_rpn_cls: 0.032  loss_rpn_loc: 0.069  time: 0.5598  data_time: 0.0072  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:22:09 d2.utils.events]: [0m eta: 0:06:31  iter: 1299  total_loss: 0.707  loss_cls: 0.206  loss_box_reg: 0.335  loss_rpn_cls: 0.045  loss_rpn_loc: 0.105  time: 0.5597  data_time: 0.0078  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:22:20 d2.utils.events]: [0m eta: 0:06:20  iter: 1319  total_loss: 0.743  loss_cls: 0.260  loss_box_reg: 0.423  loss_rpn_cls: 0.033  loss_rpn_loc: 0.061  time: 0.5595  data_time: 0.0085  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:22:31 d2.utils.events]: [0m eta: 0:06:09  iter: 1339  total_loss: 0.738  loss_cls: 0.273  loss_box_reg: 0.386  loss_rpn_cls: 0.029  loss_rpn_loc: 0.089  time: 0.5593  data_time: 0.0071  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:22:42 d2.utils.events]: [0m eta: 0:05:58  iter: 1359  total_loss: 0.667  loss_cls: 0.245  loss_box_reg: 0.328  loss_rpn_cls: 0.040  loss_rpn_loc: 0.059  time: 0.5594  data_time: 0.0067  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:22:54 d2.utils.events]: [0m eta: 0:05:46  iter: 1379  total_loss: 0.717  loss_cls: 0.257  loss_box_reg: 0.363  loss_rpn_cls: 0.053  loss_rpn_loc: 0.071  time: 0.5593  data_time: 0.0082  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:23:05 d2.utils.events]: [0m eta: 0:05:35  iter: 1399  total_loss: 0.757  loss_cls: 0.234  loss_box_reg: 0.343  loss_rpn_cls: 0.038  loss_rpn_loc: 0.077  time: 0.5592  data_time: 0.0068  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:23:16 d2.utils.events]: [0m eta: 0:05:24  iter: 1419  total_loss: 0.642  loss_cls: 0.243  loss_box_reg: 0.304  loss_rpn_cls: 0.047  loss_rpn_loc: 0.062  time: 0.5590  data_time: 0.0066  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:23:27 d2.utils.events]: [0m eta: 0:05:13  iter: 1439  total_loss: 0.649  loss_cls: 0.224  loss_box_reg: 0.296  loss_rpn_cls: 0.037  loss_rpn_loc: 0.068  time: 0.5592  data_time: 0.0070  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:23:38 d2.utils.events]: [0m eta: 0:05:01  iter: 1459  total_loss: 0.781  loss_cls: 0.241  loss_box_reg: 0.391  loss_rpn_cls: 0.045  loss_rpn_loc: 0.069  time: 0.5588  data_time: 0.0071  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:23:49 d2.utils.events]: [0m eta: 0:04:50  iter: 1479  total_loss: 0.884  loss_cls: 0.317  loss_box_reg: 0.430  loss_rpn_cls: 0.034  loss_rpn_loc: 0.066  time: 0.5588  data_time: 0.0075  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:24:01 d2.utils.events]: [0m eta: 0:04:39  iter: 1499  total_loss: 0.747  loss_cls: 0.226  loss_box_reg: 0.366  loss_rpn_cls: 0.036  loss_rpn_loc: 0.078  time: 0.5591  data_time: 0.0076  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:24:12 d2.utils.events]: [0m eta: 0:04:29  iter: 1519  total_loss: 0.833  loss_cls: 0.288  loss_box_reg: 0.393  loss_rpn_cls: 0.055  loss_rpn_loc: 0.075  time: 0.5592  data_time: 0.0069  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:24:23 d2.utils.events]: [0m eta: 0:04:17  iter: 1539  total_loss: 0.674  loss_cls: 0.228  loss_box_reg: 0.327  loss_rpn_cls: 0.059  loss_rpn_loc: 0.067  time: 0.5591  data_time: 0.0064  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:24:35 d2.utils.events]: [0m eta: 0:04:06  iter: 1559  total_loss: 0.843  loss_cls: 0.263  loss_box_reg: 0.386  loss_rpn_cls: 0.044  loss_rpn_loc: 0.089  time: 0.5590  data_time: 0.0084  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:24:46 d2.utils.events]: [0m eta: 0:03:55  iter: 1579  total_loss: 0.838  loss_cls: 0.279  loss_box_reg: 0.441  loss_rpn_cls: 0.035  loss_rpn_loc: 0.074  time: 0.5592  data_time: 0.0075  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:24:57 d2.utils.events]: [0m eta: 0:03:44  iter: 1599  total_loss: 0.712  loss_cls: 0.250  loss_box_reg: 0.361  loss_rpn_cls: 0.053  loss_rpn_loc: 0.060  time: 0.5589  data_time: 0.0074  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:25:08 d2.utils.events]: [0m eta: 0:03:33  iter: 1619  total_loss: 0.697  loss_cls: 0.227  loss_box_reg: 0.288  loss_rpn_cls: 0.035  loss_rpn_loc: 0.068  time: 0.5590  data_time: 0.0064  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:25:20 d2.utils.events]: [0m eta: 0:03:21  iter: 1639  total_loss: 0.722  loss_cls: 0.227  loss_box_reg: 0.309  loss_rpn_cls: 0.055  loss_rpn_loc: 0.070  time: 0.5589  data_time: 0.0066  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:25:34 d2.utils.events]: [0m eta: 0:03:10  iter: 1659  total_loss: 0.857  loss_cls: 0.300  loss_box_reg: 0.395  loss_rpn_cls: 0.043  loss_rpn_loc: 0.096  time: 0.5588  data_time: 0.0079  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:25:45 d2.utils.events]: [0m eta: 0:02:59  iter: 1679  total_loss: 0.618  loss_cls: 0.215  loss_box_reg: 0.310  loss_rpn_cls: 0.041  loss_rpn_loc: 0.063  time: 0.5587  data_time: 0.0421  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:25:57 d2.utils.events]: [0m eta: 0:02:48  iter: 1699  total_loss: 0.748  loss_cls: 0.258  loss_box_reg: 0.343  loss_rpn_cls: 0.044  loss_rpn_loc: 0.064  time: 0.5589  data_time: 0.0078  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:26:08 d2.utils.events]: [0m eta: 0:02:37  iter: 1719  total_loss: 0.748  loss_cls: 0.264  loss_box_reg: 0.364  loss_rpn_cls: 0.049  loss_rpn_loc: 0.072  time: 0.5587  data_time: 0.0067  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:26:19 d2.utils.events]: [0m eta: 0:02:26  iter: 1739  total_loss: 0.734  loss_cls: 0.268  loss_box_reg: 0.369  loss_rpn_cls: 0.045  loss_rpn_loc: 0.076  time: 0.5587  data_time: 0.0073  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:26:36 d2.utils.events]: [0m eta: 0:02:14  iter: 1759  total_loss: 0.758  loss_cls: 0.273  loss_box_reg: 0.375  loss_rpn_cls: 0.050  loss_rpn_loc: 0.061  time: 0.5584  data_time: 0.0079  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:26:47 d2.utils.events]: [0m eta: 0:02:03  iter: 1779  total_loss: 0.844  loss_cls: 0.282  loss_box_reg: 0.381  loss_rpn_cls: 0.040  loss_rpn_loc: 0.077  time: 0.5581  data_time: 0.0074  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:26:58 d2.utils.events]: [0m eta: 0:01:52  iter: 1799  total_loss: 0.817  loss_cls: 0.289  loss_box_reg: 0.418  loss_rpn_cls: 0.044  loss_rpn_loc: 0.075  time: 0.5581  data_time: 0.0144  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:27:08 d2.utils.events]: [0m eta: 0:01:41  iter: 1819  total_loss: 0.796  loss_cls: 0.232  loss_box_reg: 0.365  loss_rpn_cls: 0.059  loss_rpn_loc: 0.095  time: 0.5577  data_time: 0.0070  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:27:20 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.782  loss_cls: 0.270  loss_box_reg: 0.382  loss_rpn_cls: 0.045  loss_rpn_loc: 0.069  time: 0.5577  data_time: 0.0090  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:27:30 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.541  loss_cls: 0.168  loss_box_reg: 0.234  loss_rpn_cls: 0.046  loss_rpn_loc: 0.078  time: 0.5574  data_time: 0.0073  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:27:42 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.718  loss_cls: 0.267  loss_box_reg: 0.376  loss_rpn_cls: 0.047  loss_rpn_loc: 0.082  time: 0.5573  data_time: 0.0090  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:27:53 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.794  loss_cls: 0.292  loss_box_reg: 0.375  loss_rpn_cls: 0.037  loss_rpn_loc: 0.088  time: 0.5571  data_time: 0.0072  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:28:04 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.737  loss_cls: 0.262  loss_box_reg: 0.351  loss_rpn_cls: 0.033  loss_rpn_loc: 0.070  time: 0.5571  data_time: 0.0071  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:28:15 d2.utils.events]: [0m eta: 0:00:34  iter: 1939  total_loss: 0.835  loss_cls: 0.272  loss_box_reg: 0.420  loss_rpn_cls: 0.046  loss_rpn_loc: 0.060  time: 0.5571  data_time: 0.0072  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:28:26 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.725  loss_cls: 0.258  loss_box_reg: 0.366  loss_rpn_cls: 0.050  loss_rpn_loc: 0.060  time: 0.5570  data_time: 0.0074  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:28:37 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.767  loss_cls: 0.256  loss_box_reg: 0.379  loss_rpn_cls: 0.045  loss_rpn_loc: 0.088  time: 0.5570  data_time: 0.0069  lr: 0.010000  max_mem: 3068M
[5m[31mWARNING[0m [32m[04/15 06:28:58 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:28:58 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 06:28:58 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 06:28:59 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.662  loss_cls: 0.238  loss_box_reg: 0.346  loss_rpn_cls: 0.036  loss_rpn_loc: 0.062  time: 0.5571  data_time: 0.0086  lr: 0.010000  max_mem: 3068M
[32m[04/15 06:29:03 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:33 (0.5574 s / it)
[32m[04/15 06:29:03 d2.engine.hooks]: [0mTotal training time: 0:19:06 (0:00:33 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 06:29:18 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:29:18 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 06:29:18 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 06:29:20 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1235 s / img. ETA=0:02:37
[32m[04/15 06:29:25 d2.evaluation.evaluator]: [0mInference done 51/1257. 0.1218 s / img. ETA=0:02:31
[32m[04/15 06:29:30 d2.evaluation.evaluator]: [0mInference done 91/1257. 0.1225 s / img. ETA=0:02:26
[32m[04/15 06:29:35 d2.evaluation.evaluator]: [0mInference done 130/1257. 0.1237 s / img. ETA=0:02:23
[32m[04/15 06:29:40 d2.evaluation.evaluator]: [0mInference done 170/1257. 0.1237 s / img. ETA=0:02:18
[32m[04/15 06:29:45 d2.evaluation.evaluator]: [0mInference done 210/1257. 0.1234 s / img. ETA=0:02:12
[32m[04/15 06:29:50 d2.evaluation.evaluator]: [0mInference done 250/1257. 0.1233 s / img. ETA=0:02:07
[32m[04/15 06:29:55 d2.evaluation.evaluator]: [0mInference done 287/1257. 0.1234 s / img. ETA=0:02:04
[32m[04/15 06:30:01 d2.evaluation.evaluator]: [0mInference done 322/1257. 0.1234 s / img. ETA=0:02:01
[32m[04/15 06:30:06 d2.evaluation.evaluator]: [0mInference done 363/1257. 0.1231 s / img. ETA=0:01:55
[32m[04/15 06:30:11 d2.evaluation.evaluator]: [0mInference done 403/1257. 0.1229 s / img. ETA=0:01:50
[32m[04/15 06:30:16 d2.evaluation.evaluator]: [0mInference done 443/1257. 0.1229 s / img. ETA=0:01:44
[32m[04/15 06:30:21 d2.evaluation.evaluator]: [0mInference done 483/1257. 0.1229 s / img. ETA=0:01:39
[32m[04/15 06:30:26 d2.evaluation.evaluator]: [0mInference done 523/1257. 0.1229 s / img. ETA=0:01:34
[32m[04/15 06:30:31 d2.evaluation.evaluator]: [0mInference done 562/1257. 0.1230 s / img. ETA=0:01:29
[32m[04/15 06:30:36 d2.evaluation.evaluator]: [0mInference done 602/1257. 0.1231 s / img. ETA=0:01:24
[32m[04/15 06:30:41 d2.evaluation.evaluator]: [0mInference done 641/1257. 0.1232 s / img. ETA=0:01:19
[32m[04/15 06:30:46 d2.evaluation.evaluator]: [0mInference done 680/1257. 0.1235 s / img. ETA=0:01:14
[32m[04/15 06:30:51 d2.evaluation.evaluator]: [0mInference done 720/1257. 0.1235 s / img. ETA=0:01:08
[32m[04/15 06:30:56 d2.evaluation.evaluator]: [0mInference done 759/1257. 0.1236 s / img. ETA=0:01:04
[32m[04/15 06:31:01 d2.evaluation.evaluator]: [0mInference done 798/1257. 0.1238 s / img. ETA=0:00:59
[32m[04/15 06:31:06 d2.evaluation.evaluator]: [0mInference done 837/1257. 0.1239 s / img. ETA=0:00:54
[32m[04/15 06:31:12 d2.evaluation.evaluator]: [0mInference done 877/1257. 0.1238 s / img. ETA=0:00:48
[32m[04/15 06:31:17 d2.evaluation.evaluator]: [0mInference done 915/1257. 0.1240 s / img. ETA=0:00:44
[32m[04/15 06:31:22 d2.evaluation.evaluator]: [0mInference done 954/1257. 0.1241 s / img. ETA=0:00:39
[32m[04/15 06:31:27 d2.evaluation.evaluator]: [0mInference done 994/1257. 0.1241 s / img. ETA=0:00:33
[32m[04/15 06:31:32 d2.evaluation.evaluator]: [0mInference done 1033/1257. 0.1242 s / img. ETA=0:00:28
[32m[04/15 06:31:37 d2.evaluation.evaluator]: [0mInference done 1072/1257. 0.1242 s / img. ETA=0:00:23
[32m[04/15 06:31:42 d2.evaluation.evaluator]: [0mInference done 1111/1257. 0.1243 s / img. ETA=0:00:18
[32m[04/15 06:31:47 d2.evaluation.evaluator]: [0mInference done 1150/1257. 0.1243 s / img. ETA=0:00:13
[32m[04/15 06:31:52 d2.evaluation.evaluator]: [0mInference done 1190/1257. 0.1243 s / img. ETA=0:00:08
[32m[04/15 06:31:57 d2.evaluation.evaluator]: [0mInference done 1229/1257. 0.1244 s / img. ETA=0:00:03
[32m[04/15 06:32:01 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:41.438659 (0.128945 s / img per device, on 1 devices)
[32m[04/15 06:32:01 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:35 (0.124442 s / img per device, on 1 devices)
[32m[04/15 06:32:01 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 06:32:01 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 06:32:01 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.03s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.67s).
Accumulating evaluation results...
DONE (t=0.83s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.304
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.120
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.080
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.181
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.385
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.085
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.192
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.193
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.116
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.429
[32m[04/15 06:32:08 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 14.428 | 30.358 | 11.960 | 7.989 | 18.082 | 38.530 |
[32m[04/15 06:32:08 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 19.536 | bicycle       | 6.040 | car            | 32.137 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  3  *  2000  iterations ============
4 channel input
[32m[04/15 06:32:09 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[5m[31mWARNING[0m [32m[04/15 06:32:10 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:32:10 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 06:32:11 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 06:32:11 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 06:32:11 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 06:32:11 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 06:32:23 d2.utils.events]: [0m eta: 0:18:23  iter: 19  total_loss: 0.804  loss_cls: 0.236  loss_box_reg: 0.381  loss_rpn_cls: 0.045  loss_rpn_loc: 0.076  time: 0.5561  data_time: 0.0304  lr: 0.000200  max_mem: 3070M
[32m[04/15 06:32:33 d2.utils.events]: [0m eta: 0:18:02  iter: 39  total_loss: 0.738  loss_cls: 0.253  loss_box_reg: 0.357  loss_rpn_cls: 0.043  loss_rpn_loc: 0.066  time: 0.5436  data_time: 0.0065  lr: 0.000400  max_mem: 3070M
[32m[04/15 06:32:45 d2.utils.events]: [0m eta: 0:17:48  iter: 59  total_loss: 0.704  loss_cls: 0.225  loss_box_reg: 0.343  loss_rpn_cls: 0.041  loss_rpn_loc: 0.071  time: 0.5451  data_time: 0.0093  lr: 0.000599  max_mem: 3070M
[32m[04/15 06:32:56 d2.utils.events]: [0m eta: 0:17:43  iter: 79  total_loss: 0.788  loss_cls: 0.250  loss_box_reg: 0.403  loss_rpn_cls: 0.046  loss_rpn_loc: 0.065  time: 0.5497  data_time: 0.0071  lr: 0.000799  max_mem: 3070M
[32m[04/15 06:33:07 d2.utils.events]: [0m eta: 0:17:32  iter: 99  total_loss: 0.635  loss_cls: 0.213  loss_box_reg: 0.328  loss_rpn_cls: 0.033  loss_rpn_loc: 0.060  time: 0.5486  data_time: 0.0068  lr: 0.000999  max_mem: 3070M
[32m[04/15 06:33:18 d2.utils.events]: [0m eta: 0:17:18  iter: 119  total_loss: 0.833  loss_cls: 0.258  loss_box_reg: 0.400  loss_rpn_cls: 0.031  loss_rpn_loc: 0.095  time: 0.5476  data_time: 0.0064  lr: 0.001199  max_mem: 3070M
[32m[04/15 06:33:29 d2.utils.events]: [0m eta: 0:17:06  iter: 139  total_loss: 0.742  loss_cls: 0.229  loss_box_reg: 0.378  loss_rpn_cls: 0.040  loss_rpn_loc: 0.067  time: 0.5480  data_time: 0.0072  lr: 0.001399  max_mem: 3070M
[32m[04/15 06:33:40 d2.utils.events]: [0m eta: 0:16:52  iter: 159  total_loss: 0.693  loss_cls: 0.233  loss_box_reg: 0.379  loss_rpn_cls: 0.030  loss_rpn_loc: 0.068  time: 0.5457  data_time: 0.0047  lr: 0.001598  max_mem: 3070M
[32m[04/15 06:33:51 d2.utils.events]: [0m eta: 0:16:42  iter: 179  total_loss: 0.676  loss_cls: 0.230  loss_box_reg: 0.348  loss_rpn_cls: 0.037  loss_rpn_loc: 0.047  time: 0.5468  data_time: 0.0064  lr: 0.001798  max_mem: 3070M
[32m[04/15 06:34:02 d2.utils.events]: [0m eta: 0:16:28  iter: 199  total_loss: 0.765  loss_cls: 0.246  loss_box_reg: 0.424  loss_rpn_cls: 0.028  loss_rpn_loc: 0.060  time: 0.5458  data_time: 0.0068  lr: 0.001998  max_mem: 3070M
[32m[04/15 06:34:13 d2.utils.events]: [0m eta: 0:16:22  iter: 219  total_loss: 0.672  loss_cls: 0.225  loss_box_reg: 0.344  loss_rpn_cls: 0.041  loss_rpn_loc: 0.057  time: 0.5480  data_time: 0.0132  lr: 0.002198  max_mem: 3070M
[32m[04/15 06:34:24 d2.utils.events]: [0m eta: 0:16:12  iter: 239  total_loss: 0.676  loss_cls: 0.215  loss_box_reg: 0.368  loss_rpn_cls: 0.028  loss_rpn_loc: 0.052  time: 0.5495  data_time: 0.0071  lr: 0.002398  max_mem: 3070M
[32m[04/15 06:34:36 d2.utils.events]: [0m eta: 0:16:01  iter: 259  total_loss: 0.727  loss_cls: 0.269  loss_box_reg: 0.333  loss_rpn_cls: 0.037  loss_rpn_loc: 0.081  time: 0.5497  data_time: 0.0071  lr: 0.002597  max_mem: 3070M
[32m[04/15 06:34:47 d2.utils.events]: [0m eta: 0:15:51  iter: 279  total_loss: 0.542  loss_cls: 0.197  loss_box_reg: 0.248  loss_rpn_cls: 0.030  loss_rpn_loc: 0.049  time: 0.5507  data_time: 0.0072  lr: 0.002797  max_mem: 3070M
[32m[04/15 06:34:58 d2.utils.events]: [0m eta: 0:15:40  iter: 299  total_loss: 0.732  loss_cls: 0.235  loss_box_reg: 0.355  loss_rpn_cls: 0.027  loss_rpn_loc: 0.069  time: 0.5510  data_time: 0.0094  lr: 0.002997  max_mem: 3070M
[32m[04/15 06:35:10 d2.utils.events]: [0m eta: 0:15:31  iter: 319  total_loss: 0.653  loss_cls: 0.226  loss_box_reg: 0.349  loss_rpn_cls: 0.028  loss_rpn_loc: 0.053  time: 0.5525  data_time: 0.0074  lr: 0.003197  max_mem: 3070M
[32m[04/15 06:35:21 d2.utils.events]: [0m eta: 0:15:23  iter: 339  total_loss: 0.787  loss_cls: 0.251  loss_box_reg: 0.430  loss_rpn_cls: 0.025  loss_rpn_loc: 0.059  time: 0.5535  data_time: 0.0096  lr: 0.003397  max_mem: 3070M
[32m[04/15 06:35:32 d2.utils.events]: [0m eta: 0:15:09  iter: 359  total_loss: 0.806  loss_cls: 0.280  loss_box_reg: 0.378  loss_rpn_cls: 0.029  loss_rpn_loc: 0.084  time: 0.5528  data_time: 0.0069  lr: 0.003596  max_mem: 3070M
[32m[04/15 06:35:44 d2.utils.events]: [0m eta: 0:14:59  iter: 379  total_loss: 0.760  loss_cls: 0.261  loss_box_reg: 0.365  loss_rpn_cls: 0.037  loss_rpn_loc: 0.074  time: 0.5536  data_time: 0.0102  lr: 0.003796  max_mem: 3070M
[32m[04/15 06:35:55 d2.utils.events]: [0m eta: 0:14:49  iter: 399  total_loss: 0.538  loss_cls: 0.169  loss_box_reg: 0.265  loss_rpn_cls: 0.021  loss_rpn_loc: 0.040  time: 0.5537  data_time: 0.0097  lr: 0.003996  max_mem: 3070M
[32m[04/15 06:36:06 d2.utils.events]: [0m eta: 0:14:38  iter: 419  total_loss: 0.681  loss_cls: 0.220  loss_box_reg: 0.371  loss_rpn_cls: 0.025  loss_rpn_loc: 0.070  time: 0.5538  data_time: 0.0064  lr: 0.004196  max_mem: 3070M
[32m[04/15 06:36:17 d2.utils.events]: [0m eta: 0:14:27  iter: 439  total_loss: 0.688  loss_cls: 0.237  loss_box_reg: 0.368  loss_rpn_cls: 0.033  loss_rpn_loc: 0.072  time: 0.5539  data_time: 0.0074  lr: 0.004396  max_mem: 3070M
[32m[04/15 06:36:30 d2.utils.events]: [0m eta: 0:14:17  iter: 459  total_loss: 0.714  loss_cls: 0.249  loss_box_reg: 0.357  loss_rpn_cls: 0.027  loss_rpn_loc: 0.061  time: 0.5570  data_time: 0.1293  lr: 0.004595  max_mem: 3070M
[32m[04/15 06:36:41 d2.utils.events]: [0m eta: 0:14:08  iter: 479  total_loss: 0.726  loss_cls: 0.260  loss_box_reg: 0.369  loss_rpn_cls: 0.025  loss_rpn_loc: 0.061  time: 0.5576  data_time: 0.0101  lr: 0.004795  max_mem: 3070M
[32m[04/15 06:36:53 d2.utils.events]: [0m eta: 0:13:57  iter: 499  total_loss: 0.619  loss_cls: 0.219  loss_box_reg: 0.303  loss_rpn_cls: 0.035  loss_rpn_loc: 0.065  time: 0.5577  data_time: 0.0077  lr: 0.004995  max_mem: 3070M
[32m[04/15 06:37:04 d2.utils.events]: [0m eta: 0:13:46  iter: 519  total_loss: 0.744  loss_cls: 0.242  loss_box_reg: 0.390  loss_rpn_cls: 0.030  loss_rpn_loc: 0.073  time: 0.5579  data_time: 0.0077  lr: 0.005195  max_mem: 3070M
[32m[04/15 06:37:15 d2.utils.events]: [0m eta: 0:13:35  iter: 539  total_loss: 0.614  loss_cls: 0.201  loss_box_reg: 0.347  loss_rpn_cls: 0.028  loss_rpn_loc: 0.058  time: 0.5576  data_time: 0.0088  lr: 0.005395  max_mem: 3070M
[32m[04/15 06:37:26 d2.utils.events]: [0m eta: 0:13:23  iter: 559  total_loss: 0.699  loss_cls: 0.217  loss_box_reg: 0.357  loss_rpn_cls: 0.040  loss_rpn_loc: 0.069  time: 0.5571  data_time: 0.0083  lr: 0.005594  max_mem: 3070M
[32m[04/15 06:37:38 d2.utils.events]: [0m eta: 0:13:13  iter: 579  total_loss: 0.697  loss_cls: 0.217  loss_box_reg: 0.355  loss_rpn_cls: 0.031  loss_rpn_loc: 0.059  time: 0.5574  data_time: 0.0080  lr: 0.005794  max_mem: 3070M
[32m[04/15 06:37:49 d2.utils.events]: [0m eta: 0:13:02  iter: 599  total_loss: 0.736  loss_cls: 0.237  loss_box_reg: 0.392  loss_rpn_cls: 0.030  loss_rpn_loc: 0.075  time: 0.5576  data_time: 0.0081  lr: 0.005994  max_mem: 3070M
[32m[04/15 06:38:00 d2.utils.events]: [0m eta: 0:12:51  iter: 619  total_loss: 0.545  loss_cls: 0.193  loss_box_reg: 0.272  loss_rpn_cls: 0.033  loss_rpn_loc: 0.049  time: 0.5575  data_time: 0.0079  lr: 0.006194  max_mem: 3070M
[32m[04/15 06:38:11 d2.utils.events]: [0m eta: 0:12:39  iter: 639  total_loss: 0.603  loss_cls: 0.196  loss_box_reg: 0.307  loss_rpn_cls: 0.034  loss_rpn_loc: 0.048  time: 0.5573  data_time: 0.0069  lr: 0.006394  max_mem: 3070M
[32m[04/15 06:38:23 d2.utils.events]: [0m eta: 0:12:28  iter: 659  total_loss: 0.723  loss_cls: 0.245  loss_box_reg: 0.374  loss_rpn_cls: 0.033  loss_rpn_loc: 0.059  time: 0.5576  data_time: 0.0071  lr: 0.006593  max_mem: 3070M
[32m[04/15 06:38:34 d2.utils.events]: [0m eta: 0:12:17  iter: 679  total_loss: 0.712  loss_cls: 0.239  loss_box_reg: 0.333  loss_rpn_cls: 0.035  loss_rpn_loc: 0.069  time: 0.5577  data_time: 0.0072  lr: 0.006793  max_mem: 3070M
[32m[04/15 06:38:45 d2.utils.events]: [0m eta: 0:12:06  iter: 699  total_loss: 0.781  loss_cls: 0.259  loss_box_reg: 0.435  loss_rpn_cls: 0.028  loss_rpn_loc: 0.070  time: 0.5576  data_time: 0.0073  lr: 0.006993  max_mem: 3070M
[32m[04/15 06:38:56 d2.utils.events]: [0m eta: 0:11:54  iter: 719  total_loss: 0.801  loss_cls: 0.270  loss_box_reg: 0.389  loss_rpn_cls: 0.040  loss_rpn_loc: 0.062  time: 0.5570  data_time: 0.0068  lr: 0.007193  max_mem: 3070M
[32m[04/15 06:39:08 d2.utils.events]: [0m eta: 0:11:44  iter: 739  total_loss: 0.727  loss_cls: 0.225  loss_box_reg: 0.373  loss_rpn_cls: 0.035  loss_rpn_loc: 0.054  time: 0.5575  data_time: 0.0082  lr: 0.007393  max_mem: 3070M
[32m[04/15 06:39:19 d2.utils.events]: [0m eta: 0:11:33  iter: 759  total_loss: 0.794  loss_cls: 0.231  loss_box_reg: 0.315  loss_rpn_cls: 0.039  loss_rpn_loc: 0.066  time: 0.5577  data_time: 0.0080  lr: 0.007592  max_mem: 3070M
[32m[04/15 06:39:30 d2.utils.events]: [0m eta: 0:11:22  iter: 779  total_loss: 0.765  loss_cls: 0.260  loss_box_reg: 0.383  loss_rpn_cls: 0.033  loss_rpn_loc: 0.074  time: 0.5577  data_time: 0.0067  lr: 0.007792  max_mem: 3070M
[32m[04/15 06:39:41 d2.utils.events]: [0m eta: 0:11:10  iter: 799  total_loss: 0.785  loss_cls: 0.268  loss_box_reg: 0.382  loss_rpn_cls: 0.040  loss_rpn_loc: 0.083  time: 0.5575  data_time: 0.0089  lr: 0.007992  max_mem: 3070M
[32m[04/15 06:39:53 d2.utils.events]: [0m eta: 0:10:59  iter: 819  total_loss: 0.791  loss_cls: 0.239  loss_box_reg: 0.396  loss_rpn_cls: 0.033  loss_rpn_loc: 0.095  time: 0.5575  data_time: 0.0078  lr: 0.008192  max_mem: 3070M
[32m[04/15 06:40:04 d2.utils.events]: [0m eta: 0:10:48  iter: 839  total_loss: 0.785  loss_cls: 0.280  loss_box_reg: 0.391  loss_rpn_cls: 0.031  loss_rpn_loc: 0.061  time: 0.5574  data_time: 0.0083  lr: 0.008392  max_mem: 3070M
[32m[04/15 06:40:15 d2.utils.events]: [0m eta: 0:10:36  iter: 859  total_loss: 0.779  loss_cls: 0.276  loss_box_reg: 0.376  loss_rpn_cls: 0.040  loss_rpn_loc: 0.078  time: 0.5570  data_time: 0.0067  lr: 0.008591  max_mem: 3070M
[32m[04/15 06:40:26 d2.utils.events]: [0m eta: 0:10:26  iter: 879  total_loss: 0.759  loss_cls: 0.277  loss_box_reg: 0.389  loss_rpn_cls: 0.042  loss_rpn_loc: 0.088  time: 0.5570  data_time: 0.0071  lr: 0.008791  max_mem: 3070M
[32m[04/15 06:40:37 d2.utils.events]: [0m eta: 0:10:15  iter: 899  total_loss: 0.791  loss_cls: 0.233  loss_box_reg: 0.362  loss_rpn_cls: 0.037  loss_rpn_loc: 0.076  time: 0.5572  data_time: 0.0081  lr: 0.008991  max_mem: 3070M
[32m[04/15 06:40:49 d2.utils.events]: [0m eta: 0:10:04  iter: 919  total_loss: 0.724  loss_cls: 0.227  loss_box_reg: 0.359  loss_rpn_cls: 0.035  loss_rpn_loc: 0.087  time: 0.5575  data_time: 0.0066  lr: 0.009191  max_mem: 3070M
[32m[04/15 06:41:00 d2.utils.events]: [0m eta: 0:09:53  iter: 939  total_loss: 0.629  loss_cls: 0.215  loss_box_reg: 0.341  loss_rpn_cls: 0.044  loss_rpn_loc: 0.065  time: 0.5574  data_time: 0.0068  lr: 0.009391  max_mem: 3070M
[32m[04/15 06:41:12 d2.utils.events]: [0m eta: 0:09:43  iter: 959  total_loss: 0.777  loss_cls: 0.244  loss_box_reg: 0.368  loss_rpn_cls: 0.038  loss_rpn_loc: 0.080  time: 0.5578  data_time: 0.0069  lr: 0.009590  max_mem: 3070M
[32m[04/15 06:41:23 d2.utils.events]: [0m eta: 0:09:31  iter: 979  total_loss: 0.677  loss_cls: 0.242  loss_box_reg: 0.331  loss_rpn_cls: 0.044  loss_rpn_loc: 0.088  time: 0.5577  data_time: 0.0065  lr: 0.009790  max_mem: 3070M
[32m[04/15 06:41:34 d2.utils.events]: [0m eta: 0:09:20  iter: 999  total_loss: 0.767  loss_cls: 0.236  loss_box_reg: 0.377  loss_rpn_cls: 0.034  loss_rpn_loc: 0.060  time: 0.5576  data_time: 0.0087  lr: 0.009990  max_mem: 3070M
[32m[04/15 06:41:45 d2.utils.events]: [0m eta: 0:09:09  iter: 1019  total_loss: 0.706  loss_cls: 0.218  loss_box_reg: 0.311  loss_rpn_cls: 0.047  loss_rpn_loc: 0.081  time: 0.5578  data_time: 0.0062  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:41:57 d2.utils.events]: [0m eta: 0:08:59  iter: 1039  total_loss: 0.739  loss_cls: 0.226  loss_box_reg: 0.367  loss_rpn_cls: 0.047  loss_rpn_loc: 0.082  time: 0.5581  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:42:08 d2.utils.events]: [0m eta: 0:08:47  iter: 1059  total_loss: 0.789  loss_cls: 0.255  loss_box_reg: 0.386  loss_rpn_cls: 0.043  loss_rpn_loc: 0.076  time: 0.5575  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:42:19 d2.utils.events]: [0m eta: 0:08:36  iter: 1079  total_loss: 0.651  loss_cls: 0.176  loss_box_reg: 0.272  loss_rpn_cls: 0.032  loss_rpn_loc: 0.094  time: 0.5576  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:42:30 d2.utils.events]: [0m eta: 0:08:24  iter: 1099  total_loss: 0.650  loss_cls: 0.217  loss_box_reg: 0.307  loss_rpn_cls: 0.026  loss_rpn_loc: 0.058  time: 0.5575  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:42:41 d2.utils.events]: [0m eta: 0:08:13  iter: 1119  total_loss: 0.757  loss_cls: 0.251  loss_box_reg: 0.419  loss_rpn_cls: 0.036  loss_rpn_loc: 0.085  time: 0.5575  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:42:52 d2.utils.events]: [0m eta: 0:08:02  iter: 1139  total_loss: 0.818  loss_cls: 0.268  loss_box_reg: 0.427  loss_rpn_cls: 0.034  loss_rpn_loc: 0.088  time: 0.5571  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:43:03 d2.utils.events]: [0m eta: 0:07:51  iter: 1159  total_loss: 0.684  loss_cls: 0.271  loss_box_reg: 0.327  loss_rpn_cls: 0.028  loss_rpn_loc: 0.051  time: 0.5571  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:43:14 d2.utils.events]: [0m eta: 0:07:40  iter: 1179  total_loss: 0.685  loss_cls: 0.267  loss_box_reg: 0.318  loss_rpn_cls: 0.035  loss_rpn_loc: 0.072  time: 0.5570  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:43:26 d2.utils.events]: [0m eta: 0:07:29  iter: 1199  total_loss: 0.618  loss_cls: 0.227  loss_box_reg: 0.303  loss_rpn_cls: 0.042  loss_rpn_loc: 0.052  time: 0.5571  data_time: 0.0111  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:43:37 d2.utils.events]: [0m eta: 0:07:17  iter: 1219  total_loss: 0.789  loss_cls: 0.269  loss_box_reg: 0.396  loss_rpn_cls: 0.058  loss_rpn_loc: 0.069  time: 0.5570  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:43:48 d2.utils.events]: [0m eta: 0:07:06  iter: 1239  total_loss: 0.787  loss_cls: 0.246  loss_box_reg: 0.391  loss_rpn_cls: 0.037  loss_rpn_loc: 0.070  time: 0.5569  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:43:58 d2.utils.events]: [0m eta: 0:06:54  iter: 1259  total_loss: 0.502  loss_cls: 0.168  loss_box_reg: 0.231  loss_rpn_cls: 0.031  loss_rpn_loc: 0.063  time: 0.5563  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:44:10 d2.utils.events]: [0m eta: 0:06:43  iter: 1279  total_loss: 0.594  loss_cls: 0.187  loss_box_reg: 0.291  loss_rpn_cls: 0.039  loss_rpn_loc: 0.059  time: 0.5566  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:44:21 d2.utils.events]: [0m eta: 0:06:32  iter: 1299  total_loss: 0.791  loss_cls: 0.277  loss_box_reg: 0.395  loss_rpn_cls: 0.039  loss_rpn_loc: 0.067  time: 0.5567  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:44:33 d2.utils.events]: [0m eta: 0:06:21  iter: 1319  total_loss: 0.739  loss_cls: 0.226  loss_box_reg: 0.316  loss_rpn_cls: 0.043  loss_rpn_loc: 0.071  time: 0.5570  data_time: 0.0090  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:44:44 d2.utils.events]: [0m eta: 0:06:09  iter: 1339  total_loss: 0.656  loss_cls: 0.196  loss_box_reg: 0.333  loss_rpn_cls: 0.030  loss_rpn_loc: 0.063  time: 0.5568  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:44:55 d2.utils.events]: [0m eta: 0:05:58  iter: 1359  total_loss: 0.865  loss_cls: 0.282  loss_box_reg: 0.450  loss_rpn_cls: 0.041  loss_rpn_loc: 0.075  time: 0.5569  data_time: 0.0099  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:45:07 d2.utils.events]: [0m eta: 0:05:47  iter: 1379  total_loss: 0.604  loss_cls: 0.204  loss_box_reg: 0.281  loss_rpn_cls: 0.037  loss_rpn_loc: 0.060  time: 0.5570  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:45:18 d2.utils.events]: [0m eta: 0:05:36  iter: 1399  total_loss: 0.619  loss_cls: 0.226  loss_box_reg: 0.283  loss_rpn_cls: 0.028  loss_rpn_loc: 0.075  time: 0.5571  data_time: 0.0103  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:45:29 d2.utils.events]: [0m eta: 0:05:25  iter: 1419  total_loss: 0.688  loss_cls: 0.237  loss_box_reg: 0.353  loss_rpn_cls: 0.039  loss_rpn_loc: 0.070  time: 0.5572  data_time: 0.0111  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:45:40 d2.utils.events]: [0m eta: 0:05:13  iter: 1439  total_loss: 0.913  loss_cls: 0.258  loss_box_reg: 0.427  loss_rpn_cls: 0.051  loss_rpn_loc: 0.105  time: 0.5570  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:45:52 d2.utils.events]: [0m eta: 0:05:02  iter: 1459  total_loss: 0.858  loss_cls: 0.275  loss_box_reg: 0.415  loss_rpn_cls: 0.051  loss_rpn_loc: 0.090  time: 0.5571  data_time: 0.0090  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:46:03 d2.utils.events]: [0m eta: 0:04:50  iter: 1479  total_loss: 0.707  loss_cls: 0.218  loss_box_reg: 0.353  loss_rpn_cls: 0.046  loss_rpn_loc: 0.069  time: 0.5570  data_time: 0.0099  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:46:14 d2.utils.events]: [0m eta: 0:04:39  iter: 1499  total_loss: 0.866  loss_cls: 0.270  loss_box_reg: 0.456  loss_rpn_cls: 0.036  loss_rpn_loc: 0.097  time: 0.5571  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:46:26 d2.utils.events]: [0m eta: 0:04:28  iter: 1519  total_loss: 0.706  loss_cls: 0.237  loss_box_reg: 0.322  loss_rpn_cls: 0.046  loss_rpn_loc: 0.071  time: 0.5573  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:46:37 d2.utils.events]: [0m eta: 0:04:17  iter: 1539  total_loss: 0.579  loss_cls: 0.201  loss_box_reg: 0.322  loss_rpn_cls: 0.028  loss_rpn_loc: 0.051  time: 0.5573  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:46:48 d2.utils.events]: [0m eta: 0:04:06  iter: 1559  total_loss: 0.711  loss_cls: 0.216  loss_box_reg: 0.320  loss_rpn_cls: 0.035  loss_rpn_loc: 0.071  time: 0.5575  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:47:00 d2.utils.events]: [0m eta: 0:03:55  iter: 1579  total_loss: 0.829  loss_cls: 0.258  loss_box_reg: 0.471  loss_rpn_cls: 0.036  loss_rpn_loc: 0.072  time: 0.5577  data_time: 0.0125  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:47:11 d2.utils.events]: [0m eta: 0:03:44  iter: 1599  total_loss: 0.668  loss_cls: 0.231  loss_box_reg: 0.357  loss_rpn_cls: 0.031  loss_rpn_loc: 0.072  time: 0.5576  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:47:22 d2.utils.events]: [0m eta: 0:03:32  iter: 1619  total_loss: 0.786  loss_cls: 0.266  loss_box_reg: 0.366  loss_rpn_cls: 0.043  loss_rpn_loc: 0.061  time: 0.5574  data_time: 0.0092  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:47:33 d2.utils.events]: [0m eta: 0:03:21  iter: 1639  total_loss: 0.821  loss_cls: 0.248  loss_box_reg: 0.421  loss_rpn_cls: 0.038  loss_rpn_loc: 0.062  time: 0.5573  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:47:44 d2.utils.events]: [0m eta: 0:03:09  iter: 1659  total_loss: 0.802  loss_cls: 0.280  loss_box_reg: 0.368  loss_rpn_cls: 0.037  loss_rpn_loc: 0.086  time: 0.5569  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:47:55 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.868  loss_cls: 0.290  loss_box_reg: 0.458  loss_rpn_cls: 0.034  loss_rpn_loc: 0.077  time: 0.5569  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:48:06 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.716  loss_cls: 0.255  loss_box_reg: 0.333  loss_rpn_cls: 0.043  loss_rpn_loc: 0.076  time: 0.5571  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:48:17 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.719  loss_cls: 0.223  loss_box_reg: 0.373  loss_rpn_cls: 0.033  loss_rpn_loc: 0.065  time: 0.5569  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:48:36 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.759  loss_cls: 0.254  loss_box_reg: 0.387  loss_rpn_cls: 0.044  loss_rpn_loc: 0.071  time: 0.5567  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:48:47 d2.utils.events]: [0m eta: 0:02:14  iter: 1759  total_loss: 0.732  loss_cls: 0.232  loss_box_reg: 0.360  loss_rpn_cls: 0.038  loss_rpn_loc: 0.080  time: 0.5568  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:48:58 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.674  loss_cls: 0.227  loss_box_reg: 0.317  loss_rpn_cls: 0.054  loss_rpn_loc: 0.067  time: 0.5567  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:49:13 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.704  loss_cls: 0.227  loss_box_reg: 0.358  loss_rpn_cls: 0.035  loss_rpn_loc: 0.051  time: 0.5564  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:49:24 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.749  loss_cls: 0.244  loss_box_reg: 0.374  loss_rpn_cls: 0.031  loss_rpn_loc: 0.070  time: 0.5563  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:49:36 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.654  loss_cls: 0.222  loss_box_reg: 0.344  loss_rpn_cls: 0.025  loss_rpn_loc: 0.055  time: 0.5565  data_time: 0.0099  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:49:47 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.692  loss_cls: 0.245  loss_box_reg: 0.342  loss_rpn_cls: 0.046  loss_rpn_loc: 0.073  time: 0.5565  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:49:58 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.753  loss_cls: 0.249  loss_box_reg: 0.386  loss_rpn_cls: 0.035  loss_rpn_loc: 0.071  time: 0.5564  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:50:08 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.854  loss_cls: 0.260  loss_box_reg: 0.423  loss_rpn_cls: 0.046  loss_rpn_loc: 0.095  time: 0.5559  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:50:20 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.727  loss_cls: 0.250  loss_box_reg: 0.366  loss_rpn_cls: 0.036  loss_rpn_loc: 0.069  time: 0.5560  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:50:31 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.737  loss_cls: 0.227  loss_box_reg: 0.363  loss_rpn_cls: 0.028  loss_rpn_loc: 0.072  time: 0.5559  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:50:42 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.541  loss_cls: 0.176  loss_box_reg: 0.291  loss_rpn_cls: 0.031  loss_rpn_loc: 0.044  time: 0.5559  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:50:53 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.814  loss_cls: 0.301  loss_box_reg: 0.432  loss_rpn_cls: 0.031  loss_rpn_loc: 0.085  time: 0.5559  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 06:51:28 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:51:28 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 06:51:29 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 06:51:29 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.707  loss_cls: 0.212  loss_box_reg: 0.357  loss_rpn_cls: 0.031  loss_rpn_loc: 0.070  time: 0.5559  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 06:51:32 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:30 (0.5562 s / it)
[32m[04/15 06:51:32 d2.engine.hooks]: [0mTotal training time: 0:19:19 (0:00:48 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 06:51:44 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:51:44 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 06:51:44 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 06:51:46 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1250 s / img. ETA=0:02:39
[32m[04/15 06:51:51 d2.evaluation.evaluator]: [0mInference done 51/1257. 0.1244 s / img. ETA=0:02:34
[32m[04/15 06:51:56 d2.evaluation.evaluator]: [0mInference done 91/1257. 0.1243 s / img. ETA=0:02:29
[32m[04/15 06:52:01 d2.evaluation.evaluator]: [0mInference done 129/1257. 0.1248 s / img. ETA=0:02:25
[32m[04/15 06:52:07 d2.evaluation.evaluator]: [0mInference done 136/1257. 0.1244 s / img. ETA=0:03:06
[32m[04/15 06:52:12 d2.evaluation.evaluator]: [0mInference done 143/1257. 0.1243 s / img. ETA=0:03:37
[32m[04/15 06:52:17 d2.evaluation.evaluator]: [0mInference done 173/1257. 0.1243 s / img. ETA=0:03:25
[32m[04/15 06:52:22 d2.evaluation.evaluator]: [0mInference done 213/1257. 0.1239 s / img. ETA=0:03:05
[32m[04/15 06:52:27 d2.evaluation.evaluator]: [0mInference done 252/1257. 0.1239 s / img. ETA=0:02:50
[32m[04/15 06:52:32 d2.evaluation.evaluator]: [0mInference done 292/1257. 0.1239 s / img. ETA=0:02:38
[32m[04/15 06:52:37 d2.evaluation.evaluator]: [0mInference done 330/1257. 0.1237 s / img. ETA=0:02:28
[32m[04/15 06:52:42 d2.evaluation.evaluator]: [0mInference done 368/1257. 0.1234 s / img. ETA=0:02:20
[32m[04/15 06:52:47 d2.evaluation.evaluator]: [0mInference done 403/1257. 0.1233 s / img. ETA=0:02:13
[32m[04/15 06:52:52 d2.evaluation.evaluator]: [0mInference done 436/1257. 0.1232 s / img. ETA=0:02:08
[32m[04/15 06:52:58 d2.evaluation.evaluator]: [0mInference done 464/1257. 0.1229 s / img. ETA=0:02:05
[32m[04/15 06:53:03 d2.evaluation.evaluator]: [0mInference done 495/1257. 0.1228 s / img. ETA=0:02:00
[32m[04/15 06:53:08 d2.evaluation.evaluator]: [0mInference done 524/1257. 0.1228 s / img. ETA=0:01:57
[32m[04/15 06:53:13 d2.evaluation.evaluator]: [0mInference done 553/1257. 0.1228 s / img. ETA=0:01:52
[32m[04/15 06:53:18 d2.evaluation.evaluator]: [0mInference done 585/1257. 0.1228 s / img. ETA=0:01:47
[32m[04/15 06:53:23 d2.evaluation.evaluator]: [0mInference done 614/1257. 0.1227 s / img. ETA=0:01:43
[32m[04/15 06:53:28 d2.evaluation.evaluator]: [0mInference done 644/1257. 0.1227 s / img. ETA=0:01:38
[32m[04/15 06:53:33 d2.evaluation.evaluator]: [0mInference done 674/1257. 0.1227 s / img. ETA=0:01:34
[32m[04/15 06:53:38 d2.evaluation.evaluator]: [0mInference done 704/1257. 0.1227 s / img. ETA=0:01:29
[32m[04/15 06:53:43 d2.evaluation.evaluator]: [0mInference done 738/1257. 0.1227 s / img. ETA=0:01:23
[32m[04/15 06:53:48 d2.evaluation.evaluator]: [0mInference done 773/1257. 0.1227 s / img. ETA=0:01:17
[32m[04/15 06:53:54 d2.evaluation.evaluator]: [0mInference done 812/1257. 0.1226 s / img. ETA=0:01:10
[32m[04/15 06:53:59 d2.evaluation.evaluator]: [0mInference done 852/1257. 0.1226 s / img. ETA=0:01:03
[32m[04/15 06:54:04 d2.evaluation.evaluator]: [0mInference done 892/1257. 0.1226 s / img. ETA=0:00:57
[32m[04/15 06:54:09 d2.evaluation.evaluator]: [0mInference done 928/1257. 0.1226 s / img. ETA=0:00:51
[32m[04/15 06:54:14 d2.evaluation.evaluator]: [0mInference done 968/1257. 0.1226 s / img. ETA=0:00:44
[32m[04/15 06:54:19 d2.evaluation.evaluator]: [0mInference done 1006/1257. 0.1227 s / img. ETA=0:00:38
[32m[04/15 06:54:24 d2.evaluation.evaluator]: [0mInference done 1046/1257. 0.1227 s / img. ETA=0:00:32
[32m[04/15 06:54:29 d2.evaluation.evaluator]: [0mInference done 1085/1257. 0.1228 s / img. ETA=0:00:26
[32m[04/15 06:54:34 d2.evaluation.evaluator]: [0mInference done 1117/1257. 0.1228 s / img. ETA=0:00:21
[32m[04/15 06:54:39 d2.evaluation.evaluator]: [0mInference done 1149/1257. 0.1228 s / img. ETA=0:00:16
[32m[04/15 06:54:44 d2.evaluation.evaluator]: [0mInference done 1187/1257. 0.1229 s / img. ETA=0:00:10
[32m[04/15 06:54:49 d2.evaluation.evaluator]: [0mInference done 1226/1257. 0.1230 s / img. ETA=0:00:04
[32m[04/15 06:54:54 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:08.496232 (0.150556 s / img per device, on 1 devices)
[32m[04/15 06:54:54 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:34 (0.123082 s / img per device, on 1 devices)
[32m[04/15 06:54:54 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 06:54:54 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 06:54:54 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.03s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.10s).
Accumulating evaluation results...
DONE (t=0.62s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.158
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.319
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.134
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.081
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.198
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.430
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.198
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.199
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.124
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.242
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.467
[32m[04/15 06:55:00 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 15.834 | 31.867 | 13.429 | 8.140 | 19.775 | 42.955 |
[32m[04/15 06:55:00 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 23.175 | bicycle       | 6.111 | car            | 34.051 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  4  *  2000  iterations ============
4 channel input
[32m[04/15 06:55:01 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[5m[31mWARNING[0m [32m[04/15 06:55:02 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:55:02 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 06:55:03 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 06:55:03 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 06:55:03 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 06:55:07 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 06:55:19 d2.utils.events]: [0m eta: 0:18:32  iter: 19  total_loss: 0.696  loss_cls: 0.230  loss_box_reg: 0.347  loss_rpn_cls: 0.042  loss_rpn_loc: 0.090  time: 0.5609  data_time: 0.0402  lr: 0.000200  max_mem: 3070M
[32m[04/15 06:55:29 d2.utils.events]: [0m eta: 0:17:53  iter: 39  total_loss: 0.731  loss_cls: 0.242  loss_box_reg: 0.389  loss_rpn_cls: 0.041  loss_rpn_loc: 0.079  time: 0.5480  data_time: 0.0068  lr: 0.000400  max_mem: 3070M
[32m[04/15 06:55:40 d2.utils.events]: [0m eta: 0:17:42  iter: 59  total_loss: 0.623  loss_cls: 0.210  loss_box_reg: 0.335  loss_rpn_cls: 0.033  loss_rpn_loc: 0.052  time: 0.5487  data_time: 0.0078  lr: 0.000599  max_mem: 3070M
[32m[04/15 06:55:51 d2.utils.events]: [0m eta: 0:17:26  iter: 79  total_loss: 0.635  loss_cls: 0.210  loss_box_reg: 0.330  loss_rpn_cls: 0.032  loss_rpn_loc: 0.053  time: 0.5469  data_time: 0.0074  lr: 0.000799  max_mem: 3070M
[32m[04/15 06:56:02 d2.utils.events]: [0m eta: 0:17:15  iter: 99  total_loss: 0.793  loss_cls: 0.258  loss_box_reg: 0.399  loss_rpn_cls: 0.032  loss_rpn_loc: 0.067  time: 0.5475  data_time: 0.0075  lr: 0.000999  max_mem: 3070M
[32m[04/15 06:56:13 d2.utils.events]: [0m eta: 0:17:08  iter: 119  total_loss: 0.624  loss_cls: 0.216  loss_box_reg: 0.306  loss_rpn_cls: 0.020  loss_rpn_loc: 0.073  time: 0.5467  data_time: 0.0077  lr: 0.001199  max_mem: 3070M
[32m[04/15 06:56:25 d2.utils.events]: [0m eta: 0:17:00  iter: 139  total_loss: 0.828  loss_cls: 0.259  loss_box_reg: 0.444  loss_rpn_cls: 0.028  loss_rpn_loc: 0.067  time: 0.5467  data_time: 0.0082  lr: 0.001399  max_mem: 3070M
[32m[04/15 06:56:36 d2.utils.events]: [0m eta: 0:16:50  iter: 159  total_loss: 0.657  loss_cls: 0.226  loss_box_reg: 0.368  loss_rpn_cls: 0.027  loss_rpn_loc: 0.056  time: 0.5479  data_time: 0.0069  lr: 0.001598  max_mem: 3070M
[32m[04/15 06:56:47 d2.utils.events]: [0m eta: 0:16:37  iter: 179  total_loss: 0.727  loss_cls: 0.232  loss_box_reg: 0.372  loss_rpn_cls: 0.025  loss_rpn_loc: 0.081  time: 0.5472  data_time: 0.0061  lr: 0.001798  max_mem: 3070M
[32m[04/15 06:56:58 d2.utils.events]: [0m eta: 0:16:26  iter: 199  total_loss: 0.684  loss_cls: 0.227  loss_box_reg: 0.372  loss_rpn_cls: 0.025  loss_rpn_loc: 0.055  time: 0.5468  data_time: 0.0065  lr: 0.001998  max_mem: 3070M
[32m[04/15 06:57:08 d2.utils.events]: [0m eta: 0:16:09  iter: 219  total_loss: 0.660  loss_cls: 0.222  loss_box_reg: 0.337  loss_rpn_cls: 0.022  loss_rpn_loc: 0.054  time: 0.5444  data_time: 0.0064  lr: 0.002198  max_mem: 3070M
[32m[04/15 06:57:19 d2.utils.events]: [0m eta: 0:15:58  iter: 239  total_loss: 0.696  loss_cls: 0.246  loss_box_reg: 0.382  loss_rpn_cls: 0.031  loss_rpn_loc: 0.058  time: 0.5449  data_time: 0.0074  lr: 0.002398  max_mem: 3070M
[32m[04/15 06:57:30 d2.utils.events]: [0m eta: 0:15:46  iter: 259  total_loss: 0.724  loss_cls: 0.226  loss_box_reg: 0.377  loss_rpn_cls: 0.034  loss_rpn_loc: 0.069  time: 0.5450  data_time: 0.0076  lr: 0.002597  max_mem: 3070M
[32m[04/15 06:57:42 d2.utils.events]: [0m eta: 0:15:36  iter: 279  total_loss: 0.668  loss_cls: 0.234  loss_box_reg: 0.341  loss_rpn_cls: 0.028  loss_rpn_loc: 0.067  time: 0.5465  data_time: 0.0081  lr: 0.002797  max_mem: 3070M
[32m[04/15 06:57:53 d2.utils.events]: [0m eta: 0:15:30  iter: 299  total_loss: 0.649  loss_cls: 0.213  loss_box_reg: 0.344  loss_rpn_cls: 0.022  loss_rpn_loc: 0.046  time: 0.5472  data_time: 0.0063  lr: 0.002997  max_mem: 3070M
[32m[04/15 06:58:04 d2.utils.events]: [0m eta: 0:15:22  iter: 319  total_loss: 0.621  loss_cls: 0.205  loss_box_reg: 0.331  loss_rpn_cls: 0.023  loss_rpn_loc: 0.051  time: 0.5489  data_time: 0.0068  lr: 0.003197  max_mem: 3070M
[32m[04/15 06:58:16 d2.utils.events]: [0m eta: 0:15:16  iter: 339  total_loss: 0.773  loss_cls: 0.262  loss_box_reg: 0.429  loss_rpn_cls: 0.026  loss_rpn_loc: 0.054  time: 0.5506  data_time: 0.0074  lr: 0.003397  max_mem: 3070M
[32m[04/15 06:58:28 d2.utils.events]: [0m eta: 0:15:07  iter: 359  total_loss: 0.686  loss_cls: 0.230  loss_box_reg: 0.362  loss_rpn_cls: 0.035  loss_rpn_loc: 0.074  time: 0.5512  data_time: 0.0068  lr: 0.003596  max_mem: 3070M
[32m[04/15 06:58:39 d2.utils.events]: [0m eta: 0:14:57  iter: 379  total_loss: 0.743  loss_cls: 0.241  loss_box_reg: 0.371  loss_rpn_cls: 0.026  loss_rpn_loc: 0.066  time: 0.5518  data_time: 0.0081  lr: 0.003796  max_mem: 3070M
[32m[04/15 06:58:50 d2.utils.events]: [0m eta: 0:14:48  iter: 399  total_loss: 0.746  loss_cls: 0.250  loss_box_reg: 0.386  loss_rpn_cls: 0.038  loss_rpn_loc: 0.072  time: 0.5519  data_time: 0.0072  lr: 0.003996  max_mem: 3070M
[32m[04/15 06:59:01 d2.utils.events]: [0m eta: 0:14:37  iter: 419  total_loss: 0.565  loss_cls: 0.197  loss_box_reg: 0.313  loss_rpn_cls: 0.023  loss_rpn_loc: 0.064  time: 0.5518  data_time: 0.0066  lr: 0.004196  max_mem: 3070M
[32m[04/15 06:59:12 d2.utils.events]: [0m eta: 0:14:27  iter: 439  total_loss: 0.623  loss_cls: 0.195  loss_box_reg: 0.331  loss_rpn_cls: 0.033  loss_rpn_loc: 0.056  time: 0.5521  data_time: 0.0091  lr: 0.004396  max_mem: 3070M
[32m[04/15 06:59:24 d2.utils.events]: [0m eta: 0:14:16  iter: 459  total_loss: 0.665  loss_cls: 0.221  loss_box_reg: 0.370  loss_rpn_cls: 0.026  loss_rpn_loc: 0.044  time: 0.5525  data_time: 0.0076  lr: 0.004595  max_mem: 3070M
[32m[04/15 06:59:35 d2.utils.events]: [0m eta: 0:14:06  iter: 479  total_loss: 0.616  loss_cls: 0.199  loss_box_reg: 0.310  loss_rpn_cls: 0.023  loss_rpn_loc: 0.053  time: 0.5530  data_time: 0.0063  lr: 0.004795  max_mem: 3070M
[32m[04/15 06:59:46 d2.utils.events]: [0m eta: 0:13:56  iter: 499  total_loss: 0.756  loss_cls: 0.253  loss_box_reg: 0.398  loss_rpn_cls: 0.026  loss_rpn_loc: 0.059  time: 0.5534  data_time: 0.0077  lr: 0.004995  max_mem: 3070M
[32m[04/15 06:59:58 d2.utils.events]: [0m eta: 0:13:46  iter: 519  total_loss: 0.651  loss_cls: 0.212  loss_box_reg: 0.350  loss_rpn_cls: 0.022  loss_rpn_loc: 0.057  time: 0.5534  data_time: 0.0079  lr: 0.005195  max_mem: 3070M
[32m[04/15 07:00:09 d2.utils.events]: [0m eta: 0:13:35  iter: 539  total_loss: 0.736  loss_cls: 0.252  loss_box_reg: 0.377  loss_rpn_cls: 0.033  loss_rpn_loc: 0.064  time: 0.5532  data_time: 0.0079  lr: 0.005395  max_mem: 3070M
[32m[04/15 07:00:20 d2.utils.events]: [0m eta: 0:13:24  iter: 559  total_loss: 0.734  loss_cls: 0.233  loss_box_reg: 0.380  loss_rpn_cls: 0.027  loss_rpn_loc: 0.066  time: 0.5536  data_time: 0.0075  lr: 0.005594  max_mem: 3070M
[32m[04/15 07:00:31 d2.utils.events]: [0m eta: 0:13:12  iter: 579  total_loss: 0.652  loss_cls: 0.222  loss_box_reg: 0.334  loss_rpn_cls: 0.024  loss_rpn_loc: 0.061  time: 0.5534  data_time: 0.0095  lr: 0.005794  max_mem: 3070M
[32m[04/15 07:00:42 d2.utils.events]: [0m eta: 0:13:01  iter: 599  total_loss: 0.672  loss_cls: 0.216  loss_box_reg: 0.324  loss_rpn_cls: 0.027  loss_rpn_loc: 0.070  time: 0.5536  data_time: 0.0078  lr: 0.005994  max_mem: 3070M
[32m[04/15 07:00:53 d2.utils.events]: [0m eta: 0:12:49  iter: 619  total_loss: 0.583  loss_cls: 0.214  loss_box_reg: 0.311  loss_rpn_cls: 0.018  loss_rpn_loc: 0.038  time: 0.5530  data_time: 0.0084  lr: 0.006194  max_mem: 3070M
[32m[04/15 07:01:05 d2.utils.events]: [0m eta: 0:12:39  iter: 639  total_loss: 0.579  loss_cls: 0.211  loss_box_reg: 0.287  loss_rpn_cls: 0.043  loss_rpn_loc: 0.047  time: 0.5538  data_time: 0.0075  lr: 0.006394  max_mem: 3070M
[32m[04/15 07:01:16 d2.utils.events]: [0m eta: 0:12:28  iter: 659  total_loss: 0.664  loss_cls: 0.221  loss_box_reg: 0.355  loss_rpn_cls: 0.026  loss_rpn_loc: 0.060  time: 0.5543  data_time: 0.0073  lr: 0.006593  max_mem: 3070M
[32m[04/15 07:01:28 d2.utils.events]: [0m eta: 0:12:17  iter: 679  total_loss: 0.710  loss_cls: 0.242  loss_box_reg: 0.381  loss_rpn_cls: 0.031  loss_rpn_loc: 0.061  time: 0.5543  data_time: 0.0081  lr: 0.006793  max_mem: 3070M
[32m[04/15 07:01:39 d2.utils.events]: [0m eta: 0:12:06  iter: 699  total_loss: 0.729  loss_cls: 0.243  loss_box_reg: 0.388  loss_rpn_cls: 0.034  loss_rpn_loc: 0.058  time: 0.5543  data_time: 0.0081  lr: 0.006993  max_mem: 3070M
[32m[04/15 07:01:50 d2.utils.events]: [0m eta: 0:11:55  iter: 719  total_loss: 0.638  loss_cls: 0.199  loss_box_reg: 0.318  loss_rpn_cls: 0.030  loss_rpn_loc: 0.068  time: 0.5547  data_time: 0.0077  lr: 0.007193  max_mem: 3070M
[32m[04/15 07:02:01 d2.utils.events]: [0m eta: 0:11:44  iter: 739  total_loss: 0.623  loss_cls: 0.229  loss_box_reg: 0.355  loss_rpn_cls: 0.026  loss_rpn_loc: 0.052  time: 0.5546  data_time: 0.0070  lr: 0.007393  max_mem: 3070M
[32m[04/15 07:02:13 d2.utils.events]: [0m eta: 0:11:32  iter: 759  total_loss: 0.660  loss_cls: 0.230  loss_box_reg: 0.352  loss_rpn_cls: 0.031  loss_rpn_loc: 0.052  time: 0.5549  data_time: 0.0081  lr: 0.007592  max_mem: 3070M
[32m[04/15 07:02:24 d2.utils.events]: [0m eta: 0:11:22  iter: 779  total_loss: 0.709  loss_cls: 0.237  loss_box_reg: 0.381  loss_rpn_cls: 0.035  loss_rpn_loc: 0.054  time: 0.5553  data_time: 0.0081  lr: 0.007792  max_mem: 3070M
[32m[04/15 07:02:35 d2.utils.events]: [0m eta: 0:11:12  iter: 799  total_loss: 0.814  loss_cls: 0.266  loss_box_reg: 0.401  loss_rpn_cls: 0.049  loss_rpn_loc: 0.098  time: 0.5554  data_time: 0.0073  lr: 0.007992  max_mem: 3070M
[32m[04/15 07:02:47 d2.utils.events]: [0m eta: 0:11:00  iter: 819  total_loss: 0.621  loss_cls: 0.221  loss_box_reg: 0.339  loss_rpn_cls: 0.030  loss_rpn_loc: 0.062  time: 0.5556  data_time: 0.0076  lr: 0.008192  max_mem: 3070M
[32m[04/15 07:02:58 d2.utils.events]: [0m eta: 0:10:49  iter: 839  total_loss: 0.674  loss_cls: 0.237  loss_box_reg: 0.356  loss_rpn_cls: 0.031  loss_rpn_loc: 0.072  time: 0.5558  data_time: 0.0065  lr: 0.008392  max_mem: 3070M
[32m[04/15 07:03:10 d2.utils.events]: [0m eta: 0:10:38  iter: 859  total_loss: 0.774  loss_cls: 0.244  loss_box_reg: 0.421  loss_rpn_cls: 0.025  loss_rpn_loc: 0.062  time: 0.5564  data_time: 0.0067  lr: 0.008591  max_mem: 3070M
[32m[04/15 07:03:21 d2.utils.events]: [0m eta: 0:10:27  iter: 879  total_loss: 0.772  loss_cls: 0.263  loss_box_reg: 0.385  loss_rpn_cls: 0.038  loss_rpn_loc: 0.081  time: 0.5561  data_time: 0.0066  lr: 0.008791  max_mem: 3070M
[32m[04/15 07:03:32 d2.utils.events]: [0m eta: 0:10:16  iter: 899  total_loss: 0.751  loss_cls: 0.261  loss_box_reg: 0.389  loss_rpn_cls: 0.031  loss_rpn_loc: 0.066  time: 0.5558  data_time: 0.0074  lr: 0.008991  max_mem: 3070M
[32m[04/15 07:03:43 d2.utils.events]: [0m eta: 0:10:05  iter: 919  total_loss: 0.624  loss_cls: 0.205  loss_box_reg: 0.308  loss_rpn_cls: 0.031  loss_rpn_loc: 0.048  time: 0.5560  data_time: 0.0093  lr: 0.009191  max_mem: 3070M
[32m[04/15 07:03:54 d2.utils.events]: [0m eta: 0:09:53  iter: 939  total_loss: 0.730  loss_cls: 0.222  loss_box_reg: 0.366  loss_rpn_cls: 0.033  loss_rpn_loc: 0.080  time: 0.5559  data_time: 0.0065  lr: 0.009391  max_mem: 3070M
[32m[04/15 07:04:06 d2.utils.events]: [0m eta: 0:09:42  iter: 959  total_loss: 0.601  loss_cls: 0.216  loss_box_reg: 0.295  loss_rpn_cls: 0.040  loss_rpn_loc: 0.043  time: 0.5558  data_time: 0.0068  lr: 0.009590  max_mem: 3070M
[32m[04/15 07:04:17 d2.utils.events]: [0m eta: 0:09:31  iter: 979  total_loss: 0.640  loss_cls: 0.211  loss_box_reg: 0.310  loss_rpn_cls: 0.044  loss_rpn_loc: 0.062  time: 0.5558  data_time: 0.0071  lr: 0.009790  max_mem: 3070M
[32m[04/15 07:04:28 d2.utils.events]: [0m eta: 0:09:20  iter: 999  total_loss: 0.791  loss_cls: 0.272  loss_box_reg: 0.399  loss_rpn_cls: 0.029  loss_rpn_loc: 0.057  time: 0.5560  data_time: 0.0081  lr: 0.009990  max_mem: 3070M
[32m[04/15 07:04:40 d2.utils.events]: [0m eta: 0:09:09  iter: 1019  total_loss: 0.624  loss_cls: 0.211  loss_box_reg: 0.315  loss_rpn_cls: 0.031  loss_rpn_loc: 0.065  time: 0.5563  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:04:51 d2.utils.events]: [0m eta: 0:08:58  iter: 1039  total_loss: 0.676  loss_cls: 0.231  loss_box_reg: 0.317  loss_rpn_cls: 0.040  loss_rpn_loc: 0.066  time: 0.5564  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:05:02 d2.utils.events]: [0m eta: 0:08:47  iter: 1059  total_loss: 0.697  loss_cls: 0.226  loss_box_reg: 0.361  loss_rpn_cls: 0.033  loss_rpn_loc: 0.065  time: 0.5566  data_time: 0.0096  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:05:13 d2.utils.events]: [0m eta: 0:08:36  iter: 1079  total_loss: 0.773  loss_cls: 0.269  loss_box_reg: 0.378  loss_rpn_cls: 0.032  loss_rpn_loc: 0.074  time: 0.5563  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:05:25 d2.utils.events]: [0m eta: 0:08:25  iter: 1099  total_loss: 0.686  loss_cls: 0.222  loss_box_reg: 0.374  loss_rpn_cls: 0.032  loss_rpn_loc: 0.062  time: 0.5564  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:05:36 d2.utils.events]: [0m eta: 0:08:15  iter: 1119  total_loss: 0.714  loss_cls: 0.240  loss_box_reg: 0.349  loss_rpn_cls: 0.030  loss_rpn_loc: 0.076  time: 0.5567  data_time: 0.0098  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:05:48 d2.utils.events]: [0m eta: 0:08:04  iter: 1139  total_loss: 0.725  loss_cls: 0.222  loss_box_reg: 0.334  loss_rpn_cls: 0.028  loss_rpn_loc: 0.062  time: 0.5568  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:05:59 d2.utils.events]: [0m eta: 0:07:53  iter: 1159  total_loss: 0.819  loss_cls: 0.265  loss_box_reg: 0.374  loss_rpn_cls: 0.043  loss_rpn_loc: 0.079  time: 0.5571  data_time: 0.0096  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:06:10 d2.utils.events]: [0m eta: 0:07:42  iter: 1179  total_loss: 0.657  loss_cls: 0.220  loss_box_reg: 0.331  loss_rpn_cls: 0.029  loss_rpn_loc: 0.065  time: 0.5571  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:06:22 d2.utils.events]: [0m eta: 0:07:31  iter: 1199  total_loss: 0.721  loss_cls: 0.227  loss_box_reg: 0.380  loss_rpn_cls: 0.038  loss_rpn_loc: 0.078  time: 0.5570  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:06:33 d2.utils.events]: [0m eta: 0:07:20  iter: 1219  total_loss: 0.799  loss_cls: 0.256  loss_box_reg: 0.398  loss_rpn_cls: 0.038  loss_rpn_loc: 0.080  time: 0.5569  data_time: 0.0059  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:06:44 d2.utils.events]: [0m eta: 0:07:09  iter: 1239  total_loss: 0.702  loss_cls: 0.219  loss_box_reg: 0.377  loss_rpn_cls: 0.028  loss_rpn_loc: 0.066  time: 0.5570  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:06:55 d2.utils.events]: [0m eta: 0:06:58  iter: 1259  total_loss: 0.808  loss_cls: 0.284  loss_box_reg: 0.398  loss_rpn_cls: 0.032  loss_rpn_loc: 0.080  time: 0.5572  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:07:07 d2.utils.events]: [0m eta: 0:06:46  iter: 1279  total_loss: 0.828  loss_cls: 0.262  loss_box_reg: 0.450  loss_rpn_cls: 0.032  loss_rpn_loc: 0.095  time: 0.5574  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:07:18 d2.utils.events]: [0m eta: 0:06:35  iter: 1299  total_loss: 0.790  loss_cls: 0.262  loss_box_reg: 0.399  loss_rpn_cls: 0.036  loss_rpn_loc: 0.078  time: 0.5575  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:07:30 d2.utils.events]: [0m eta: 0:06:23  iter: 1319  total_loss: 0.711  loss_cls: 0.245  loss_box_reg: 0.351  loss_rpn_cls: 0.031  loss_rpn_loc: 0.061  time: 0.5575  data_time: 0.0160  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:07:41 d2.utils.events]: [0m eta: 0:06:11  iter: 1339  total_loss: 0.790  loss_cls: 0.266  loss_box_reg: 0.395  loss_rpn_cls: 0.037  loss_rpn_loc: 0.080  time: 0.5573  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:07:52 d2.utils.events]: [0m eta: 0:06:00  iter: 1359  total_loss: 0.887  loss_cls: 0.311  loss_box_reg: 0.461  loss_rpn_cls: 0.031  loss_rpn_loc: 0.081  time: 0.5575  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:08:04 d2.utils.events]: [0m eta: 0:05:49  iter: 1379  total_loss: 0.707  loss_cls: 0.258  loss_box_reg: 0.353  loss_rpn_cls: 0.035  loss_rpn_loc: 0.052  time: 0.5578  data_time: 0.0338  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:08:15 d2.utils.events]: [0m eta: 0:05:38  iter: 1399  total_loss: 0.808  loss_cls: 0.277  loss_box_reg: 0.440  loss_rpn_cls: 0.036  loss_rpn_loc: 0.083  time: 0.5581  data_time: 0.0403  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:08:27 d2.utils.events]: [0m eta: 0:05:27  iter: 1419  total_loss: 0.747  loss_cls: 0.272  loss_box_reg: 0.348  loss_rpn_cls: 0.036  loss_rpn_loc: 0.074  time: 0.5584  data_time: 0.0113  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:08:38 d2.utils.events]: [0m eta: 0:05:16  iter: 1439  total_loss: 0.734  loss_cls: 0.260  loss_box_reg: 0.360  loss_rpn_cls: 0.051  loss_rpn_loc: 0.080  time: 0.5584  data_time: 0.0100  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:08:49 d2.utils.events]: [0m eta: 0:05:04  iter: 1459  total_loss: 0.660  loss_cls: 0.207  loss_box_reg: 0.311  loss_rpn_cls: 0.044  loss_rpn_loc: 0.064  time: 0.5582  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:09:00 d2.utils.events]: [0m eta: 0:04:53  iter: 1479  total_loss: 0.605  loss_cls: 0.200  loss_box_reg: 0.292  loss_rpn_cls: 0.052  loss_rpn_loc: 0.084  time: 0.5579  data_time: 0.0102  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:09:11 d2.utils.events]: [0m eta: 0:04:41  iter: 1499  total_loss: 0.699  loss_cls: 0.244  loss_box_reg: 0.354  loss_rpn_cls: 0.046  loss_rpn_loc: 0.072  time: 0.5578  data_time: 0.0062  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:09:23 d2.utils.events]: [0m eta: 0:04:30  iter: 1519  total_loss: 0.824  loss_cls: 0.259  loss_box_reg: 0.408  loss_rpn_cls: 0.039  loss_rpn_loc: 0.087  time: 0.5578  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:09:34 d2.utils.events]: [0m eta: 0:04:19  iter: 1539  total_loss: 0.866  loss_cls: 0.292  loss_box_reg: 0.414  loss_rpn_cls: 0.044  loss_rpn_loc: 0.078  time: 0.5578  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:09:45 d2.utils.events]: [0m eta: 0:04:08  iter: 1559  total_loss: 0.817  loss_cls: 0.269  loss_box_reg: 0.407  loss_rpn_cls: 0.051  loss_rpn_loc: 0.086  time: 0.5580  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:09:56 d2.utils.events]: [0m eta: 0:03:56  iter: 1579  total_loss: 0.731  loss_cls: 0.238  loss_box_reg: 0.379  loss_rpn_cls: 0.035  loss_rpn_loc: 0.069  time: 0.5579  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:10:08 d2.utils.events]: [0m eta: 0:03:45  iter: 1599  total_loss: 0.681  loss_cls: 0.243  loss_box_reg: 0.345  loss_rpn_cls: 0.032  loss_rpn_loc: 0.069  time: 0.5581  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:10:19 d2.utils.events]: [0m eta: 0:03:34  iter: 1619  total_loss: 0.716  loss_cls: 0.235  loss_box_reg: 0.340  loss_rpn_cls: 0.040  loss_rpn_loc: 0.085  time: 0.5578  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:10:30 d2.utils.events]: [0m eta: 0:03:23  iter: 1639  total_loss: 0.826  loss_cls: 0.270  loss_box_reg: 0.394  loss_rpn_cls: 0.031  loss_rpn_loc: 0.069  time: 0.5578  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:10:42 d2.utils.events]: [0m eta: 0:03:11  iter: 1659  total_loss: 0.873  loss_cls: 0.287  loss_box_reg: 0.498  loss_rpn_cls: 0.037  loss_rpn_loc: 0.086  time: 0.5579  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:10:53 d2.utils.events]: [0m eta: 0:03:00  iter: 1679  total_loss: 0.935  loss_cls: 0.305  loss_box_reg: 0.465  loss_rpn_cls: 0.046  loss_rpn_loc: 0.092  time: 0.5581  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:11:04 d2.utils.events]: [0m eta: 0:02:49  iter: 1699  total_loss: 0.787  loss_cls: 0.270  loss_box_reg: 0.387  loss_rpn_cls: 0.034  loss_rpn_loc: 0.081  time: 0.5582  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:11:21 d2.utils.events]: [0m eta: 0:02:38  iter: 1719  total_loss: 0.748  loss_cls: 0.258  loss_box_reg: 0.412  loss_rpn_cls: 0.035  loss_rpn_loc: 0.070  time: 0.5578  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:11:32 d2.utils.events]: [0m eta: 0:02:26  iter: 1739  total_loss: 0.556  loss_cls: 0.197  loss_box_reg: 0.287  loss_rpn_cls: 0.026  loss_rpn_loc: 0.048  time: 0.5577  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:11:43 d2.utils.events]: [0m eta: 0:02:15  iter: 1759  total_loss: 0.767  loss_cls: 0.236  loss_box_reg: 0.358  loss_rpn_cls: 0.046  loss_rpn_loc: 0.070  time: 0.5576  data_time: 0.0060  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:11:57 d2.utils.events]: [0m eta: 0:02:04  iter: 1779  total_loss: 0.668  loss_cls: 0.244  loss_box_reg: 0.300  loss_rpn_cls: 0.030  loss_rpn_loc: 0.060  time: 0.5577  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:12:17 d2.utils.events]: [0m eta: 0:01:52  iter: 1799  total_loss: 0.718  loss_cls: 0.239  loss_box_reg: 0.347  loss_rpn_cls: 0.033  loss_rpn_loc: 0.070  time: 0.5575  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:12:28 d2.utils.events]: [0m eta: 0:01:41  iter: 1819  total_loss: 0.695  loss_cls: 0.243  loss_box_reg: 0.354  loss_rpn_cls: 0.034  loss_rpn_loc: 0.066  time: 0.5574  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:12:39 d2.utils.events]: [0m eta: 0:01:30  iter: 1839  total_loss: 0.682  loss_cls: 0.210  loss_box_reg: 0.349  loss_rpn_cls: 0.036  loss_rpn_loc: 0.059  time: 0.5573  data_time: 0.0133  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:12:50 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.623  loss_cls: 0.205  loss_box_reg: 0.322  loss_rpn_cls: 0.025  loss_rpn_loc: 0.058  time: 0.5570  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:13:01 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.702  loss_cls: 0.228  loss_box_reg: 0.335  loss_rpn_cls: 0.037  loss_rpn_loc: 0.066  time: 0.5569  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:13:12 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.752  loss_cls: 0.230  loss_box_reg: 0.385  loss_rpn_cls: 0.044  loss_rpn_loc: 0.067  time: 0.5568  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:13:23 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.719  loss_cls: 0.256  loss_box_reg: 0.367  loss_rpn_cls: 0.038  loss_rpn_loc: 0.066  time: 0.5568  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:13:34 d2.utils.events]: [0m eta: 0:00:34  iter: 1939  total_loss: 0.685  loss_cls: 0.238  loss_box_reg: 0.342  loss_rpn_cls: 0.039  loss_rpn_loc: 0.080  time: 0.5567  data_time: 0.0092  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:13:45 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.728  loss_cls: 0.250  loss_box_reg: 0.391  loss_rpn_cls: 0.036  loss_rpn_loc: 0.073  time: 0.5566  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:13:57 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.782  loss_cls: 0.254  loss_box_reg: 0.381  loss_rpn_cls: 0.044  loss_rpn_loc: 0.088  time: 0.5565  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 07:14:41 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 07:14:41 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 07:14:42 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 07:14:42 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.688  loss_cls: 0.218  loss_box_reg: 0.347  loss_rpn_cls: 0.029  loss_rpn_loc: 0.075  time: 0.5565  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:14:45 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:31 (0.5568 s / it)
[32m[04/15 07:14:45 d2.engine.hooks]: [0mTotal training time: 0:19:36 (0:01:04 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 07:14:58 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 07:14:58 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 07:14:58 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 07:15:01 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1183 s / img. ETA=0:06:42
[32m[04/15 07:15:06 d2.evaluation.evaluator]: [0mInference done 35/1257. 0.1200 s / img. ETA=0:04:43
[32m[04/15 07:15:11 d2.evaluation.evaluator]: [0mInference done 68/1257. 0.1211 s / img. ETA=0:03:46
[32m[04/15 07:15:16 d2.evaluation.evaluator]: [0mInference done 98/1257. 0.1207 s / img. ETA=0:03:34
[32m[04/15 07:15:21 d2.evaluation.evaluator]: [0mInference done 125/1257. 0.1204 s / img. ETA=0:03:30
[32m[04/15 07:15:27 d2.evaluation.evaluator]: [0mInference done 154/1257. 0.1199 s / img. ETA=0:03:22
[32m[04/15 07:15:32 d2.evaluation.evaluator]: [0mInference done 189/1257. 0.1203 s / img. ETA=0:03:08
[32m[04/15 07:15:37 d2.evaluation.evaluator]: [0mInference done 226/1257. 0.1206 s / img. ETA=0:02:54
[32m[04/15 07:15:42 d2.evaluation.evaluator]: [0mInference done 263/1257. 0.1209 s / img. ETA=0:02:43
[32m[04/15 07:15:47 d2.evaluation.evaluator]: [0mInference done 303/1257. 0.1210 s / img. ETA=0:02:32
[32m[04/15 07:15:52 d2.evaluation.evaluator]: [0mInference done 340/1257. 0.1212 s / img. ETA=0:02:24
[32m[04/15 07:15:57 d2.evaluation.evaluator]: [0mInference done 378/1257. 0.1212 s / img. ETA=0:02:16
[32m[04/15 07:16:02 d2.evaluation.evaluator]: [0mInference done 412/1257. 0.1212 s / img. ETA=0:02:11
[32m[04/15 07:16:07 d2.evaluation.evaluator]: [0mInference done 442/1257. 0.1211 s / img. ETA=0:02:07
[32m[04/15 07:16:12 d2.evaluation.evaluator]: [0mInference done 472/1257. 0.1210 s / img. ETA=0:02:03
[32m[04/15 07:16:18 d2.evaluation.evaluator]: [0mInference done 502/1257. 0.1208 s / img. ETA=0:01:58
[32m[04/15 07:16:23 d2.evaluation.evaluator]: [0mInference done 529/1257. 0.1208 s / img. ETA=0:01:55
[32m[04/15 07:16:28 d2.evaluation.evaluator]: [0mInference done 560/1257. 0.1208 s / img. ETA=0:01:51
[32m[04/15 07:16:33 d2.evaluation.evaluator]: [0mInference done 591/1257. 0.1208 s / img. ETA=0:01:46
[32m[04/15 07:16:38 d2.evaluation.evaluator]: [0mInference done 622/1257. 0.1208 s / img. ETA=0:01:41
[32m[04/15 07:16:43 d2.evaluation.evaluator]: [0mInference done 653/1257. 0.1208 s / img. ETA=0:01:36
[32m[04/15 07:16:48 d2.evaluation.evaluator]: [0mInference done 678/1257. 0.1208 s / img. ETA=0:01:33
[32m[04/15 07:16:53 d2.evaluation.evaluator]: [0mInference done 713/1257. 0.1209 s / img. ETA=0:01:27
[32m[04/15 07:16:58 d2.evaluation.evaluator]: [0mInference done 741/1257. 0.1209 s / img. ETA=0:01:23
[32m[04/15 07:17:03 d2.evaluation.evaluator]: [0mInference done 777/1257. 0.1210 s / img. ETA=0:01:17
[32m[04/15 07:17:08 d2.evaluation.evaluator]: [0mInference done 817/1257. 0.1210 s / img. ETA=0:01:09
[32m[04/15 07:17:13 d2.evaluation.evaluator]: [0mInference done 857/1257. 0.1210 s / img. ETA=0:01:03
[32m[04/15 07:17:18 d2.evaluation.evaluator]: [0mInference done 897/1257. 0.1211 s / img. ETA=0:00:56
[32m[04/15 07:17:24 d2.evaluation.evaluator]: [0mInference done 936/1257. 0.1211 s / img. ETA=0:00:49
[32m[04/15 07:17:29 d2.evaluation.evaluator]: [0mInference done 975/1257. 0.1212 s / img. ETA=0:00:43
[32m[04/15 07:17:34 d2.evaluation.evaluator]: [0mInference done 1014/1257. 0.1212 s / img. ETA=0:00:37
[32m[04/15 07:17:39 d2.evaluation.evaluator]: [0mInference done 1053/1257. 0.1212 s / img. ETA=0:00:31
[32m[04/15 07:17:44 d2.evaluation.evaluator]: [0mInference done 1090/1257. 0.1213 s / img. ETA=0:00:25
[32m[04/15 07:17:49 d2.evaluation.evaluator]: [0mInference done 1125/1257. 0.1214 s / img. ETA=0:00:20
[32m[04/15 07:17:54 d2.evaluation.evaluator]: [0mInference done 1162/1257. 0.1216 s / img. ETA=0:00:14
[32m[04/15 07:17:59 d2.evaluation.evaluator]: [0mInference done 1200/1257. 0.1217 s / img. ETA=0:00:08
[32m[04/15 07:18:04 d2.evaluation.evaluator]: [0mInference done 1237/1257. 0.1217 s / img. ETA=0:00:03
[32m[04/15 07:18:07 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:07.774211 (0.149979 s / img per device, on 1 devices)
[32m[04/15 07:18:07 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:32 (0.121778 s / img per device, on 1 devices)
[32m[04/15 07:18:07 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 07:18:07 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 07:18:07 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.03s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.49s).
Accumulating evaluation results...
DONE (t=0.64s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.296
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.124
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.178
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.322
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.080
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.191
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.192
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.130
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.232
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.367
[32m[04/15 07:18:13 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 14.503 | 29.596 | 12.386 | 8.705 | 17.821 | 32.225 |
[32m[04/15 07:18:13 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 19.716 | bicycle       | 3.119 | car            | 35.176 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  5  *  2000  iterations ============
4 channel input
[32m[04/15 07:18:15 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 07:18:16 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.29 seconds.
[5m[31mWARNING[0m [32m[04/15 07:18:16 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 07:18:16 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 07:18:17 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 07:18:17 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 07:18:17 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 07:18:21 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 07:18:33 d2.utils.events]: [0m eta: 0:18:26  iter: 19  total_loss: 0.778  loss_cls: 0.262  loss_box_reg: 0.367  loss_rpn_cls: 0.030  loss_rpn_loc: 0.067  time: 0.5622  data_time: 0.0424  lr: 0.000200  max_mem: 3070M
[32m[04/15 07:18:44 d2.utils.events]: [0m eta: 0:17:50  iter: 39  total_loss: 0.721  loss_cls: 0.228  loss_box_reg: 0.392  loss_rpn_cls: 0.028  loss_rpn_loc: 0.069  time: 0.5504  data_time: 0.0103  lr: 0.000400  max_mem: 3070M
[32m[04/15 07:18:55 d2.utils.events]: [0m eta: 0:17:41  iter: 59  total_loss: 0.711  loss_cls: 0.240  loss_box_reg: 0.373  loss_rpn_cls: 0.029  loss_rpn_loc: 0.067  time: 0.5502  data_time: 0.0075  lr: 0.000599  max_mem: 3070M
[32m[04/15 07:19:06 d2.utils.events]: [0m eta: 0:17:36  iter: 79  total_loss: 0.543  loss_cls: 0.168  loss_box_reg: 0.281  loss_rpn_cls: 0.032  loss_rpn_loc: 0.052  time: 0.5521  data_time: 0.0068  lr: 0.000799  max_mem: 3070M
[32m[04/15 07:19:17 d2.utils.events]: [0m eta: 0:17:22  iter: 99  total_loss: 0.664  loss_cls: 0.225  loss_box_reg: 0.364  loss_rpn_cls: 0.027  loss_rpn_loc: 0.053  time: 0.5494  data_time: 0.0071  lr: 0.000999  max_mem: 3070M
[32m[04/15 07:19:28 d2.utils.events]: [0m eta: 0:17:20  iter: 119  total_loss: 0.728  loss_cls: 0.256  loss_box_reg: 0.382  loss_rpn_cls: 0.029  loss_rpn_loc: 0.062  time: 0.5527  data_time: 0.0069  lr: 0.001199  max_mem: 3070M
[32m[04/15 07:19:40 d2.utils.events]: [0m eta: 0:17:08  iter: 139  total_loss: 0.734  loss_cls: 0.225  loss_box_reg: 0.384  loss_rpn_cls: 0.033  loss_rpn_loc: 0.061  time: 0.5526  data_time: 0.0079  lr: 0.001399  max_mem: 3070M
[32m[04/15 07:19:51 d2.utils.events]: [0m eta: 0:16:59  iter: 159  total_loss: 0.654  loss_cls: 0.209  loss_box_reg: 0.342  loss_rpn_cls: 0.033  loss_rpn_loc: 0.057  time: 0.5536  data_time: 0.0080  lr: 0.001598  max_mem: 3070M
[32m[04/15 07:20:02 d2.utils.events]: [0m eta: 0:16:46  iter: 179  total_loss: 0.709  loss_cls: 0.222  loss_box_reg: 0.353  loss_rpn_cls: 0.037  loss_rpn_loc: 0.073  time: 0.5516  data_time: 0.0069  lr: 0.001798  max_mem: 3070M
[32m[04/15 07:20:13 d2.utils.events]: [0m eta: 0:16:40  iter: 199  total_loss: 0.611  loss_cls: 0.198  loss_box_reg: 0.340  loss_rpn_cls: 0.019  loss_rpn_loc: 0.055  time: 0.5551  data_time: 0.0071  lr: 0.001998  max_mem: 3070M
[32m[04/15 07:20:24 d2.utils.events]: [0m eta: 0:16:28  iter: 219  total_loss: 0.732  loss_cls: 0.239  loss_box_reg: 0.398  loss_rpn_cls: 0.032  loss_rpn_loc: 0.075  time: 0.5538  data_time: 0.0076  lr: 0.002198  max_mem: 3070M
[32m[04/15 07:20:36 d2.utils.events]: [0m eta: 0:16:19  iter: 239  total_loss: 0.615  loss_cls: 0.198  loss_box_reg: 0.319  loss_rpn_cls: 0.024  loss_rpn_loc: 0.055  time: 0.5551  data_time: 0.0072  lr: 0.002398  max_mem: 3070M
[32m[04/15 07:20:47 d2.utils.events]: [0m eta: 0:16:11  iter: 259  total_loss: 0.665  loss_cls: 0.203  loss_box_reg: 0.366  loss_rpn_cls: 0.021  loss_rpn_loc: 0.045  time: 0.5567  data_time: 0.0080  lr: 0.002597  max_mem: 3070M
[32m[04/15 07:20:59 d2.utils.events]: [0m eta: 0:16:04  iter: 279  total_loss: 0.745  loss_cls: 0.246  loss_box_reg: 0.390  loss_rpn_cls: 0.019  loss_rpn_loc: 0.060  time: 0.5580  data_time: 0.0067  lr: 0.002797  max_mem: 3070M
[32m[04/15 07:21:10 d2.utils.events]: [0m eta: 0:15:53  iter: 299  total_loss: 0.697  loss_cls: 0.232  loss_box_reg: 0.392  loss_rpn_cls: 0.022  loss_rpn_loc: 0.052  time: 0.5581  data_time: 0.0072  lr: 0.002997  max_mem: 3070M
[32m[04/15 07:21:22 d2.utils.events]: [0m eta: 0:15:42  iter: 319  total_loss: 0.710  loss_cls: 0.230  loss_box_reg: 0.373  loss_rpn_cls: 0.029  loss_rpn_loc: 0.059  time: 0.5590  data_time: 0.0073  lr: 0.003197  max_mem: 3070M
[32m[04/15 07:21:33 d2.utils.events]: [0m eta: 0:15:31  iter: 339  total_loss: 0.683  loss_cls: 0.251  loss_box_reg: 0.364  loss_rpn_cls: 0.032  loss_rpn_loc: 0.050  time: 0.5590  data_time: 0.0068  lr: 0.003397  max_mem: 3070M
[32m[04/15 07:21:44 d2.utils.events]: [0m eta: 0:15:19  iter: 359  total_loss: 0.538  loss_cls: 0.174  loss_box_reg: 0.306  loss_rpn_cls: 0.023  loss_rpn_loc: 0.045  time: 0.5579  data_time: 0.0071  lr: 0.003596  max_mem: 3070M
[32m[04/15 07:21:55 d2.utils.events]: [0m eta: 0:15:07  iter: 379  total_loss: 0.658  loss_cls: 0.247  loss_box_reg: 0.323  loss_rpn_cls: 0.028  loss_rpn_loc: 0.059  time: 0.5569  data_time: 0.0082  lr: 0.003796  max_mem: 3070M
[32m[04/15 07:22:06 d2.utils.events]: [0m eta: 0:14:57  iter: 399  total_loss: 0.706  loss_cls: 0.237  loss_box_reg: 0.367  loss_rpn_cls: 0.027  loss_rpn_loc: 0.061  time: 0.5573  data_time: 0.0064  lr: 0.003996  max_mem: 3070M
[32m[04/15 07:22:17 d2.utils.events]: [0m eta: 0:14:45  iter: 419  total_loss: 0.779  loss_cls: 0.249  loss_box_reg: 0.383  loss_rpn_cls: 0.028  loss_rpn_loc: 0.061  time: 0.5563  data_time: 0.0060  lr: 0.004196  max_mem: 3070M
[32m[04/15 07:22:28 d2.utils.events]: [0m eta: 0:14:34  iter: 439  total_loss: 0.570  loss_cls: 0.182  loss_box_reg: 0.318  loss_rpn_cls: 0.027  loss_rpn_loc: 0.045  time: 0.5567  data_time: 0.0082  lr: 0.004396  max_mem: 3070M
[32m[04/15 07:22:39 d2.utils.events]: [0m eta: 0:14:22  iter: 459  total_loss: 0.570  loss_cls: 0.191  loss_box_reg: 0.279  loss_rpn_cls: 0.024  loss_rpn_loc: 0.043  time: 0.5562  data_time: 0.0062  lr: 0.004595  max_mem: 3070M
[32m[04/15 07:22:50 d2.utils.events]: [0m eta: 0:14:06  iter: 479  total_loss: 0.677  loss_cls: 0.217  loss_box_reg: 0.365  loss_rpn_cls: 0.026  loss_rpn_loc: 0.066  time: 0.5554  data_time: 0.0076  lr: 0.004795  max_mem: 3070M
[32m[04/15 07:23:01 d2.utils.events]: [0m eta: 0:13:57  iter: 499  total_loss: 0.745  loss_cls: 0.241  loss_box_reg: 0.384  loss_rpn_cls: 0.033  loss_rpn_loc: 0.071  time: 0.5554  data_time: 0.0067  lr: 0.004995  max_mem: 3070M
[32m[04/15 07:23:13 d2.utils.events]: [0m eta: 0:13:49  iter: 519  total_loss: 0.720  loss_cls: 0.242  loss_box_reg: 0.383  loss_rpn_cls: 0.030  loss_rpn_loc: 0.048  time: 0.5562  data_time: 0.0075  lr: 0.005195  max_mem: 3070M
[32m[04/15 07:23:25 d2.utils.events]: [0m eta: 0:13:38  iter: 539  total_loss: 0.659  loss_cls: 0.209  loss_box_reg: 0.335  loss_rpn_cls: 0.020  loss_rpn_loc: 0.061  time: 0.5565  data_time: 0.0068  lr: 0.005395  max_mem: 3070M
[32m[04/15 07:23:36 d2.utils.events]: [0m eta: 0:13:27  iter: 559  total_loss: 0.547  loss_cls: 0.200  loss_box_reg: 0.300  loss_rpn_cls: 0.025  loss_rpn_loc: 0.046  time: 0.5567  data_time: 0.0065  lr: 0.005594  max_mem: 3070M
[32m[04/15 07:23:47 d2.utils.events]: [0m eta: 0:13:15  iter: 579  total_loss: 0.595  loss_cls: 0.214  loss_box_reg: 0.321  loss_rpn_cls: 0.028  loss_rpn_loc: 0.042  time: 0.5565  data_time: 0.0087  lr: 0.005794  max_mem: 3070M
[32m[04/15 07:23:58 d2.utils.events]: [0m eta: 0:13:05  iter: 599  total_loss: 0.592  loss_cls: 0.217  loss_box_reg: 0.298  loss_rpn_cls: 0.021  loss_rpn_loc: 0.043  time: 0.5567  data_time: 0.0070  lr: 0.005994  max_mem: 3070M
[32m[04/15 07:24:10 d2.utils.events]: [0m eta: 0:12:54  iter: 619  total_loss: 0.646  loss_cls: 0.222  loss_box_reg: 0.341  loss_rpn_cls: 0.029  loss_rpn_loc: 0.053  time: 0.5574  data_time: 0.0062  lr: 0.006194  max_mem: 3070M
[32m[04/15 07:24:21 d2.utils.events]: [0m eta: 0:12:43  iter: 639  total_loss: 0.533  loss_cls: 0.180  loss_box_reg: 0.298  loss_rpn_cls: 0.033  loss_rpn_loc: 0.059  time: 0.5570  data_time: 0.0099  lr: 0.006394  max_mem: 3070M
[32m[04/15 07:24:37 d2.utils.events]: [0m eta: 0:12:32  iter: 659  total_loss: 0.723  loss_cls: 0.221  loss_box_reg: 0.368  loss_rpn_cls: 0.024  loss_rpn_loc: 0.075  time: 0.5598  data_time: 0.1413  lr: 0.006593  max_mem: 3070M
[32m[04/15 07:24:48 d2.utils.events]: [0m eta: 0:12:20  iter: 679  total_loss: 0.636  loss_cls: 0.201  loss_box_reg: 0.352  loss_rpn_cls: 0.032  loss_rpn_loc: 0.051  time: 0.5596  data_time: 0.0086  lr: 0.006793  max_mem: 3070M
[32m[04/15 07:25:00 d2.utils.events]: [0m eta: 0:12:09  iter: 699  total_loss: 0.686  loss_cls: 0.219  loss_box_reg: 0.362  loss_rpn_cls: 0.027  loss_rpn_loc: 0.079  time: 0.5596  data_time: 0.0068  lr: 0.006993  max_mem: 3070M
[32m[04/15 07:25:11 d2.utils.events]: [0m eta: 0:11:58  iter: 719  total_loss: 0.822  loss_cls: 0.234  loss_box_reg: 0.447  loss_rpn_cls: 0.026  loss_rpn_loc: 0.078  time: 0.5599  data_time: 0.0100  lr: 0.007193  max_mem: 3070M
[32m[04/15 07:25:22 d2.utils.events]: [0m eta: 0:11:46  iter: 739  total_loss: 0.703  loss_cls: 0.221  loss_box_reg: 0.321  loss_rpn_cls: 0.030  loss_rpn_loc: 0.078  time: 0.5594  data_time: 0.0068  lr: 0.007393  max_mem: 3070M
[32m[04/15 07:25:33 d2.utils.events]: [0m eta: 0:11:35  iter: 759  total_loss: 0.652  loss_cls: 0.208  loss_box_reg: 0.358  loss_rpn_cls: 0.032  loss_rpn_loc: 0.054  time: 0.5592  data_time: 0.0080  lr: 0.007592  max_mem: 3070M
[32m[04/15 07:25:44 d2.utils.events]: [0m eta: 0:11:24  iter: 779  total_loss: 0.712  loss_cls: 0.253  loss_box_reg: 0.380  loss_rpn_cls: 0.027  loss_rpn_loc: 0.063  time: 0.5590  data_time: 0.0082  lr: 0.007792  max_mem: 3070M
[32m[04/15 07:25:55 d2.utils.events]: [0m eta: 0:11:12  iter: 799  total_loss: 0.681  loss_cls: 0.232  loss_box_reg: 0.350  loss_rpn_cls: 0.023  loss_rpn_loc: 0.059  time: 0.5583  data_time: 0.0077  lr: 0.007992  max_mem: 3070M
[32m[04/15 07:26:06 d2.utils.events]: [0m eta: 0:11:01  iter: 819  total_loss: 0.892  loss_cls: 0.318  loss_box_reg: 0.464  loss_rpn_cls: 0.028  loss_rpn_loc: 0.098  time: 0.5580  data_time: 0.0090  lr: 0.008192  max_mem: 3070M
[32m[04/15 07:26:17 d2.utils.events]: [0m eta: 0:10:49  iter: 839  total_loss: 0.660  loss_cls: 0.203  loss_box_reg: 0.355  loss_rpn_cls: 0.030  loss_rpn_loc: 0.071  time: 0.5577  data_time: 0.0085  lr: 0.008392  max_mem: 3070M
[32m[04/15 07:26:28 d2.utils.events]: [0m eta: 0:10:38  iter: 859  total_loss: 0.721  loss_cls: 0.224  loss_box_reg: 0.382  loss_rpn_cls: 0.034  loss_rpn_loc: 0.080  time: 0.5573  data_time: 0.0070  lr: 0.008591  max_mem: 3070M
[32m[04/15 07:26:39 d2.utils.events]: [0m eta: 0:10:26  iter: 879  total_loss: 0.704  loss_cls: 0.254  loss_box_reg: 0.370  loss_rpn_cls: 0.027  loss_rpn_loc: 0.074  time: 0.5569  data_time: 0.0058  lr: 0.008791  max_mem: 3070M
[32m[04/15 07:26:50 d2.utils.events]: [0m eta: 0:10:15  iter: 899  total_loss: 0.655  loss_cls: 0.227  loss_box_reg: 0.333  loss_rpn_cls: 0.036  loss_rpn_loc: 0.072  time: 0.5568  data_time: 0.0040  lr: 0.008991  max_mem: 3070M
[32m[04/15 07:27:01 d2.utils.events]: [0m eta: 0:10:03  iter: 919  total_loss: 0.739  loss_cls: 0.237  loss_box_reg: 0.364  loss_rpn_cls: 0.034  loss_rpn_loc: 0.079  time: 0.5563  data_time: 0.0073  lr: 0.009191  max_mem: 3070M
[32m[04/15 07:27:13 d2.utils.events]: [0m eta: 0:09:52  iter: 939  total_loss: 0.793  loss_cls: 0.254  loss_box_reg: 0.402  loss_rpn_cls: 0.030  loss_rpn_loc: 0.076  time: 0.5567  data_time: 0.0074  lr: 0.009391  max_mem: 3070M
[32m[04/15 07:27:23 d2.utils.events]: [0m eta: 0:09:41  iter: 959  total_loss: 0.842  loss_cls: 0.294  loss_box_reg: 0.427  loss_rpn_cls: 0.043  loss_rpn_loc: 0.088  time: 0.5563  data_time: 0.0070  lr: 0.009590  max_mem: 3070M
[32m[04/15 07:27:37 d2.utils.events]: [0m eta: 0:09:30  iter: 979  total_loss: 0.697  loss_cls: 0.225  loss_box_reg: 0.377  loss_rpn_cls: 0.030  loss_rpn_loc: 0.069  time: 0.5565  data_time: 0.0070  lr: 0.009790  max_mem: 3070M
[32m[04/15 07:27:49 d2.utils.events]: [0m eta: 0:09:19  iter: 999  total_loss: 0.696  loss_cls: 0.239  loss_box_reg: 0.340  loss_rpn_cls: 0.034  loss_rpn_loc: 0.065  time: 0.5566  data_time: 0.0074  lr: 0.009990  max_mem: 3070M
[32m[04/15 07:28:00 d2.utils.events]: [0m eta: 0:09:08  iter: 1019  total_loss: 0.776  loss_cls: 0.259  loss_box_reg: 0.350  loss_rpn_cls: 0.038  loss_rpn_loc: 0.071  time: 0.5564  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:28:11 d2.utils.events]: [0m eta: 0:08:58  iter: 1039  total_loss: 0.695  loss_cls: 0.228  loss_box_reg: 0.362  loss_rpn_cls: 0.024  loss_rpn_loc: 0.061  time: 0.5565  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:28:22 d2.utils.events]: [0m eta: 0:08:47  iter: 1059  total_loss: 0.817  loss_cls: 0.272  loss_box_reg: 0.418  loss_rpn_cls: 0.050  loss_rpn_loc: 0.060  time: 0.5565  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:28:33 d2.utils.events]: [0m eta: 0:08:35  iter: 1079  total_loss: 0.736  loss_cls: 0.236  loss_box_reg: 0.377  loss_rpn_cls: 0.030  loss_rpn_loc: 0.067  time: 0.5564  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:28:44 d2.utils.events]: [0m eta: 0:08:24  iter: 1099  total_loss: 0.690  loss_cls: 0.226  loss_box_reg: 0.378  loss_rpn_cls: 0.030  loss_rpn_loc: 0.057  time: 0.5563  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:28:56 d2.utils.events]: [0m eta: 0:08:13  iter: 1119  total_loss: 0.761  loss_cls: 0.256  loss_box_reg: 0.373  loss_rpn_cls: 0.032  loss_rpn_loc: 0.095  time: 0.5565  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:29:07 d2.utils.events]: [0m eta: 0:08:01  iter: 1139  total_loss: 0.773  loss_cls: 0.259  loss_box_reg: 0.387  loss_rpn_cls: 0.033  loss_rpn_loc: 0.081  time: 0.5561  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:29:18 d2.utils.events]: [0m eta: 0:07:50  iter: 1159  total_loss: 0.621  loss_cls: 0.187  loss_box_reg: 0.279  loss_rpn_cls: 0.022  loss_rpn_loc: 0.058  time: 0.5560  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:29:29 d2.utils.events]: [0m eta: 0:07:40  iter: 1179  total_loss: 0.673  loss_cls: 0.216  loss_box_reg: 0.334  loss_rpn_cls: 0.039  loss_rpn_loc: 0.063  time: 0.5563  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:29:41 d2.utils.events]: [0m eta: 0:07:28  iter: 1199  total_loss: 0.533  loss_cls: 0.166  loss_box_reg: 0.302  loss_rpn_cls: 0.036  loss_rpn_loc: 0.048  time: 0.5564  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:29:52 d2.utils.events]: [0m eta: 0:07:17  iter: 1219  total_loss: 0.656  loss_cls: 0.228  loss_box_reg: 0.332  loss_rpn_cls: 0.035  loss_rpn_loc: 0.060  time: 0.5566  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:30:04 d2.utils.events]: [0m eta: 0:07:06  iter: 1239  total_loss: 0.828  loss_cls: 0.275  loss_box_reg: 0.416  loss_rpn_cls: 0.041  loss_rpn_loc: 0.085  time: 0.5566  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:30:15 d2.utils.events]: [0m eta: 0:06:54  iter: 1259  total_loss: 0.754  loss_cls: 0.252  loss_box_reg: 0.383  loss_rpn_cls: 0.040  loss_rpn_loc: 0.069  time: 0.5567  data_time: 0.0093  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:30:26 d2.utils.events]: [0m eta: 0:06:43  iter: 1279  total_loss: 0.575  loss_cls: 0.195  loss_box_reg: 0.291  loss_rpn_cls: 0.038  loss_rpn_loc: 0.055  time: 0.5565  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:30:37 d2.utils.events]: [0m eta: 0:06:31  iter: 1299  total_loss: 0.718  loss_cls: 0.213  loss_box_reg: 0.381  loss_rpn_cls: 0.037  loss_rpn_loc: 0.074  time: 0.5567  data_time: 0.0061  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:30:49 d2.utils.events]: [0m eta: 0:06:20  iter: 1319  total_loss: 0.774  loss_cls: 0.263  loss_box_reg: 0.405  loss_rpn_cls: 0.039  loss_rpn_loc: 0.082  time: 0.5566  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:30:59 d2.utils.events]: [0m eta: 0:06:08  iter: 1339  total_loss: 0.682  loss_cls: 0.229  loss_box_reg: 0.301  loss_rpn_cls: 0.036  loss_rpn_loc: 0.063  time: 0.5564  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:31:11 d2.utils.events]: [0m eta: 0:05:57  iter: 1359  total_loss: 0.574  loss_cls: 0.181  loss_box_reg: 0.280  loss_rpn_cls: 0.046  loss_rpn_loc: 0.077  time: 0.5564  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:31:22 d2.utils.events]: [0m eta: 0:05:46  iter: 1379  total_loss: 0.652  loss_cls: 0.215  loss_box_reg: 0.335  loss_rpn_cls: 0.030  loss_rpn_loc: 0.066  time: 0.5563  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:31:33 d2.utils.events]: [0m eta: 0:05:35  iter: 1399  total_loss: 0.691  loss_cls: 0.242  loss_box_reg: 0.334  loss_rpn_cls: 0.039  loss_rpn_loc: 0.063  time: 0.5563  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:31:45 d2.utils.events]: [0m eta: 0:05:24  iter: 1419  total_loss: 0.791  loss_cls: 0.259  loss_box_reg: 0.395  loss_rpn_cls: 0.036  loss_rpn_loc: 0.082  time: 0.5564  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:31:56 d2.utils.events]: [0m eta: 0:05:13  iter: 1439  total_loss: 0.657  loss_cls: 0.221  loss_box_reg: 0.339  loss_rpn_cls: 0.028  loss_rpn_loc: 0.061  time: 0.5563  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:32:07 d2.utils.events]: [0m eta: 0:05:02  iter: 1459  total_loss: 0.822  loss_cls: 0.265  loss_box_reg: 0.433  loss_rpn_cls: 0.036  loss_rpn_loc: 0.080  time: 0.5564  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:32:18 d2.utils.events]: [0m eta: 0:04:50  iter: 1479  total_loss: 0.687  loss_cls: 0.228  loss_box_reg: 0.354  loss_rpn_cls: 0.023  loss_rpn_loc: 0.053  time: 0.5561  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:32:29 d2.utils.events]: [0m eta: 0:04:39  iter: 1499  total_loss: 0.804  loss_cls: 0.263  loss_box_reg: 0.439  loss_rpn_cls: 0.034  loss_rpn_loc: 0.087  time: 0.5563  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:32:40 d2.utils.events]: [0m eta: 0:04:28  iter: 1519  total_loss: 0.734  loss_cls: 0.215  loss_box_reg: 0.375  loss_rpn_cls: 0.027  loss_rpn_loc: 0.080  time: 0.5562  data_time: 0.0095  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:32:51 d2.utils.events]: [0m eta: 0:04:17  iter: 1539  total_loss: 0.754  loss_cls: 0.231  loss_box_reg: 0.372  loss_rpn_cls: 0.026  loss_rpn_loc: 0.085  time: 0.5560  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:33:03 d2.utils.events]: [0m eta: 0:04:06  iter: 1559  total_loss: 0.702  loss_cls: 0.215  loss_box_reg: 0.353  loss_rpn_cls: 0.025  loss_rpn_loc: 0.065  time: 0.5562  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:33:14 d2.utils.events]: [0m eta: 0:03:55  iter: 1579  total_loss: 0.811  loss_cls: 0.261  loss_box_reg: 0.403  loss_rpn_cls: 0.042  loss_rpn_loc: 0.080  time: 0.5563  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:33:26 d2.utils.events]: [0m eta: 0:03:43  iter: 1599  total_loss: 0.692  loss_cls: 0.230  loss_box_reg: 0.336  loss_rpn_cls: 0.039  loss_rpn_loc: 0.066  time: 0.5564  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:33:37 d2.utils.events]: [0m eta: 0:03:32  iter: 1619  total_loss: 0.522  loss_cls: 0.179  loss_box_reg: 0.262  loss_rpn_cls: 0.033  loss_rpn_loc: 0.055  time: 0.5563  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:33:48 d2.utils.events]: [0m eta: 0:03:20  iter: 1639  total_loss: 0.820  loss_cls: 0.260  loss_box_reg: 0.422  loss_rpn_cls: 0.036  loss_rpn_loc: 0.075  time: 0.5562  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:33:59 d2.utils.events]: [0m eta: 0:03:09  iter: 1659  total_loss: 0.719  loss_cls: 0.224  loss_box_reg: 0.362  loss_rpn_cls: 0.034  loss_rpn_loc: 0.060  time: 0.5563  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:34:10 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.769  loss_cls: 0.252  loss_box_reg: 0.396  loss_rpn_cls: 0.046  loss_rpn_loc: 0.068  time: 0.5562  data_time: 0.0108  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:34:25 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.660  loss_cls: 0.222  loss_box_reg: 0.331  loss_rpn_cls: 0.042  loss_rpn_loc: 0.071  time: 0.5565  data_time: 0.0572  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:34:36 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.710  loss_cls: 0.220  loss_box_reg: 0.367  loss_rpn_cls: 0.037  loss_rpn_loc: 0.077  time: 0.5563  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:34:47 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.701  loss_cls: 0.201  loss_box_reg: 0.309  loss_rpn_cls: 0.032  loss_rpn_loc: 0.063  time: 0.5561  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:34:58 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.583  loss_cls: 0.184  loss_box_reg: 0.300  loss_rpn_cls: 0.046  loss_rpn_loc: 0.057  time: 0.5560  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:35:15 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.668  loss_cls: 0.244  loss_box_reg: 0.324  loss_rpn_cls: 0.032  loss_rpn_loc: 0.054  time: 0.5559  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:35:26 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.721  loss_cls: 0.223  loss_box_reg: 0.377  loss_rpn_cls: 0.046  loss_rpn_loc: 0.073  time: 0.5557  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:35:37 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.580  loss_cls: 0.201  loss_box_reg: 0.300  loss_rpn_cls: 0.035  loss_rpn_loc: 0.059  time: 0.5556  data_time: 0.0244  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:35:48 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.694  loss_cls: 0.219  loss_box_reg: 0.368  loss_rpn_cls: 0.043  loss_rpn_loc: 0.062  time: 0.5554  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:35:59 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.799  loss_cls: 0.259  loss_box_reg: 0.388  loss_rpn_cls: 0.043  loss_rpn_loc: 0.077  time: 0.5553  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:36:10 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.626  loss_cls: 0.205  loss_box_reg: 0.328  loss_rpn_cls: 0.035  loss_rpn_loc: 0.069  time: 0.5552  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:36:21 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.644  loss_cls: 0.231  loss_box_reg: 0.344  loss_rpn_cls: 0.023  loss_rpn_loc: 0.047  time: 0.5551  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:36:32 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.783  loss_cls: 0.259  loss_box_reg: 0.379  loss_rpn_cls: 0.038  loss_rpn_loc: 0.067  time: 0.5550  data_time: 0.0089  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:36:43 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.629  loss_cls: 0.225  loss_box_reg: 0.349  loss_rpn_cls: 0.037  loss_rpn_loc: 0.060  time: 0.5551  data_time: 0.0111  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:36:55 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.787  loss_cls: 0.266  loss_box_reg: 0.396  loss_rpn_cls: 0.029  loss_rpn_loc: 0.071  time: 0.5551  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:37:06 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.653  loss_cls: 0.222  loss_box_reg: 0.329  loss_rpn_cls: 0.027  loss_rpn_loc: 0.063  time: 0.5552  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 07:37:31 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 07:37:31 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 07:37:31 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 07:37:31 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.746  loss_cls: 0.262  loss_box_reg: 0.373  loss_rpn_cls: 0.035  loss_rpn_loc: 0.077  time: 0.5552  data_time: 0.0090  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:37:35 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:29 (0.5556 s / it)
[32m[04/15 07:37:35 d2.engine.hooks]: [0mTotal training time: 0:19:11 (0:00:42 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 07:37:47 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 07:37:47 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 07:37:47 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 07:37:49 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1206 s / img. ETA=0:02:33
[32m[04/15 07:37:54 d2.evaluation.evaluator]: [0mInference done 51/1257. 0.1218 s / img. ETA=0:02:30
[32m[04/15 07:37:59 d2.evaluation.evaluator]: [0mInference done 91/1257. 0.1221 s / img. ETA=0:02:26
[32m[04/15 07:38:04 d2.evaluation.evaluator]: [0mInference done 131/1257. 0.1224 s / img. ETA=0:02:21
[32m[04/15 07:38:09 d2.evaluation.evaluator]: [0mInference done 171/1257. 0.1224 s / img. ETA=0:02:16
[32m[04/15 07:38:14 d2.evaluation.evaluator]: [0mInference done 211/1257. 0.1226 s / img. ETA=0:02:11
[32m[04/15 07:38:19 d2.evaluation.evaluator]: [0mInference done 251/1257. 0.1227 s / img. ETA=0:02:06
[32m[04/15 07:38:24 d2.evaluation.evaluator]: [0mInference done 291/1257. 0.1226 s / img. ETA=0:02:01
[32m[04/15 07:38:29 d2.evaluation.evaluator]: [0mInference done 331/1257. 0.1225 s / img. ETA=0:01:56
[32m[04/15 07:38:34 d2.evaluation.evaluator]: [0mInference done 371/1257. 0.1224 s / img. ETA=0:01:51
[32m[04/15 07:38:39 d2.evaluation.evaluator]: [0mInference done 411/1257. 0.1225 s / img. ETA=0:01:46
[32m[04/15 07:38:44 d2.evaluation.evaluator]: [0mInference done 451/1257. 0.1225 s / img. ETA=0:01:41
[32m[04/15 07:38:49 d2.evaluation.evaluator]: [0mInference done 491/1257. 0.1225 s / img. ETA=0:01:36
[32m[04/15 07:38:54 d2.evaluation.evaluator]: [0mInference done 531/1257. 0.1227 s / img. ETA=0:01:31
[32m[04/15 07:38:59 d2.evaluation.evaluator]: [0mInference done 570/1257. 0.1229 s / img. ETA=0:01:26
[32m[04/15 07:39:05 d2.evaluation.evaluator]: [0mInference done 609/1257. 0.1231 s / img. ETA=0:01:22
[32m[04/15 07:39:10 d2.evaluation.evaluator]: [0mInference done 648/1257. 0.1234 s / img. ETA=0:01:17
[32m[04/15 07:39:15 d2.evaluation.evaluator]: [0mInference done 687/1257. 0.1236 s / img. ETA=0:01:12
[32m[04/15 07:39:20 d2.evaluation.evaluator]: [0mInference done 726/1257. 0.1237 s / img. ETA=0:01:07
[32m[04/15 07:39:25 d2.evaluation.evaluator]: [0mInference done 766/1257. 0.1237 s / img. ETA=0:01:02
[32m[04/15 07:39:30 d2.evaluation.evaluator]: [0mInference done 805/1257. 0.1238 s / img. ETA=0:00:57
[32m[04/15 07:39:35 d2.evaluation.evaluator]: [0mInference done 844/1257. 0.1239 s / img. ETA=0:00:52
[32m[04/15 07:39:40 d2.evaluation.evaluator]: [0mInference done 883/1257. 0.1240 s / img. ETA=0:00:47
[32m[04/15 07:39:45 d2.evaluation.evaluator]: [0mInference done 922/1257. 0.1242 s / img. ETA=0:00:42
[32m[04/15 07:39:50 d2.evaluation.evaluator]: [0mInference done 961/1257. 0.1242 s / img. ETA=0:00:37
[32m[04/15 07:39:55 d2.evaluation.evaluator]: [0mInference done 1000/1257. 0.1243 s / img. ETA=0:00:32
[32m[04/15 07:40:00 d2.evaluation.evaluator]: [0mInference done 1039/1257. 0.1243 s / img. ETA=0:00:27
[32m[04/15 07:40:05 d2.evaluation.evaluator]: [0mInference done 1078/1257. 0.1244 s / img. ETA=0:00:22
[32m[04/15 07:40:10 d2.evaluation.evaluator]: [0mInference done 1117/1257. 0.1244 s / img. ETA=0:00:17
[32m[04/15 07:40:15 d2.evaluation.evaluator]: [0mInference done 1156/1257. 0.1244 s / img. ETA=0:00:12
[32m[04/15 07:40:20 d2.evaluation.evaluator]: [0mInference done 1191/1257. 0.1246 s / img. ETA=0:00:08
[32m[04/15 07:40:26 d2.evaluation.evaluator]: [0mInference done 1197/1257. 0.1246 s / img. ETA=0:00:07
[32m[04/15 07:40:31 d2.evaluation.evaluator]: [0mInference done 1221/1257. 0.1246 s / img. ETA=0:00:04
[32m[04/15 07:40:36 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:47.844889 (0.134061 s / img per device, on 1 devices)
[32m[04/15 07:40:36 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:35 (0.124575 s / img per device, on 1 devices)
[32m[04/15 07:40:36 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 07:40:36 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 07:40:36 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.03s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.67s).
Accumulating evaluation results...
DONE (t=0.78s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.177
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.353
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.158
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.098
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.225
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.385
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.217
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.132
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.274
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.421
[32m[04/15 07:40:43 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 17.651 | 35.269 | 15.823 | 9.819 | 22.503 | 38.469 |
[32m[04/15 07:40:43 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 25.720 | bicycle       | 6.211 | car            | 38.146 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.528 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  6  *  2000  iterations ============
4 channel input
[32m[04/15 07:40:44 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 07:40:45 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.26 seconds.
[5m[31mWARNING[0m [32m[04/15 07:40:45 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 07:40:46 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 07:40:46 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 07:40:46 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 07:40:46 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 07:40:51 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 07:41:03 d2.utils.events]: [0m eta: 0:17:32  iter: 19  total_loss: 0.682  loss_cls: 0.231  loss_box_reg: 0.327  loss_rpn_cls: 0.038  loss_rpn_loc: 0.070  time: 0.5361  data_time: 0.0356  lr: 0.000200  max_mem: 3070M
[32m[04/15 07:41:13 d2.utils.events]: [0m eta: 0:17:17  iter: 39  total_loss: 0.645  loss_cls: 0.221  loss_box_reg: 0.333  loss_rpn_cls: 0.029  loss_rpn_loc: 0.058  time: 0.5332  data_time: 0.0120  lr: 0.000400  max_mem: 3070M
[32m[04/15 07:41:24 d2.utils.events]: [0m eta: 0:17:12  iter: 59  total_loss: 0.737  loss_cls: 0.241  loss_box_reg: 0.376  loss_rpn_cls: 0.040  loss_rpn_loc: 0.074  time: 0.5373  data_time: 0.0090  lr: 0.000599  max_mem: 3070M
[32m[04/15 07:41:35 d2.utils.events]: [0m eta: 0:16:59  iter: 79  total_loss: 0.698  loss_cls: 0.229  loss_box_reg: 0.336  loss_rpn_cls: 0.029  loss_rpn_loc: 0.067  time: 0.5330  data_time: 0.0101  lr: 0.000799  max_mem: 3070M
[32m[04/15 07:41:46 d2.utils.events]: [0m eta: 0:17:00  iter: 99  total_loss: 0.808  loss_cls: 0.256  loss_box_reg: 0.416  loss_rpn_cls: 0.030  loss_rpn_loc: 0.086  time: 0.5396  data_time: 0.0067  lr: 0.000999  max_mem: 3070M
[32m[04/15 07:41:57 d2.utils.events]: [0m eta: 0:16:58  iter: 119  total_loss: 0.694  loss_cls: 0.227  loss_box_reg: 0.380  loss_rpn_cls: 0.021  loss_rpn_loc: 0.058  time: 0.5411  data_time: 0.0121  lr: 0.001199  max_mem: 3070M
[32m[04/15 07:42:08 d2.utils.events]: [0m eta: 0:16:47  iter: 139  total_loss: 0.789  loss_cls: 0.233  loss_box_reg: 0.374  loss_rpn_cls: 0.030  loss_rpn_loc: 0.077  time: 0.5422  data_time: 0.0071  lr: 0.001399  max_mem: 3070M
[32m[04/15 07:42:19 d2.utils.events]: [0m eta: 0:16:40  iter: 159  total_loss: 0.791  loss_cls: 0.224  loss_box_reg: 0.332  loss_rpn_cls: 0.023  loss_rpn_loc: 0.071  time: 0.5423  data_time: 0.0084  lr: 0.001598  max_mem: 3070M
[32m[04/15 07:42:31 d2.utils.events]: [0m eta: 0:16:31  iter: 179  total_loss: 0.573  loss_cls: 0.207  loss_box_reg: 0.285  loss_rpn_cls: 0.028  loss_rpn_loc: 0.044  time: 0.5449  data_time: 0.0077  lr: 0.001798  max_mem: 3070M
[32m[04/15 07:42:42 d2.utils.events]: [0m eta: 0:16:24  iter: 199  total_loss: 0.609  loss_cls: 0.206  loss_box_reg: 0.315  loss_rpn_cls: 0.020  loss_rpn_loc: 0.045  time: 0.5459  data_time: 0.0064  lr: 0.001998  max_mem: 3070M
[32m[04/15 07:42:53 d2.utils.events]: [0m eta: 0:16:18  iter: 219  total_loss: 0.649  loss_cls: 0.212  loss_box_reg: 0.361  loss_rpn_cls: 0.019  loss_rpn_loc: 0.054  time: 0.5471  data_time: 0.0096  lr: 0.002198  max_mem: 3070M
[32m[04/15 07:43:04 d2.utils.events]: [0m eta: 0:16:06  iter: 239  total_loss: 0.732  loss_cls: 0.226  loss_box_reg: 0.329  loss_rpn_cls: 0.027  loss_rpn_loc: 0.058  time: 0.5478  data_time: 0.0066  lr: 0.002398  max_mem: 3070M
[32m[04/15 07:43:16 d2.utils.events]: [0m eta: 0:15:56  iter: 259  total_loss: 0.674  loss_cls: 0.226  loss_box_reg: 0.343  loss_rpn_cls: 0.029  loss_rpn_loc: 0.060  time: 0.5487  data_time: 0.0078  lr: 0.002597  max_mem: 3070M
[32m[04/15 07:43:27 d2.utils.events]: [0m eta: 0:15:45  iter: 279  total_loss: 0.815  loss_cls: 0.272  loss_box_reg: 0.423  loss_rpn_cls: 0.022  loss_rpn_loc: 0.067  time: 0.5506  data_time: 0.0077  lr: 0.002797  max_mem: 3070M
[32m[04/15 07:43:39 d2.utils.events]: [0m eta: 0:15:35  iter: 299  total_loss: 0.636  loss_cls: 0.229  loss_box_reg: 0.337  loss_rpn_cls: 0.022  loss_rpn_loc: 0.044  time: 0.5517  data_time: 0.0070  lr: 0.002997  max_mem: 3070M
[32m[04/15 07:43:50 d2.utils.events]: [0m eta: 0:15:25  iter: 319  total_loss: 0.808  loss_cls: 0.244  loss_box_reg: 0.403  loss_rpn_cls: 0.028  loss_rpn_loc: 0.082  time: 0.5514  data_time: 0.0071  lr: 0.003197  max_mem: 3070M
[32m[04/15 07:44:01 d2.utils.events]: [0m eta: 0:15:15  iter: 339  total_loss: 0.683  loss_cls: 0.218  loss_box_reg: 0.333  loss_rpn_cls: 0.029  loss_rpn_loc: 0.047  time: 0.5514  data_time: 0.0083  lr: 0.003397  max_mem: 3070M
[32m[04/15 07:44:12 d2.utils.events]: [0m eta: 0:15:06  iter: 359  total_loss: 0.703  loss_cls: 0.221  loss_box_reg: 0.374  loss_rpn_cls: 0.025  loss_rpn_loc: 0.057  time: 0.5524  data_time: 0.0096  lr: 0.003596  max_mem: 3070M
[32m[04/15 07:44:24 d2.utils.events]: [0m eta: 0:14:55  iter: 379  total_loss: 0.627  loss_cls: 0.214  loss_box_reg: 0.310  loss_rpn_cls: 0.022  loss_rpn_loc: 0.058  time: 0.5527  data_time: 0.0079  lr: 0.003796  max_mem: 3070M
[32m[04/15 07:44:35 d2.utils.events]: [0m eta: 0:14:45  iter: 399  total_loss: 0.711  loss_cls: 0.251  loss_box_reg: 0.380  loss_rpn_cls: 0.023  loss_rpn_loc: 0.065  time: 0.5535  data_time: 0.0087  lr: 0.003996  max_mem: 3070M
[32m[04/15 07:44:47 d2.utils.events]: [0m eta: 0:14:35  iter: 419  total_loss: 0.770  loss_cls: 0.274  loss_box_reg: 0.403  loss_rpn_cls: 0.026  loss_rpn_loc: 0.071  time: 0.5538  data_time: 0.0097  lr: 0.004196  max_mem: 3070M
[32m[04/15 07:44:57 d2.utils.events]: [0m eta: 0:14:23  iter: 439  total_loss: 0.700  loss_cls: 0.220  loss_box_reg: 0.379  loss_rpn_cls: 0.022  loss_rpn_loc: 0.068  time: 0.5533  data_time: 0.0104  lr: 0.004396  max_mem: 3070M
[32m[04/15 07:45:09 d2.utils.events]: [0m eta: 0:14:14  iter: 459  total_loss: 0.654  loss_cls: 0.226  loss_box_reg: 0.362  loss_rpn_cls: 0.026  loss_rpn_loc: 0.066  time: 0.5538  data_time: 0.0071  lr: 0.004595  max_mem: 3070M
[32m[04/15 07:45:20 d2.utils.events]: [0m eta: 0:14:03  iter: 479  total_loss: 0.574  loss_cls: 0.203  loss_box_reg: 0.316  loss_rpn_cls: 0.026  loss_rpn_loc: 0.042  time: 0.5539  data_time: 0.0082  lr: 0.004795  max_mem: 3070M
[32m[04/15 07:45:31 d2.utils.events]: [0m eta: 0:13:52  iter: 499  total_loss: 0.585  loss_cls: 0.182  loss_box_reg: 0.311  loss_rpn_cls: 0.025  loss_rpn_loc: 0.061  time: 0.5539  data_time: 0.0079  lr: 0.004995  max_mem: 3070M
[32m[04/15 07:45:43 d2.utils.events]: [0m eta: 0:13:41  iter: 519  total_loss: 0.639  loss_cls: 0.190  loss_box_reg: 0.315  loss_rpn_cls: 0.018  loss_rpn_loc: 0.059  time: 0.5543  data_time: 0.0088  lr: 0.005195  max_mem: 3070M
[32m[04/15 07:45:54 d2.utils.events]: [0m eta: 0:13:31  iter: 539  total_loss: 0.742  loss_cls: 0.218  loss_box_reg: 0.360  loss_rpn_cls: 0.027  loss_rpn_loc: 0.092  time: 0.5548  data_time: 0.0092  lr: 0.005395  max_mem: 3070M
[32m[04/15 07:46:05 d2.utils.events]: [0m eta: 0:13:19  iter: 559  total_loss: 0.602  loss_cls: 0.213  loss_box_reg: 0.311  loss_rpn_cls: 0.023  loss_rpn_loc: 0.052  time: 0.5544  data_time: 0.0067  lr: 0.005594  max_mem: 3070M
[32m[04/15 07:46:16 d2.utils.events]: [0m eta: 0:13:08  iter: 579  total_loss: 0.616  loss_cls: 0.195  loss_box_reg: 0.342  loss_rpn_cls: 0.024  loss_rpn_loc: 0.053  time: 0.5543  data_time: 0.0093  lr: 0.005794  max_mem: 3070M
[32m[04/15 07:46:27 d2.utils.events]: [0m eta: 0:12:57  iter: 599  total_loss: 0.704  loss_cls: 0.209  loss_box_reg: 0.381  loss_rpn_cls: 0.023  loss_rpn_loc: 0.059  time: 0.5545  data_time: 0.0069  lr: 0.005994  max_mem: 3070M
[32m[04/15 07:46:39 d2.utils.events]: [0m eta: 0:12:48  iter: 619  total_loss: 0.706  loss_cls: 0.219  loss_box_reg: 0.357  loss_rpn_cls: 0.025  loss_rpn_loc: 0.053  time: 0.5555  data_time: 0.0068  lr: 0.006194  max_mem: 3070M
[32m[04/15 07:46:51 d2.utils.events]: [0m eta: 0:12:38  iter: 639  total_loss: 0.623  loss_cls: 0.194  loss_box_reg: 0.313  loss_rpn_cls: 0.023  loss_rpn_loc: 0.054  time: 0.5560  data_time: 0.0072  lr: 0.006394  max_mem: 3070M
[32m[04/15 07:47:02 d2.utils.events]: [0m eta: 0:12:26  iter: 659  total_loss: 0.696  loss_cls: 0.232  loss_box_reg: 0.354  loss_rpn_cls: 0.028  loss_rpn_loc: 0.062  time: 0.5559  data_time: 0.0080  lr: 0.006593  max_mem: 3070M
[32m[04/15 07:47:13 d2.utils.events]: [0m eta: 0:12:16  iter: 679  total_loss: 0.630  loss_cls: 0.204  loss_box_reg: 0.350  loss_rpn_cls: 0.022  loss_rpn_loc: 0.046  time: 0.5560  data_time: 0.0071  lr: 0.006793  max_mem: 3070M
[32m[04/15 07:47:25 d2.utils.events]: [0m eta: 0:12:06  iter: 699  total_loss: 0.728  loss_cls: 0.222  loss_box_reg: 0.402  loss_rpn_cls: 0.022  loss_rpn_loc: 0.056  time: 0.5568  data_time: 0.0063  lr: 0.006993  max_mem: 3070M
[32m[04/15 07:47:36 d2.utils.events]: [0m eta: 0:11:55  iter: 719  total_loss: 0.731  loss_cls: 0.236  loss_box_reg: 0.370  loss_rpn_cls: 0.032  loss_rpn_loc: 0.074  time: 0.5568  data_time: 0.0074  lr: 0.007193  max_mem: 3070M
[32m[04/15 07:47:47 d2.utils.events]: [0m eta: 0:11:43  iter: 739  total_loss: 0.698  loss_cls: 0.232  loss_box_reg: 0.374  loss_rpn_cls: 0.028  loss_rpn_loc: 0.051  time: 0.5568  data_time: 0.0077  lr: 0.007393  max_mem: 3070M
[32m[04/15 07:47:59 d2.utils.events]: [0m eta: 0:11:32  iter: 759  total_loss: 0.745  loss_cls: 0.235  loss_box_reg: 0.375  loss_rpn_cls: 0.028  loss_rpn_loc: 0.077  time: 0.5568  data_time: 0.0062  lr: 0.007592  max_mem: 3070M
[32m[04/15 07:48:09 d2.utils.events]: [0m eta: 0:11:20  iter: 779  total_loss: 0.743  loss_cls: 0.256  loss_box_reg: 0.386  loss_rpn_cls: 0.030  loss_rpn_loc: 0.067  time: 0.5563  data_time: 0.0070  lr: 0.007792  max_mem: 3070M
[32m[04/15 07:48:20 d2.utils.events]: [0m eta: 0:11:09  iter: 799  total_loss: 0.695  loss_cls: 0.224  loss_box_reg: 0.391  loss_rpn_cls: 0.025  loss_rpn_loc: 0.059  time: 0.5561  data_time: 0.0064  lr: 0.007992  max_mem: 3070M
[32m[04/15 07:48:32 d2.utils.events]: [0m eta: 0:10:57  iter: 819  total_loss: 0.574  loss_cls: 0.192  loss_box_reg: 0.296  loss_rpn_cls: 0.035  loss_rpn_loc: 0.060  time: 0.5559  data_time: 0.0068  lr: 0.008192  max_mem: 3070M
[32m[04/15 07:48:43 d2.utils.events]: [0m eta: 0:10:47  iter: 839  total_loss: 0.668  loss_cls: 0.232  loss_box_reg: 0.334  loss_rpn_cls: 0.033  loss_rpn_loc: 0.066  time: 0.5563  data_time: 0.0068  lr: 0.008392  max_mem: 3070M
[32m[04/15 07:48:55 d2.utils.events]: [0m eta: 0:10:36  iter: 859  total_loss: 0.739  loss_cls: 0.242  loss_box_reg: 0.377  loss_rpn_cls: 0.030  loss_rpn_loc: 0.074  time: 0.5566  data_time: 0.0079  lr: 0.008591  max_mem: 3070M
[32m[04/15 07:49:06 d2.utils.events]: [0m eta: 0:10:25  iter: 879  total_loss: 0.783  loss_cls: 0.254  loss_box_reg: 0.421  loss_rpn_cls: 0.033  loss_rpn_loc: 0.078  time: 0.5565  data_time: 0.0071  lr: 0.008791  max_mem: 3070M
[32m[04/15 07:49:17 d2.utils.events]: [0m eta: 0:10:14  iter: 899  total_loss: 0.692  loss_cls: 0.227  loss_box_reg: 0.335  loss_rpn_cls: 0.036  loss_rpn_loc: 0.066  time: 0.5567  data_time: 0.0066  lr: 0.008991  max_mem: 3070M
[32m[04/15 07:49:28 d2.utils.events]: [0m eta: 0:10:03  iter: 919  total_loss: 0.728  loss_cls: 0.237  loss_box_reg: 0.351  loss_rpn_cls: 0.032  loss_rpn_loc: 0.065  time: 0.5567  data_time: 0.0085  lr: 0.009191  max_mem: 3070M
[32m[04/15 07:49:40 d2.utils.events]: [0m eta: 0:09:51  iter: 939  total_loss: 0.814  loss_cls: 0.261  loss_box_reg: 0.444  loss_rpn_cls: 0.042  loss_rpn_loc: 0.069  time: 0.5567  data_time: 0.0079  lr: 0.009391  max_mem: 3070M
[32m[04/15 07:49:51 d2.utils.events]: [0m eta: 0:09:40  iter: 959  total_loss: 0.704  loss_cls: 0.243  loss_box_reg: 0.382  loss_rpn_cls: 0.029  loss_rpn_loc: 0.075  time: 0.5568  data_time: 0.0064  lr: 0.009590  max_mem: 3070M
[32m[04/15 07:50:02 d2.utils.events]: [0m eta: 0:09:29  iter: 979  total_loss: 0.735  loss_cls: 0.244  loss_box_reg: 0.384  loss_rpn_cls: 0.032  loss_rpn_loc: 0.070  time: 0.5567  data_time: 0.0077  lr: 0.009790  max_mem: 3070M
[32m[04/15 07:50:13 d2.utils.events]: [0m eta: 0:09:17  iter: 999  total_loss: 0.794  loss_cls: 0.247  loss_box_reg: 0.393  loss_rpn_cls: 0.043  loss_rpn_loc: 0.080  time: 0.5564  data_time: 0.0065  lr: 0.009990  max_mem: 3070M
[32m[04/15 07:50:25 d2.utils.events]: [0m eta: 0:09:07  iter: 1019  total_loss: 0.781  loss_cls: 0.231  loss_box_reg: 0.390  loss_rpn_cls: 0.044  loss_rpn_loc: 0.076  time: 0.5569  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:50:36 d2.utils.events]: [0m eta: 0:08:56  iter: 1039  total_loss: 0.759  loss_cls: 0.263  loss_box_reg: 0.393  loss_rpn_cls: 0.032  loss_rpn_loc: 0.071  time: 0.5567  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:50:47 d2.utils.events]: [0m eta: 0:08:45  iter: 1059  total_loss: 0.836  loss_cls: 0.264  loss_box_reg: 0.414  loss_rpn_cls: 0.041  loss_rpn_loc: 0.106  time: 0.5570  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:50:58 d2.utils.events]: [0m eta: 0:08:35  iter: 1079  total_loss: 0.657  loss_cls: 0.222  loss_box_reg: 0.341  loss_rpn_cls: 0.027  loss_rpn_loc: 0.063  time: 0.5567  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:51:10 d2.utils.events]: [0m eta: 0:08:23  iter: 1099  total_loss: 0.727  loss_cls: 0.223  loss_box_reg: 0.402  loss_rpn_cls: 0.027  loss_rpn_loc: 0.071  time: 0.5569  data_time: 0.0087  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:51:21 d2.utils.events]: [0m eta: 0:08:13  iter: 1119  total_loss: 0.739  loss_cls: 0.231  loss_box_reg: 0.372  loss_rpn_cls: 0.027  loss_rpn_loc: 0.065  time: 0.5572  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:51:33 d2.utils.events]: [0m eta: 0:08:02  iter: 1139  total_loss: 0.586  loss_cls: 0.207  loss_box_reg: 0.322  loss_rpn_cls: 0.026  loss_rpn_loc: 0.060  time: 0.5574  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:51:44 d2.utils.events]: [0m eta: 0:07:52  iter: 1159  total_loss: 0.625  loss_cls: 0.193  loss_box_reg: 0.314  loss_rpn_cls: 0.027  loss_rpn_loc: 0.064  time: 0.5577  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:51:55 d2.utils.events]: [0m eta: 0:07:40  iter: 1179  total_loss: 0.673  loss_cls: 0.240  loss_box_reg: 0.359  loss_rpn_cls: 0.033  loss_rpn_loc: 0.052  time: 0.5575  data_time: 0.0116  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:52:06 d2.utils.events]: [0m eta: 0:07:29  iter: 1199  total_loss: 0.659  loss_cls: 0.193  loss_box_reg: 0.358  loss_rpn_cls: 0.019  loss_rpn_loc: 0.068  time: 0.5575  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:52:18 d2.utils.events]: [0m eta: 0:07:19  iter: 1219  total_loss: 0.609  loss_cls: 0.190  loss_box_reg: 0.333  loss_rpn_cls: 0.029  loss_rpn_loc: 0.060  time: 0.5580  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:52:29 d2.utils.events]: [0m eta: 0:07:08  iter: 1239  total_loss: 0.676  loss_cls: 0.220  loss_box_reg: 0.367  loss_rpn_cls: 0.038  loss_rpn_loc: 0.071  time: 0.5581  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:52:40 d2.utils.events]: [0m eta: 0:06:56  iter: 1259  total_loss: 0.653  loss_cls: 0.207  loss_box_reg: 0.352  loss_rpn_cls: 0.029  loss_rpn_loc: 0.058  time: 0.5578  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:52:52 d2.utils.events]: [0m eta: 0:06:45  iter: 1279  total_loss: 0.659  loss_cls: 0.247  loss_box_reg: 0.335  loss_rpn_cls: 0.039  loss_rpn_loc: 0.052  time: 0.5580  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:53:03 d2.utils.events]: [0m eta: 0:06:34  iter: 1299  total_loss: 0.745  loss_cls: 0.266  loss_box_reg: 0.402  loss_rpn_cls: 0.026  loss_rpn_loc: 0.081  time: 0.5578  data_time: 0.0094  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:53:14 d2.utils.events]: [0m eta: 0:06:22  iter: 1319  total_loss: 0.739  loss_cls: 0.267  loss_box_reg: 0.396  loss_rpn_cls: 0.023  loss_rpn_loc: 0.069  time: 0.5576  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:53:25 d2.utils.events]: [0m eta: 0:06:11  iter: 1339  total_loss: 0.624  loss_cls: 0.218  loss_box_reg: 0.345  loss_rpn_cls: 0.036  loss_rpn_loc: 0.067  time: 0.5577  data_time: 0.0097  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:53:36 d2.utils.events]: [0m eta: 0:06:00  iter: 1359  total_loss: 0.646  loss_cls: 0.221  loss_box_reg: 0.331  loss_rpn_cls: 0.030  loss_rpn_loc: 0.068  time: 0.5575  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:53:47 d2.utils.events]: [0m eta: 0:05:49  iter: 1379  total_loss: 0.724  loss_cls: 0.244  loss_box_reg: 0.368  loss_rpn_cls: 0.030  loss_rpn_loc: 0.056  time: 0.5575  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:53:59 d2.utils.events]: [0m eta: 0:05:37  iter: 1399  total_loss: 0.851  loss_cls: 0.270  loss_box_reg: 0.400  loss_rpn_cls: 0.037  loss_rpn_loc: 0.089  time: 0.5579  data_time: 0.0099  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:54:11 d2.utils.events]: [0m eta: 0:05:26  iter: 1419  total_loss: 0.673  loss_cls: 0.235  loss_box_reg: 0.335  loss_rpn_cls: 0.028  loss_rpn_loc: 0.070  time: 0.5582  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:54:22 d2.utils.events]: [0m eta: 0:05:15  iter: 1439  total_loss: 0.642  loss_cls: 0.185  loss_box_reg: 0.329  loss_rpn_cls: 0.025  loss_rpn_loc: 0.069  time: 0.5581  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:54:33 d2.utils.events]: [0m eta: 0:05:04  iter: 1459  total_loss: 0.780  loss_cls: 0.255  loss_box_reg: 0.398  loss_rpn_cls: 0.027  loss_rpn_loc: 0.085  time: 0.5582  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:54:45 d2.utils.events]: [0m eta: 0:04:53  iter: 1479  total_loss: 0.689  loss_cls: 0.233  loss_box_reg: 0.317  loss_rpn_cls: 0.028  loss_rpn_loc: 0.066  time: 0.5583  data_time: 0.0062  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:54:56 d2.utils.events]: [0m eta: 0:04:41  iter: 1499  total_loss: 0.645  loss_cls: 0.216  loss_box_reg: 0.321  loss_rpn_cls: 0.025  loss_rpn_loc: 0.063  time: 0.5583  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:55:08 d2.utils.events]: [0m eta: 0:04:30  iter: 1519  total_loss: 0.643  loss_cls: 0.210  loss_box_reg: 0.336  loss_rpn_cls: 0.024  loss_rpn_loc: 0.066  time: 0.5585  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:55:19 d2.utils.events]: [0m eta: 0:04:19  iter: 1539  total_loss: 0.810  loss_cls: 0.268  loss_box_reg: 0.390  loss_rpn_cls: 0.043  loss_rpn_loc: 0.095  time: 0.5584  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:55:30 d2.utils.events]: [0m eta: 0:04:08  iter: 1559  total_loss: 0.783  loss_cls: 0.242  loss_box_reg: 0.417  loss_rpn_cls: 0.029  loss_rpn_loc: 0.070  time: 0.5583  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:55:41 d2.utils.events]: [0m eta: 0:03:56  iter: 1579  total_loss: 0.673  loss_cls: 0.241  loss_box_reg: 0.334  loss_rpn_cls: 0.028  loss_rpn_loc: 0.072  time: 0.5583  data_time: 0.0112  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:55:52 d2.utils.events]: [0m eta: 0:03:45  iter: 1599  total_loss: 0.641  loss_cls: 0.222  loss_box_reg: 0.291  loss_rpn_cls: 0.027  loss_rpn_loc: 0.064  time: 0.5583  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:56:03 d2.utils.events]: [0m eta: 0:03:34  iter: 1619  total_loss: 0.642  loss_cls: 0.200  loss_box_reg: 0.329  loss_rpn_cls: 0.033  loss_rpn_loc: 0.082  time: 0.5581  data_time: 0.0062  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:56:15 d2.utils.events]: [0m eta: 0:03:22  iter: 1639  total_loss: 0.714  loss_cls: 0.245  loss_box_reg: 0.352  loss_rpn_cls: 0.030  loss_rpn_loc: 0.066  time: 0.5583  data_time: 0.0182  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:56:26 d2.utils.events]: [0m eta: 0:03:11  iter: 1659  total_loss: 0.709  loss_cls: 0.245  loss_box_reg: 0.355  loss_rpn_cls: 0.028  loss_rpn_loc: 0.066  time: 0.5584  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:56:37 d2.utils.events]: [0m eta: 0:03:00  iter: 1679  total_loss: 0.683  loss_cls: 0.215  loss_box_reg: 0.335  loss_rpn_cls: 0.028  loss_rpn_loc: 0.067  time: 0.5582  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:56:48 d2.utils.events]: [0m eta: 0:02:48  iter: 1699  total_loss: 0.771  loss_cls: 0.226  loss_box_reg: 0.375  loss_rpn_cls: 0.027  loss_rpn_loc: 0.082  time: 0.5579  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:56:59 d2.utils.events]: [0m eta: 0:02:37  iter: 1719  total_loss: 0.607  loss_cls: 0.219  loss_box_reg: 0.310  loss_rpn_cls: 0.024  loss_rpn_loc: 0.048  time: 0.5576  data_time: 0.0059  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:57:13 d2.utils.events]: [0m eta: 0:02:26  iter: 1739  total_loss: 0.722  loss_cls: 0.233  loss_box_reg: 0.370  loss_rpn_cls: 0.041  loss_rpn_loc: 0.077  time: 0.5575  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:57:27 d2.utils.events]: [0m eta: 0:02:14  iter: 1759  total_loss: 0.750  loss_cls: 0.250  loss_box_reg: 0.374  loss_rpn_cls: 0.040  loss_rpn_loc: 0.069  time: 0.5575  data_time: 0.0334  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:57:38 d2.utils.events]: [0m eta: 0:02:03  iter: 1779  total_loss: 0.783  loss_cls: 0.209  loss_box_reg: 0.359  loss_rpn_cls: 0.041  loss_rpn_loc: 0.106  time: 0.5574  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:57:49 d2.utils.events]: [0m eta: 0:01:52  iter: 1799  total_loss: 0.772  loss_cls: 0.253  loss_box_reg: 0.440  loss_rpn_cls: 0.031  loss_rpn_loc: 0.075  time: 0.5574  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:58:18 d2.utils.events]: [0m eta: 0:01:41  iter: 1819  total_loss: 0.790  loss_cls: 0.234  loss_box_reg: 0.414  loss_rpn_cls: 0.041  loss_rpn_loc: 0.053  time: 0.5569  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:58:29 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.750  loss_cls: 0.239  loss_box_reg: 0.384  loss_rpn_cls: 0.035  loss_rpn_loc: 0.068  time: 0.5566  data_time: 0.0090  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:58:39 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.689  loss_cls: 0.215  loss_box_reg: 0.358  loss_rpn_cls: 0.034  loss_rpn_loc: 0.067  time: 0.5564  data_time: 0.0056  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:58:50 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.620  loss_cls: 0.222  loss_box_reg: 0.338  loss_rpn_cls: 0.028  loss_rpn_loc: 0.065  time: 0.5561  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:59:01 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.514  loss_cls: 0.203  loss_box_reg: 0.282  loss_rpn_cls: 0.025  loss_rpn_loc: 0.053  time: 0.5558  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:59:12 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.658  loss_cls: 0.222  loss_box_reg: 0.340  loss_rpn_cls: 0.039  loss_rpn_loc: 0.061  time: 0.5558  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:59:23 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.623  loss_cls: 0.220  loss_box_reg: 0.350  loss_rpn_cls: 0.028  loss_rpn_loc: 0.053  time: 0.5555  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:59:34 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.699  loss_cls: 0.242  loss_box_reg: 0.349  loss_rpn_cls: 0.035  loss_rpn_loc: 0.067  time: 0.5555  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 07:59:45 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.727  loss_cls: 0.238  loss_box_reg: 0.385  loss_rpn_cls: 0.029  loss_rpn_loc: 0.064  time: 0.5555  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 08:00:21 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:00:21 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 08:00:21 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 08:00:21 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.753  loss_cls: 0.252  loss_box_reg: 0.393  loss_rpn_cls: 0.031  loss_rpn_loc: 0.056  time: 0.5554  data_time: 0.0097  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:00:28 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:29 (0.5557 s / it)
[32m[04/15 08:00:28 d2.engine.hooks]: [0mTotal training time: 0:19:35 (0:01:05 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 08:00:45 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:00:45 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 08:00:45 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 08:00:47 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1205 s / img. ETA=0:02:34
[32m[04/15 08:00:52 d2.evaluation.evaluator]: [0mInference done 51/1257. 0.1216 s / img. ETA=0:02:30
[32m[04/15 08:00:57 d2.evaluation.evaluator]: [0mInference done 91/1257. 0.1216 s / img. ETA=0:02:25
[32m[04/15 08:01:02 d2.evaluation.evaluator]: [0mInference done 131/1257. 0.1219 s / img. ETA=0:02:21
[32m[04/15 08:01:07 d2.evaluation.evaluator]: [0mInference done 172/1257. 0.1217 s / img. ETA=0:02:15
[32m[04/15 08:01:12 d2.evaluation.evaluator]: [0mInference done 213/1257. 0.1214 s / img. ETA=0:02:10
[32m[04/15 08:01:17 d2.evaluation.evaluator]: [0mInference done 253/1257. 0.1216 s / img. ETA=0:02:05
[32m[04/15 08:01:22 d2.evaluation.evaluator]: [0mInference done 293/1257. 0.1220 s / img. ETA=0:02:00
[32m[04/15 08:01:27 d2.evaluation.evaluator]: [0mInference done 333/1257. 0.1221 s / img. ETA=0:01:55
[32m[04/15 08:01:32 d2.evaluation.evaluator]: [0mInference done 374/1257. 0.1220 s / img. ETA=0:01:50
[32m[04/15 08:01:37 d2.evaluation.evaluator]: [0mInference done 414/1257. 0.1220 s / img. ETA=0:01:45
[32m[04/15 08:01:42 d2.evaluation.evaluator]: [0mInference done 453/1257. 0.1224 s / img. ETA=0:01:41
[32m[04/15 08:01:47 d2.evaluation.evaluator]: [0mInference done 493/1257. 0.1224 s / img. ETA=0:01:36
[32m[04/15 08:01:53 d2.evaluation.evaluator]: [0mInference done 533/1257. 0.1225 s / img. ETA=0:01:31
[32m[04/15 08:01:58 d2.evaluation.evaluator]: [0mInference done 574/1257. 0.1224 s / img. ETA=0:01:25
[32m[04/15 08:02:03 d2.evaluation.evaluator]: [0mInference done 610/1257. 0.1233 s / img. ETA=0:01:21
[32m[04/15 08:02:08 d2.evaluation.evaluator]: [0mInference done 650/1257. 0.1233 s / img. ETA=0:01:16
[32m[04/15 08:02:13 d2.evaluation.evaluator]: [0mInference done 690/1257. 0.1233 s / img. ETA=0:01:11
[32m[04/15 08:02:18 d2.evaluation.evaluator]: [0mInference done 729/1257. 0.1235 s / img. ETA=0:01:07
[32m[04/15 08:02:23 d2.evaluation.evaluator]: [0mInference done 769/1257. 0.1236 s / img. ETA=0:01:01
[32m[04/15 08:02:28 d2.evaluation.evaluator]: [0mInference done 809/1257. 0.1237 s / img. ETA=0:00:56
[32m[04/15 08:02:33 d2.evaluation.evaluator]: [0mInference done 848/1257. 0.1238 s / img. ETA=0:00:52
[32m[04/15 08:02:38 d2.evaluation.evaluator]: [0mInference done 887/1257. 0.1239 s / img. ETA=0:00:47
[32m[04/15 08:02:43 d2.evaluation.evaluator]: [0mInference done 926/1257. 0.1240 s / img. ETA=0:00:42
[32m[04/15 08:02:48 d2.evaluation.evaluator]: [0mInference done 965/1257. 0.1240 s / img. ETA=0:00:37
[32m[04/15 08:02:54 d2.evaluation.evaluator]: [0mInference done 1004/1257. 0.1241 s / img. ETA=0:00:32
[32m[04/15 08:02:59 d2.evaluation.evaluator]: [0mInference done 1043/1257. 0.1241 s / img. ETA=0:00:27
[32m[04/15 08:03:04 d2.evaluation.evaluator]: [0mInference done 1083/1257. 0.1241 s / img. ETA=0:00:22
[32m[04/15 08:03:09 d2.evaluation.evaluator]: [0mInference done 1122/1257. 0.1242 s / img. ETA=0:00:17
[32m[04/15 08:03:14 d2.evaluation.evaluator]: [0mInference done 1161/1257. 0.1243 s / img. ETA=0:00:12
[32m[04/15 08:03:19 d2.evaluation.evaluator]: [0mInference done 1201/1257. 0.1243 s / img. ETA=0:00:07
[32m[04/15 08:03:24 d2.evaluation.evaluator]: [0mInference done 1240/1257. 0.1244 s / img. ETA=0:00:02
[32m[04/15 08:03:26 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:40.171711 (0.127933 s / img per device, on 1 devices)
[32m[04/15 08:03:26 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:35 (0.124421 s / img per device, on 1 devices)
[32m[04/15 08:03:26 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 08:03:26 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 08:03:27 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.11s).
Accumulating evaluation results...
DONE (t=0.68s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.163
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.334
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.139
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.102
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.196
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.078
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.204
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.152
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.414
[32m[04/15 08:03:33 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 16.332 | 33.405 | 13.933 | 10.238 | 19.649 | 36.886 |
[32m[04/15 08:03:33 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 21.954 | bicycle       | 4.853 | car            | 38.522 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  7  *  2000  iterations ============
4 channel input
[32m[04/15 08:03:34 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 08:03:35 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.25 seconds.
[5m[31mWARNING[0m [32m[04/15 08:03:35 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:03:35 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 08:03:36 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 08:03:36 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 08:03:36 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 08:03:37 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 08:03:48 d2.utils.events]: [0m eta: 0:17:52  iter: 19  total_loss: 0.676  loss_cls: 0.243  loss_box_reg: 0.314  loss_rpn_cls: 0.038  loss_rpn_loc: 0.071  time: 0.5417  data_time: 0.0290  lr: 0.000200  max_mem: 3070M
[32m[04/15 08:03:59 d2.utils.events]: [0m eta: 0:17:27  iter: 39  total_loss: 0.608  loss_cls: 0.208  loss_box_reg: 0.317  loss_rpn_cls: 0.031  loss_rpn_loc: 0.058  time: 0.5364  data_time: 0.0074  lr: 0.000400  max_mem: 3070M
[32m[04/15 08:04:10 d2.utils.events]: [0m eta: 0:17:19  iter: 59  total_loss: 0.856  loss_cls: 0.279  loss_box_reg: 0.443  loss_rpn_cls: 0.034  loss_rpn_loc: 0.070  time: 0.5384  data_time: 0.0086  lr: 0.000599  max_mem: 3070M
[32m[04/15 08:04:21 d2.utils.events]: [0m eta: 0:17:28  iter: 79  total_loss: 0.627  loss_cls: 0.216  loss_box_reg: 0.328  loss_rpn_cls: 0.041  loss_rpn_loc: 0.056  time: 0.5434  data_time: 0.0075  lr: 0.000799  max_mem: 3070M
[32m[04/15 08:04:32 d2.utils.events]: [0m eta: 0:17:09  iter: 99  total_loss: 0.735  loss_cls: 0.243  loss_box_reg: 0.373  loss_rpn_cls: 0.021  loss_rpn_loc: 0.061  time: 0.5420  data_time: 0.0086  lr: 0.000999  max_mem: 3070M
[32m[04/15 08:04:43 d2.utils.events]: [0m eta: 0:16:58  iter: 119  total_loss: 0.748  loss_cls: 0.248  loss_box_reg: 0.358  loss_rpn_cls: 0.032  loss_rpn_loc: 0.070  time: 0.5420  data_time: 0.0085  lr: 0.001199  max_mem: 3070M
[32m[04/15 08:04:54 d2.utils.events]: [0m eta: 0:16:56  iter: 139  total_loss: 0.695  loss_cls: 0.222  loss_box_reg: 0.361  loss_rpn_cls: 0.024  loss_rpn_loc: 0.055  time: 0.5459  data_time: 0.0073  lr: 0.001399  max_mem: 3070M
[32m[04/15 08:05:05 d2.utils.events]: [0m eta: 0:16:47  iter: 159  total_loss: 0.646  loss_cls: 0.208  loss_box_reg: 0.321  loss_rpn_cls: 0.026  loss_rpn_loc: 0.057  time: 0.5475  data_time: 0.0064  lr: 0.001598  max_mem: 3070M
[32m[04/15 08:05:16 d2.utils.events]: [0m eta: 0:16:36  iter: 179  total_loss: 0.683  loss_cls: 0.214  loss_box_reg: 0.359  loss_rpn_cls: 0.020  loss_rpn_loc: 0.043  time: 0.5481  data_time: 0.0087  lr: 0.001798  max_mem: 3070M
[32m[04/15 08:05:28 d2.utils.events]: [0m eta: 0:16:27  iter: 199  total_loss: 0.650  loss_cls: 0.204  loss_box_reg: 0.333  loss_rpn_cls: 0.023  loss_rpn_loc: 0.056  time: 0.5491  data_time: 0.0080  lr: 0.001998  max_mem: 3070M
[32m[04/15 08:05:39 d2.utils.events]: [0m eta: 0:16:21  iter: 219  total_loss: 0.683  loss_cls: 0.235  loss_box_reg: 0.385  loss_rpn_cls: 0.023  loss_rpn_loc: 0.052  time: 0.5508  data_time: 0.0093  lr: 0.002198  max_mem: 3070M
[32m[04/15 08:05:51 d2.utils.events]: [0m eta: 0:16:10  iter: 239  total_loss: 0.745  loss_cls: 0.268  loss_box_reg: 0.388  loss_rpn_cls: 0.025  loss_rpn_loc: 0.078  time: 0.5517  data_time: 0.0065  lr: 0.002398  max_mem: 3070M
[32m[04/15 08:06:02 d2.utils.events]: [0m eta: 0:15:58  iter: 259  total_loss: 0.639  loss_cls: 0.224  loss_box_reg: 0.300  loss_rpn_cls: 0.018  loss_rpn_loc: 0.048  time: 0.5518  data_time: 0.0073  lr: 0.002597  max_mem: 3070M
[32m[04/15 08:06:13 d2.utils.events]: [0m eta: 0:15:47  iter: 279  total_loss: 0.717  loss_cls: 0.248  loss_box_reg: 0.367  loss_rpn_cls: 0.021  loss_rpn_loc: 0.055  time: 0.5511  data_time: 0.0072  lr: 0.002797  max_mem: 3070M
[32m[04/15 08:06:23 d2.utils.events]: [0m eta: 0:15:33  iter: 299  total_loss: 0.773  loss_cls: 0.254  loss_box_reg: 0.406  loss_rpn_cls: 0.032  loss_rpn_loc: 0.074  time: 0.5506  data_time: 0.0071  lr: 0.002997  max_mem: 3070M
[32m[04/15 08:06:35 d2.utils.events]: [0m eta: 0:15:24  iter: 319  total_loss: 0.636  loss_cls: 0.206  loss_box_reg: 0.327  loss_rpn_cls: 0.022  loss_rpn_loc: 0.062  time: 0.5508  data_time: 0.0084  lr: 0.003197  max_mem: 3070M
[32m[04/15 08:06:45 d2.utils.events]: [0m eta: 0:15:12  iter: 339  total_loss: 0.700  loss_cls: 0.226  loss_box_reg: 0.344  loss_rpn_cls: 0.018  loss_rpn_loc: 0.066  time: 0.5499  data_time: 0.0073  lr: 0.003397  max_mem: 3070M
[32m[04/15 08:06:57 d2.utils.events]: [0m eta: 0:15:03  iter: 359  total_loss: 0.688  loss_cls: 0.245  loss_box_reg: 0.390  loss_rpn_cls: 0.017  loss_rpn_loc: 0.055  time: 0.5507  data_time: 0.0077  lr: 0.003596  max_mem: 3070M
[32m[04/15 08:07:08 d2.utils.events]: [0m eta: 0:14:51  iter: 379  total_loss: 0.742  loss_cls: 0.241  loss_box_reg: 0.400  loss_rpn_cls: 0.023  loss_rpn_loc: 0.065  time: 0.5505  data_time: 0.0072  lr: 0.003796  max_mem: 3070M
[32m[04/15 08:07:19 d2.utils.events]: [0m eta: 0:14:41  iter: 399  total_loss: 0.591  loss_cls: 0.197  loss_box_reg: 0.324  loss_rpn_cls: 0.020  loss_rpn_loc: 0.045  time: 0.5510  data_time: 0.0078  lr: 0.003996  max_mem: 3070M
[32m[04/15 08:07:30 d2.utils.events]: [0m eta: 0:14:30  iter: 419  total_loss: 0.681  loss_cls: 0.219  loss_box_reg: 0.364  loss_rpn_cls: 0.021  loss_rpn_loc: 0.071  time: 0.5511  data_time: 0.0063  lr: 0.004196  max_mem: 3070M
[32m[04/15 08:07:42 d2.utils.events]: [0m eta: 0:14:20  iter: 439  total_loss: 0.699  loss_cls: 0.231  loss_box_reg: 0.373  loss_rpn_cls: 0.021  loss_rpn_loc: 0.059  time: 0.5517  data_time: 0.0086  lr: 0.004396  max_mem: 3070M
[32m[04/15 08:07:53 d2.utils.events]: [0m eta: 0:14:11  iter: 459  total_loss: 0.555  loss_cls: 0.185  loss_box_reg: 0.321  loss_rpn_cls: 0.018  loss_rpn_loc: 0.048  time: 0.5518  data_time: 0.0078  lr: 0.004595  max_mem: 3070M
[32m[04/15 08:08:04 d2.utils.events]: [0m eta: 0:14:01  iter: 479  total_loss: 0.701  loss_cls: 0.238  loss_box_reg: 0.359  loss_rpn_cls: 0.024  loss_rpn_loc: 0.047  time: 0.5528  data_time: 0.0066  lr: 0.004795  max_mem: 3070M
[32m[04/15 08:08:16 d2.utils.events]: [0m eta: 0:13:50  iter: 499  total_loss: 0.709  loss_cls: 0.221  loss_box_reg: 0.392  loss_rpn_cls: 0.022  loss_rpn_loc: 0.065  time: 0.5532  data_time: 0.0072  lr: 0.004995  max_mem: 3070M
[32m[04/15 08:08:27 d2.utils.events]: [0m eta: 0:13:39  iter: 519  total_loss: 0.722  loss_cls: 0.205  loss_box_reg: 0.374  loss_rpn_cls: 0.016  loss_rpn_loc: 0.055  time: 0.5535  data_time: 0.0075  lr: 0.005195  max_mem: 3070M
[32m[04/15 08:08:38 d2.utils.events]: [0m eta: 0:13:28  iter: 539  total_loss: 0.682  loss_cls: 0.224  loss_box_reg: 0.361  loss_rpn_cls: 0.027  loss_rpn_loc: 0.061  time: 0.5535  data_time: 0.0079  lr: 0.005395  max_mem: 3070M
[32m[04/15 08:08:49 d2.utils.events]: [0m eta: 0:13:17  iter: 559  total_loss: 0.641  loss_cls: 0.219  loss_box_reg: 0.347  loss_rpn_cls: 0.027  loss_rpn_loc: 0.058  time: 0.5535  data_time: 0.0074  lr: 0.005594  max_mem: 3070M
[32m[04/15 08:09:01 d2.utils.events]: [0m eta: 0:13:07  iter: 579  total_loss: 0.528  loss_cls: 0.163  loss_box_reg: 0.290  loss_rpn_cls: 0.020  loss_rpn_loc: 0.044  time: 0.5539  data_time: 0.0070  lr: 0.005794  max_mem: 3070M
[32m[04/15 08:09:12 d2.utils.events]: [0m eta: 0:12:55  iter: 599  total_loss: 0.642  loss_cls: 0.210  loss_box_reg: 0.320  loss_rpn_cls: 0.026  loss_rpn_loc: 0.069  time: 0.5536  data_time: 0.0070  lr: 0.005994  max_mem: 3070M
[32m[04/15 08:09:23 d2.utils.events]: [0m eta: 0:12:44  iter: 619  total_loss: 0.733  loss_cls: 0.240  loss_box_reg: 0.388  loss_rpn_cls: 0.026  loss_rpn_loc: 0.061  time: 0.5532  data_time: 0.0075  lr: 0.006194  max_mem: 3070M
[32m[04/15 08:09:34 d2.utils.events]: [0m eta: 0:12:33  iter: 639  total_loss: 0.773  loss_cls: 0.256  loss_box_reg: 0.389  loss_rpn_cls: 0.031  loss_rpn_loc: 0.079  time: 0.5532  data_time: 0.0091  lr: 0.006394  max_mem: 3070M
[32m[04/15 08:09:45 d2.utils.events]: [0m eta: 0:12:22  iter: 659  total_loss: 0.733  loss_cls: 0.229  loss_box_reg: 0.390  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5533  data_time: 0.0087  lr: 0.006593  max_mem: 3070M
[32m[04/15 08:09:56 d2.utils.events]: [0m eta: 0:12:11  iter: 679  total_loss: 0.703  loss_cls: 0.219  loss_box_reg: 0.347  loss_rpn_cls: 0.027  loss_rpn_loc: 0.058  time: 0.5534  data_time: 0.0068  lr: 0.006793  max_mem: 3070M
[32m[04/15 08:10:08 d2.utils.events]: [0m eta: 0:12:01  iter: 699  total_loss: 0.673  loss_cls: 0.218  loss_box_reg: 0.356  loss_rpn_cls: 0.024  loss_rpn_loc: 0.076  time: 0.5537  data_time: 0.0072  lr: 0.006993  max_mem: 3070M
[32m[04/15 08:10:19 d2.utils.events]: [0m eta: 0:11:50  iter: 719  total_loss: 0.554  loss_cls: 0.192  loss_box_reg: 0.251  loss_rpn_cls: 0.025  loss_rpn_loc: 0.051  time: 0.5538  data_time: 0.0071  lr: 0.007193  max_mem: 3070M
[32m[04/15 08:10:30 d2.utils.events]: [0m eta: 0:11:39  iter: 739  total_loss: 0.721  loss_cls: 0.280  loss_box_reg: 0.381  loss_rpn_cls: 0.027  loss_rpn_loc: 0.068  time: 0.5537  data_time: 0.0079  lr: 0.007393  max_mem: 3070M
[32m[04/15 08:10:41 d2.utils.events]: [0m eta: 0:11:27  iter: 759  total_loss: 0.619  loss_cls: 0.198  loss_box_reg: 0.317  loss_rpn_cls: 0.026  loss_rpn_loc: 0.061  time: 0.5528  data_time: 0.0075  lr: 0.007592  max_mem: 3070M
[32m[04/15 08:10:52 d2.utils.events]: [0m eta: 0:11:16  iter: 779  total_loss: 0.644  loss_cls: 0.215  loss_box_reg: 0.347  loss_rpn_cls: 0.033  loss_rpn_loc: 0.054  time: 0.5529  data_time: 0.0073  lr: 0.007792  max_mem: 3070M
[32m[04/15 08:11:03 d2.utils.events]: [0m eta: 0:11:04  iter: 799  total_loss: 0.566  loss_cls: 0.204  loss_box_reg: 0.307  loss_rpn_cls: 0.021  loss_rpn_loc: 0.043  time: 0.5523  data_time: 0.0073  lr: 0.007992  max_mem: 3070M
[32m[04/15 08:11:13 d2.utils.events]: [0m eta: 0:10:53  iter: 819  total_loss: 0.722  loss_cls: 0.223  loss_box_reg: 0.378  loss_rpn_cls: 0.025  loss_rpn_loc: 0.060  time: 0.5519  data_time: 0.0085  lr: 0.008192  max_mem: 3070M
[32m[04/15 08:11:25 d2.utils.events]: [0m eta: 0:10:42  iter: 839  total_loss: 0.763  loss_cls: 0.243  loss_box_reg: 0.367  loss_rpn_cls: 0.027  loss_rpn_loc: 0.089  time: 0.5519  data_time: 0.0075  lr: 0.008392  max_mem: 3070M
[32m[04/15 08:11:35 d2.utils.events]: [0m eta: 0:10:31  iter: 859  total_loss: 0.706  loss_cls: 0.223  loss_box_reg: 0.314  loss_rpn_cls: 0.030  loss_rpn_loc: 0.067  time: 0.5515  data_time: 0.0061  lr: 0.008591  max_mem: 3070M
[32m[04/15 08:11:47 d2.utils.events]: [0m eta: 0:10:20  iter: 879  total_loss: 0.785  loss_cls: 0.250  loss_box_reg: 0.365  loss_rpn_cls: 0.037  loss_rpn_loc: 0.058  time: 0.5516  data_time: 0.0094  lr: 0.008791  max_mem: 3070M
[32m[04/15 08:11:58 d2.utils.events]: [0m eta: 0:10:09  iter: 899  total_loss: 0.620  loss_cls: 0.207  loss_box_reg: 0.322  loss_rpn_cls: 0.027  loss_rpn_loc: 0.046  time: 0.5519  data_time: 0.0073  lr: 0.008991  max_mem: 3070M
[32m[04/15 08:12:11 d2.utils.events]: [0m eta: 0:09:59  iter: 919  total_loss: 0.628  loss_cls: 0.195  loss_box_reg: 0.303  loss_rpn_cls: 0.030  loss_rpn_loc: 0.058  time: 0.5538  data_time: 0.1181  lr: 0.009191  max_mem: 3070M
[32m[04/15 08:12:22 d2.utils.events]: [0m eta: 0:09:47  iter: 939  total_loss: 0.640  loss_cls: 0.206  loss_box_reg: 0.357  loss_rpn_cls: 0.029  loss_rpn_loc: 0.054  time: 0.5539  data_time: 0.0100  lr: 0.009391  max_mem: 3070M
[32m[04/15 08:12:34 d2.utils.events]: [0m eta: 0:09:37  iter: 959  total_loss: 0.766  loss_cls: 0.230  loss_box_reg: 0.365  loss_rpn_cls: 0.037  loss_rpn_loc: 0.071  time: 0.5543  data_time: 0.0071  lr: 0.009590  max_mem: 3070M
[32m[04/15 08:12:45 d2.utils.events]: [0m eta: 0:09:25  iter: 979  total_loss: 0.654  loss_cls: 0.200  loss_box_reg: 0.341  loss_rpn_cls: 0.021  loss_rpn_loc: 0.057  time: 0.5542  data_time: 0.0079  lr: 0.009790  max_mem: 3070M
[32m[04/15 08:12:56 d2.utils.events]: [0m eta: 0:09:15  iter: 999  total_loss: 0.789  loss_cls: 0.231  loss_box_reg: 0.402  loss_rpn_cls: 0.039  loss_rpn_loc: 0.075  time: 0.5545  data_time: 0.0076  lr: 0.009990  max_mem: 3070M
[32m[04/15 08:13:07 d2.utils.events]: [0m eta: 0:09:04  iter: 1019  total_loss: 0.687  loss_cls: 0.226  loss_box_reg: 0.315  loss_rpn_cls: 0.033  loss_rpn_loc: 0.067  time: 0.5545  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:13:19 d2.utils.events]: [0m eta: 0:08:54  iter: 1039  total_loss: 0.769  loss_cls: 0.257  loss_box_reg: 0.408  loss_rpn_cls: 0.033  loss_rpn_loc: 0.057  time: 0.5548  data_time: 0.0103  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:13:30 d2.utils.events]: [0m eta: 0:08:43  iter: 1059  total_loss: 0.609  loss_cls: 0.224  loss_box_reg: 0.336  loss_rpn_cls: 0.029  loss_rpn_loc: 0.057  time: 0.5546  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:13:41 d2.utils.events]: [0m eta: 0:08:31  iter: 1079  total_loss: 0.819  loss_cls: 0.264  loss_box_reg: 0.396  loss_rpn_cls: 0.029  loss_rpn_loc: 0.093  time: 0.5542  data_time: 0.0154  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:13:52 d2.utils.events]: [0m eta: 0:08:21  iter: 1099  total_loss: 0.648  loss_cls: 0.222  loss_box_reg: 0.306  loss_rpn_cls: 0.029  loss_rpn_loc: 0.062  time: 0.5546  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:14:03 d2.utils.events]: [0m eta: 0:08:10  iter: 1119  total_loss: 0.655  loss_cls: 0.224  loss_box_reg: 0.346  loss_rpn_cls: 0.028  loss_rpn_loc: 0.056  time: 0.5545  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:14:14 d2.utils.events]: [0m eta: 0:07:59  iter: 1139  total_loss: 0.742  loss_cls: 0.237  loss_box_reg: 0.382  loss_rpn_cls: 0.033  loss_rpn_loc: 0.060  time: 0.5540  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:14:25 d2.utils.events]: [0m eta: 0:07:47  iter: 1159  total_loss: 0.689  loss_cls: 0.220  loss_box_reg: 0.361  loss_rpn_cls: 0.025  loss_rpn_loc: 0.056  time: 0.5540  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:14:37 d2.utils.events]: [0m eta: 0:07:37  iter: 1179  total_loss: 0.722  loss_cls: 0.223  loss_box_reg: 0.380  loss_rpn_cls: 0.030  loss_rpn_loc: 0.060  time: 0.5542  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:14:48 d2.utils.events]: [0m eta: 0:07:26  iter: 1199  total_loss: 0.647  loss_cls: 0.217  loss_box_reg: 0.330  loss_rpn_cls: 0.031  loss_rpn_loc: 0.056  time: 0.5543  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:14:59 d2.utils.events]: [0m eta: 0:07:14  iter: 1219  total_loss: 0.613  loss_cls: 0.192  loss_box_reg: 0.329  loss_rpn_cls: 0.029  loss_rpn_loc: 0.057  time: 0.5544  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:15:10 d2.utils.events]: [0m eta: 0:07:02  iter: 1239  total_loss: 0.714  loss_cls: 0.220  loss_box_reg: 0.349  loss_rpn_cls: 0.024  loss_rpn_loc: 0.069  time: 0.5541  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:15:21 d2.utils.events]: [0m eta: 0:06:51  iter: 1259  total_loss: 0.760  loss_cls: 0.272  loss_box_reg: 0.393  loss_rpn_cls: 0.041  loss_rpn_loc: 0.059  time: 0.5541  data_time: 0.0092  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:15:32 d2.utils.events]: [0m eta: 0:06:40  iter: 1279  total_loss: 0.606  loss_cls: 0.203  loss_box_reg: 0.298  loss_rpn_cls: 0.034  loss_rpn_loc: 0.052  time: 0.5541  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:15:44 d2.utils.events]: [0m eta: 0:06:30  iter: 1299  total_loss: 0.628  loss_cls: 0.218  loss_box_reg: 0.315  loss_rpn_cls: 0.031  loss_rpn_loc: 0.055  time: 0.5543  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:15:55 d2.utils.events]: [0m eta: 0:06:19  iter: 1319  total_loss: 0.631  loss_cls: 0.201  loss_box_reg: 0.345  loss_rpn_cls: 0.034  loss_rpn_loc: 0.055  time: 0.5542  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:16:06 d2.utils.events]: [0m eta: 0:06:08  iter: 1339  total_loss: 0.558  loss_cls: 0.165  loss_box_reg: 0.298  loss_rpn_cls: 0.026  loss_rpn_loc: 0.049  time: 0.5543  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:16:17 d2.utils.events]: [0m eta: 0:05:56  iter: 1359  total_loss: 0.665  loss_cls: 0.227  loss_box_reg: 0.355  loss_rpn_cls: 0.038  loss_rpn_loc: 0.053  time: 0.5541  data_time: 0.0090  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:16:29 d2.utils.events]: [0m eta: 0:05:45  iter: 1379  total_loss: 0.775  loss_cls: 0.254  loss_box_reg: 0.424  loss_rpn_cls: 0.030  loss_rpn_loc: 0.062  time: 0.5544  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:16:40 d2.utils.events]: [0m eta: 0:05:34  iter: 1399  total_loss: 0.737  loss_cls: 0.255  loss_box_reg: 0.390  loss_rpn_cls: 0.031  loss_rpn_loc: 0.080  time: 0.5543  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:16:51 d2.utils.events]: [0m eta: 0:05:23  iter: 1419  total_loss: 0.613  loss_cls: 0.203  loss_box_reg: 0.342  loss_rpn_cls: 0.027  loss_rpn_loc: 0.063  time: 0.5542  data_time: 0.0060  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:17:02 d2.utils.events]: [0m eta: 0:05:12  iter: 1439  total_loss: 0.719  loss_cls: 0.235  loss_box_reg: 0.400  loss_rpn_cls: 0.033  loss_rpn_loc: 0.068  time: 0.5542  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:17:13 d2.utils.events]: [0m eta: 0:05:01  iter: 1459  total_loss: 0.754  loss_cls: 0.233  loss_box_reg: 0.405  loss_rpn_cls: 0.027  loss_rpn_loc: 0.082  time: 0.5543  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:17:24 d2.utils.events]: [0m eta: 0:04:49  iter: 1479  total_loss: 0.673  loss_cls: 0.233  loss_box_reg: 0.365  loss_rpn_cls: 0.021  loss_rpn_loc: 0.067  time: 0.5543  data_time: 0.0062  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:17:36 d2.utils.events]: [0m eta: 0:04:38  iter: 1499  total_loss: 0.783  loss_cls: 0.264  loss_box_reg: 0.397  loss_rpn_cls: 0.021  loss_rpn_loc: 0.069  time: 0.5544  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:17:47 d2.utils.events]: [0m eta: 0:04:27  iter: 1519  total_loss: 0.605  loss_cls: 0.214  loss_box_reg: 0.310  loss_rpn_cls: 0.032  loss_rpn_loc: 0.067  time: 0.5545  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:17:58 d2.utils.events]: [0m eta: 0:04:16  iter: 1539  total_loss: 0.539  loss_cls: 0.184  loss_box_reg: 0.283  loss_rpn_cls: 0.027  loss_rpn_loc: 0.050  time: 0.5544  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:18:09 d2.utils.events]: [0m eta: 0:04:05  iter: 1559  total_loss: 0.659  loss_cls: 0.229  loss_box_reg: 0.327  loss_rpn_cls: 0.023  loss_rpn_loc: 0.058  time: 0.5545  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:18:20 d2.utils.events]: [0m eta: 0:03:54  iter: 1579  total_loss: 0.813  loss_cls: 0.255  loss_box_reg: 0.435  loss_rpn_cls: 0.027  loss_rpn_loc: 0.075  time: 0.5544  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:18:32 d2.utils.events]: [0m eta: 0:03:43  iter: 1599  total_loss: 0.754  loss_cls: 0.232  loss_box_reg: 0.401  loss_rpn_cls: 0.025  loss_rpn_loc: 0.063  time: 0.5545  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:18:43 d2.utils.events]: [0m eta: 0:03:32  iter: 1619  total_loss: 0.829  loss_cls: 0.255  loss_box_reg: 0.421  loss_rpn_cls: 0.029  loss_rpn_loc: 0.080  time: 0.5547  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:18:54 d2.utils.events]: [0m eta: 0:03:21  iter: 1639  total_loss: 0.691  loss_cls: 0.229  loss_box_reg: 0.363  loss_rpn_cls: 0.026  loss_rpn_loc: 0.072  time: 0.5548  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:19:06 d2.utils.events]: [0m eta: 0:03:09  iter: 1659  total_loss: 0.638  loss_cls: 0.197  loss_box_reg: 0.317  loss_rpn_cls: 0.029  loss_rpn_loc: 0.057  time: 0.5549  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:19:17 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.825  loss_cls: 0.245  loss_box_reg: 0.414  loss_rpn_cls: 0.033  loss_rpn_loc: 0.082  time: 0.5548  data_time: 0.0050  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:19:28 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.712  loss_cls: 0.220  loss_box_reg: 0.372  loss_rpn_cls: 0.034  loss_rpn_loc: 0.065  time: 0.5546  data_time: 0.0052  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:19:39 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.751  loss_cls: 0.230  loss_box_reg: 0.391  loss_rpn_cls: 0.030  loss_rpn_loc: 0.058  time: 0.5548  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:19:51 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.812  loss_cls: 0.251  loss_box_reg: 0.380  loss_rpn_cls: 0.040  loss_rpn_loc: 0.098  time: 0.5549  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:20:02 d2.utils.events]: [0m eta: 0:02:14  iter: 1759  total_loss: 0.633  loss_cls: 0.191  loss_box_reg: 0.321  loss_rpn_cls: 0.031  loss_rpn_loc: 0.073  time: 0.5549  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:20:13 d2.utils.events]: [0m eta: 0:02:03  iter: 1779  total_loss: 0.645  loss_cls: 0.212  loss_box_reg: 0.332  loss_rpn_cls: 0.022  loss_rpn_loc: 0.051  time: 0.5549  data_time: 0.0087  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:20:30 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.721  loss_cls: 0.240  loss_box_reg: 0.359  loss_rpn_cls: 0.036  loss_rpn_loc: 0.059  time: 0.5550  data_time: 0.0363  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:20:41 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.615  loss_cls: 0.201  loss_box_reg: 0.337  loss_rpn_cls: 0.028  loss_rpn_loc: 0.054  time: 0.5549  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:20:52 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.724  loss_cls: 0.241  loss_box_reg: 0.362  loss_rpn_cls: 0.028  loss_rpn_loc: 0.069  time: 0.5549  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:21:15 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.685  loss_cls: 0.245  loss_box_reg: 0.361  loss_rpn_cls: 0.027  loss_rpn_loc: 0.063  time: 0.5548  data_time: 0.0115  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:21:26 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.854  loss_cls: 0.292  loss_box_reg: 0.447  loss_rpn_cls: 0.039  loss_rpn_loc: 0.097  time: 0.5547  data_time: 0.0098  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:21:37 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.719  loss_cls: 0.237  loss_box_reg: 0.371  loss_rpn_cls: 0.026  loss_rpn_loc: 0.066  time: 0.5547  data_time: 0.0091  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:21:48 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.695  loss_cls: 0.221  loss_box_reg: 0.348  loss_rpn_cls: 0.034  loss_rpn_loc: 0.075  time: 0.5546  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:21:59 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.675  loss_cls: 0.212  loss_box_reg: 0.338  loss_rpn_cls: 0.034  loss_rpn_loc: 0.060  time: 0.5545  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:22:11 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.758  loss_cls: 0.251  loss_box_reg: 0.371  loss_rpn_cls: 0.039  loss_rpn_loc: 0.062  time: 0.5547  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:22:22 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.697  loss_cls: 0.225  loss_box_reg: 0.356  loss_rpn_cls: 0.030  loss_rpn_loc: 0.070  time: 0.5548  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 08:22:47 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:22:47 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 08:22:48 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 08:22:48 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.578  loss_cls: 0.214  loss_box_reg: 0.298  loss_rpn_cls: 0.025  loss_rpn_loc: 0.046  time: 0.5548  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:22:58 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:28 (0.5551 s / it)
[32m[04/15 08:22:58 d2.engine.hooks]: [0mTotal training time: 0:19:20 (0:00:51 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 08:23:16 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:23:16 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 08:23:17 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 08:23:18 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1260 s / img. ETA=0:02:43
[32m[04/15 08:23:23 d2.evaluation.evaluator]: [0mInference done 51/1257. 0.1241 s / img. ETA=0:02:34
[32m[04/15 08:23:29 d2.evaluation.evaluator]: [0mInference done 91/1257. 0.1242 s / img. ETA=0:02:29
[32m[04/15 08:23:34 d2.evaluation.evaluator]: [0mInference done 131/1257. 0.1236 s / img. ETA=0:02:23
[32m[04/15 08:23:39 d2.evaluation.evaluator]: [0mInference done 172/1257. 0.1229 s / img. ETA=0:02:17
[32m[04/15 08:23:44 d2.evaluation.evaluator]: [0mInference done 213/1257. 0.1222 s / img. ETA=0:02:11
[32m[04/15 08:23:49 d2.evaluation.evaluator]: [0mInference done 253/1257. 0.1225 s / img. ETA=0:02:06
[32m[04/15 08:23:54 d2.evaluation.evaluator]: [0mInference done 293/1257. 0.1228 s / img. ETA=0:02:01
[32m[04/15 08:23:59 d2.evaluation.evaluator]: [0mInference done 333/1257. 0.1229 s / img. ETA=0:01:56
[32m[04/15 08:24:04 d2.evaluation.evaluator]: [0mInference done 373/1257. 0.1229 s / img. ETA=0:01:51
[32m[04/15 08:24:09 d2.evaluation.evaluator]: [0mInference done 413/1257. 0.1229 s / img. ETA=0:01:46
[32m[04/15 08:24:14 d2.evaluation.evaluator]: [0mInference done 453/1257. 0.1229 s / img. ETA=0:01:41
[32m[04/15 08:24:19 d2.evaluation.evaluator]: [0mInference done 494/1257. 0.1228 s / img. ETA=0:01:36
[32m[04/15 08:24:24 d2.evaluation.evaluator]: [0mInference done 534/1257. 0.1228 s / img. ETA=0:01:31
[32m[04/15 08:24:29 d2.evaluation.evaluator]: [0mInference done 574/1257. 0.1228 s / img. ETA=0:01:26
[32m[04/15 08:24:34 d2.evaluation.evaluator]: [0mInference done 614/1257. 0.1229 s / img. ETA=0:01:21
[32m[04/15 08:24:40 d2.evaluation.evaluator]: [0mInference done 653/1257. 0.1230 s / img. ETA=0:01:16
[32m[04/15 08:24:45 d2.evaluation.evaluator]: [0mInference done 691/1257. 0.1233 s / img. ETA=0:01:11
[32m[04/15 08:24:50 d2.evaluation.evaluator]: [0mInference done 730/1257. 0.1234 s / img. ETA=0:01:06
[32m[04/15 08:24:55 d2.evaluation.evaluator]: [0mInference done 769/1257. 0.1235 s / img. ETA=0:01:02
[32m[04/15 08:25:00 d2.evaluation.evaluator]: [0mInference done 809/1257. 0.1235 s / img. ETA=0:00:56
[32m[04/15 08:25:05 d2.evaluation.evaluator]: [0mInference done 847/1257. 0.1237 s / img. ETA=0:00:52
[32m[04/15 08:25:10 d2.evaluation.evaluator]: [0mInference done 886/1257. 0.1239 s / img. ETA=0:00:47
[32m[04/15 08:25:15 d2.evaluation.evaluator]: [0mInference done 926/1257. 0.1239 s / img. ETA=0:00:42
[32m[04/15 08:25:20 d2.evaluation.evaluator]: [0mInference done 965/1257. 0.1240 s / img. ETA=0:00:37
[32m[04/15 08:25:25 d2.evaluation.evaluator]: [0mInference done 1005/1257. 0.1240 s / img. ETA=0:00:32
[32m[04/15 08:25:30 d2.evaluation.evaluator]: [0mInference done 1044/1257. 0.1241 s / img. ETA=0:00:27
[32m[04/15 08:25:35 d2.evaluation.evaluator]: [0mInference done 1083/1257. 0.1241 s / img. ETA=0:00:22
[32m[04/15 08:25:40 d2.evaluation.evaluator]: [0mInference done 1122/1257. 0.1242 s / img. ETA=0:00:17
[32m[04/15 08:25:45 d2.evaluation.evaluator]: [0mInference done 1161/1257. 0.1242 s / img. ETA=0:00:12
[32m[04/15 08:25:50 d2.evaluation.evaluator]: [0mInference done 1200/1257. 0.1243 s / img. ETA=0:00:07
[32m[04/15 08:25:55 d2.evaluation.evaluator]: [0mInference done 1239/1257. 0.1244 s / img. ETA=0:00:02
[32m[04/15 08:25:58 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:40.520594 (0.128211 s / img per device, on 1 devices)
[32m[04/15 08:25:58 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:35 (0.124412 s / img per device, on 1 devices)
[32m[04/15 08:25:58 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 08:25:58 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 08:25:58 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.48s).
Accumulating evaluation results...
DONE (t=0.90s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.166
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.337
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.143
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.106
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.207
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.084
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.216
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.161
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.249
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.411
[32m[04/15 08:26:05 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 16.634 | 33.713 | 14.304 | 10.552 | 20.723 | 37.882 |
[32m[04/15 08:26:05 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 25.338 | bicycle       | 3.920 | car            | 37.155 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.122 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  8  *  2000  iterations ============
4 channel input
[32m[04/15 08:26:07 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 08:26:08 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.11 seconds.
[5m[31mWARNING[0m [32m[04/15 08:26:08 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:26:08 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 08:26:09 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 08:26:09 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 08:26:09 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 08:26:13 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 08:26:24 d2.utils.events]: [0m eta: 0:18:08  iter: 19  total_loss: 0.739  loss_cls: 0.260  loss_box_reg: 0.364  loss_rpn_cls: 0.033  loss_rpn_loc: 0.061  time: 0.5402  data_time: 0.0382  lr: 0.000200  max_mem: 3070M
[32m[04/15 08:26:35 d2.utils.events]: [0m eta: 0:18:06  iter: 39  total_loss: 0.689  loss_cls: 0.226  loss_box_reg: 0.372  loss_rpn_cls: 0.027  loss_rpn_loc: 0.061  time: 0.5527  data_time: 0.0068  lr: 0.000400  max_mem: 3070M
[32m[04/15 08:26:46 d2.utils.events]: [0m eta: 0:17:42  iter: 59  total_loss: 0.707  loss_cls: 0.227  loss_box_reg: 0.378  loss_rpn_cls: 0.023  loss_rpn_loc: 0.069  time: 0.5452  data_time: 0.0071  lr: 0.000599  max_mem: 3070M
[32m[04/15 08:26:57 d2.utils.events]: [0m eta: 0:17:31  iter: 79  total_loss: 0.694  loss_cls: 0.227  loss_box_reg: 0.378  loss_rpn_cls: 0.026  loss_rpn_loc: 0.065  time: 0.5457  data_time: 0.0095  lr: 0.000799  max_mem: 3070M
[32m[04/15 08:27:08 d2.utils.events]: [0m eta: 0:17:21  iter: 99  total_loss: 0.738  loss_cls: 0.261  loss_box_reg: 0.397  loss_rpn_cls: 0.027  loss_rpn_loc: 0.059  time: 0.5444  data_time: 0.0069  lr: 0.000999  max_mem: 3070M
[32m[04/15 08:27:19 d2.utils.events]: [0m eta: 0:17:11  iter: 119  total_loss: 0.737  loss_cls: 0.252  loss_box_reg: 0.394  loss_rpn_cls: 0.023  loss_rpn_loc: 0.060  time: 0.5457  data_time: 0.0074  lr: 0.001199  max_mem: 3070M
[32m[04/15 08:27:30 d2.utils.events]: [0m eta: 0:17:10  iter: 139  total_loss: 0.590  loss_cls: 0.196  loss_box_reg: 0.343  loss_rpn_cls: 0.027  loss_rpn_loc: 0.050  time: 0.5482  data_time: 0.0071  lr: 0.001399  max_mem: 3070M
[32m[04/15 08:27:41 d2.utils.events]: [0m eta: 0:16:59  iter: 159  total_loss: 0.832  loss_cls: 0.286  loss_box_reg: 0.424  loss_rpn_cls: 0.026  loss_rpn_loc: 0.063  time: 0.5486  data_time: 0.0064  lr: 0.001598  max_mem: 3070M
[32m[04/15 08:27:52 d2.utils.events]: [0m eta: 0:16:49  iter: 179  total_loss: 0.705  loss_cls: 0.225  loss_box_reg: 0.402  loss_rpn_cls: 0.028  loss_rpn_loc: 0.073  time: 0.5483  data_time: 0.0094  lr: 0.001798  max_mem: 3070M
[32m[04/15 08:28:05 d2.utils.events]: [0m eta: 0:16:39  iter: 199  total_loss: 0.548  loss_cls: 0.169  loss_box_reg: 0.282  loss_rpn_cls: 0.017  loss_rpn_loc: 0.040  time: 0.5567  data_time: 0.0796  lr: 0.001998  max_mem: 3070M
[32m[04/15 08:28:17 d2.utils.events]: [0m eta: 0:16:29  iter: 219  total_loss: 0.720  loss_cls: 0.237  loss_box_reg: 0.376  loss_rpn_cls: 0.027  loss_rpn_loc: 0.076  time: 0.5562  data_time: 0.0065  lr: 0.002198  max_mem: 3070M
[32m[04/15 08:28:28 d2.utils.events]: [0m eta: 0:16:19  iter: 239  total_loss: 0.731  loss_cls: 0.219  loss_box_reg: 0.386  loss_rpn_cls: 0.026  loss_rpn_loc: 0.060  time: 0.5564  data_time: 0.0077  lr: 0.002398  max_mem: 3070M
[32m[04/15 08:28:40 d2.utils.events]: [0m eta: 0:16:08  iter: 259  total_loss: 0.675  loss_cls: 0.239  loss_box_reg: 0.354  loss_rpn_cls: 0.023  loss_rpn_loc: 0.070  time: 0.5555  data_time: 0.0083  lr: 0.002597  max_mem: 3070M
[32m[04/15 08:28:51 d2.utils.events]: [0m eta: 0:15:54  iter: 279  total_loss: 0.643  loss_cls: 0.222  loss_box_reg: 0.353  loss_rpn_cls: 0.018  loss_rpn_loc: 0.071  time: 0.5540  data_time: 0.0072  lr: 0.002797  max_mem: 3070M
[32m[04/15 08:29:02 d2.utils.events]: [0m eta: 0:15:44  iter: 299  total_loss: 0.621  loss_cls: 0.194  loss_box_reg: 0.328  loss_rpn_cls: 0.020  loss_rpn_loc: 0.062  time: 0.5541  data_time: 0.0087  lr: 0.002997  max_mem: 3070M
[32m[04/15 08:29:13 d2.utils.events]: [0m eta: 0:15:33  iter: 319  total_loss: 0.537  loss_cls: 0.191  loss_box_reg: 0.270  loss_rpn_cls: 0.024  loss_rpn_loc: 0.039  time: 0.5536  data_time: 0.0077  lr: 0.003197  max_mem: 3070M
[32m[04/15 08:29:25 d2.utils.events]: [0m eta: 0:15:24  iter: 339  total_loss: 0.688  loss_cls: 0.201  loss_box_reg: 0.339  loss_rpn_cls: 0.026  loss_rpn_loc: 0.055  time: 0.5547  data_time: 0.0081  lr: 0.003397  max_mem: 3070M
[32m[04/15 08:29:36 d2.utils.events]: [0m eta: 0:15:12  iter: 359  total_loss: 0.576  loss_cls: 0.191  loss_box_reg: 0.323  loss_rpn_cls: 0.019  loss_rpn_loc: 0.053  time: 0.5545  data_time: 0.0071  lr: 0.003596  max_mem: 3070M
[32m[04/15 08:29:47 d2.utils.events]: [0m eta: 0:15:00  iter: 379  total_loss: 0.774  loss_cls: 0.251  loss_box_reg: 0.381  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5545  data_time: 0.0074  lr: 0.003796  max_mem: 3070M
[32m[04/15 08:29:58 d2.utils.events]: [0m eta: 0:14:49  iter: 399  total_loss: 0.685  loss_cls: 0.242  loss_box_reg: 0.367  loss_rpn_cls: 0.016  loss_rpn_loc: 0.060  time: 0.5541  data_time: 0.0127  lr: 0.003996  max_mem: 3070M
[32m[04/15 08:30:10 d2.utils.events]: [0m eta: 0:14:39  iter: 419  total_loss: 0.657  loss_cls: 0.205  loss_box_reg: 0.334  loss_rpn_cls: 0.023  loss_rpn_loc: 0.056  time: 0.5544  data_time: 0.0109  lr: 0.004196  max_mem: 3070M
[32m[04/15 08:30:21 d2.utils.events]: [0m eta: 0:14:27  iter: 439  total_loss: 0.677  loss_cls: 0.213  loss_box_reg: 0.360  loss_rpn_cls: 0.022  loss_rpn_loc: 0.065  time: 0.5542  data_time: 0.0071  lr: 0.004396  max_mem: 3070M
[32m[04/15 08:30:32 d2.utils.events]: [0m eta: 0:14:16  iter: 459  total_loss: 0.561  loss_cls: 0.169  loss_box_reg: 0.292  loss_rpn_cls: 0.022  loss_rpn_loc: 0.045  time: 0.5542  data_time: 0.0076  lr: 0.004595  max_mem: 3070M
[32m[04/15 08:30:43 d2.utils.events]: [0m eta: 0:14:05  iter: 479  total_loss: 0.645  loss_cls: 0.220  loss_box_reg: 0.369  loss_rpn_cls: 0.022  loss_rpn_loc: 0.058  time: 0.5544  data_time: 0.0071  lr: 0.004795  max_mem: 3070M
[32m[04/15 08:30:54 d2.utils.events]: [0m eta: 0:13:54  iter: 499  total_loss: 0.726  loss_cls: 0.225  loss_box_reg: 0.379  loss_rpn_cls: 0.028  loss_rpn_loc: 0.065  time: 0.5543  data_time: 0.0072  lr: 0.004995  max_mem: 3070M
[32m[04/15 08:31:05 d2.utils.events]: [0m eta: 0:13:42  iter: 519  total_loss: 0.744  loss_cls: 0.226  loss_box_reg: 0.387  loss_rpn_cls: 0.025  loss_rpn_loc: 0.074  time: 0.5540  data_time: 0.0071  lr: 0.005195  max_mem: 3070M
[32m[04/15 08:31:16 d2.utils.events]: [0m eta: 0:13:31  iter: 539  total_loss: 0.618  loss_cls: 0.231  loss_box_reg: 0.335  loss_rpn_cls: 0.024  loss_rpn_loc: 0.060  time: 0.5538  data_time: 0.0066  lr: 0.005395  max_mem: 3070M
[32m[04/15 08:31:27 d2.utils.events]: [0m eta: 0:13:20  iter: 559  total_loss: 0.596  loss_cls: 0.194  loss_box_reg: 0.314  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5539  data_time: 0.0070  lr: 0.005594  max_mem: 3070M
[32m[04/15 08:31:39 d2.utils.events]: [0m eta: 0:13:09  iter: 579  total_loss: 0.618  loss_cls: 0.215  loss_box_reg: 0.323  loss_rpn_cls: 0.019  loss_rpn_loc: 0.045  time: 0.5538  data_time: 0.0068  lr: 0.005794  max_mem: 3070M
[32m[04/15 08:31:50 d2.utils.events]: [0m eta: 0:12:58  iter: 599  total_loss: 0.625  loss_cls: 0.180  loss_box_reg: 0.347  loss_rpn_cls: 0.021  loss_rpn_loc: 0.061  time: 0.5536  data_time: 0.0072  lr: 0.005994  max_mem: 3070M
[32m[04/15 08:32:00 d2.utils.events]: [0m eta: 0:12:46  iter: 619  total_loss: 0.742  loss_cls: 0.245  loss_box_reg: 0.421  loss_rpn_cls: 0.018  loss_rpn_loc: 0.093  time: 0.5530  data_time: 0.0075  lr: 0.006194  max_mem: 3070M
[32m[04/15 08:32:12 d2.utils.events]: [0m eta: 0:12:35  iter: 639  total_loss: 0.627  loss_cls: 0.201  loss_box_reg: 0.312  loss_rpn_cls: 0.024  loss_rpn_loc: 0.050  time: 0.5531  data_time: 0.0087  lr: 0.006394  max_mem: 3070M
[32m[04/15 08:32:23 d2.utils.events]: [0m eta: 0:12:24  iter: 659  total_loss: 0.626  loss_cls: 0.218  loss_box_reg: 0.344  loss_rpn_cls: 0.024  loss_rpn_loc: 0.053  time: 0.5529  data_time: 0.0063  lr: 0.006593  max_mem: 3070M
[32m[04/15 08:32:34 d2.utils.events]: [0m eta: 0:12:13  iter: 679  total_loss: 0.719  loss_cls: 0.221  loss_box_reg: 0.395  loss_rpn_cls: 0.023  loss_rpn_loc: 0.059  time: 0.5533  data_time: 0.0069  lr: 0.006793  max_mem: 3070M
[32m[04/15 08:32:46 d2.utils.events]: [0m eta: 0:12:02  iter: 699  total_loss: 0.680  loss_cls: 0.236  loss_box_reg: 0.344  loss_rpn_cls: 0.020  loss_rpn_loc: 0.049  time: 0.5537  data_time: 0.0085  lr: 0.006993  max_mem: 3070M
[32m[04/15 08:32:57 d2.utils.events]: [0m eta: 0:11:51  iter: 719  total_loss: 0.578  loss_cls: 0.187  loss_box_reg: 0.339  loss_rpn_cls: 0.018  loss_rpn_loc: 0.061  time: 0.5538  data_time: 0.0066  lr: 0.007193  max_mem: 3070M
[32m[04/15 08:33:09 d2.utils.events]: [0m eta: 0:11:40  iter: 739  total_loss: 0.636  loss_cls: 0.214  loss_box_reg: 0.325  loss_rpn_cls: 0.020  loss_rpn_loc: 0.058  time: 0.5542  data_time: 0.0107  lr: 0.007393  max_mem: 3070M
[32m[04/15 08:33:20 d2.utils.events]: [0m eta: 0:11:29  iter: 759  total_loss: 0.609  loss_cls: 0.226  loss_box_reg: 0.342  loss_rpn_cls: 0.020  loss_rpn_loc: 0.048  time: 0.5546  data_time: 0.0070  lr: 0.007592  max_mem: 3070M
[32m[04/15 08:33:31 d2.utils.events]: [0m eta: 0:11:18  iter: 779  total_loss: 0.723  loss_cls: 0.226  loss_box_reg: 0.381  loss_rpn_cls: 0.025  loss_rpn_loc: 0.061  time: 0.5548  data_time: 0.0066  lr: 0.007792  max_mem: 3070M
[32m[04/15 08:33:43 d2.utils.events]: [0m eta: 0:11:07  iter: 799  total_loss: 0.668  loss_cls: 0.223  loss_box_reg: 0.379  loss_rpn_cls: 0.024  loss_rpn_loc: 0.055  time: 0.5551  data_time: 0.0065  lr: 0.007992  max_mem: 3070M
[32m[04/15 08:33:55 d2.utils.events]: [0m eta: 0:10:56  iter: 819  total_loss: 0.630  loss_cls: 0.221  loss_box_reg: 0.333  loss_rpn_cls: 0.026  loss_rpn_loc: 0.068  time: 0.5556  data_time: 0.0116  lr: 0.008192  max_mem: 3070M
[32m[04/15 08:34:06 d2.utils.events]: [0m eta: 0:10:46  iter: 839  total_loss: 0.639  loss_cls: 0.203  loss_box_reg: 0.355  loss_rpn_cls: 0.020  loss_rpn_loc: 0.055  time: 0.5555  data_time: 0.0073  lr: 0.008392  max_mem: 3070M
[32m[04/15 08:34:17 d2.utils.events]: [0m eta: 0:10:34  iter: 859  total_loss: 0.615  loss_cls: 0.203  loss_box_reg: 0.333  loss_rpn_cls: 0.021  loss_rpn_loc: 0.055  time: 0.5553  data_time: 0.0066  lr: 0.008591  max_mem: 3070M
[32m[04/15 08:34:28 d2.utils.events]: [0m eta: 0:10:23  iter: 879  total_loss: 0.684  loss_cls: 0.227  loss_box_reg: 0.339  loss_rpn_cls: 0.036  loss_rpn_loc: 0.079  time: 0.5554  data_time: 0.0069  lr: 0.008791  max_mem: 3070M
[32m[04/15 08:34:40 d2.utils.events]: [0m eta: 0:10:12  iter: 899  total_loss: 0.510  loss_cls: 0.178  loss_box_reg: 0.265  loss_rpn_cls: 0.027  loss_rpn_loc: 0.056  time: 0.5557  data_time: 0.0080  lr: 0.008991  max_mem: 3070M
[32m[04/15 08:34:51 d2.utils.events]: [0m eta: 0:10:01  iter: 919  total_loss: 0.730  loss_cls: 0.245  loss_box_reg: 0.362  loss_rpn_cls: 0.023  loss_rpn_loc: 0.076  time: 0.5556  data_time: 0.0060  lr: 0.009191  max_mem: 3070M
[32m[04/15 08:35:02 d2.utils.events]: [0m eta: 0:09:50  iter: 939  total_loss: 0.655  loss_cls: 0.218  loss_box_reg: 0.304  loss_rpn_cls: 0.029  loss_rpn_loc: 0.061  time: 0.5555  data_time: 0.0128  lr: 0.009391  max_mem: 3070M
[32m[04/15 08:35:13 d2.utils.events]: [0m eta: 0:09:39  iter: 959  total_loss: 0.627  loss_cls: 0.217  loss_box_reg: 0.354  loss_rpn_cls: 0.030  loss_rpn_loc: 0.072  time: 0.5558  data_time: 0.0096  lr: 0.009590  max_mem: 3070M
[32m[04/15 08:35:25 d2.utils.events]: [0m eta: 0:09:28  iter: 979  total_loss: 0.643  loss_cls: 0.234  loss_box_reg: 0.309  loss_rpn_cls: 0.027  loss_rpn_loc: 0.060  time: 0.5562  data_time: 0.0075  lr: 0.009790  max_mem: 3070M
[32m[04/15 08:35:37 d2.utils.events]: [0m eta: 0:09:17  iter: 999  total_loss: 0.701  loss_cls: 0.236  loss_box_reg: 0.345  loss_rpn_cls: 0.027  loss_rpn_loc: 0.080  time: 0.5563  data_time: 0.0066  lr: 0.009990  max_mem: 3070M
[32m[04/15 08:35:48 d2.utils.events]: [0m eta: 0:09:06  iter: 1019  total_loss: 0.752  loss_cls: 0.224  loss_box_reg: 0.366  loss_rpn_cls: 0.023  loss_rpn_loc: 0.070  time: 0.5565  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:36:00 d2.utils.events]: [0m eta: 0:08:55  iter: 1039  total_loss: 0.651  loss_cls: 0.206  loss_box_reg: 0.356  loss_rpn_cls: 0.024  loss_rpn_loc: 0.075  time: 0.5569  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:36:11 d2.utils.events]: [0m eta: 0:08:45  iter: 1059  total_loss: 0.759  loss_cls: 0.245  loss_box_reg: 0.412  loss_rpn_cls: 0.024  loss_rpn_loc: 0.057  time: 0.5568  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:36:22 d2.utils.events]: [0m eta: 0:08:34  iter: 1079  total_loss: 0.686  loss_cls: 0.232  loss_box_reg: 0.353  loss_rpn_cls: 0.026  loss_rpn_loc: 0.062  time: 0.5568  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:36:33 d2.utils.events]: [0m eta: 0:08:23  iter: 1099  total_loss: 0.655  loss_cls: 0.209  loss_box_reg: 0.358  loss_rpn_cls: 0.023  loss_rpn_loc: 0.065  time: 0.5567  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:36:44 d2.utils.events]: [0m eta: 0:08:11  iter: 1119  total_loss: 0.621  loss_cls: 0.216  loss_box_reg: 0.328  loss_rpn_cls: 0.024  loss_rpn_loc: 0.050  time: 0.5565  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:36:55 d2.utils.events]: [0m eta: 0:08:00  iter: 1139  total_loss: 0.694  loss_cls: 0.240  loss_box_reg: 0.345  loss_rpn_cls: 0.033  loss_rpn_loc: 0.071  time: 0.5565  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:37:06 d2.utils.events]: [0m eta: 0:07:48  iter: 1159  total_loss: 0.657  loss_cls: 0.218  loss_box_reg: 0.334  loss_rpn_cls: 0.029  loss_rpn_loc: 0.067  time: 0.5563  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:37:18 d2.utils.events]: [0m eta: 0:07:38  iter: 1179  total_loss: 0.676  loss_cls: 0.219  loss_box_reg: 0.372  loss_rpn_cls: 0.027  loss_rpn_loc: 0.069  time: 0.5566  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:37:29 d2.utils.events]: [0m eta: 0:07:27  iter: 1199  total_loss: 0.687  loss_cls: 0.210  loss_box_reg: 0.345  loss_rpn_cls: 0.027  loss_rpn_loc: 0.086  time: 0.5565  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:37:40 d2.utils.events]: [0m eta: 0:07:15  iter: 1219  total_loss: 0.668  loss_cls: 0.217  loss_box_reg: 0.375  loss_rpn_cls: 0.020  loss_rpn_loc: 0.048  time: 0.5565  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:37:52 d2.utils.events]: [0m eta: 0:07:04  iter: 1239  total_loss: 0.708  loss_cls: 0.242  loss_box_reg: 0.380  loss_rpn_cls: 0.023  loss_rpn_loc: 0.065  time: 0.5566  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:38:03 d2.utils.events]: [0m eta: 0:06:53  iter: 1259  total_loss: 0.699  loss_cls: 0.234  loss_box_reg: 0.361  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5566  data_time: 0.0059  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:38:15 d2.utils.events]: [0m eta: 0:06:42  iter: 1279  total_loss: 0.703  loss_cls: 0.237  loss_box_reg: 0.308  loss_rpn_cls: 0.044  loss_rpn_loc: 0.066  time: 0.5569  data_time: 0.0087  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:38:26 d2.utils.events]: [0m eta: 0:06:31  iter: 1299  total_loss: 0.686  loss_cls: 0.211  loss_box_reg: 0.359  loss_rpn_cls: 0.028  loss_rpn_loc: 0.060  time: 0.5569  data_time: 0.0092  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:38:37 d2.utils.events]: [0m eta: 0:06:20  iter: 1319  total_loss: 0.901  loss_cls: 0.306  loss_box_reg: 0.416  loss_rpn_cls: 0.030  loss_rpn_loc: 0.086  time: 0.5568  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:38:48 d2.utils.events]: [0m eta: 0:06:08  iter: 1339  total_loss: 0.725  loss_cls: 0.241  loss_box_reg: 0.368  loss_rpn_cls: 0.029  loss_rpn_loc: 0.072  time: 0.5567  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:38:59 d2.utils.events]: [0m eta: 0:05:57  iter: 1359  total_loss: 0.741  loss_cls: 0.234  loss_box_reg: 0.380  loss_rpn_cls: 0.037  loss_rpn_loc: 0.065  time: 0.5564  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:39:10 d2.utils.events]: [0m eta: 0:05:46  iter: 1379  total_loss: 0.664  loss_cls: 0.231  loss_box_reg: 0.356  loss_rpn_cls: 0.038  loss_rpn_loc: 0.062  time: 0.5565  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:39:21 d2.utils.events]: [0m eta: 0:05:35  iter: 1399  total_loss: 0.744  loss_cls: 0.236  loss_box_reg: 0.384  loss_rpn_cls: 0.034  loss_rpn_loc: 0.072  time: 0.5566  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:39:32 d2.utils.events]: [0m eta: 0:05:24  iter: 1419  total_loss: 0.638  loss_cls: 0.187  loss_box_reg: 0.329  loss_rpn_cls: 0.023  loss_rpn_loc: 0.059  time: 0.5564  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:39:43 d2.utils.events]: [0m eta: 0:05:12  iter: 1439  total_loss: 0.603  loss_cls: 0.205  loss_box_reg: 0.320  loss_rpn_cls: 0.028  loss_rpn_loc: 0.066  time: 0.5558  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:39:54 d2.utils.events]: [0m eta: 0:05:01  iter: 1459  total_loss: 0.583  loss_cls: 0.203  loss_box_reg: 0.318  loss_rpn_cls: 0.033  loss_rpn_loc: 0.068  time: 0.5557  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:40:05 d2.utils.events]: [0m eta: 0:04:50  iter: 1479  total_loss: 0.633  loss_cls: 0.213  loss_box_reg: 0.327  loss_rpn_cls: 0.023  loss_rpn_loc: 0.075  time: 0.5558  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:40:16 d2.utils.events]: [0m eta: 0:04:39  iter: 1499  total_loss: 0.596  loss_cls: 0.181  loss_box_reg: 0.309  loss_rpn_cls: 0.021  loss_rpn_loc: 0.060  time: 0.5557  data_time: 0.0096  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:40:27 d2.utils.events]: [0m eta: 0:04:27  iter: 1519  total_loss: 0.574  loss_cls: 0.186  loss_box_reg: 0.302  loss_rpn_cls: 0.035  loss_rpn_loc: 0.050  time: 0.5555  data_time: 0.0058  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:40:38 d2.utils.events]: [0m eta: 0:04:16  iter: 1539  total_loss: 0.573  loss_cls: 0.204  loss_box_reg: 0.323  loss_rpn_cls: 0.025  loss_rpn_loc: 0.050  time: 0.5554  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:40:49 d2.utils.events]: [0m eta: 0:04:05  iter: 1559  total_loss: 0.793  loss_cls: 0.237  loss_box_reg: 0.374  loss_rpn_cls: 0.035  loss_rpn_loc: 0.064  time: 0.5553  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:41:01 d2.utils.events]: [0m eta: 0:03:54  iter: 1579  total_loss: 0.650  loss_cls: 0.199  loss_box_reg: 0.315  loss_rpn_cls: 0.026  loss_rpn_loc: 0.057  time: 0.5555  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:41:12 d2.utils.events]: [0m eta: 0:03:43  iter: 1599  total_loss: 0.672  loss_cls: 0.210  loss_box_reg: 0.312  loss_rpn_cls: 0.032  loss_rpn_loc: 0.122  time: 0.5554  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:41:23 d2.utils.events]: [0m eta: 0:03:32  iter: 1619  total_loss: 0.614  loss_cls: 0.207  loss_box_reg: 0.300  loss_rpn_cls: 0.037  loss_rpn_loc: 0.068  time: 0.5554  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:41:34 d2.utils.events]: [0m eta: 0:03:20  iter: 1639  total_loss: 0.697  loss_cls: 0.218  loss_box_reg: 0.364  loss_rpn_cls: 0.029  loss_rpn_loc: 0.077  time: 0.5550  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:41:45 d2.utils.events]: [0m eta: 0:03:09  iter: 1659  total_loss: 0.739  loss_cls: 0.215  loss_box_reg: 0.342  loss_rpn_cls: 0.032  loss_rpn_loc: 0.069  time: 0.5552  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:41:56 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.683  loss_cls: 0.219  loss_box_reg: 0.379  loss_rpn_cls: 0.037  loss_rpn_loc: 0.086  time: 0.5550  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:42:07 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.748  loss_cls: 0.218  loss_box_reg: 0.429  loss_rpn_cls: 0.024  loss_rpn_loc: 0.078  time: 0.5550  data_time: 0.0106  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:42:19 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.792  loss_cls: 0.232  loss_box_reg: 0.388  loss_rpn_cls: 0.034  loss_rpn_loc: 0.056  time: 0.5550  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:42:29 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.652  loss_cls: 0.201  loss_box_reg: 0.315  loss_rpn_cls: 0.033  loss_rpn_loc: 0.054  time: 0.5548  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:42:40 d2.utils.events]: [0m eta: 0:02:14  iter: 1759  total_loss: 0.661  loss_cls: 0.235  loss_box_reg: 0.323  loss_rpn_cls: 0.038  loss_rpn_loc: 0.073  time: 0.5547  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:42:51 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.642  loss_cls: 0.226  loss_box_reg: 0.329  loss_rpn_cls: 0.036  loss_rpn_loc: 0.063  time: 0.5545  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:43:06 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.825  loss_cls: 0.271  loss_box_reg: 0.445  loss_rpn_cls: 0.021  loss_rpn_loc: 0.073  time: 0.5544  data_time: 0.0099  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:43:17 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.693  loss_cls: 0.225  loss_box_reg: 0.365  loss_rpn_cls: 0.031  loss_rpn_loc: 0.076  time: 0.5543  data_time: 0.0121  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:43:29 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.776  loss_cls: 0.235  loss_box_reg: 0.420  loss_rpn_cls: 0.035  loss_rpn_loc: 0.071  time: 0.5545  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:43:39 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.543  loss_cls: 0.167  loss_box_reg: 0.315  loss_rpn_cls: 0.022  loss_rpn_loc: 0.056  time: 0.5541  data_time: 0.0061  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:44:10 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.877  loss_cls: 0.252  loss_box_reg: 0.423  loss_rpn_cls: 0.042  loss_rpn_loc: 0.091  time: 0.5539  data_time: 0.0109  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:44:21 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.678  loss_cls: 0.223  loss_box_reg: 0.352  loss_rpn_cls: 0.025  loss_rpn_loc: 0.070  time: 0.5538  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:44:40 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.692  loss_cls: 0.237  loss_box_reg: 0.368  loss_rpn_cls: 0.024  loss_rpn_loc: 0.068  time: 0.5536  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:44:51 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.635  loss_cls: 0.195  loss_box_reg: 0.323  loss_rpn_cls: 0.027  loss_rpn_loc: 0.078  time: 0.5535  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:45:02 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.800  loss_cls: 0.260  loss_box_reg: 0.429  loss_rpn_cls: 0.031  loss_rpn_loc: 0.075  time: 0.5532  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:45:13 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.746  loss_cls: 0.232  loss_box_reg: 0.365  loss_rpn_cls: 0.023  loss_rpn_loc: 0.072  time: 0.5531  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 08:45:56 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:45:56 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 08:45:56 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 08:45:56 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.754  loss_cls: 0.250  loss_box_reg: 0.422  loss_rpn_cls: 0.035  loss_rpn_loc: 0.070  time: 0.5531  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:46:03 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:25 (0.5534 s / it)
[32m[04/15 08:46:03 d2.engine.hooks]: [0mTotal training time: 0:19:48 (0:01:23 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 08:46:25 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:46:25 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 08:46:25 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 08:46:29 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1213 s / img. ETA=0:06:48
[32m[04/15 08:46:35 d2.evaluation.evaluator]: [0mInference done 39/1257. 0.1219 s / img. ETA=0:04:11
[32m[04/15 08:46:40 d2.evaluation.evaluator]: [0mInference done 64/1257. 0.1217 s / img. ETA=0:04:04
[32m[04/15 08:46:45 d2.evaluation.evaluator]: [0mInference done 96/1257. 0.1213 s / img. ETA=0:03:39
[32m[04/15 08:46:50 d2.evaluation.evaluator]: [0mInference done 130/1257. 0.1209 s / img. ETA=0:03:21
[32m[04/15 08:46:55 d2.evaluation.evaluator]: [0mInference done 145/1257. 0.1205 s / img. ETA=0:03:39
[32m[04/15 08:47:00 d2.evaluation.evaluator]: [0mInference done 179/1257. 0.1206 s / img. ETA=0:03:22
[32m[04/15 08:47:05 d2.evaluation.evaluator]: [0mInference done 218/1257. 0.1206 s / img. ETA=0:03:04
[32m[04/15 08:47:10 d2.evaluation.evaluator]: [0mInference done 255/1257. 0.1209 s / img. ETA=0:02:51
[32m[04/15 08:47:15 d2.evaluation.evaluator]: [0mInference done 294/1257. 0.1213 s / img. ETA=0:02:39
[32m[04/15 08:47:20 d2.evaluation.evaluator]: [0mInference done 334/1257. 0.1214 s / img. ETA=0:02:28
[32m[04/15 08:47:26 d2.evaluation.evaluator]: [0mInference done 374/1257. 0.1217 s / img. ETA=0:02:18
[32m[04/15 08:47:31 d2.evaluation.evaluator]: [0mInference done 414/1257. 0.1218 s / img. ETA=0:02:09
[32m[04/15 08:47:36 d2.evaluation.evaluator]: [0mInference done 454/1257. 0.1219 s / img. ETA=0:02:01
[32m[04/15 08:47:41 d2.evaluation.evaluator]: [0mInference done 494/1257. 0.1219 s / img. ETA=0:01:54
[32m[04/15 08:47:46 d2.evaluation.evaluator]: [0mInference done 535/1257. 0.1219 s / img. ETA=0:01:46
[32m[04/15 08:47:51 d2.evaluation.evaluator]: [0mInference done 574/1257. 0.1221 s / img. ETA=0:01:39
[32m[04/15 08:47:56 d2.evaluation.evaluator]: [0mInference done 614/1257. 0.1221 s / img. ETA=0:01:33
[32m[04/15 08:48:01 d2.evaluation.evaluator]: [0mInference done 654/1257. 0.1222 s / img. ETA=0:01:26
[32m[04/15 08:48:06 d2.evaluation.evaluator]: [0mInference done 694/1257. 0.1222 s / img. ETA=0:01:20
[32m[04/15 08:48:11 d2.evaluation.evaluator]: [0mInference done 735/1257. 0.1222 s / img. ETA=0:01:14
[32m[04/15 08:48:16 d2.evaluation.evaluator]: [0mInference done 775/1257. 0.1223 s / img. ETA=0:01:07
[32m[04/15 08:48:21 d2.evaluation.evaluator]: [0mInference done 816/1257. 0.1222 s / img. ETA=0:01:01
[32m[04/15 08:48:26 d2.evaluation.evaluator]: [0mInference done 856/1257. 0.1223 s / img. ETA=0:00:55
[32m[04/15 08:48:31 d2.evaluation.evaluator]: [0mInference done 896/1257. 0.1224 s / img. ETA=0:00:50
[32m[04/15 08:48:36 d2.evaluation.evaluator]: [0mInference done 935/1257. 0.1225 s / img. ETA=0:00:44
[32m[04/15 08:48:41 d2.evaluation.evaluator]: [0mInference done 974/1257. 0.1226 s / img. ETA=0:00:39
[32m[04/15 08:48:47 d2.evaluation.evaluator]: [0mInference done 1014/1257. 0.1227 s / img. ETA=0:00:33
[32m[04/15 08:48:52 d2.evaluation.evaluator]: [0mInference done 1054/1257. 0.1227 s / img. ETA=0:00:27
[32m[04/15 08:48:57 d2.evaluation.evaluator]: [0mInference done 1094/1257. 0.1228 s / img. ETA=0:00:22
[32m[04/15 08:49:02 d2.evaluation.evaluator]: [0mInference done 1134/1257. 0.1229 s / img. ETA=0:00:16
[32m[04/15 08:49:07 d2.evaluation.evaluator]: [0mInference done 1173/1257. 0.1230 s / img. ETA=0:00:11
[32m[04/15 08:49:12 d2.evaluation.evaluator]: [0mInference done 1213/1257. 0.1230 s / img. ETA=0:00:05
[32m[04/15 08:49:17 d2.evaluation.evaluator]: [0mInference done 1252/1257. 0.1231 s / img. ETA=0:00:00
[32m[04/15 08:49:18 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:50.341674 (0.136056 s / img per device, on 1 devices)
[32m[04/15 08:49:18 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:34 (0.123123 s / img per device, on 1 devices)
[32m[04/15 08:49:18 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 08:49:18 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 08:49:18 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.89s).
Accumulating evaluation results...
DONE (t=0.80s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.171
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.351
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.106
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.082
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.150
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.255
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.405
[32m[04/15 08:49:25 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 17.124 | 35.119 | 14.033 | 10.584 | 21.173 | 37.713 |
[32m[04/15 08:49:25 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 22.559 | bicycle       | 4.988 | car            | 40.947 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  9  *  2000  iterations ============
4 channel input
[32m[04/15 08:49:26 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[5m[31mWARNING[0m [32m[04/15 08:49:27 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:49:27 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 08:49:28 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 08:49:28 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 08:49:28 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 08:49:29 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 08:49:40 d2.utils.events]: [0m eta: 0:18:17  iter: 19  total_loss: 0.718  loss_cls: 0.224  loss_box_reg: 0.380  loss_rpn_cls: 0.034  loss_rpn_loc: 0.071  time: 0.5517  data_time: 0.0318  lr: 0.000200  max_mem: 3070M
[32m[04/15 08:49:51 d2.utils.events]: [0m eta: 0:17:50  iter: 39  total_loss: 0.706  loss_cls: 0.219  loss_box_reg: 0.348  loss_rpn_cls: 0.024  loss_rpn_loc: 0.061  time: 0.5441  data_time: 0.0099  lr: 0.000400  max_mem: 3070M
[32m[04/15 08:50:02 d2.utils.events]: [0m eta: 0:17:47  iter: 59  total_loss: 0.769  loss_cls: 0.249  loss_box_reg: 0.414  loss_rpn_cls: 0.027  loss_rpn_loc: 0.069  time: 0.5469  data_time: 0.0099  lr: 0.000599  max_mem: 3070M
[32m[04/15 08:50:13 d2.utils.events]: [0m eta: 0:17:33  iter: 79  total_loss: 0.711  loss_cls: 0.204  loss_box_reg: 0.344  loss_rpn_cls: 0.028  loss_rpn_loc: 0.084  time: 0.5455  data_time: 0.0062  lr: 0.000799  max_mem: 3070M
[32m[04/15 08:50:24 d2.utils.events]: [0m eta: 0:17:29  iter: 99  total_loss: 0.710  loss_cls: 0.218  loss_box_reg: 0.379  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5466  data_time: 0.0095  lr: 0.000999  max_mem: 3070M
[32m[04/15 08:50:35 d2.utils.events]: [0m eta: 0:17:19  iter: 119  total_loss: 0.698  loss_cls: 0.233  loss_box_reg: 0.389  loss_rpn_cls: 0.023  loss_rpn_loc: 0.061  time: 0.5474  data_time: 0.0078  lr: 0.001199  max_mem: 3070M
[32m[04/15 08:50:46 d2.utils.events]: [0m eta: 0:17:08  iter: 139  total_loss: 0.547  loss_cls: 0.189  loss_box_reg: 0.275  loss_rpn_cls: 0.021  loss_rpn_loc: 0.053  time: 0.5476  data_time: 0.0080  lr: 0.001399  max_mem: 3070M
[32m[04/15 08:50:58 d2.utils.events]: [0m eta: 0:16:58  iter: 159  total_loss: 0.640  loss_cls: 0.227  loss_box_reg: 0.327  loss_rpn_cls: 0.021  loss_rpn_loc: 0.050  time: 0.5493  data_time: 0.0070  lr: 0.001598  max_mem: 3070M
[32m[04/15 08:51:09 d2.utils.events]: [0m eta: 0:16:47  iter: 179  total_loss: 0.761  loss_cls: 0.219  loss_box_reg: 0.388  loss_rpn_cls: 0.024  loss_rpn_loc: 0.068  time: 0.5500  data_time: 0.0090  lr: 0.001798  max_mem: 3070M
[32m[04/15 08:51:20 d2.utils.events]: [0m eta: 0:16:37  iter: 199  total_loss: 0.588  loss_cls: 0.192  loss_box_reg: 0.325  loss_rpn_cls: 0.018  loss_rpn_loc: 0.051  time: 0.5522  data_time: 0.0081  lr: 0.001998  max_mem: 3070M
[32m[04/15 08:51:32 d2.utils.events]: [0m eta: 0:16:28  iter: 219  total_loss: 0.679  loss_cls: 0.232  loss_box_reg: 0.350  loss_rpn_cls: 0.018  loss_rpn_loc: 0.057  time: 0.5523  data_time: 0.0074  lr: 0.002198  max_mem: 3070M
[32m[04/15 08:51:43 d2.utils.events]: [0m eta: 0:16:15  iter: 239  total_loss: 0.775  loss_cls: 0.250  loss_box_reg: 0.396  loss_rpn_cls: 0.025  loss_rpn_loc: 0.084  time: 0.5523  data_time: 0.0068  lr: 0.002398  max_mem: 3070M
[32m[04/15 08:51:54 d2.utils.events]: [0m eta: 0:16:06  iter: 259  total_loss: 0.534  loss_cls: 0.186  loss_box_reg: 0.296  loss_rpn_cls: 0.014  loss_rpn_loc: 0.044  time: 0.5528  data_time: 0.0087  lr: 0.002597  max_mem: 3070M
[32m[04/15 08:52:05 d2.utils.events]: [0m eta: 0:15:55  iter: 279  total_loss: 0.599  loss_cls: 0.215  loss_box_reg: 0.330  loss_rpn_cls: 0.019  loss_rpn_loc: 0.053  time: 0.5527  data_time: 0.0063  lr: 0.002797  max_mem: 3070M
[32m[04/15 08:52:16 d2.utils.events]: [0m eta: 0:15:42  iter: 299  total_loss: 0.603  loss_cls: 0.189  loss_box_reg: 0.334  loss_rpn_cls: 0.020  loss_rpn_loc: 0.057  time: 0.5512  data_time: 0.0077  lr: 0.002997  max_mem: 3070M
[32m[04/15 08:52:27 d2.utils.events]: [0m eta: 0:15:33  iter: 319  total_loss: 0.675  loss_cls: 0.217  loss_box_reg: 0.381  loss_rpn_cls: 0.021  loss_rpn_loc: 0.052  time: 0.5532  data_time: 0.0077  lr: 0.003197  max_mem: 3070M
[32m[04/15 08:52:38 d2.utils.events]: [0m eta: 0:15:22  iter: 339  total_loss: 0.619  loss_cls: 0.205  loss_box_reg: 0.319  loss_rpn_cls: 0.021  loss_rpn_loc: 0.047  time: 0.5526  data_time: 0.0075  lr: 0.003397  max_mem: 3070M
[32m[04/15 08:52:50 d2.utils.events]: [0m eta: 0:15:11  iter: 359  total_loss: 0.809  loss_cls: 0.241  loss_box_reg: 0.391  loss_rpn_cls: 0.028  loss_rpn_loc: 0.060  time: 0.5530  data_time: 0.0059  lr: 0.003596  max_mem: 3070M
[32m[04/15 08:53:01 d2.utils.events]: [0m eta: 0:15:00  iter: 379  total_loss: 0.616  loss_cls: 0.216  loss_box_reg: 0.337  loss_rpn_cls: 0.020  loss_rpn_loc: 0.045  time: 0.5538  data_time: 0.0071  lr: 0.003796  max_mem: 3070M
[32m[04/15 08:53:12 d2.utils.events]: [0m eta: 0:14:49  iter: 399  total_loss: 0.668  loss_cls: 0.203  loss_box_reg: 0.346  loss_rpn_cls: 0.026  loss_rpn_loc: 0.050  time: 0.5526  data_time: 0.0076  lr: 0.003996  max_mem: 3070M
[32m[04/15 08:53:23 d2.utils.events]: [0m eta: 0:14:36  iter: 419  total_loss: 0.605  loss_cls: 0.205  loss_box_reg: 0.304  loss_rpn_cls: 0.021  loss_rpn_loc: 0.066  time: 0.5514  data_time: 0.0063  lr: 0.004196  max_mem: 3070M
[32m[04/15 08:53:34 d2.utils.events]: [0m eta: 0:14:25  iter: 439  total_loss: 0.689  loss_cls: 0.210  loss_box_reg: 0.350  loss_rpn_cls: 0.025  loss_rpn_loc: 0.070  time: 0.5509  data_time: 0.0062  lr: 0.004396  max_mem: 3070M
[32m[04/15 08:53:45 d2.utils.events]: [0m eta: 0:14:14  iter: 459  total_loss: 0.751  loss_cls: 0.234  loss_box_reg: 0.411  loss_rpn_cls: 0.026  loss_rpn_loc: 0.071  time: 0.5509  data_time: 0.0072  lr: 0.004595  max_mem: 3070M
[32m[04/15 08:53:56 d2.utils.events]: [0m eta: 0:14:04  iter: 479  total_loss: 0.606  loss_cls: 0.195  loss_box_reg: 0.330  loss_rpn_cls: 0.018  loss_rpn_loc: 0.042  time: 0.5518  data_time: 0.0069  lr: 0.004795  max_mem: 3070M
[32m[04/15 08:54:07 d2.utils.events]: [0m eta: 0:13:53  iter: 499  total_loss: 0.809  loss_cls: 0.253  loss_box_reg: 0.425  loss_rpn_cls: 0.025  loss_rpn_loc: 0.065  time: 0.5520  data_time: 0.0073  lr: 0.004995  max_mem: 3070M
[32m[04/15 08:54:19 d2.utils.events]: [0m eta: 0:13:42  iter: 519  total_loss: 0.670  loss_cls: 0.212  loss_box_reg: 0.380  loss_rpn_cls: 0.019  loss_rpn_loc: 0.046  time: 0.5520  data_time: 0.0077  lr: 0.005195  max_mem: 3070M
[32m[04/15 08:54:30 d2.utils.events]: [0m eta: 0:13:30  iter: 539  total_loss: 0.655  loss_cls: 0.215  loss_box_reg: 0.359  loss_rpn_cls: 0.017  loss_rpn_loc: 0.052  time: 0.5520  data_time: 0.0067  lr: 0.005395  max_mem: 3070M
[32m[04/15 08:54:41 d2.utils.events]: [0m eta: 0:13:20  iter: 559  total_loss: 0.623  loss_cls: 0.190  loss_box_reg: 0.326  loss_rpn_cls: 0.023  loss_rpn_loc: 0.050  time: 0.5526  data_time: 0.0059  lr: 0.005594  max_mem: 3070M
[32m[04/15 08:54:53 d2.utils.events]: [0m eta: 0:13:09  iter: 579  total_loss: 0.684  loss_cls: 0.211  loss_box_reg: 0.365  loss_rpn_cls: 0.024  loss_rpn_loc: 0.063  time: 0.5531  data_time: 0.0074  lr: 0.005794  max_mem: 3070M
[32m[04/15 08:55:04 d2.utils.events]: [0m eta: 0:12:58  iter: 599  total_loss: 0.621  loss_cls: 0.208  loss_box_reg: 0.307  loss_rpn_cls: 0.023  loss_rpn_loc: 0.052  time: 0.5529  data_time: 0.0080  lr: 0.005994  max_mem: 3070M
[32m[04/15 08:55:15 d2.utils.events]: [0m eta: 0:12:47  iter: 619  total_loss: 0.819  loss_cls: 0.239  loss_box_reg: 0.390  loss_rpn_cls: 0.027  loss_rpn_loc: 0.060  time: 0.5532  data_time: 0.0084  lr: 0.006194  max_mem: 3070M
[32m[04/15 08:55:26 d2.utils.events]: [0m eta: 0:12:36  iter: 639  total_loss: 0.628  loss_cls: 0.192  loss_box_reg: 0.332  loss_rpn_cls: 0.019  loss_rpn_loc: 0.083  time: 0.5527  data_time: 0.0064  lr: 0.006394  max_mem: 3070M
[32m[04/15 08:55:37 d2.utils.events]: [0m eta: 0:12:25  iter: 659  total_loss: 0.723  loss_cls: 0.233  loss_box_reg: 0.374  loss_rpn_cls: 0.026  loss_rpn_loc: 0.067  time: 0.5529  data_time: 0.0108  lr: 0.006593  max_mem: 3070M
[32m[04/15 08:55:49 d2.utils.events]: [0m eta: 0:12:14  iter: 679  total_loss: 0.630  loss_cls: 0.201  loss_box_reg: 0.350  loss_rpn_cls: 0.019  loss_rpn_loc: 0.064  time: 0.5534  data_time: 0.0076  lr: 0.006793  max_mem: 3070M
[32m[04/15 08:56:00 d2.utils.events]: [0m eta: 0:12:03  iter: 699  total_loss: 0.622  loss_cls: 0.196  loss_box_reg: 0.314  loss_rpn_cls: 0.018  loss_rpn_loc: 0.060  time: 0.5537  data_time: 0.0093  lr: 0.006993  max_mem: 3070M
[32m[04/15 08:56:11 d2.utils.events]: [0m eta: 0:11:52  iter: 719  total_loss: 0.695  loss_cls: 0.213  loss_box_reg: 0.365  loss_rpn_cls: 0.022  loss_rpn_loc: 0.071  time: 0.5537  data_time: 0.0094  lr: 0.007193  max_mem: 3070M
[32m[04/15 08:56:22 d2.utils.events]: [0m eta: 0:11:41  iter: 739  total_loss: 0.659  loss_cls: 0.214  loss_box_reg: 0.360  loss_rpn_cls: 0.023  loss_rpn_loc: 0.052  time: 0.5534  data_time: 0.0071  lr: 0.007393  max_mem: 3070M
[32m[04/15 08:56:33 d2.utils.events]: [0m eta: 0:11:30  iter: 759  total_loss: 0.731  loss_cls: 0.243  loss_box_reg: 0.370  loss_rpn_cls: 0.030  loss_rpn_loc: 0.066  time: 0.5536  data_time: 0.0068  lr: 0.007592  max_mem: 3070M
[32m[04/15 08:56:44 d2.utils.events]: [0m eta: 0:11:19  iter: 779  total_loss: 0.696  loss_cls: 0.224  loss_box_reg: 0.349  loss_rpn_cls: 0.028  loss_rpn_loc: 0.062  time: 0.5535  data_time: 0.0079  lr: 0.007792  max_mem: 3070M
[32m[04/15 08:56:56 d2.utils.events]: [0m eta: 0:11:08  iter: 799  total_loss: 0.564  loss_cls: 0.187  loss_box_reg: 0.322  loss_rpn_cls: 0.018  loss_rpn_loc: 0.050  time: 0.5537  data_time: 0.0074  lr: 0.007992  max_mem: 3070M
[32m[04/15 08:57:07 d2.utils.events]: [0m eta: 0:10:56  iter: 819  total_loss: 0.665  loss_cls: 0.215  loss_box_reg: 0.340  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5537  data_time: 0.0084  lr: 0.008192  max_mem: 3070M
[32m[04/15 08:57:18 d2.utils.events]: [0m eta: 0:10:45  iter: 839  total_loss: 0.597  loss_cls: 0.221  loss_box_reg: 0.325  loss_rpn_cls: 0.020  loss_rpn_loc: 0.046  time: 0.5539  data_time: 0.0073  lr: 0.008392  max_mem: 3070M
[32m[04/15 08:57:30 d2.utils.events]: [0m eta: 0:10:35  iter: 859  total_loss: 0.595  loss_cls: 0.187  loss_box_reg: 0.324  loss_rpn_cls: 0.019  loss_rpn_loc: 0.062  time: 0.5541  data_time: 0.0074  lr: 0.008591  max_mem: 3070M
[32m[04/15 08:57:41 d2.utils.events]: [0m eta: 0:10:23  iter: 879  total_loss: 0.625  loss_cls: 0.213  loss_box_reg: 0.319  loss_rpn_cls: 0.027  loss_rpn_loc: 0.059  time: 0.5539  data_time: 0.0069  lr: 0.008791  max_mem: 3070M
[32m[04/15 08:57:52 d2.utils.events]: [0m eta: 0:10:13  iter: 899  total_loss: 0.587  loss_cls: 0.176  loss_box_reg: 0.315  loss_rpn_cls: 0.022  loss_rpn_loc: 0.063  time: 0.5544  data_time: 0.0067  lr: 0.008991  max_mem: 3070M
[32m[04/15 08:58:03 d2.utils.events]: [0m eta: 0:10:01  iter: 919  total_loss: 0.635  loss_cls: 0.219  loss_box_reg: 0.384  loss_rpn_cls: 0.021  loss_rpn_loc: 0.060  time: 0.5544  data_time: 0.0081  lr: 0.009191  max_mem: 3070M
[32m[04/15 08:58:14 d2.utils.events]: [0m eta: 0:09:50  iter: 939  total_loss: 0.554  loss_cls: 0.202  loss_box_reg: 0.307  loss_rpn_cls: 0.021  loss_rpn_loc: 0.038  time: 0.5542  data_time: 0.0098  lr: 0.009391  max_mem: 3070M
[32m[04/15 08:58:25 d2.utils.events]: [0m eta: 0:09:39  iter: 959  total_loss: 0.601  loss_cls: 0.218  loss_box_reg: 0.321  loss_rpn_cls: 0.019  loss_rpn_loc: 0.056  time: 0.5539  data_time: 0.0063  lr: 0.009590  max_mem: 3070M
[32m[04/15 08:58:36 d2.utils.events]: [0m eta: 0:09:28  iter: 979  total_loss: 0.686  loss_cls: 0.245  loss_box_reg: 0.356  loss_rpn_cls: 0.024  loss_rpn_loc: 0.071  time: 0.5538  data_time: 0.0078  lr: 0.009790  max_mem: 3070M
[32m[04/15 08:58:48 d2.utils.events]: [0m eta: 0:09:17  iter: 999  total_loss: 0.729  loss_cls: 0.221  loss_box_reg: 0.420  loss_rpn_cls: 0.029  loss_rpn_loc: 0.061  time: 0.5539  data_time: 0.0077  lr: 0.009990  max_mem: 3070M
[32m[04/15 08:58:59 d2.utils.events]: [0m eta: 0:09:06  iter: 1019  total_loss: 0.638  loss_cls: 0.206  loss_box_reg: 0.364  loss_rpn_cls: 0.020  loss_rpn_loc: 0.044  time: 0.5542  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:59:10 d2.utils.events]: [0m eta: 0:08:55  iter: 1039  total_loss: 0.705  loss_cls: 0.237  loss_box_reg: 0.349  loss_rpn_cls: 0.030  loss_rpn_loc: 0.057  time: 0.5544  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:59:22 d2.utils.events]: [0m eta: 0:08:44  iter: 1059  total_loss: 0.673  loss_cls: 0.238  loss_box_reg: 0.311  loss_rpn_cls: 0.031  loss_rpn_loc: 0.049  time: 0.5548  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:59:33 d2.utils.events]: [0m eta: 0:08:32  iter: 1079  total_loss: 0.687  loss_cls: 0.239  loss_box_reg: 0.349  loss_rpn_cls: 0.031  loss_rpn_loc: 0.081  time: 0.5542  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:59:43 d2.utils.events]: [0m eta: 0:08:21  iter: 1099  total_loss: 0.817  loss_cls: 0.262  loss_box_reg: 0.433  loss_rpn_cls: 0.027  loss_rpn_loc: 0.088  time: 0.5539  data_time: 0.0060  lr: 0.010000  max_mem: 3070M
[32m[04/15 08:59:55 d2.utils.events]: [0m eta: 0:08:10  iter: 1119  total_loss: 0.639  loss_cls: 0.207  loss_box_reg: 0.348  loss_rpn_cls: 0.022  loss_rpn_loc: 0.053  time: 0.5541  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:00:07 d2.utils.events]: [0m eta: 0:07:59  iter: 1139  total_loss: 0.703  loss_cls: 0.226  loss_box_reg: 0.365  loss_rpn_cls: 0.024  loss_rpn_loc: 0.057  time: 0.5547  data_time: 0.0807  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:00:19 d2.utils.events]: [0m eta: 0:07:48  iter: 1159  total_loss: 0.677  loss_cls: 0.209  loss_box_reg: 0.359  loss_rpn_cls: 0.021  loss_rpn_loc: 0.060  time: 0.5558  data_time: 0.0405  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:00:30 d2.utils.events]: [0m eta: 0:07:37  iter: 1179  total_loss: 0.681  loss_cls: 0.226  loss_box_reg: 0.347  loss_rpn_cls: 0.037  loss_rpn_loc: 0.077  time: 0.5556  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:00:41 d2.utils.events]: [0m eta: 0:07:25  iter: 1199  total_loss: 0.679  loss_cls: 0.214  loss_box_reg: 0.372  loss_rpn_cls: 0.028  loss_rpn_loc: 0.066  time: 0.5555  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:00:53 d2.utils.events]: [0m eta: 0:07:14  iter: 1219  total_loss: 0.814  loss_cls: 0.261  loss_box_reg: 0.453  loss_rpn_cls: 0.023  loss_rpn_loc: 0.080  time: 0.5556  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:01:03 d2.utils.events]: [0m eta: 0:07:03  iter: 1239  total_loss: 0.686  loss_cls: 0.245  loss_box_reg: 0.388  loss_rpn_cls: 0.036  loss_rpn_loc: 0.066  time: 0.5552  data_time: 0.0111  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:01:15 d2.utils.events]: [0m eta: 0:06:52  iter: 1259  total_loss: 0.792  loss_cls: 0.254  loss_box_reg: 0.391  loss_rpn_cls: 0.036  loss_rpn_loc: 0.082  time: 0.5556  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:01:26 d2.utils.events]: [0m eta: 0:06:41  iter: 1279  total_loss: 0.642  loss_cls: 0.207  loss_box_reg: 0.331  loss_rpn_cls: 0.027  loss_rpn_loc: 0.061  time: 0.5556  data_time: 0.0099  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:01:37 d2.utils.events]: [0m eta: 0:06:30  iter: 1299  total_loss: 0.673  loss_cls: 0.227  loss_box_reg: 0.359  loss_rpn_cls: 0.040  loss_rpn_loc: 0.080  time: 0.5555  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:01:49 d2.utils.events]: [0m eta: 0:06:19  iter: 1319  total_loss: 0.710  loss_cls: 0.254  loss_box_reg: 0.378  loss_rpn_cls: 0.023  loss_rpn_loc: 0.070  time: 0.5556  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:02:00 d2.utils.events]: [0m eta: 0:06:08  iter: 1339  total_loss: 0.614  loss_cls: 0.218  loss_box_reg: 0.314  loss_rpn_cls: 0.038  loss_rpn_loc: 0.057  time: 0.5556  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:02:12 d2.utils.events]: [0m eta: 0:05:57  iter: 1359  total_loss: 0.530  loss_cls: 0.170  loss_box_reg: 0.280  loss_rpn_cls: 0.025  loss_rpn_loc: 0.041  time: 0.5557  data_time: 0.0118  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:02:22 d2.utils.events]: [0m eta: 0:05:45  iter: 1379  total_loss: 0.555  loss_cls: 0.173  loss_box_reg: 0.270  loss_rpn_cls: 0.030  loss_rpn_loc: 0.054  time: 0.5555  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:02:34 d2.utils.events]: [0m eta: 0:05:35  iter: 1399  total_loss: 0.720  loss_cls: 0.211  loss_box_reg: 0.366  loss_rpn_cls: 0.032  loss_rpn_loc: 0.077  time: 0.5558  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:02:45 d2.utils.events]: [0m eta: 0:05:24  iter: 1419  total_loss: 0.588  loss_cls: 0.190  loss_box_reg: 0.321  loss_rpn_cls: 0.024  loss_rpn_loc: 0.052  time: 0.5558  data_time: 0.0113  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:02:56 d2.utils.events]: [0m eta: 0:05:13  iter: 1439  total_loss: 0.607  loss_cls: 0.193  loss_box_reg: 0.340  loss_rpn_cls: 0.031  loss_rpn_loc: 0.062  time: 0.5556  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:03:07 d2.utils.events]: [0m eta: 0:05:01  iter: 1459  total_loss: 0.623  loss_cls: 0.193  loss_box_reg: 0.362  loss_rpn_cls: 0.019  loss_rpn_loc: 0.042  time: 0.5555  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:03:19 d2.utils.events]: [0m eta: 0:04:50  iter: 1479  total_loss: 0.770  loss_cls: 0.267  loss_box_reg: 0.410  loss_rpn_cls: 0.031  loss_rpn_loc: 0.081  time: 0.5556  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:03:30 d2.utils.events]: [0m eta: 0:04:39  iter: 1499  total_loss: 0.855  loss_cls: 0.255  loss_box_reg: 0.439  loss_rpn_cls: 0.027  loss_rpn_loc: 0.081  time: 0.5555  data_time: 0.0062  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:03:41 d2.utils.events]: [0m eta: 0:04:28  iter: 1519  total_loss: 0.700  loss_cls: 0.219  loss_box_reg: 0.342  loss_rpn_cls: 0.040  loss_rpn_loc: 0.060  time: 0.5554  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:03:52 d2.utils.events]: [0m eta: 0:04:16  iter: 1539  total_loss: 0.794  loss_cls: 0.244  loss_box_reg: 0.392  loss_rpn_cls: 0.039  loss_rpn_loc: 0.085  time: 0.5552  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:04:03 d2.utils.events]: [0m eta: 0:04:05  iter: 1559  total_loss: 0.663  loss_cls: 0.228  loss_box_reg: 0.339  loss_rpn_cls: 0.029  loss_rpn_loc: 0.069  time: 0.5554  data_time: 0.0103  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:04:15 d2.utils.events]: [0m eta: 0:03:54  iter: 1579  total_loss: 0.709  loss_cls: 0.223  loss_box_reg: 0.378  loss_rpn_cls: 0.032  loss_rpn_loc: 0.067  time: 0.5556  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:04:25 d2.utils.events]: [0m eta: 0:03:43  iter: 1599  total_loss: 0.587  loss_cls: 0.171  loss_box_reg: 0.291  loss_rpn_cls: 0.026  loss_rpn_loc: 0.064  time: 0.5552  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:04:36 d2.utils.events]: [0m eta: 0:03:32  iter: 1619  total_loss: 0.641  loss_cls: 0.204  loss_box_reg: 0.347  loss_rpn_cls: 0.025  loss_rpn_loc: 0.052  time: 0.5552  data_time: 0.0109  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:04:47 d2.utils.events]: [0m eta: 0:03:21  iter: 1639  total_loss: 0.712  loss_cls: 0.246  loss_box_reg: 0.349  loss_rpn_cls: 0.021  loss_rpn_loc: 0.067  time: 0.5551  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:04:59 d2.utils.events]: [0m eta: 0:03:10  iter: 1659  total_loss: 0.759  loss_cls: 0.230  loss_box_reg: 0.410  loss_rpn_cls: 0.025  loss_rpn_loc: 0.080  time: 0.5551  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:05:10 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.613  loss_cls: 0.217  loss_box_reg: 0.308  loss_rpn_cls: 0.023  loss_rpn_loc: 0.043  time: 0.5552  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:05:21 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.721  loss_cls: 0.217  loss_box_reg: 0.373  loss_rpn_cls: 0.031  loss_rpn_loc: 0.063  time: 0.5552  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:05:32 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.484  loss_cls: 0.159  loss_box_reg: 0.274  loss_rpn_cls: 0.025  loss_rpn_loc: 0.040  time: 0.5550  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:05:44 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.775  loss_cls: 0.260  loss_box_reg: 0.410  loss_rpn_cls: 0.031  loss_rpn_loc: 0.072  time: 0.5553  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:05:55 d2.utils.events]: [0m eta: 0:02:14  iter: 1759  total_loss: 0.616  loss_cls: 0.197  loss_box_reg: 0.342  loss_rpn_cls: 0.028  loss_rpn_loc: 0.052  time: 0.5553  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:06:06 d2.utils.events]: [0m eta: 0:02:03  iter: 1779  total_loss: 0.780  loss_cls: 0.242  loss_box_reg: 0.395  loss_rpn_cls: 0.044  loss_rpn_loc: 0.090  time: 0.5553  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:06:19 d2.utils.events]: [0m eta: 0:01:52  iter: 1799  total_loss: 0.691  loss_cls: 0.224  loss_box_reg: 0.378  loss_rpn_cls: 0.030  loss_rpn_loc: 0.066  time: 0.5552  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:06:31 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.703  loss_cls: 0.216  loss_box_reg: 0.386  loss_rpn_cls: 0.020  loss_rpn_loc: 0.085  time: 0.5551  data_time: 0.0425  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:06:42 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.642  loss_cls: 0.234  loss_box_reg: 0.346  loss_rpn_cls: 0.031  loss_rpn_loc: 0.066  time: 0.5549  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:06:53 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.652  loss_cls: 0.211  loss_box_reg: 0.342  loss_rpn_cls: 0.025  loss_rpn_loc: 0.064  time: 0.5548  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:07:04 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.540  loss_cls: 0.178  loss_box_reg: 0.273  loss_rpn_cls: 0.023  loss_rpn_loc: 0.046  time: 0.5548  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:07:24 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.754  loss_cls: 0.259  loss_box_reg: 0.384  loss_rpn_cls: 0.035  loss_rpn_loc: 0.070  time: 0.5546  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:07:35 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.626  loss_cls: 0.201  loss_box_reg: 0.320  loss_rpn_cls: 0.031  loss_rpn_loc: 0.066  time: 0.5545  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:07:46 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.703  loss_cls: 0.234  loss_box_reg: 0.413  loss_rpn_cls: 0.046  loss_rpn_loc: 0.058  time: 0.5543  data_time: 0.0094  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:07:57 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.626  loss_cls: 0.224  loss_box_reg: 0.336  loss_rpn_cls: 0.024  loss_rpn_loc: 0.060  time: 0.5541  data_time: 0.0092  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:08:08 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.716  loss_cls: 0.246  loss_box_reg: 0.384  loss_rpn_cls: 0.029  loss_rpn_loc: 0.072  time: 0.5540  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 09:08:47 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:08:47 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 09:08:47 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 09:08:47 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.815  loss_cls: 0.265  loss_box_reg: 0.414  loss_rpn_cls: 0.031  loss_rpn_loc: 0.077  time: 0.5540  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:08:52 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:27 (0.5543 s / it)
[32m[04/15 09:08:52 d2.engine.hooks]: [0mTotal training time: 0:19:21 (0:00:54 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 09:09:11 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:09:11 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 09:09:11 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 09:09:13 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1229 s / img. ETA=0:02:35
[32m[04/15 09:09:18 d2.evaluation.evaluator]: [0mInference done 46/1257. 0.1228 s / img. ETA=0:02:51
[32m[04/15 09:09:24 d2.evaluation.evaluator]: [0mInference done 76/1257. 0.1220 s / img. ETA=0:03:02
[32m[04/15 09:09:29 d2.evaluation.evaluator]: [0mInference done 107/1257. 0.1221 s / img. ETA=0:03:01
[32m[04/15 09:09:34 d2.evaluation.evaluator]: [0mInference done 139/1257. 0.1221 s / img. ETA=0:02:56
[32m[04/15 09:09:39 d2.evaluation.evaluator]: [0mInference done 172/1257. 0.1223 s / img. ETA=0:02:50
[32m[04/15 09:09:44 d2.evaluation.evaluator]: [0mInference done 211/1257. 0.1226 s / img. ETA=0:02:39
[32m[04/15 09:09:49 d2.evaluation.evaluator]: [0mInference done 248/1257. 0.1225 s / img. ETA=0:02:31
[32m[04/15 09:09:54 d2.evaluation.evaluator]: [0mInference done 288/1257. 0.1224 s / img. ETA=0:02:22
[32m[04/15 09:09:59 d2.evaluation.evaluator]: [0mInference done 327/1257. 0.1222 s / img. ETA=0:02:14
[32m[04/15 09:10:04 d2.evaluation.evaluator]: [0mInference done 368/1257. 0.1221 s / img. ETA=0:02:06
[32m[04/15 09:10:09 d2.evaluation.evaluator]: [0mInference done 408/1257. 0.1222 s / img. ETA=0:01:59
[32m[04/15 09:10:15 d2.evaluation.evaluator]: [0mInference done 440/1257. 0.1222 s / img. ETA=0:01:57
[32m[04/15 09:10:20 d2.evaluation.evaluator]: [0mInference done 477/1257. 0.1222 s / img. ETA=0:01:51
[32m[04/15 09:10:25 d2.evaluation.evaluator]: [0mInference done 517/1257. 0.1224 s / img. ETA=0:01:44
[32m[04/15 09:10:30 d2.evaluation.evaluator]: [0mInference done 557/1257. 0.1225 s / img. ETA=0:01:38
[32m[04/15 09:10:35 d2.evaluation.evaluator]: [0mInference done 597/1257. 0.1226 s / img. ETA=0:01:32
[32m[04/15 09:10:40 d2.evaluation.evaluator]: [0mInference done 637/1257. 0.1226 s / img. ETA=0:01:25
[32m[04/15 09:10:45 d2.evaluation.evaluator]: [0mInference done 666/1257. 0.1225 s / img. ETA=0:01:22
[32m[04/15 09:10:50 d2.evaluation.evaluator]: [0mInference done 702/1257. 0.1226 s / img. ETA=0:01:17
[32m[04/15 09:10:55 d2.evaluation.evaluator]: [0mInference done 739/1257. 0.1228 s / img. ETA=0:01:12
[32m[04/15 09:11:00 d2.evaluation.evaluator]: [0mInference done 779/1257. 0.1228 s / img. ETA=0:01:06
[32m[04/15 09:11:06 d2.evaluation.evaluator]: [0mInference done 819/1257. 0.1229 s / img. ETA=0:01:00
[32m[04/15 09:11:11 d2.evaluation.evaluator]: [0mInference done 858/1257. 0.1230 s / img. ETA=0:00:55
[32m[04/15 09:11:16 d2.evaluation.evaluator]: [0mInference done 897/1257. 0.1231 s / img. ETA=0:00:49
[32m[04/15 09:11:21 d2.evaluation.evaluator]: [0mInference done 937/1257. 0.1232 s / img. ETA=0:00:43
[32m[04/15 09:11:26 d2.evaluation.evaluator]: [0mInference done 976/1257. 0.1233 s / img. ETA=0:00:38
[32m[04/15 09:11:31 d2.evaluation.evaluator]: [0mInference done 1015/1257. 0.1234 s / img. ETA=0:00:33
[32m[04/15 09:11:36 d2.evaluation.evaluator]: [0mInference done 1054/1257. 0.1235 s / img. ETA=0:00:27
[32m[04/15 09:11:41 d2.evaluation.evaluator]: [0mInference done 1093/1257. 0.1236 s / img. ETA=0:00:22
[32m[04/15 09:11:46 d2.evaluation.evaluator]: [0mInference done 1132/1257. 0.1237 s / img. ETA=0:00:17
[32m[04/15 09:11:51 d2.evaluation.evaluator]: [0mInference done 1171/1257. 0.1237 s / img. ETA=0:00:11
[32m[04/15 09:11:56 d2.evaluation.evaluator]: [0mInference done 1210/1257. 0.1238 s / img. ETA=0:00:06
[32m[04/15 09:12:01 d2.evaluation.evaluator]: [0mInference done 1250/1257. 0.1238 s / img. ETA=0:00:00
[32m[04/15 09:12:02 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:49.485119 (0.135372 s / img per device, on 1 devices)
[32m[04/15 09:12:02 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:34 (0.123790 s / img per device, on 1 devices)
[32m[04/15 09:12:02 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 09:12:02 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 09:12:02 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.34s).
Accumulating evaluation results...
DONE (t=0.58s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.195
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.383
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.169
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.125
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.233
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.434
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.097
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.246
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.172
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.292
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.484
[32m[04/15 09:12:08 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.456 | 38.323 | 16.907 | 12.452 | 23.338 | 43.404 |
[32m[04/15 09:12:08 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 27.404 | bicycle       | 8.127 | car            | 42.296 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  10  *  2000  iterations ============
4 channel input
[32m[04/15 09:12:10 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[5m[31mWARNING[0m [32m[04/15 09:12:11 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:12:11 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 09:12:11 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 09:12:11 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 09:12:11 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 09:12:14 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 09:12:24 d2.utils.events]: [0m eta: 0:17:16  iter: 19  total_loss: 0.799  loss_cls: 0.273  loss_box_reg: 0.390  loss_rpn_cls: 0.027  loss_rpn_loc: 0.086  time: 0.5260  data_time: 0.0217  lr: 0.000200  max_mem: 3070M
[32m[04/15 09:12:35 d2.utils.events]: [0m eta: 0:17:09  iter: 39  total_loss: 0.644  loss_cls: 0.223  loss_box_reg: 0.339  loss_rpn_cls: 0.024  loss_rpn_loc: 0.063  time: 0.5294  data_time: 0.0054  lr: 0.000400  max_mem: 3070M
[32m[04/15 09:12:46 d2.utils.events]: [0m eta: 0:17:11  iter: 59  total_loss: 0.677  loss_cls: 0.218  loss_box_reg: 0.354  loss_rpn_cls: 0.023  loss_rpn_loc: 0.051  time: 0.5327  data_time: 0.0076  lr: 0.000599  max_mem: 3070M
[32m[04/15 09:12:57 d2.utils.events]: [0m eta: 0:17:19  iter: 79  total_loss: 0.704  loss_cls: 0.224  loss_box_reg: 0.366  loss_rpn_cls: 0.026  loss_rpn_loc: 0.057  time: 0.5393  data_time: 0.0064  lr: 0.000799  max_mem: 3070M
[32m[04/15 09:13:08 d2.utils.events]: [0m eta: 0:17:16  iter: 99  total_loss: 0.747  loss_cls: 0.214  loss_box_reg: 0.404  loss_rpn_cls: 0.020  loss_rpn_loc: 0.064  time: 0.5432  data_time: 0.0102  lr: 0.000999  max_mem: 3070M
[32m[04/15 09:13:19 d2.utils.events]: [0m eta: 0:17:02  iter: 119  total_loss: 0.547  loss_cls: 0.196  loss_box_reg: 0.303  loss_rpn_cls: 0.020  loss_rpn_loc: 0.035  time: 0.5436  data_time: 0.0065  lr: 0.001199  max_mem: 3070M
[32m[04/15 09:13:31 d2.utils.events]: [0m eta: 0:16:59  iter: 139  total_loss: 0.718  loss_cls: 0.246  loss_box_reg: 0.388  loss_rpn_cls: 0.023  loss_rpn_loc: 0.064  time: 0.5474  data_time: 0.0072  lr: 0.001399  max_mem: 3070M
[32m[04/15 09:13:42 d2.utils.events]: [0m eta: 0:16:48  iter: 159  total_loss: 0.688  loss_cls: 0.219  loss_box_reg: 0.358  loss_rpn_cls: 0.026  loss_rpn_loc: 0.055  time: 0.5480  data_time: 0.0070  lr: 0.001598  max_mem: 3070M
[32m[04/15 09:13:53 d2.utils.events]: [0m eta: 0:16:39  iter: 179  total_loss: 0.588  loss_cls: 0.182  loss_box_reg: 0.347  loss_rpn_cls: 0.021  loss_rpn_loc: 0.046  time: 0.5479  data_time: 0.0091  lr: 0.001798  max_mem: 3070M
[32m[04/15 09:14:04 d2.utils.events]: [0m eta: 0:16:30  iter: 199  total_loss: 0.751  loss_cls: 0.240  loss_box_reg: 0.381  loss_rpn_cls: 0.021  loss_rpn_loc: 0.059  time: 0.5477  data_time: 0.0062  lr: 0.001998  max_mem: 3070M
[32m[04/15 09:14:15 d2.utils.events]: [0m eta: 0:16:17  iter: 219  total_loss: 0.570  loss_cls: 0.205  loss_box_reg: 0.306  loss_rpn_cls: 0.021  loss_rpn_loc: 0.046  time: 0.5486  data_time: 0.0093  lr: 0.002198  max_mem: 3070M
[32m[04/15 09:14:27 d2.utils.events]: [0m eta: 0:16:08  iter: 239  total_loss: 0.700  loss_cls: 0.207  loss_box_reg: 0.373  loss_rpn_cls: 0.017  loss_rpn_loc: 0.050  time: 0.5510  data_time: 0.0081  lr: 0.002398  max_mem: 3070M
[32m[04/15 09:14:38 d2.utils.events]: [0m eta: 0:15:57  iter: 259  total_loss: 0.630  loss_cls: 0.206  loss_box_reg: 0.340  loss_rpn_cls: 0.017  loss_rpn_loc: 0.048  time: 0.5512  data_time: 0.0082  lr: 0.002597  max_mem: 3070M
[32m[04/15 09:14:49 d2.utils.events]: [0m eta: 0:15:46  iter: 279  total_loss: 0.732  loss_cls: 0.231  loss_box_reg: 0.388  loss_rpn_cls: 0.020  loss_rpn_loc: 0.063  time: 0.5522  data_time: 0.0063  lr: 0.002797  max_mem: 3070M
[32m[04/15 09:15:00 d2.utils.events]: [0m eta: 0:15:35  iter: 299  total_loss: 0.635  loss_cls: 0.187  loss_box_reg: 0.364  loss_rpn_cls: 0.023  loss_rpn_loc: 0.053  time: 0.5507  data_time: 0.0116  lr: 0.002997  max_mem: 3070M
[32m[04/15 09:15:11 d2.utils.events]: [0m eta: 0:15:23  iter: 319  total_loss: 0.688  loss_cls: 0.208  loss_box_reg: 0.375  loss_rpn_cls: 0.020  loss_rpn_loc: 0.062  time: 0.5496  data_time: 0.0081  lr: 0.003197  max_mem: 3070M
[32m[04/15 09:15:22 d2.utils.events]: [0m eta: 0:15:09  iter: 339  total_loss: 0.655  loss_cls: 0.183  loss_box_reg: 0.347  loss_rpn_cls: 0.017  loss_rpn_loc: 0.072  time: 0.5485  data_time: 0.0101  lr: 0.003397  max_mem: 3070M
[32m[04/15 09:15:33 d2.utils.events]: [0m eta: 0:15:02  iter: 359  total_loss: 0.541  loss_cls: 0.168  loss_box_reg: 0.317  loss_rpn_cls: 0.026  loss_rpn_loc: 0.048  time: 0.5497  data_time: 0.0063  lr: 0.003596  max_mem: 3070M
[32m[04/15 09:15:45 d2.utils.events]: [0m eta: 0:14:53  iter: 379  total_loss: 0.696  loss_cls: 0.243  loss_box_reg: 0.349  loss_rpn_cls: 0.019  loss_rpn_loc: 0.063  time: 0.5510  data_time: 0.0089  lr: 0.003796  max_mem: 3070M
[32m[04/15 09:15:57 d2.utils.events]: [0m eta: 0:14:42  iter: 399  total_loss: 0.589  loss_cls: 0.172  loss_box_reg: 0.278  loss_rpn_cls: 0.022  loss_rpn_loc: 0.045  time: 0.5530  data_time: 0.0603  lr: 0.003996  max_mem: 3070M
[32m[04/15 09:16:08 d2.utils.events]: [0m eta: 0:14:32  iter: 419  total_loss: 0.703  loss_cls: 0.226  loss_box_reg: 0.370  loss_rpn_cls: 0.031  loss_rpn_loc: 0.053  time: 0.5540  data_time: 0.0158  lr: 0.004196  max_mem: 3070M
[32m[04/15 09:16:19 d2.utils.events]: [0m eta: 0:14:22  iter: 439  total_loss: 0.665  loss_cls: 0.232  loss_box_reg: 0.383  loss_rpn_cls: 0.025  loss_rpn_loc: 0.059  time: 0.5541  data_time: 0.0067  lr: 0.004396  max_mem: 3070M
[32m[04/15 09:16:31 d2.utils.events]: [0m eta: 0:14:12  iter: 459  total_loss: 0.621  loss_cls: 0.218  loss_box_reg: 0.354  loss_rpn_cls: 0.012  loss_rpn_loc: 0.068  time: 0.5544  data_time: 0.0091  lr: 0.004595  max_mem: 3070M
[32m[04/15 09:16:42 d2.utils.events]: [0m eta: 0:14:02  iter: 479  total_loss: 0.722  loss_cls: 0.221  loss_box_reg: 0.384  loss_rpn_cls: 0.020  loss_rpn_loc: 0.077  time: 0.5545  data_time: 0.0089  lr: 0.004795  max_mem: 3070M
[32m[04/15 09:16:53 d2.utils.events]: [0m eta: 0:13:52  iter: 499  total_loss: 0.655  loss_cls: 0.201  loss_box_reg: 0.377  loss_rpn_cls: 0.019  loss_rpn_loc: 0.060  time: 0.5550  data_time: 0.0076  lr: 0.004995  max_mem: 3070M
[32m[04/15 09:17:05 d2.utils.events]: [0m eta: 0:13:41  iter: 519  total_loss: 0.582  loss_cls: 0.170  loss_box_reg: 0.289  loss_rpn_cls: 0.021  loss_rpn_loc: 0.047  time: 0.5551  data_time: 0.0078  lr: 0.005195  max_mem: 3070M
[32m[04/15 09:17:16 d2.utils.events]: [0m eta: 0:13:31  iter: 539  total_loss: 0.702  loss_cls: 0.222  loss_box_reg: 0.354  loss_rpn_cls: 0.021  loss_rpn_loc: 0.056  time: 0.5551  data_time: 0.0075  lr: 0.005395  max_mem: 3070M
[32m[04/15 09:17:27 d2.utils.events]: [0m eta: 0:13:20  iter: 559  total_loss: 0.579  loss_cls: 0.191  loss_box_reg: 0.330  loss_rpn_cls: 0.020  loss_rpn_loc: 0.042  time: 0.5550  data_time: 0.0071  lr: 0.005594  max_mem: 3070M
[32m[04/15 09:17:38 d2.utils.events]: [0m eta: 0:13:09  iter: 579  total_loss: 0.809  loss_cls: 0.247  loss_box_reg: 0.434  loss_rpn_cls: 0.032  loss_rpn_loc: 0.076  time: 0.5552  data_time: 0.0083  lr: 0.005794  max_mem: 3070M
[32m[04/15 09:17:50 d2.utils.events]: [0m eta: 0:12:59  iter: 599  total_loss: 0.609  loss_cls: 0.192  loss_box_reg: 0.313  loss_rpn_cls: 0.016  loss_rpn_loc: 0.050  time: 0.5554  data_time: 0.0078  lr: 0.005994  max_mem: 3070M
[32m[04/15 09:18:01 d2.utils.events]: [0m eta: 0:12:47  iter: 619  total_loss: 0.654  loss_cls: 0.201  loss_box_reg: 0.330  loss_rpn_cls: 0.029  loss_rpn_loc: 0.077  time: 0.5553  data_time: 0.0070  lr: 0.006194  max_mem: 3070M
[32m[04/15 09:18:12 d2.utils.events]: [0m eta: 0:12:37  iter: 639  total_loss: 0.609  loss_cls: 0.201  loss_box_reg: 0.355  loss_rpn_cls: 0.020  loss_rpn_loc: 0.066  time: 0.5557  data_time: 0.0073  lr: 0.006394  max_mem: 3070M
[32m[04/15 09:18:23 d2.utils.events]: [0m eta: 0:12:27  iter: 659  total_loss: 0.630  loss_cls: 0.198  loss_box_reg: 0.339  loss_rpn_cls: 0.028  loss_rpn_loc: 0.054  time: 0.5559  data_time: 0.0072  lr: 0.006593  max_mem: 3070M
[32m[04/15 09:18:34 d2.utils.events]: [0m eta: 0:12:15  iter: 679  total_loss: 0.532  loss_cls: 0.181  loss_box_reg: 0.292  loss_rpn_cls: 0.016  loss_rpn_loc: 0.061  time: 0.5555  data_time: 0.0088  lr: 0.006793  max_mem: 3070M
[32m[04/15 09:18:46 d2.utils.events]: [0m eta: 0:12:06  iter: 699  total_loss: 0.557  loss_cls: 0.182  loss_box_reg: 0.312  loss_rpn_cls: 0.022  loss_rpn_loc: 0.033  time: 0.5559  data_time: 0.0063  lr: 0.006993  max_mem: 3070M
[32m[04/15 09:18:57 d2.utils.events]: [0m eta: 0:11:55  iter: 719  total_loss: 0.551  loss_cls: 0.193  loss_box_reg: 0.296  loss_rpn_cls: 0.023  loss_rpn_loc: 0.054  time: 0.5563  data_time: 0.0076  lr: 0.007193  max_mem: 3070M
[32m[04/15 09:19:09 d2.utils.events]: [0m eta: 0:11:44  iter: 739  total_loss: 0.729  loss_cls: 0.238  loss_box_reg: 0.375  loss_rpn_cls: 0.022  loss_rpn_loc: 0.068  time: 0.5564  data_time: 0.0070  lr: 0.007393  max_mem: 3070M
[32m[04/15 09:19:20 d2.utils.events]: [0m eta: 0:11:33  iter: 759  total_loss: 0.651  loss_cls: 0.201  loss_box_reg: 0.355  loss_rpn_cls: 0.019  loss_rpn_loc: 0.052  time: 0.5569  data_time: 0.0067  lr: 0.007592  max_mem: 3070M
[32m[04/15 09:19:32 d2.utils.events]: [0m eta: 0:11:22  iter: 779  total_loss: 0.791  loss_cls: 0.246  loss_box_reg: 0.420  loss_rpn_cls: 0.025  loss_rpn_loc: 0.083  time: 0.5571  data_time: 0.0072  lr: 0.007792  max_mem: 3070M
[32m[04/15 09:19:43 d2.utils.events]: [0m eta: 0:11:11  iter: 799  total_loss: 0.690  loss_cls: 0.230  loss_box_reg: 0.366  loss_rpn_cls: 0.021  loss_rpn_loc: 0.054  time: 0.5568  data_time: 0.0093  lr: 0.007992  max_mem: 3070M
[32m[04/15 09:19:54 d2.utils.events]: [0m eta: 0:11:00  iter: 819  total_loss: 0.628  loss_cls: 0.199  loss_box_reg: 0.357  loss_rpn_cls: 0.022  loss_rpn_loc: 0.049  time: 0.5570  data_time: 0.0066  lr: 0.008192  max_mem: 3070M
[32m[04/15 09:20:05 d2.utils.events]: [0m eta: 0:10:48  iter: 839  total_loss: 0.615  loss_cls: 0.207  loss_box_reg: 0.341  loss_rpn_cls: 0.019  loss_rpn_loc: 0.050  time: 0.5563  data_time: 0.0087  lr: 0.008392  max_mem: 3070M
[32m[04/15 09:20:16 d2.utils.events]: [0m eta: 0:10:37  iter: 859  total_loss: 0.705  loss_cls: 0.244  loss_box_reg: 0.372  loss_rpn_cls: 0.033  loss_rpn_loc: 0.079  time: 0.5563  data_time: 0.0102  lr: 0.008591  max_mem: 3070M
[32m[04/15 09:20:27 d2.utils.events]: [0m eta: 0:10:25  iter: 879  total_loss: 0.723  loss_cls: 0.253  loss_box_reg: 0.396  loss_rpn_cls: 0.028  loss_rpn_loc: 0.049  time: 0.5561  data_time: 0.0069  lr: 0.008791  max_mem: 3070M
[32m[04/15 09:20:38 d2.utils.events]: [0m eta: 0:10:14  iter: 899  total_loss: 0.812  loss_cls: 0.238  loss_box_reg: 0.435  loss_rpn_cls: 0.025  loss_rpn_loc: 0.085  time: 0.5560  data_time: 0.0096  lr: 0.008991  max_mem: 3070M
[32m[04/15 09:20:50 d2.utils.events]: [0m eta: 0:10:04  iter: 919  total_loss: 0.740  loss_cls: 0.229  loss_box_reg: 0.403  loss_rpn_cls: 0.024  loss_rpn_loc: 0.054  time: 0.5564  data_time: 0.0071  lr: 0.009191  max_mem: 3070M
[32m[04/15 09:21:01 d2.utils.events]: [0m eta: 0:09:53  iter: 939  total_loss: 0.795  loss_cls: 0.268  loss_box_reg: 0.422  loss_rpn_cls: 0.029  loss_rpn_loc: 0.075  time: 0.5564  data_time: 0.0084  lr: 0.009391  max_mem: 3070M
[32m[04/15 09:21:13 d2.utils.events]: [0m eta: 0:09:42  iter: 959  total_loss: 0.771  loss_cls: 0.247  loss_box_reg: 0.414  loss_rpn_cls: 0.027  loss_rpn_loc: 0.067  time: 0.5569  data_time: 0.0112  lr: 0.009590  max_mem: 3070M
[32m[04/15 09:21:23 d2.utils.events]: [0m eta: 0:09:30  iter: 979  total_loss: 0.708  loss_cls: 0.234  loss_box_reg: 0.417  loss_rpn_cls: 0.022  loss_rpn_loc: 0.067  time: 0.5564  data_time: 0.0078  lr: 0.009790  max_mem: 3070M
[32m[04/15 09:21:35 d2.utils.events]: [0m eta: 0:09:19  iter: 999  total_loss: 0.650  loss_cls: 0.204  loss_box_reg: 0.359  loss_rpn_cls: 0.038  loss_rpn_loc: 0.079  time: 0.5562  data_time: 0.0074  lr: 0.009990  max_mem: 3070M
[32m[04/15 09:21:46 d2.utils.events]: [0m eta: 0:09:08  iter: 1019  total_loss: 0.574  loss_cls: 0.181  loss_box_reg: 0.319  loss_rpn_cls: 0.028  loss_rpn_loc: 0.045  time: 0.5562  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:21:57 d2.utils.events]: [0m eta: 0:08:57  iter: 1039  total_loss: 0.691  loss_cls: 0.243  loss_box_reg: 0.338  loss_rpn_cls: 0.038  loss_rpn_loc: 0.067  time: 0.5562  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:22:08 d2.utils.events]: [0m eta: 0:08:46  iter: 1059  total_loss: 0.733  loss_cls: 0.247  loss_box_reg: 0.420  loss_rpn_cls: 0.027  loss_rpn_loc: 0.048  time: 0.5560  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:22:19 d2.utils.events]: [0m eta: 0:08:35  iter: 1079  total_loss: 0.671  loss_cls: 0.208  loss_box_reg: 0.358  loss_rpn_cls: 0.033  loss_rpn_loc: 0.063  time: 0.5557  data_time: 0.0112  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:22:30 d2.utils.events]: [0m eta: 0:08:24  iter: 1099  total_loss: 0.715  loss_cls: 0.233  loss_box_reg: 0.342  loss_rpn_cls: 0.030  loss_rpn_loc: 0.082  time: 0.5558  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:22:41 d2.utils.events]: [0m eta: 0:08:13  iter: 1119  total_loss: 0.763  loss_cls: 0.245  loss_box_reg: 0.398  loss_rpn_cls: 0.035  loss_rpn_loc: 0.060  time: 0.5556  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:22:53 d2.utils.events]: [0m eta: 0:08:02  iter: 1139  total_loss: 0.649  loss_cls: 0.209  loss_box_reg: 0.350  loss_rpn_cls: 0.024  loss_rpn_loc: 0.061  time: 0.5558  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:23:04 d2.utils.events]: [0m eta: 0:07:51  iter: 1159  total_loss: 0.722  loss_cls: 0.240  loss_box_reg: 0.368  loss_rpn_cls: 0.024  loss_rpn_loc: 0.074  time: 0.5559  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:23:15 d2.utils.events]: [0m eta: 0:07:40  iter: 1179  total_loss: 0.573  loss_cls: 0.197  loss_box_reg: 0.296  loss_rpn_cls: 0.019  loss_rpn_loc: 0.041  time: 0.5559  data_time: 0.0095  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:23:26 d2.utils.events]: [0m eta: 0:07:28  iter: 1199  total_loss: 0.693  loss_cls: 0.225  loss_box_reg: 0.342  loss_rpn_cls: 0.041  loss_rpn_loc: 0.074  time: 0.5557  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:23:38 d2.utils.events]: [0m eta: 0:07:17  iter: 1219  total_loss: 0.643  loss_cls: 0.238  loss_box_reg: 0.341  loss_rpn_cls: 0.026  loss_rpn_loc: 0.060  time: 0.5560  data_time: 0.0092  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:23:49 d2.utils.events]: [0m eta: 0:07:06  iter: 1239  total_loss: 0.724  loss_cls: 0.232  loss_box_reg: 0.390  loss_rpn_cls: 0.026  loss_rpn_loc: 0.077  time: 0.5558  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:24:00 d2.utils.events]: [0m eta: 0:06:55  iter: 1259  total_loss: 0.674  loss_cls: 0.204  loss_box_reg: 0.370  loss_rpn_cls: 0.026  loss_rpn_loc: 0.073  time: 0.5558  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:24:11 d2.utils.events]: [0m eta: 0:06:44  iter: 1279  total_loss: 0.802  loss_cls: 0.235  loss_box_reg: 0.386  loss_rpn_cls: 0.043  loss_rpn_loc: 0.081  time: 0.5558  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:24:23 d2.utils.events]: [0m eta: 0:06:33  iter: 1299  total_loss: 0.695  loss_cls: 0.238  loss_box_reg: 0.375  loss_rpn_cls: 0.023  loss_rpn_loc: 0.056  time: 0.5559  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:24:34 d2.utils.events]: [0m eta: 0:06:21  iter: 1319  total_loss: 0.813  loss_cls: 0.227  loss_box_reg: 0.393  loss_rpn_cls: 0.037  loss_rpn_loc: 0.087  time: 0.5556  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:24:44 d2.utils.events]: [0m eta: 0:06:10  iter: 1339  total_loss: 0.663  loss_cls: 0.196  loss_box_reg: 0.339  loss_rpn_cls: 0.024  loss_rpn_loc: 0.053  time: 0.5553  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:24:55 d2.utils.events]: [0m eta: 0:05:59  iter: 1359  total_loss: 0.744  loss_cls: 0.233  loss_box_reg: 0.384  loss_rpn_cls: 0.018  loss_rpn_loc: 0.060  time: 0.5553  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:25:07 d2.utils.events]: [0m eta: 0:05:47  iter: 1379  total_loss: 0.739  loss_cls: 0.230  loss_box_reg: 0.358  loss_rpn_cls: 0.023  loss_rpn_loc: 0.069  time: 0.5554  data_time: 0.0062  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:25:18 d2.utils.events]: [0m eta: 0:05:36  iter: 1399  total_loss: 0.632  loss_cls: 0.203  loss_box_reg: 0.312  loss_rpn_cls: 0.030  loss_rpn_loc: 0.074  time: 0.5555  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:25:29 d2.utils.events]: [0m eta: 0:05:25  iter: 1419  total_loss: 0.695  loss_cls: 0.209  loss_box_reg: 0.376  loss_rpn_cls: 0.022  loss_rpn_loc: 0.067  time: 0.5553  data_time: 0.0100  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:25:40 d2.utils.events]: [0m eta: 0:05:14  iter: 1439  total_loss: 0.802  loss_cls: 0.231  loss_box_reg: 0.372  loss_rpn_cls: 0.034  loss_rpn_loc: 0.089  time: 0.5554  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:25:52 d2.utils.events]: [0m eta: 0:05:03  iter: 1459  total_loss: 0.797  loss_cls: 0.275  loss_box_reg: 0.369  loss_rpn_cls: 0.031  loss_rpn_loc: 0.059  time: 0.5556  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:26:03 d2.utils.events]: [0m eta: 0:04:51  iter: 1479  total_loss: 0.627  loss_cls: 0.205  loss_box_reg: 0.332  loss_rpn_cls: 0.028  loss_rpn_loc: 0.062  time: 0.5554  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:26:14 d2.utils.events]: [0m eta: 0:04:40  iter: 1499  total_loss: 0.764  loss_cls: 0.244  loss_box_reg: 0.402  loss_rpn_cls: 0.037  loss_rpn_loc: 0.061  time: 0.5555  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:26:25 d2.utils.events]: [0m eta: 0:04:29  iter: 1519  total_loss: 0.687  loss_cls: 0.228  loss_box_reg: 0.404  loss_rpn_cls: 0.029  loss_rpn_loc: 0.057  time: 0.5555  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:26:37 d2.utils.events]: [0m eta: 0:04:17  iter: 1539  total_loss: 0.774  loss_cls: 0.227  loss_box_reg: 0.395  loss_rpn_cls: 0.027  loss_rpn_loc: 0.076  time: 0.5555  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:26:48 d2.utils.events]: [0m eta: 0:04:06  iter: 1559  total_loss: 0.687  loss_cls: 0.241  loss_box_reg: 0.362  loss_rpn_cls: 0.026  loss_rpn_loc: 0.058  time: 0.5555  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:26:59 d2.utils.events]: [0m eta: 0:03:55  iter: 1579  total_loss: 0.729  loss_cls: 0.242  loss_box_reg: 0.384  loss_rpn_cls: 0.031  loss_rpn_loc: 0.070  time: 0.5558  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:27:11 d2.utils.events]: [0m eta: 0:03:44  iter: 1599  total_loss: 0.640  loss_cls: 0.217  loss_box_reg: 0.356  loss_rpn_cls: 0.029  loss_rpn_loc: 0.065  time: 0.5559  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:27:22 d2.utils.events]: [0m eta: 0:03:33  iter: 1619  total_loss: 0.803  loss_cls: 0.257  loss_box_reg: 0.368  loss_rpn_cls: 0.031  loss_rpn_loc: 0.069  time: 0.5556  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:27:33 d2.utils.events]: [0m eta: 0:03:21  iter: 1639  total_loss: 0.824  loss_cls: 0.292  loss_box_reg: 0.428  loss_rpn_cls: 0.033  loss_rpn_loc: 0.072  time: 0.5557  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:27:44 d2.utils.events]: [0m eta: 0:03:10  iter: 1659  total_loss: 0.740  loss_cls: 0.252  loss_box_reg: 0.362  loss_rpn_cls: 0.031  loss_rpn_loc: 0.068  time: 0.5557  data_time: 0.0123  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:27:55 d2.utils.events]: [0m eta: 0:02:59  iter: 1679  total_loss: 0.736  loss_cls: 0.240  loss_box_reg: 0.377  loss_rpn_cls: 0.023  loss_rpn_loc: 0.066  time: 0.5557  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:28:06 d2.utils.events]: [0m eta: 0:02:48  iter: 1699  total_loss: 0.757  loss_cls: 0.239  loss_box_reg: 0.385  loss_rpn_cls: 0.032  loss_rpn_loc: 0.077  time: 0.5556  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:28:18 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.663  loss_cls: 0.218  loss_box_reg: 0.372  loss_rpn_cls: 0.027  loss_rpn_loc: 0.046  time: 0.5558  data_time: 0.0098  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:28:29 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.663  loss_cls: 0.211  loss_box_reg: 0.331  loss_rpn_cls: 0.025  loss_rpn_loc: 0.067  time: 0.5557  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:28:41 d2.utils.events]: [0m eta: 0:02:14  iter: 1759  total_loss: 0.670  loss_cls: 0.206  loss_box_reg: 0.377  loss_rpn_cls: 0.028  loss_rpn_loc: 0.054  time: 0.5561  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:28:52 d2.utils.events]: [0m eta: 0:02:03  iter: 1779  total_loss: 0.689  loss_cls: 0.224  loss_box_reg: 0.355  loss_rpn_cls: 0.036  loss_rpn_loc: 0.073  time: 0.5563  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:29:03 d2.utils.events]: [0m eta: 0:01:52  iter: 1799  total_loss: 0.573  loss_cls: 0.187  loss_box_reg: 0.308  loss_rpn_cls: 0.025  loss_rpn_loc: 0.048  time: 0.5562  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:29:15 d2.utils.events]: [0m eta: 0:01:41  iter: 1819  total_loss: 0.916  loss_cls: 0.318  loss_box_reg: 0.499  loss_rpn_cls: 0.032  loss_rpn_loc: 0.078  time: 0.5562  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:29:31 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.663  loss_cls: 0.219  loss_box_reg: 0.338  loss_rpn_cls: 0.025  loss_rpn_loc: 0.059  time: 0.5566  data_time: 0.0638  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:29:42 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.743  loss_cls: 0.250  loss_box_reg: 0.402  loss_rpn_cls: 0.024  loss_rpn_loc: 0.064  time: 0.5564  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:29:53 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.811  loss_cls: 0.262  loss_box_reg: 0.406  loss_rpn_cls: 0.037  loss_rpn_loc: 0.086  time: 0.5563  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:30:14 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.686  loss_cls: 0.209  loss_box_reg: 0.336  loss_rpn_cls: 0.036  loss_rpn_loc: 0.061  time: 0.5561  data_time: 0.0097  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:30:25 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.678  loss_cls: 0.225  loss_box_reg: 0.334  loss_rpn_cls: 0.041  loss_rpn_loc: 0.088  time: 0.5560  data_time: 0.0094  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:30:35 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.571  loss_cls: 0.194  loss_box_reg: 0.310  loss_rpn_cls: 0.031  loss_rpn_loc: 0.054  time: 0.5556  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:30:46 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.752  loss_cls: 0.244  loss_box_reg: 0.384  loss_rpn_cls: 0.034  loss_rpn_loc: 0.082  time: 0.5554  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:30:58 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.662  loss_cls: 0.220  loss_box_reg: 0.346  loss_rpn_cls: 0.017  loss_rpn_loc: 0.063  time: 0.5554  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 09:31:49 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:31:49 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 09:31:49 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 09:31:49 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.633  loss_cls: 0.217  loss_box_reg: 0.298  loss_rpn_cls: 0.032  loss_rpn_loc: 0.063  time: 0.5553  data_time: 0.0107  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:31:53 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:29 (0.5556 s / it)
[32m[04/15 09:31:53 d2.engine.hooks]: [0mTotal training time: 0:19:38 (0:01:08 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 09:32:06 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:32:06 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 09:32:06 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 09:32:09 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1201 s / img. ETA=0:03:26
[32m[04/15 09:32:14 d2.evaluation.evaluator]: [0mInference done 39/1257. 0.1204 s / img. ETA=0:03:44
[32m[04/15 09:32:20 d2.evaluation.evaluator]: [0mInference done 71/1257. 0.1220 s / img. ETA=0:03:24
[32m[04/15 09:32:25 d2.evaluation.evaluator]: [0mInference done 102/1257. 0.1223 s / img. ETA=0:03:15
[32m[04/15 09:32:30 d2.evaluation.evaluator]: [0mInference done 137/1257. 0.1224 s / img. ETA=0:03:08
[32m[04/15 09:32:36 d2.evaluation.evaluator]: [0mInference done 171/1257. 0.1224 s / img. ETA=0:02:59
[32m[04/15 09:32:41 d2.evaluation.evaluator]: [0mInference done 210/1257. 0.1224 s / img. ETA=0:02:46
[32m[04/15 09:32:46 d2.evaluation.evaluator]: [0mInference done 248/1257. 0.1228 s / img. ETA=0:02:36
[32m[04/15 09:32:51 d2.evaluation.evaluator]: [0mInference done 286/1257. 0.1228 s / img. ETA=0:02:27
[32m[04/15 09:32:56 d2.evaluation.evaluator]: [0mInference done 320/1257. 0.1229 s / img. ETA=0:02:22
[32m[04/15 09:33:01 d2.evaluation.evaluator]: [0mInference done 360/1257. 0.1228 s / img. ETA=0:02:13
[32m[04/15 09:33:06 d2.evaluation.evaluator]: [0mInference done 392/1257. 0.1227 s / img. ETA=0:02:09
[32m[04/15 09:33:11 d2.evaluation.evaluator]: [0mInference done 419/1257. 0.1226 s / img. ETA=0:02:07
[32m[04/15 09:33:16 d2.evaluation.evaluator]: [0mInference done 459/1257. 0.1228 s / img. ETA=0:01:59
[32m[04/15 09:33:21 d2.evaluation.evaluator]: [0mInference done 499/1257. 0.1229 s / img. ETA=0:01:52
[32m[04/15 09:33:26 d2.evaluation.evaluator]: [0mInference done 539/1257. 0.1228 s / img. ETA=0:01:45
[32m[04/15 09:33:31 d2.evaluation.evaluator]: [0mInference done 578/1257. 0.1230 s / img. ETA=0:01:38
[32m[04/15 09:33:36 d2.evaluation.evaluator]: [0mInference done 618/1257. 0.1230 s / img. ETA=0:01:31
[32m[04/15 09:33:41 d2.evaluation.evaluator]: [0mInference done 657/1257. 0.1232 s / img. ETA=0:01:25
[32m[04/15 09:33:47 d2.evaluation.evaluator]: [0mInference done 697/1257. 0.1233 s / img. ETA=0:01:19
[32m[04/15 09:33:52 d2.evaluation.evaluator]: [0mInference done 736/1257. 0.1235 s / img. ETA=0:01:13
[32m[04/15 09:33:57 d2.evaluation.evaluator]: [0mInference done 776/1257. 0.1234 s / img. ETA=0:01:07
[32m[04/15 09:34:02 d2.evaluation.evaluator]: [0mInference done 815/1257. 0.1235 s / img. ETA=0:01:01
[32m[04/15 09:34:07 d2.evaluation.evaluator]: [0mInference done 855/1257. 0.1235 s / img. ETA=0:00:56
[32m[04/15 09:34:12 d2.evaluation.evaluator]: [0mInference done 895/1257. 0.1236 s / img. ETA=0:00:50
[32m[04/15 09:34:17 d2.evaluation.evaluator]: [0mInference done 935/1257. 0.1235 s / img. ETA=0:00:44
[32m[04/15 09:34:22 d2.evaluation.evaluator]: [0mInference done 974/1257. 0.1237 s / img. ETA=0:00:39
[32m[04/15 09:34:27 d2.evaluation.evaluator]: [0mInference done 1014/1257. 0.1237 s / img. ETA=0:00:33
[32m[04/15 09:34:32 d2.evaluation.evaluator]: [0mInference done 1054/1257. 0.1237 s / img. ETA=0:00:27
[32m[04/15 09:34:37 d2.evaluation.evaluator]: [0mInference done 1094/1257. 0.1237 s / img. ETA=0:00:22
[32m[04/15 09:34:43 d2.evaluation.evaluator]: [0mInference done 1133/1257. 0.1239 s / img. ETA=0:00:16
[32m[04/15 09:34:48 d2.evaluation.evaluator]: [0mInference done 1172/1257. 0.1240 s / img. ETA=0:00:11
[32m[04/15 09:34:53 d2.evaluation.evaluator]: [0mInference done 1211/1257. 0.1240 s / img. ETA=0:00:06
[32m[04/15 09:34:58 d2.evaluation.evaluator]: [0mInference done 1250/1257. 0.1241 s / img. ETA=0:00:00
[32m[04/15 09:34:59 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:50.535809 (0.136211 s / img per device, on 1 devices)
[32m[04/15 09:34:59 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:35 (0.124106 s / img per device, on 1 devices)
[32m[04/15 09:34:59 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 09:34:59 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 09:34:59 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.05s).
Accumulating evaluation results...
DONE (t=0.67s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.179
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.355
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.486
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.098
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.232
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.234
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.151
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.282
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.550
[32m[04/15 09:35:05 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 17.877 | 35.516 | 15.971 | 10.020 | 22.327 | 48.615 |
[32m[04/15 09:35:05 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 24.273 | bicycle       | 8.594 | car            | 38.639 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  11  *  2000  iterations ============
4 channel input
[32m[04/15 09:35:07 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[5m[31mWARNING[0m [32m[04/15 09:35:08 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:35:08 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 09:35:08 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 09:35:09 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 09:35:09 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 09:35:09 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 09:35:20 d2.utils.events]: [0m eta: 0:18:13  iter: 19  total_loss: 0.686  loss_cls: 0.225  loss_box_reg: 0.347  loss_rpn_cls: 0.025  loss_rpn_loc: 0.051  time: 0.5478  data_time: 0.0367  lr: 0.000200  max_mem: 3070M
[32m[04/15 09:35:31 d2.utils.events]: [0m eta: 0:17:26  iter: 39  total_loss: 0.680  loss_cls: 0.214  loss_box_reg: 0.390  loss_rpn_cls: 0.025  loss_rpn_loc: 0.066  time: 0.5362  data_time: 0.0058  lr: 0.000400  max_mem: 3070M
[32m[04/15 09:35:42 d2.utils.events]: [0m eta: 0:17:16  iter: 59  total_loss: 0.668  loss_cls: 0.212  loss_box_reg: 0.350  loss_rpn_cls: 0.018  loss_rpn_loc: 0.059  time: 0.5352  data_time: 0.0061  lr: 0.000599  max_mem: 3070M
[32m[04/15 09:35:53 d2.utils.events]: [0m eta: 0:17:08  iter: 79  total_loss: 0.610  loss_cls: 0.191  loss_box_reg: 0.314  loss_rpn_cls: 0.023  loss_rpn_loc: 0.053  time: 0.5382  data_time: 0.0082  lr: 0.000799  max_mem: 3070M
[32m[04/15 09:36:04 d2.utils.events]: [0m eta: 0:17:09  iter: 99  total_loss: 0.696  loss_cls: 0.231  loss_box_reg: 0.361  loss_rpn_cls: 0.019  loss_rpn_loc: 0.075  time: 0.5425  data_time: 0.0074  lr: 0.000999  max_mem: 3070M
[32m[04/15 09:36:15 d2.utils.events]: [0m eta: 0:17:13  iter: 119  total_loss: 0.698  loss_cls: 0.210  loss_box_reg: 0.340  loss_rpn_cls: 0.026  loss_rpn_loc: 0.052  time: 0.5442  data_time: 0.0071  lr: 0.001199  max_mem: 3070M
[32m[04/15 09:36:26 d2.utils.events]: [0m eta: 0:17:03  iter: 139  total_loss: 0.591  loss_cls: 0.209  loss_box_reg: 0.309  loss_rpn_cls: 0.029  loss_rpn_loc: 0.054  time: 0.5465  data_time: 0.0123  lr: 0.001399  max_mem: 3070M
[32m[04/15 09:36:37 d2.utils.events]: [0m eta: 0:16:50  iter: 159  total_loss: 0.567  loss_cls: 0.189  loss_box_reg: 0.294  loss_rpn_cls: 0.023  loss_rpn_loc: 0.049  time: 0.5448  data_time: 0.0064  lr: 0.001598  max_mem: 3070M
[32m[04/15 09:36:48 d2.utils.events]: [0m eta: 0:16:39  iter: 179  total_loss: 0.554  loss_cls: 0.169  loss_box_reg: 0.309  loss_rpn_cls: 0.021  loss_rpn_loc: 0.045  time: 0.5449  data_time: 0.0083  lr: 0.001798  max_mem: 3070M
[32m[04/15 09:36:59 d2.utils.events]: [0m eta: 0:16:28  iter: 199  total_loss: 0.569  loss_cls: 0.182  loss_box_reg: 0.303  loss_rpn_cls: 0.018  loss_rpn_loc: 0.048  time: 0.5451  data_time: 0.0075  lr: 0.001998  max_mem: 3070M
[32m[04/15 09:37:10 d2.utils.events]: [0m eta: 0:16:18  iter: 219  total_loss: 0.674  loss_cls: 0.220  loss_box_reg: 0.389  loss_rpn_cls: 0.025  loss_rpn_loc: 0.068  time: 0.5456  data_time: 0.0066  lr: 0.002198  max_mem: 3070M
[32m[04/15 09:37:22 d2.utils.events]: [0m eta: 0:16:08  iter: 239  total_loss: 0.699  loss_cls: 0.232  loss_box_reg: 0.349  loss_rpn_cls: 0.025  loss_rpn_loc: 0.073  time: 0.5464  data_time: 0.0065  lr: 0.002398  max_mem: 3070M
[32m[04/15 09:37:32 d2.utils.events]: [0m eta: 0:15:57  iter: 259  total_loss: 0.634  loss_cls: 0.218  loss_box_reg: 0.332  loss_rpn_cls: 0.023  loss_rpn_loc: 0.052  time: 0.5459  data_time: 0.0092  lr: 0.002597  max_mem: 3070M
[32m[04/15 09:37:43 d2.utils.events]: [0m eta: 0:15:45  iter: 279  total_loss: 0.529  loss_cls: 0.194  loss_box_reg: 0.294  loss_rpn_cls: 0.017  loss_rpn_loc: 0.043  time: 0.5457  data_time: 0.0075  lr: 0.002797  max_mem: 3070M
[32m[04/15 09:37:55 d2.utils.events]: [0m eta: 0:15:37  iter: 299  total_loss: 0.514  loss_cls: 0.176  loss_box_reg: 0.301  loss_rpn_cls: 0.017  loss_rpn_loc: 0.046  time: 0.5468  data_time: 0.0067  lr: 0.002997  max_mem: 3070M
[32m[04/15 09:38:06 d2.utils.events]: [0m eta: 0:15:27  iter: 319  total_loss: 0.565  loss_cls: 0.186  loss_box_reg: 0.282  loss_rpn_cls: 0.021  loss_rpn_loc: 0.047  time: 0.5476  data_time: 0.0101  lr: 0.003197  max_mem: 3070M
[32m[04/15 09:38:17 d2.utils.events]: [0m eta: 0:15:16  iter: 339  total_loss: 0.611  loss_cls: 0.208  loss_box_reg: 0.333  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5483  data_time: 0.0074  lr: 0.003397  max_mem: 3070M
[32m[04/15 09:38:28 d2.utils.events]: [0m eta: 0:15:05  iter: 359  total_loss: 0.597  loss_cls: 0.194  loss_box_reg: 0.327  loss_rpn_cls: 0.020  loss_rpn_loc: 0.052  time: 0.5485  data_time: 0.0063  lr: 0.003596  max_mem: 3070M
[32m[04/15 09:38:39 d2.utils.events]: [0m eta: 0:14:55  iter: 379  total_loss: 0.653  loss_cls: 0.226  loss_box_reg: 0.354  loss_rpn_cls: 0.020  loss_rpn_loc: 0.058  time: 0.5482  data_time: 0.0075  lr: 0.003796  max_mem: 3070M
[32m[04/15 09:38:50 d2.utils.events]: [0m eta: 0:14:44  iter: 399  total_loss: 0.725  loss_cls: 0.200  loss_box_reg: 0.389  loss_rpn_cls: 0.020  loss_rpn_loc: 0.065  time: 0.5485  data_time: 0.0092  lr: 0.003996  max_mem: 3070M
[32m[04/15 09:39:02 d2.utils.events]: [0m eta: 0:14:34  iter: 419  total_loss: 0.664  loss_cls: 0.200  loss_box_reg: 0.339  loss_rpn_cls: 0.030  loss_rpn_loc: 0.064  time: 0.5495  data_time: 0.0103  lr: 0.004196  max_mem: 3070M
[32m[04/15 09:39:13 d2.utils.events]: [0m eta: 0:14:23  iter: 439  total_loss: 0.720  loss_cls: 0.259  loss_box_reg: 0.393  loss_rpn_cls: 0.018  loss_rpn_loc: 0.066  time: 0.5496  data_time: 0.0083  lr: 0.004396  max_mem: 3070M
[32m[04/15 09:39:25 d2.utils.events]: [0m eta: 0:14:13  iter: 459  total_loss: 0.554  loss_cls: 0.174  loss_box_reg: 0.317  loss_rpn_cls: 0.015  loss_rpn_loc: 0.048  time: 0.5504  data_time: 0.0068  lr: 0.004595  max_mem: 3070M
[32m[04/15 09:39:36 d2.utils.events]: [0m eta: 0:14:02  iter: 479  total_loss: 0.679  loss_cls: 0.237  loss_box_reg: 0.356  loss_rpn_cls: 0.025  loss_rpn_loc: 0.061  time: 0.5506  data_time: 0.0070  lr: 0.004795  max_mem: 3070M
[32m[04/15 09:39:47 d2.utils.events]: [0m eta: 0:13:51  iter: 499  total_loss: 0.628  loss_cls: 0.233  loss_box_reg: 0.365  loss_rpn_cls: 0.015  loss_rpn_loc: 0.054  time: 0.5508  data_time: 0.0066  lr: 0.004995  max_mem: 3070M
[32m[04/15 09:39:58 d2.utils.events]: [0m eta: 0:13:41  iter: 519  total_loss: 0.619  loss_cls: 0.192  loss_box_reg: 0.347  loss_rpn_cls: 0.020  loss_rpn_loc: 0.054  time: 0.5512  data_time: 0.0087  lr: 0.005195  max_mem: 3070M
[32m[04/15 09:40:09 d2.utils.events]: [0m eta: 0:13:29  iter: 539  total_loss: 0.724  loss_cls: 0.214  loss_box_reg: 0.408  loss_rpn_cls: 0.020  loss_rpn_loc: 0.069  time: 0.5510  data_time: 0.0083  lr: 0.005395  max_mem: 3070M
[32m[04/15 09:40:20 d2.utils.events]: [0m eta: 0:13:17  iter: 559  total_loss: 0.722  loss_cls: 0.222  loss_box_reg: 0.362  loss_rpn_cls: 0.027  loss_rpn_loc: 0.064  time: 0.5506  data_time: 0.0069  lr: 0.005594  max_mem: 3070M
[32m[04/15 09:40:31 d2.utils.events]: [0m eta: 0:13:06  iter: 579  total_loss: 0.633  loss_cls: 0.183  loss_box_reg: 0.330  loss_rpn_cls: 0.019  loss_rpn_loc: 0.051  time: 0.5508  data_time: 0.0077  lr: 0.005794  max_mem: 3070M
[32m[04/15 09:40:43 d2.utils.events]: [0m eta: 0:12:55  iter: 599  total_loss: 0.741  loss_cls: 0.237  loss_box_reg: 0.400  loss_rpn_cls: 0.027  loss_rpn_loc: 0.077  time: 0.5509  data_time: 0.0074  lr: 0.005994  max_mem: 3070M
[32m[04/15 09:40:54 d2.utils.events]: [0m eta: 0:12:44  iter: 619  total_loss: 0.679  loss_cls: 0.220  loss_box_reg: 0.362  loss_rpn_cls: 0.017  loss_rpn_loc: 0.066  time: 0.5515  data_time: 0.0077  lr: 0.006194  max_mem: 3070M
[32m[04/15 09:41:05 d2.utils.events]: [0m eta: 0:12:34  iter: 639  total_loss: 0.725  loss_cls: 0.243  loss_box_reg: 0.380  loss_rpn_cls: 0.022  loss_rpn_loc: 0.063  time: 0.5513  data_time: 0.0069  lr: 0.006394  max_mem: 3070M
[32m[04/15 09:41:17 d2.utils.events]: [0m eta: 0:12:23  iter: 659  total_loss: 0.631  loss_cls: 0.192  loss_box_reg: 0.355  loss_rpn_cls: 0.020  loss_rpn_loc: 0.043  time: 0.5521  data_time: 0.0072  lr: 0.006593  max_mem: 3070M
[32m[04/15 09:41:28 d2.utils.events]: [0m eta: 0:12:12  iter: 679  total_loss: 0.728  loss_cls: 0.220  loss_box_reg: 0.390  loss_rpn_cls: 0.018  loss_rpn_loc: 0.069  time: 0.5521  data_time: 0.0077  lr: 0.006793  max_mem: 3070M
[32m[04/15 09:41:39 d2.utils.events]: [0m eta: 0:12:01  iter: 699  total_loss: 0.661  loss_cls: 0.230  loss_box_reg: 0.334  loss_rpn_cls: 0.022  loss_rpn_loc: 0.057  time: 0.5523  data_time: 0.0065  lr: 0.006993  max_mem: 3070M
[32m[04/15 09:41:50 d2.utils.events]: [0m eta: 0:11:50  iter: 719  total_loss: 0.747  loss_cls: 0.240  loss_box_reg: 0.389  loss_rpn_cls: 0.023  loss_rpn_loc: 0.082  time: 0.5523  data_time: 0.0081  lr: 0.007193  max_mem: 3070M
[32m[04/15 09:42:02 d2.utils.events]: [0m eta: 0:11:40  iter: 739  total_loss: 0.719  loss_cls: 0.235  loss_box_reg: 0.371  loss_rpn_cls: 0.032  loss_rpn_loc: 0.078  time: 0.5530  data_time: 0.0072  lr: 0.007393  max_mem: 3070M
[32m[04/15 09:42:13 d2.utils.events]: [0m eta: 0:11:29  iter: 759  total_loss: 0.801  loss_cls: 0.249  loss_box_reg: 0.423  loss_rpn_cls: 0.025  loss_rpn_loc: 0.060  time: 0.5531  data_time: 0.0064  lr: 0.007592  max_mem: 3070M
[32m[04/15 09:42:24 d2.utils.events]: [0m eta: 0:11:18  iter: 779  total_loss: 0.663  loss_cls: 0.197  loss_box_reg: 0.337  loss_rpn_cls: 0.022  loss_rpn_loc: 0.062  time: 0.5533  data_time: 0.0069  lr: 0.007792  max_mem: 3070M
[32m[04/15 09:42:36 d2.utils.events]: [0m eta: 0:11:07  iter: 799  total_loss: 0.683  loss_cls: 0.211  loss_box_reg: 0.370  loss_rpn_cls: 0.019  loss_rpn_loc: 0.070  time: 0.5536  data_time: 0.0072  lr: 0.007992  max_mem: 3070M
[32m[04/15 09:42:47 d2.utils.events]: [0m eta: 0:10:56  iter: 819  total_loss: 0.676  loss_cls: 0.222  loss_box_reg: 0.347  loss_rpn_cls: 0.028  loss_rpn_loc: 0.074  time: 0.5535  data_time: 0.0071  lr: 0.008192  max_mem: 3070M
[32m[04/15 09:42:58 d2.utils.events]: [0m eta: 0:10:45  iter: 839  total_loss: 0.762  loss_cls: 0.224  loss_box_reg: 0.404  loss_rpn_cls: 0.026  loss_rpn_loc: 0.085  time: 0.5534  data_time: 0.0084  lr: 0.008392  max_mem: 3070M
[32m[04/15 09:43:09 d2.utils.events]: [0m eta: 0:10:33  iter: 859  total_loss: 0.577  loss_cls: 0.190  loss_box_reg: 0.324  loss_rpn_cls: 0.017  loss_rpn_loc: 0.048  time: 0.5534  data_time: 0.0075  lr: 0.008591  max_mem: 3070M
[32m[04/15 09:43:20 d2.utils.events]: [0m eta: 0:10:22  iter: 879  total_loss: 0.690  loss_cls: 0.236  loss_box_reg: 0.349  loss_rpn_cls: 0.025  loss_rpn_loc: 0.060  time: 0.5534  data_time: 0.0086  lr: 0.008791  max_mem: 3070M
[32m[04/15 09:43:32 d2.utils.events]: [0m eta: 0:10:11  iter: 899  total_loss: 0.671  loss_cls: 0.206  loss_box_reg: 0.337  loss_rpn_cls: 0.021  loss_rpn_loc: 0.048  time: 0.5533  data_time: 0.0068  lr: 0.008991  max_mem: 3070M
[32m[04/15 09:43:43 d2.utils.events]: [0m eta: 0:10:00  iter: 919  total_loss: 0.763  loss_cls: 0.260  loss_box_reg: 0.421  loss_rpn_cls: 0.028  loss_rpn_loc: 0.066  time: 0.5534  data_time: 0.0131  lr: 0.009191  max_mem: 3070M
[32m[04/15 09:43:54 d2.utils.events]: [0m eta: 0:09:48  iter: 939  total_loss: 0.704  loss_cls: 0.207  loss_box_reg: 0.348  loss_rpn_cls: 0.037  loss_rpn_loc: 0.056  time: 0.5529  data_time: 0.0098  lr: 0.009391  max_mem: 3070M
[32m[04/15 09:44:05 d2.utils.events]: [0m eta: 0:09:37  iter: 959  total_loss: 0.697  loss_cls: 0.223  loss_box_reg: 0.367  loss_rpn_cls: 0.024  loss_rpn_loc: 0.049  time: 0.5528  data_time: 0.0104  lr: 0.009590  max_mem: 3070M
[32m[04/15 09:44:16 d2.utils.events]: [0m eta: 0:09:26  iter: 979  total_loss: 0.653  loss_cls: 0.218  loss_box_reg: 0.349  loss_rpn_cls: 0.030  loss_rpn_loc: 0.058  time: 0.5529  data_time: 0.0066  lr: 0.009790  max_mem: 3070M
[32m[04/15 09:44:27 d2.utils.events]: [0m eta: 0:09:15  iter: 999  total_loss: 0.673  loss_cls: 0.225  loss_box_reg: 0.373  loss_rpn_cls: 0.023  loss_rpn_loc: 0.056  time: 0.5527  data_time: 0.0086  lr: 0.009990  max_mem: 3070M
[32m[04/15 09:44:38 d2.utils.events]: [0m eta: 0:09:04  iter: 1019  total_loss: 0.656  loss_cls: 0.205  loss_box_reg: 0.355  loss_rpn_cls: 0.030  loss_rpn_loc: 0.061  time: 0.5527  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:44:49 d2.utils.events]: [0m eta: 0:08:54  iter: 1039  total_loss: 0.735  loss_cls: 0.224  loss_box_reg: 0.407  loss_rpn_cls: 0.023  loss_rpn_loc: 0.064  time: 0.5529  data_time: 0.0062  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:45:01 d2.utils.events]: [0m eta: 0:08:43  iter: 1059  total_loss: 0.627  loss_cls: 0.204  loss_box_reg: 0.338  loss_rpn_cls: 0.020  loss_rpn_loc: 0.059  time: 0.5529  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:45:12 d2.utils.events]: [0m eta: 0:08:32  iter: 1079  total_loss: 0.712  loss_cls: 0.225  loss_box_reg: 0.349  loss_rpn_cls: 0.022  loss_rpn_loc: 0.074  time: 0.5530  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:45:23 d2.utils.events]: [0m eta: 0:08:21  iter: 1099  total_loss: 0.885  loss_cls: 0.298  loss_box_reg: 0.462  loss_rpn_cls: 0.039  loss_rpn_loc: 0.091  time: 0.5532  data_time: 0.0118  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:45:35 d2.utils.events]: [0m eta: 0:08:10  iter: 1119  total_loss: 0.561  loss_cls: 0.183  loss_box_reg: 0.315  loss_rpn_cls: 0.028  loss_rpn_loc: 0.058  time: 0.5535  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:45:46 d2.utils.events]: [0m eta: 0:07:59  iter: 1139  total_loss: 0.531  loss_cls: 0.186  loss_box_reg: 0.274  loss_rpn_cls: 0.030  loss_rpn_loc: 0.059  time: 0.5536  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:45:57 d2.utils.events]: [0m eta: 0:07:48  iter: 1159  total_loss: 0.614  loss_cls: 0.182  loss_box_reg: 0.316  loss_rpn_cls: 0.023  loss_rpn_loc: 0.052  time: 0.5533  data_time: 0.0091  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:46:08 d2.utils.events]: [0m eta: 0:07:37  iter: 1179  total_loss: 0.511  loss_cls: 0.163  loss_box_reg: 0.269  loss_rpn_cls: 0.025  loss_rpn_loc: 0.044  time: 0.5534  data_time: 0.0101  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:46:19 d2.utils.events]: [0m eta: 0:07:25  iter: 1199  total_loss: 0.827  loss_cls: 0.257  loss_box_reg: 0.439  loss_rpn_cls: 0.021  loss_rpn_loc: 0.074  time: 0.5533  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:46:30 d2.utils.events]: [0m eta: 0:07:14  iter: 1219  total_loss: 0.817  loss_cls: 0.238  loss_box_reg: 0.423  loss_rpn_cls: 0.041  loss_rpn_loc: 0.061  time: 0.5533  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:46:42 d2.utils.events]: [0m eta: 0:07:03  iter: 1239  total_loss: 0.634  loss_cls: 0.204  loss_box_reg: 0.354  loss_rpn_cls: 0.023  loss_rpn_loc: 0.070  time: 0.5535  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:46:52 d2.utils.events]: [0m eta: 0:06:52  iter: 1259  total_loss: 0.679  loss_cls: 0.206  loss_box_reg: 0.355  loss_rpn_cls: 0.035  loss_rpn_loc: 0.082  time: 0.5532  data_time: 0.0089  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:47:03 d2.utils.events]: [0m eta: 0:06:41  iter: 1279  total_loss: 0.632  loss_cls: 0.214  loss_box_reg: 0.341  loss_rpn_cls: 0.027  loss_rpn_loc: 0.069  time: 0.5530  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:47:15 d2.utils.events]: [0m eta: 0:06:29  iter: 1299  total_loss: 0.682  loss_cls: 0.234  loss_box_reg: 0.347  loss_rpn_cls: 0.034  loss_rpn_loc: 0.055  time: 0.5530  data_time: 0.0097  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:47:26 d2.utils.events]: [0m eta: 0:06:18  iter: 1319  total_loss: 0.560  loss_cls: 0.180  loss_box_reg: 0.275  loss_rpn_cls: 0.024  loss_rpn_loc: 0.038  time: 0.5530  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:47:38 d2.utils.events]: [0m eta: 0:06:07  iter: 1339  total_loss: 0.600  loss_cls: 0.186  loss_box_reg: 0.307  loss_rpn_cls: 0.029  loss_rpn_loc: 0.080  time: 0.5538  data_time: 0.0595  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:47:49 d2.utils.events]: [0m eta: 0:05:56  iter: 1359  total_loss: 0.869  loss_cls: 0.262  loss_box_reg: 0.468  loss_rpn_cls: 0.033  loss_rpn_loc: 0.126  time: 0.5538  data_time: 0.0106  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:48:01 d2.utils.events]: [0m eta: 0:05:45  iter: 1379  total_loss: 0.748  loss_cls: 0.252  loss_box_reg: 0.369  loss_rpn_cls: 0.027  loss_rpn_loc: 0.066  time: 0.5539  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:48:12 d2.utils.events]: [0m eta: 0:05:34  iter: 1399  total_loss: 0.729  loss_cls: 0.238  loss_box_reg: 0.390  loss_rpn_cls: 0.029  loss_rpn_loc: 0.078  time: 0.5543  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:48:24 d2.utils.events]: [0m eta: 0:05:23  iter: 1419  total_loss: 0.638  loss_cls: 0.213  loss_box_reg: 0.315  loss_rpn_cls: 0.026  loss_rpn_loc: 0.055  time: 0.5544  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:48:35 d2.utils.events]: [0m eta: 0:05:12  iter: 1439  total_loss: 0.716  loss_cls: 0.229  loss_box_reg: 0.329  loss_rpn_cls: 0.034  loss_rpn_loc: 0.065  time: 0.5544  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:48:46 d2.utils.events]: [0m eta: 0:05:01  iter: 1459  total_loss: 0.804  loss_cls: 0.267  loss_box_reg: 0.410  loss_rpn_cls: 0.027  loss_rpn_loc: 0.069  time: 0.5546  data_time: 0.0091  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:48:57 d2.utils.events]: [0m eta: 0:04:50  iter: 1479  total_loss: 0.685  loss_cls: 0.219  loss_box_reg: 0.376  loss_rpn_cls: 0.029  loss_rpn_loc: 0.057  time: 0.5546  data_time: 0.0112  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:49:08 d2.utils.events]: [0m eta: 0:04:38  iter: 1499  total_loss: 0.628  loss_cls: 0.211  loss_box_reg: 0.353  loss_rpn_cls: 0.024  loss_rpn_loc: 0.045  time: 0.5544  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:49:19 d2.utils.events]: [0m eta: 0:04:27  iter: 1519  total_loss: 0.688  loss_cls: 0.200  loss_box_reg: 0.330  loss_rpn_cls: 0.026  loss_rpn_loc: 0.058  time: 0.5545  data_time: 0.0102  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:49:31 d2.utils.events]: [0m eta: 0:04:16  iter: 1539  total_loss: 0.674  loss_cls: 0.226  loss_box_reg: 0.376  loss_rpn_cls: 0.022  loss_rpn_loc: 0.069  time: 0.5544  data_time: 0.0106  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:49:42 d2.utils.events]: [0m eta: 0:04:05  iter: 1559  total_loss: 0.793  loss_cls: 0.246  loss_box_reg: 0.426  loss_rpn_cls: 0.030  loss_rpn_loc: 0.072  time: 0.5544  data_time: 0.0095  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:49:53 d2.utils.events]: [0m eta: 0:03:54  iter: 1579  total_loss: 0.660  loss_cls: 0.192  loss_box_reg: 0.378  loss_rpn_cls: 0.025  loss_rpn_loc: 0.070  time: 0.5545  data_time: 0.0098  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:50:04 d2.utils.events]: [0m eta: 0:03:43  iter: 1599  total_loss: 0.742  loss_cls: 0.235  loss_box_reg: 0.417  loss_rpn_cls: 0.027  loss_rpn_loc: 0.065  time: 0.5543  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:50:15 d2.utils.events]: [0m eta: 0:03:31  iter: 1619  total_loss: 0.749  loss_cls: 0.236  loss_box_reg: 0.371  loss_rpn_cls: 0.034  loss_rpn_loc: 0.077  time: 0.5542  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:50:26 d2.utils.events]: [0m eta: 0:03:20  iter: 1639  total_loss: 0.736  loss_cls: 0.236  loss_box_reg: 0.400  loss_rpn_cls: 0.025  loss_rpn_loc: 0.063  time: 0.5543  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:50:37 d2.utils.events]: [0m eta: 0:03:09  iter: 1659  total_loss: 0.685  loss_cls: 0.216  loss_box_reg: 0.372  loss_rpn_cls: 0.025  loss_rpn_loc: 0.080  time: 0.5543  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:50:49 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.749  loss_cls: 0.249  loss_box_reg: 0.398  loss_rpn_cls: 0.028  loss_rpn_loc: 0.077  time: 0.5543  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:51:00 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.733  loss_cls: 0.219  loss_box_reg: 0.392  loss_rpn_cls: 0.028  loss_rpn_loc: 0.074  time: 0.5542  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:51:11 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.610  loss_cls: 0.193  loss_box_reg: 0.341  loss_rpn_cls: 0.022  loss_rpn_loc: 0.054  time: 0.5541  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:51:22 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.654  loss_cls: 0.191  loss_box_reg: 0.354  loss_rpn_cls: 0.019  loss_rpn_loc: 0.091  time: 0.5542  data_time: 0.0095  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:51:33 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.685  loss_cls: 0.220  loss_box_reg: 0.342  loss_rpn_cls: 0.030  loss_rpn_loc: 0.054  time: 0.5541  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:51:45 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.683  loss_cls: 0.227  loss_box_reg: 0.356  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5543  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:51:55 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.715  loss_cls: 0.227  loss_box_reg: 0.395  loss_rpn_cls: 0.029  loss_rpn_loc: 0.072  time: 0.5541  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:52:06 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.714  loss_cls: 0.232  loss_box_reg: 0.367  loss_rpn_cls: 0.032  loss_rpn_loc: 0.066  time: 0.5540  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:52:18 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.623  loss_cls: 0.213  loss_box_reg: 0.309  loss_rpn_cls: 0.022  loss_rpn_loc: 0.059  time: 0.5541  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:52:32 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.727  loss_cls: 0.224  loss_box_reg: 0.368  loss_rpn_cls: 0.039  loss_rpn_loc: 0.077  time: 0.5538  data_time: 0.0060  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:52:43 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.742  loss_cls: 0.236  loss_box_reg: 0.400  loss_rpn_cls: 0.029  loss_rpn_loc: 0.081  time: 0.5539  data_time: 0.0176  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:52:55 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.690  loss_cls: 0.227  loss_box_reg: 0.377  loss_rpn_cls: 0.026  loss_rpn_loc: 0.075  time: 0.5539  data_time: 0.0062  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:53:06 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.665  loss_cls: 0.214  loss_box_reg: 0.358  loss_rpn_cls: 0.026  loss_rpn_loc: 0.063  time: 0.5538  data_time: 0.0087  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:53:27 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.681  loss_cls: 0.209  loss_box_reg: 0.355  loss_rpn_cls: 0.018  loss_rpn_loc: 0.063  time: 0.5537  data_time: 0.0108  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:53:38 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.619  loss_cls: 0.206  loss_box_reg: 0.325  loss_rpn_cls: 0.028  loss_rpn_loc: 0.052  time: 0.5537  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:53:49 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.605  loss_cls: 0.199  loss_box_reg: 0.332  loss_rpn_cls: 0.040  loss_rpn_loc: 0.058  time: 0.5535  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 09:54:19 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:54:19 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 09:54:19 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 09:54:19 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.768  loss_cls: 0.252  loss_box_reg: 0.405  loss_rpn_cls: 0.033  loss_rpn_loc: 0.066  time: 0.5535  data_time: 0.0061  lr: 0.010000  max_mem: 3070M
[32m[04/15 09:54:29 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:25 (0.5538 s / it)
[32m[04/15 09:54:29 d2.engine.hooks]: [0mTotal training time: 0:19:18 (0:00:52 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 09:54:49 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:54:49 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 09:54:49 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 09:54:51 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1233 s / img. ETA=0:02:37
[32m[04/15 09:54:56 d2.evaluation.evaluator]: [0mInference done 50/1257. 0.1255 s / img. ETA=0:02:35
[32m[04/15 09:55:01 d2.evaluation.evaluator]: [0mInference done 89/1257. 0.1254 s / img. ETA=0:02:30
[32m[04/15 09:55:06 d2.evaluation.evaluator]: [0mInference done 127/1257. 0.1270 s / img. ETA=0:02:27
[32m[04/15 09:55:11 d2.evaluation.evaluator]: [0mInference done 167/1257. 0.1262 s / img. ETA=0:02:21
[32m[04/15 09:55:16 d2.evaluation.evaluator]: [0mInference done 207/1257. 0.1257 s / img. ETA=0:02:15
[32m[04/15 09:55:21 d2.evaluation.evaluator]: [0mInference done 247/1257. 0.1254 s / img. ETA=0:02:10
[32m[04/15 09:55:26 d2.evaluation.evaluator]: [0mInference done 285/1257. 0.1256 s / img. ETA=0:02:05
[32m[04/15 09:55:31 d2.evaluation.evaluator]: [0mInference done 326/1257. 0.1250 s / img. ETA=0:01:59
[32m[04/15 09:55:37 d2.evaluation.evaluator]: [0mInference done 366/1257. 0.1248 s / img. ETA=0:01:54
[32m[04/15 09:55:42 d2.evaluation.evaluator]: [0mInference done 406/1257. 0.1246 s / img. ETA=0:01:49
[32m[04/15 09:55:47 d2.evaluation.evaluator]: [0mInference done 445/1257. 0.1249 s / img. ETA=0:01:44
[32m[04/15 09:55:52 d2.evaluation.evaluator]: [0mInference done 484/1257. 0.1248 s / img. ETA=0:01:39
[32m[04/15 09:55:57 d2.evaluation.evaluator]: [0mInference done 524/1257. 0.1248 s / img. ETA=0:01:34
[32m[04/15 09:56:02 d2.evaluation.evaluator]: [0mInference done 564/1257. 0.1246 s / img. ETA=0:01:28
[32m[04/15 09:56:07 d2.evaluation.evaluator]: [0mInference done 604/1257. 0.1245 s / img. ETA=0:01:23
[32m[04/15 09:56:12 d2.evaluation.evaluator]: [0mInference done 645/1257. 0.1242 s / img. ETA=0:01:18
[32m[04/15 09:56:17 d2.evaluation.evaluator]: [0mInference done 685/1257. 0.1241 s / img. ETA=0:01:13
[32m[04/15 09:56:22 d2.evaluation.evaluator]: [0mInference done 724/1257. 0.1243 s / img. ETA=0:01:08
[32m[04/15 09:56:27 d2.evaluation.evaluator]: [0mInference done 763/1257. 0.1245 s / img. ETA=0:01:03
[32m[04/15 09:56:32 d2.evaluation.evaluator]: [0mInference done 801/1257. 0.1247 s / img. ETA=0:00:58
[32m[04/15 09:56:37 d2.evaluation.evaluator]: [0mInference done 840/1257. 0.1247 s / img. ETA=0:00:53
[32m[04/15 09:56:42 d2.evaluation.evaluator]: [0mInference done 879/1257. 0.1248 s / img. ETA=0:00:48
[32m[04/15 09:56:47 d2.evaluation.evaluator]: [0mInference done 917/1257. 0.1250 s / img. ETA=0:00:43
[32m[04/15 09:56:52 d2.evaluation.evaluator]: [0mInference done 956/1257. 0.1250 s / img. ETA=0:00:38
[32m[04/15 09:56:58 d2.evaluation.evaluator]: [0mInference done 995/1257. 0.1251 s / img. ETA=0:00:33
[32m[04/15 09:57:03 d2.evaluation.evaluator]: [0mInference done 1034/1257. 0.1250 s / img. ETA=0:00:28
[32m[04/15 09:57:08 d2.evaluation.evaluator]: [0mInference done 1073/1257. 0.1251 s / img. ETA=0:00:23
[32m[04/15 09:57:13 d2.evaluation.evaluator]: [0mInference done 1113/1257. 0.1251 s / img. ETA=0:00:18
[32m[04/15 09:57:18 d2.evaluation.evaluator]: [0mInference done 1150/1257. 0.1253 s / img. ETA=0:00:13
[32m[04/15 09:57:23 d2.evaluation.evaluator]: [0mInference done 1189/1257. 0.1253 s / img. ETA=0:00:08
[32m[04/15 09:57:28 d2.evaluation.evaluator]: [0mInference done 1228/1257. 0.1253 s / img. ETA=0:00:03
[32m[04/15 09:57:32 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:41.541156 (0.129026 s / img per device, on 1 devices)
[32m[04/15 09:57:32 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:36 (0.125252 s / img per device, on 1 devices)
[32m[04/15 09:57:32 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 09:57:32 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 09:57:32 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.03s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=4.99s).
Accumulating evaluation results...
DONE (t=0.58s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.166
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.334
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.149
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.099
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.371
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.085
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.204
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.205
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.129
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.255
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.418
[32m[04/15 09:57:38 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 16.596 | 33.359 | 14.936 | 9.856 | 20.820 | 37.135 |
[32m[04/15 09:57:38 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 22.232 | bicycle       | 6.970 | car            | 37.182 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  12  *  2000  iterations ============
4 channel input
[32m[04/15 09:57:39 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[5m[31mWARNING[0m [32m[04/15 09:57:40 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:57:40 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 09:57:41 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 09:57:41 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 09:57:41 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 09:57:41 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 09:57:52 d2.utils.events]: [0m eta: 0:17:37  iter: 19  total_loss: 0.613  loss_cls: 0.191  loss_box_reg: 0.318  loss_rpn_cls: 0.026  loss_rpn_loc: 0.069  time: 0.5326  data_time: 0.0356  lr: 0.000200  max_mem: 3070M
[32m[04/15 09:58:03 d2.utils.events]: [0m eta: 0:17:30  iter: 39  total_loss: 0.553  loss_cls: 0.170  loss_box_reg: 0.277  loss_rpn_cls: 0.020  loss_rpn_loc: 0.053  time: 0.5398  data_time: 0.0075  lr: 0.000400  max_mem: 3070M
[32m[04/15 09:58:14 d2.utils.events]: [0m eta: 0:17:24  iter: 59  total_loss: 0.731  loss_cls: 0.237  loss_box_reg: 0.385  loss_rpn_cls: 0.030  loss_rpn_loc: 0.068  time: 0.5403  data_time: 0.0107  lr: 0.000599  max_mem: 3070M
[32m[04/15 09:58:26 d2.utils.events]: [0m eta: 0:17:21  iter: 79  total_loss: 0.752  loss_cls: 0.237  loss_box_reg: 0.405  loss_rpn_cls: 0.021  loss_rpn_loc: 0.066  time: 0.5439  data_time: 0.0094  lr: 0.000799  max_mem: 3070M
[32m[04/15 09:58:37 d2.utils.events]: [0m eta: 0:17:08  iter: 99  total_loss: 0.546  loss_cls: 0.168  loss_box_reg: 0.299  loss_rpn_cls: 0.023  loss_rpn_loc: 0.051  time: 0.5431  data_time: 0.0073  lr: 0.000999  max_mem: 3070M
[32m[04/15 09:58:47 d2.utils.events]: [0m eta: 0:16:51  iter: 119  total_loss: 0.837  loss_cls: 0.261  loss_box_reg: 0.418  loss_rpn_cls: 0.027  loss_rpn_loc: 0.065  time: 0.5391  data_time: 0.0066  lr: 0.001199  max_mem: 3070M
[32m[04/15 09:58:58 d2.utils.events]: [0m eta: 0:16:43  iter: 139  total_loss: 0.673  loss_cls: 0.221  loss_box_reg: 0.367  loss_rpn_cls: 0.024  loss_rpn_loc: 0.061  time: 0.5405  data_time: 0.0084  lr: 0.001399  max_mem: 3070M
[32m[04/15 09:59:10 d2.utils.events]: [0m eta: 0:16:39  iter: 159  total_loss: 0.724  loss_cls: 0.219  loss_box_reg: 0.382  loss_rpn_cls: 0.021  loss_rpn_loc: 0.050  time: 0.5454  data_time: 0.0074  lr: 0.001598  max_mem: 3070M
[32m[04/15 09:59:21 d2.utils.events]: [0m eta: 0:16:29  iter: 179  total_loss: 0.570  loss_cls: 0.190  loss_box_reg: 0.291  loss_rpn_cls: 0.025  loss_rpn_loc: 0.048  time: 0.5476  data_time: 0.0072  lr: 0.001798  max_mem: 3070M
[32m[04/15 09:59:33 d2.utils.events]: [0m eta: 0:16:29  iter: 199  total_loss: 0.497  loss_cls: 0.162  loss_box_reg: 0.280  loss_rpn_cls: 0.021  loss_rpn_loc: 0.041  time: 0.5502  data_time: 0.0066  lr: 0.001998  max_mem: 3070M
[32m[04/15 09:59:44 d2.utils.events]: [0m eta: 0:16:18  iter: 219  total_loss: 0.802  loss_cls: 0.258  loss_box_reg: 0.377  loss_rpn_cls: 0.022  loss_rpn_loc: 0.057  time: 0.5496  data_time: 0.0070  lr: 0.002198  max_mem: 3070M
[32m[04/15 09:59:55 d2.utils.events]: [0m eta: 0:16:02  iter: 239  total_loss: 0.691  loss_cls: 0.227  loss_box_reg: 0.360  loss_rpn_cls: 0.017  loss_rpn_loc: 0.052  time: 0.5487  data_time: 0.0083  lr: 0.002398  max_mem: 3070M
[32m[04/15 10:00:05 d2.utils.events]: [0m eta: 0:15:47  iter: 259  total_loss: 0.759  loss_cls: 0.209  loss_box_reg: 0.364  loss_rpn_cls: 0.020  loss_rpn_loc: 0.061  time: 0.5475  data_time: 0.0070  lr: 0.002597  max_mem: 3070M
[32m[04/15 10:00:16 d2.utils.events]: [0m eta: 0:15:40  iter: 279  total_loss: 0.638  loss_cls: 0.203  loss_box_reg: 0.319  loss_rpn_cls: 0.022  loss_rpn_loc: 0.068  time: 0.5474  data_time: 0.0075  lr: 0.002797  max_mem: 3070M
[32m[04/15 10:00:27 d2.utils.events]: [0m eta: 0:15:29  iter: 299  total_loss: 0.621  loss_cls: 0.199  loss_box_reg: 0.338  loss_rpn_cls: 0.019  loss_rpn_loc: 0.048  time: 0.5478  data_time: 0.0088  lr: 0.002997  max_mem: 3070M
[32m[04/15 10:00:39 d2.utils.events]: [0m eta: 0:15:23  iter: 319  total_loss: 0.716  loss_cls: 0.234  loss_box_reg: 0.380  loss_rpn_cls: 0.016  loss_rpn_loc: 0.054  time: 0.5495  data_time: 0.0070  lr: 0.003197  max_mem: 3070M
[32m[04/15 10:00:50 d2.utils.events]: [0m eta: 0:15:11  iter: 339  total_loss: 0.651  loss_cls: 0.211  loss_box_reg: 0.373  loss_rpn_cls: 0.019  loss_rpn_loc: 0.061  time: 0.5490  data_time: 0.0074  lr: 0.003397  max_mem: 3070M
[32m[04/15 10:01:02 d2.utils.events]: [0m eta: 0:15:01  iter: 359  total_loss: 0.686  loss_cls: 0.217  loss_box_reg: 0.370  loss_rpn_cls: 0.014  loss_rpn_loc: 0.059  time: 0.5503  data_time: 0.0074  lr: 0.003596  max_mem: 3070M
[32m[04/15 10:01:13 d2.utils.events]: [0m eta: 0:14:51  iter: 379  total_loss: 0.629  loss_cls: 0.200  loss_box_reg: 0.318  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5513  data_time: 0.0067  lr: 0.003796  max_mem: 3070M
[32m[04/15 10:01:24 d2.utils.events]: [0m eta: 0:14:39  iter: 399  total_loss: 0.692  loss_cls: 0.209  loss_box_reg: 0.373  loss_rpn_cls: 0.024  loss_rpn_loc: 0.055  time: 0.5503  data_time: 0.0066  lr: 0.003996  max_mem: 3070M
[32m[04/15 10:01:35 d2.utils.events]: [0m eta: 0:14:28  iter: 419  total_loss: 0.570  loss_cls: 0.185  loss_box_reg: 0.287  loss_rpn_cls: 0.020  loss_rpn_loc: 0.046  time: 0.5503  data_time: 0.0072  lr: 0.004196  max_mem: 3070M
[32m[04/15 10:01:46 d2.utils.events]: [0m eta: 0:14:18  iter: 439  total_loss: 0.591  loss_cls: 0.190  loss_box_reg: 0.328  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5509  data_time: 0.0089  lr: 0.004396  max_mem: 3070M
[32m[04/15 10:01:57 d2.utils.events]: [0m eta: 0:14:06  iter: 459  total_loss: 0.576  loss_cls: 0.194  loss_box_reg: 0.301  loss_rpn_cls: 0.014  loss_rpn_loc: 0.065  time: 0.5510  data_time: 0.0075  lr: 0.004595  max_mem: 3070M
[32m[04/15 10:02:09 d2.utils.events]: [0m eta: 0:13:56  iter: 479  total_loss: 0.751  loss_cls: 0.257  loss_box_reg: 0.392  loss_rpn_cls: 0.019  loss_rpn_loc: 0.061  time: 0.5515  data_time: 0.0072  lr: 0.004795  max_mem: 3070M
[32m[04/15 10:02:20 d2.utils.events]: [0m eta: 0:13:48  iter: 499  total_loss: 0.679  loss_cls: 0.219  loss_box_reg: 0.358  loss_rpn_cls: 0.021  loss_rpn_loc: 0.078  time: 0.5524  data_time: 0.0084  lr: 0.004995  max_mem: 3070M
[32m[04/15 10:02:31 d2.utils.events]: [0m eta: 0:13:34  iter: 519  total_loss: 0.552  loss_cls: 0.184  loss_box_reg: 0.304  loss_rpn_cls: 0.021  loss_rpn_loc: 0.042  time: 0.5518  data_time: 0.0065  lr: 0.005195  max_mem: 3070M
[32m[04/15 10:02:42 d2.utils.events]: [0m eta: 0:13:25  iter: 539  total_loss: 0.600  loss_cls: 0.177  loss_box_reg: 0.354  loss_rpn_cls: 0.017  loss_rpn_loc: 0.051  time: 0.5525  data_time: 0.0070  lr: 0.005395  max_mem: 3070M
[32m[04/15 10:02:54 d2.utils.events]: [0m eta: 0:13:16  iter: 559  total_loss: 0.679  loss_cls: 0.231  loss_box_reg: 0.369  loss_rpn_cls: 0.017  loss_rpn_loc: 0.056  time: 0.5529  data_time: 0.0070  lr: 0.005594  max_mem: 3070M
[32m[04/15 10:03:05 d2.utils.events]: [0m eta: 0:13:04  iter: 579  total_loss: 0.778  loss_cls: 0.262  loss_box_reg: 0.392  loss_rpn_cls: 0.033  loss_rpn_loc: 0.072  time: 0.5522  data_time: 0.0091  lr: 0.005794  max_mem: 3070M
[32m[04/15 10:03:16 d2.utils.events]: [0m eta: 0:12:54  iter: 599  total_loss: 0.598  loss_cls: 0.194  loss_box_reg: 0.344  loss_rpn_cls: 0.020  loss_rpn_loc: 0.051  time: 0.5527  data_time: 0.0069  lr: 0.005994  max_mem: 3070M
[32m[04/15 10:03:28 d2.utils.events]: [0m eta: 0:12:40  iter: 619  total_loss: 0.668  loss_cls: 0.213  loss_box_reg: 0.353  loss_rpn_cls: 0.024  loss_rpn_loc: 0.071  time: 0.5539  data_time: 0.0810  lr: 0.006194  max_mem: 3070M
[32m[04/15 10:03:39 d2.utils.events]: [0m eta: 0:12:31  iter: 639  total_loss: 0.655  loss_cls: 0.201  loss_box_reg: 0.337  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5542  data_time: 0.0148  lr: 0.006394  max_mem: 3070M
[32m[04/15 10:03:51 d2.utils.events]: [0m eta: 0:12:21  iter: 659  total_loss: 0.663  loss_cls: 0.228  loss_box_reg: 0.364  loss_rpn_cls: 0.017  loss_rpn_loc: 0.064  time: 0.5544  data_time: 0.0117  lr: 0.006593  max_mem: 3070M
[32m[04/15 10:04:02 d2.utils.events]: [0m eta: 0:12:10  iter: 679  total_loss: 0.665  loss_cls: 0.207  loss_box_reg: 0.395  loss_rpn_cls: 0.016  loss_rpn_loc: 0.050  time: 0.5548  data_time: 0.0070  lr: 0.006793  max_mem: 3070M
[32m[04/15 10:04:13 d2.utils.events]: [0m eta: 0:12:00  iter: 699  total_loss: 0.638  loss_cls: 0.218  loss_box_reg: 0.341  loss_rpn_cls: 0.020  loss_rpn_loc: 0.062  time: 0.5548  data_time: 0.0074  lr: 0.006993  max_mem: 3070M
[32m[04/15 10:04:25 d2.utils.events]: [0m eta: 0:11:50  iter: 719  total_loss: 0.731  loss_cls: 0.203  loss_box_reg: 0.379  loss_rpn_cls: 0.018  loss_rpn_loc: 0.069  time: 0.5552  data_time: 0.0082  lr: 0.007193  max_mem: 3070M
[32m[04/15 10:04:36 d2.utils.events]: [0m eta: 0:11:39  iter: 739  total_loss: 0.616  loss_cls: 0.205  loss_box_reg: 0.365  loss_rpn_cls: 0.020  loss_rpn_loc: 0.049  time: 0.5555  data_time: 0.0066  lr: 0.007393  max_mem: 3070M
[32m[04/15 10:04:48 d2.utils.events]: [0m eta: 0:11:29  iter: 759  total_loss: 0.657  loss_cls: 0.211  loss_box_reg: 0.354  loss_rpn_cls: 0.024  loss_rpn_loc: 0.068  time: 0.5559  data_time: 0.0072  lr: 0.007592  max_mem: 3070M
[32m[04/15 10:04:58 d2.utils.events]: [0m eta: 0:11:17  iter: 779  total_loss: 0.613  loss_cls: 0.203  loss_box_reg: 0.315  loss_rpn_cls: 0.022  loss_rpn_loc: 0.068  time: 0.5554  data_time: 0.0067  lr: 0.007792  max_mem: 3070M
[32m[04/15 10:05:09 d2.utils.events]: [0m eta: 0:11:06  iter: 799  total_loss: 0.535  loss_cls: 0.178  loss_box_reg: 0.300  loss_rpn_cls: 0.012  loss_rpn_loc: 0.051  time: 0.5547  data_time: 0.0042  lr: 0.007992  max_mem: 3070M
[32m[04/15 10:05:20 d2.utils.events]: [0m eta: 0:10:55  iter: 819  total_loss: 0.617  loss_cls: 0.237  loss_box_reg: 0.325  loss_rpn_cls: 0.017  loss_rpn_loc: 0.048  time: 0.5546  data_time: 0.0088  lr: 0.008192  max_mem: 3070M
[32m[04/15 10:05:32 d2.utils.events]: [0m eta: 0:10:44  iter: 839  total_loss: 0.717  loss_cls: 0.247  loss_box_reg: 0.373  loss_rpn_cls: 0.021  loss_rpn_loc: 0.059  time: 0.5549  data_time: 0.0081  lr: 0.008392  max_mem: 3070M
[32m[04/15 10:05:43 d2.utils.events]: [0m eta: 0:10:33  iter: 859  total_loss: 0.800  loss_cls: 0.252  loss_box_reg: 0.390  loss_rpn_cls: 0.018  loss_rpn_loc: 0.086  time: 0.5552  data_time: 0.0081  lr: 0.008591  max_mem: 3070M
[32m[04/15 10:05:56 d2.utils.events]: [0m eta: 0:10:21  iter: 879  total_loss: 0.663  loss_cls: 0.222  loss_box_reg: 0.354  loss_rpn_cls: 0.024  loss_rpn_loc: 0.071  time: 0.5547  data_time: 0.0080  lr: 0.008791  max_mem: 3070M
[32m[04/15 10:06:07 d2.utils.events]: [0m eta: 0:10:10  iter: 899  total_loss: 0.706  loss_cls: 0.217  loss_box_reg: 0.361  loss_rpn_cls: 0.026  loss_rpn_loc: 0.085  time: 0.5546  data_time: 0.0089  lr: 0.008991  max_mem: 3070M
[32m[04/15 10:06:18 d2.utils.events]: [0m eta: 0:09:59  iter: 919  total_loss: 0.695  loss_cls: 0.218  loss_box_reg: 0.388  loss_rpn_cls: 0.019  loss_rpn_loc: 0.050  time: 0.5546  data_time: 0.0066  lr: 0.009191  max_mem: 3070M
[32m[04/15 10:06:29 d2.utils.events]: [0m eta: 0:09:48  iter: 939  total_loss: 0.646  loss_cls: 0.220  loss_box_reg: 0.326  loss_rpn_cls: 0.021  loss_rpn_loc: 0.050  time: 0.5548  data_time: 0.0070  lr: 0.009391  max_mem: 3070M
[32m[04/15 10:06:40 d2.utils.events]: [0m eta: 0:09:38  iter: 959  total_loss: 0.636  loss_cls: 0.211  loss_box_reg: 0.334  loss_rpn_cls: 0.022  loss_rpn_loc: 0.057  time: 0.5547  data_time: 0.0072  lr: 0.009590  max_mem: 3070M
[32m[04/15 10:06:52 d2.utils.events]: [0m eta: 0:09:27  iter: 979  total_loss: 0.743  loss_cls: 0.224  loss_box_reg: 0.408  loss_rpn_cls: 0.018  loss_rpn_loc: 0.065  time: 0.5553  data_time: 0.0075  lr: 0.009790  max_mem: 3070M
[32m[04/15 10:07:03 d2.utils.events]: [0m eta: 0:09:16  iter: 999  total_loss: 0.533  loss_cls: 0.167  loss_box_reg: 0.290  loss_rpn_cls: 0.019  loss_rpn_loc: 0.055  time: 0.5554  data_time: 0.0068  lr: 0.009990  max_mem: 3070M
[32m[04/15 10:07:15 d2.utils.events]: [0m eta: 0:09:05  iter: 1019  total_loss: 0.592  loss_cls: 0.214  loss_box_reg: 0.348  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5554  data_time: 0.0099  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:07:26 d2.utils.events]: [0m eta: 0:08:55  iter: 1039  total_loss: 0.635  loss_cls: 0.213  loss_box_reg: 0.332  loss_rpn_cls: 0.025  loss_rpn_loc: 0.070  time: 0.5556  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:07:37 d2.utils.events]: [0m eta: 0:08:44  iter: 1059  total_loss: 0.590  loss_cls: 0.205  loss_box_reg: 0.313  loss_rpn_cls: 0.021  loss_rpn_loc: 0.055  time: 0.5554  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:07:48 d2.utils.events]: [0m eta: 0:08:33  iter: 1079  total_loss: 0.773  loss_cls: 0.269  loss_box_reg: 0.415  loss_rpn_cls: 0.021  loss_rpn_loc: 0.074  time: 0.5556  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:08:00 d2.utils.events]: [0m eta: 0:08:23  iter: 1099  total_loss: 0.711  loss_cls: 0.234  loss_box_reg: 0.387  loss_rpn_cls: 0.027  loss_rpn_loc: 0.064  time: 0.5559  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:08:11 d2.utils.events]: [0m eta: 0:08:12  iter: 1119  total_loss: 0.733  loss_cls: 0.256  loss_box_reg: 0.388  loss_rpn_cls: 0.024  loss_rpn_loc: 0.061  time: 0.5559  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:08:23 d2.utils.events]: [0m eta: 0:08:02  iter: 1139  total_loss: 0.624  loss_cls: 0.220  loss_box_reg: 0.327  loss_rpn_cls: 0.022  loss_rpn_loc: 0.082  time: 0.5565  data_time: 0.0135  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:08:34 d2.utils.events]: [0m eta: 0:07:50  iter: 1159  total_loss: 0.736  loss_cls: 0.234  loss_box_reg: 0.391  loss_rpn_cls: 0.032  loss_rpn_loc: 0.061  time: 0.5564  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:08:45 d2.utils.events]: [0m eta: 0:07:39  iter: 1179  total_loss: 0.588  loss_cls: 0.197  loss_box_reg: 0.319  loss_rpn_cls: 0.023  loss_rpn_loc: 0.046  time: 0.5565  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:08:57 d2.utils.events]: [0m eta: 0:07:27  iter: 1199  total_loss: 0.638  loss_cls: 0.198  loss_box_reg: 0.332  loss_rpn_cls: 0.037  loss_rpn_loc: 0.066  time: 0.5563  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:09:08 d2.utils.events]: [0m eta: 0:07:17  iter: 1219  total_loss: 0.633  loss_cls: 0.199  loss_box_reg: 0.354  loss_rpn_cls: 0.024  loss_rpn_loc: 0.058  time: 0.5566  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:09:19 d2.utils.events]: [0m eta: 0:07:06  iter: 1239  total_loss: 0.744  loss_cls: 0.253  loss_box_reg: 0.370  loss_rpn_cls: 0.033  loss_rpn_loc: 0.072  time: 0.5567  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:09:30 d2.utils.events]: [0m eta: 0:06:55  iter: 1259  total_loss: 0.596  loss_cls: 0.216  loss_box_reg: 0.296  loss_rpn_cls: 0.021  loss_rpn_loc: 0.048  time: 0.5563  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:09:42 d2.utils.events]: [0m eta: 0:06:44  iter: 1279  total_loss: 0.802  loss_cls: 0.272  loss_box_reg: 0.421  loss_rpn_cls: 0.030  loss_rpn_loc: 0.082  time: 0.5564  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:09:53 d2.utils.events]: [0m eta: 0:06:33  iter: 1299  total_loss: 0.684  loss_cls: 0.211  loss_box_reg: 0.357  loss_rpn_cls: 0.026  loss_rpn_loc: 0.056  time: 0.5565  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:10:04 d2.utils.events]: [0m eta: 0:06:21  iter: 1319  total_loss: 0.703  loss_cls: 0.232  loss_box_reg: 0.390  loss_rpn_cls: 0.025  loss_rpn_loc: 0.077  time: 0.5564  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:10:15 d2.utils.events]: [0m eta: 0:06:10  iter: 1339  total_loss: 0.775  loss_cls: 0.253  loss_box_reg: 0.414  loss_rpn_cls: 0.022  loss_rpn_loc: 0.063  time: 0.5564  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:10:27 d2.utils.events]: [0m eta: 0:05:59  iter: 1359  total_loss: 0.852  loss_cls: 0.271  loss_box_reg: 0.463  loss_rpn_cls: 0.028  loss_rpn_loc: 0.076  time: 0.5565  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:10:38 d2.utils.events]: [0m eta: 0:05:47  iter: 1379  total_loss: 0.696  loss_cls: 0.209  loss_box_reg: 0.373  loss_rpn_cls: 0.021  loss_rpn_loc: 0.061  time: 0.5564  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:10:49 d2.utils.events]: [0m eta: 0:05:37  iter: 1399  total_loss: 0.785  loss_cls: 0.249  loss_box_reg: 0.463  loss_rpn_cls: 0.024  loss_rpn_loc: 0.072  time: 0.5565  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:11:00 d2.utils.events]: [0m eta: 0:05:25  iter: 1419  total_loss: 0.584  loss_cls: 0.218  loss_box_reg: 0.310  loss_rpn_cls: 0.020  loss_rpn_loc: 0.054  time: 0.5565  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:11:11 d2.utils.events]: [0m eta: 0:05:14  iter: 1439  total_loss: 0.685  loss_cls: 0.225  loss_box_reg: 0.410  loss_rpn_cls: 0.017  loss_rpn_loc: 0.062  time: 0.5564  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:11:23 d2.utils.events]: [0m eta: 0:05:03  iter: 1459  total_loss: 0.642  loss_cls: 0.192  loss_box_reg: 0.383  loss_rpn_cls: 0.018  loss_rpn_loc: 0.048  time: 0.5566  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:11:34 d2.utils.events]: [0m eta: 0:04:51  iter: 1479  total_loss: 0.661  loss_cls: 0.220  loss_box_reg: 0.364  loss_rpn_cls: 0.026  loss_rpn_loc: 0.064  time: 0.5563  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:11:45 d2.utils.events]: [0m eta: 0:04:40  iter: 1499  total_loss: 0.643  loss_cls: 0.233  loss_box_reg: 0.348  loss_rpn_cls: 0.027  loss_rpn_loc: 0.065  time: 0.5562  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:11:56 d2.utils.events]: [0m eta: 0:04:29  iter: 1519  total_loss: 0.628  loss_cls: 0.208  loss_box_reg: 0.318  loss_rpn_cls: 0.025  loss_rpn_loc: 0.045  time: 0.5563  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:12:07 d2.utils.events]: [0m eta: 0:04:18  iter: 1539  total_loss: 0.636  loss_cls: 0.236  loss_box_reg: 0.333  loss_rpn_cls: 0.028  loss_rpn_loc: 0.060  time: 0.5564  data_time: 0.0104  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:12:19 d2.utils.events]: [0m eta: 0:04:06  iter: 1559  total_loss: 0.784  loss_cls: 0.250  loss_box_reg: 0.401  loss_rpn_cls: 0.033  loss_rpn_loc: 0.082  time: 0.5564  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:12:30 d2.utils.events]: [0m eta: 0:03:55  iter: 1579  total_loss: 0.607  loss_cls: 0.208  loss_box_reg: 0.339  loss_rpn_cls: 0.030  loss_rpn_loc: 0.052  time: 0.5563  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:12:41 d2.utils.events]: [0m eta: 0:03:44  iter: 1599  total_loss: 0.798  loss_cls: 0.244  loss_box_reg: 0.405  loss_rpn_cls: 0.024  loss_rpn_loc: 0.074  time: 0.5564  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:12:52 d2.utils.events]: [0m eta: 0:03:33  iter: 1619  total_loss: 0.665  loss_cls: 0.216  loss_box_reg: 0.394  loss_rpn_cls: 0.034  loss_rpn_loc: 0.065  time: 0.5564  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:13:04 d2.utils.events]: [0m eta: 0:03:22  iter: 1639  total_loss: 0.817  loss_cls: 0.278  loss_box_reg: 0.447  loss_rpn_cls: 0.031  loss_rpn_loc: 0.085  time: 0.5565  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:13:15 d2.utils.events]: [0m eta: 0:03:11  iter: 1659  total_loss: 0.715  loss_cls: 0.235  loss_box_reg: 0.397  loss_rpn_cls: 0.019  loss_rpn_loc: 0.051  time: 0.5565  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:13:26 d2.utils.events]: [0m eta: 0:02:59  iter: 1679  total_loss: 0.723  loss_cls: 0.230  loss_box_reg: 0.371  loss_rpn_cls: 0.023  loss_rpn_loc: 0.077  time: 0.5563  data_time: 0.0136  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:13:37 d2.utils.events]: [0m eta: 0:02:48  iter: 1699  total_loss: 0.730  loss_cls: 0.242  loss_box_reg: 0.386  loss_rpn_cls: 0.025  loss_rpn_loc: 0.058  time: 0.5564  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:13:48 d2.utils.events]: [0m eta: 0:02:37  iter: 1719  total_loss: 0.728  loss_cls: 0.245  loss_box_reg: 0.327  loss_rpn_cls: 0.026  loss_rpn_loc: 0.072  time: 0.5564  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:14:00 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.682  loss_cls: 0.206  loss_box_reg: 0.401  loss_rpn_cls: 0.026  loss_rpn_loc: 0.069  time: 0.5565  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:14:11 d2.utils.events]: [0m eta: 0:02:14  iter: 1759  total_loss: 0.625  loss_cls: 0.203  loss_box_reg: 0.324  loss_rpn_cls: 0.023  loss_rpn_loc: 0.056  time: 0.5568  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:14:23 d2.utils.events]: [0m eta: 0:02:03  iter: 1779  total_loss: 0.630  loss_cls: 0.211  loss_box_reg: 0.339  loss_rpn_cls: 0.021  loss_rpn_loc: 0.053  time: 0.5568  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:14:34 d2.utils.events]: [0m eta: 0:01:52  iter: 1799  total_loss: 0.689  loss_cls: 0.211  loss_box_reg: 0.368  loss_rpn_cls: 0.025  loss_rpn_loc: 0.075  time: 0.5568  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:14:45 d2.utils.events]: [0m eta: 0:01:41  iter: 1819  total_loss: 0.698  loss_cls: 0.220  loss_box_reg: 0.395  loss_rpn_cls: 0.021  loss_rpn_loc: 0.050  time: 0.5565  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:14:56 d2.utils.events]: [0m eta: 0:01:30  iter: 1839  total_loss: 0.627  loss_cls: 0.208  loss_box_reg: 0.331  loss_rpn_cls: 0.019  loss_rpn_loc: 0.045  time: 0.5567  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:15:08 d2.utils.events]: [0m eta: 0:01:19  iter: 1859  total_loss: 0.754  loss_cls: 0.214  loss_box_reg: 0.402  loss_rpn_cls: 0.029  loss_rpn_loc: 0.071  time: 0.5568  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:15:19 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.678  loss_cls: 0.195  loss_box_reg: 0.368  loss_rpn_cls: 0.019  loss_rpn_loc: 0.064  time: 0.5570  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:15:48 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.616  loss_cls: 0.208  loss_box_reg: 0.325  loss_rpn_cls: 0.029  loss_rpn_loc: 0.049  time: 0.5568  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:15:59 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.723  loss_cls: 0.246  loss_box_reg: 0.396  loss_rpn_cls: 0.023  loss_rpn_loc: 0.060  time: 0.5569  data_time: 0.0329  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:16:10 d2.utils.events]: [0m eta: 0:00:34  iter: 1939  total_loss: 0.671  loss_cls: 0.215  loss_box_reg: 0.345  loss_rpn_cls: 0.026  loss_rpn_loc: 0.057  time: 0.5566  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:16:21 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.713  loss_cls: 0.206  loss_box_reg: 0.384  loss_rpn_cls: 0.027  loss_rpn_loc: 0.076  time: 0.5563  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:16:43 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.602  loss_cls: 0.213  loss_box_reg: 0.326  loss_rpn_cls: 0.021  loss_rpn_loc: 0.071  time: 0.5560  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 10:17:29 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 10:17:29 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 10:17:29 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 10:17:29 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.679  loss_cls: 0.215  loss_box_reg: 0.364  loss_rpn_cls: 0.025  loss_rpn_loc: 0.061  time: 0.5559  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:17:29 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:30 (0.5562 s / it)
[32m[04/15 10:17:29 d2.engine.hooks]: [0mTotal training time: 0:19:46 (0:01:15 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 10:17:48 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 10:17:48 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 10:17:48 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 10:17:50 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1216 s / img. ETA=0:02:35
[32m[04/15 10:17:55 d2.evaluation.evaluator]: [0mInference done 51/1257. 0.1229 s / img. ETA=0:02:32
[32m[04/15 10:18:00 d2.evaluation.evaluator]: [0mInference done 91/1257. 0.1232 s / img. ETA=0:02:27
[32m[04/15 10:18:05 d2.evaluation.evaluator]: [0mInference done 131/1257. 0.1232 s / img. ETA=0:02:22
[32m[04/15 10:18:10 d2.evaluation.evaluator]: [0mInference done 171/1257. 0.1235 s / img. ETA=0:02:17
[32m[04/15 10:18:15 d2.evaluation.evaluator]: [0mInference done 210/1257. 0.1239 s / img. ETA=0:02:13
[32m[04/15 10:18:20 d2.evaluation.evaluator]: [0mInference done 249/1257. 0.1242 s / img. ETA=0:02:08
[32m[04/15 10:18:25 d2.evaluation.evaluator]: [0mInference done 289/1257. 0.1240 s / img. ETA=0:02:03
[32m[04/15 10:18:30 d2.evaluation.evaluator]: [0mInference done 328/1257. 0.1240 s / img. ETA=0:01:58
[32m[04/15 10:18:36 d2.evaluation.evaluator]: [0mInference done 368/1257. 0.1240 s / img. ETA=0:01:53
[32m[04/15 10:18:41 d2.evaluation.evaluator]: [0mInference done 408/1257. 0.1238 s / img. ETA=0:01:48
[32m[04/15 10:18:46 d2.evaluation.evaluator]: [0mInference done 447/1257. 0.1239 s / img. ETA=0:01:43
[32m[04/15 10:18:51 d2.evaluation.evaluator]: [0mInference done 487/1257. 0.1239 s / img. ETA=0:01:38
[32m[04/15 10:18:56 d2.evaluation.evaluator]: [0mInference done 526/1257. 0.1241 s / img. ETA=0:01:33
[32m[04/15 10:19:01 d2.evaluation.evaluator]: [0mInference done 565/1257. 0.1242 s / img. ETA=0:01:28
[32m[04/15 10:19:06 d2.evaluation.evaluator]: [0mInference done 604/1257. 0.1242 s / img. ETA=0:01:23
[32m[04/15 10:19:11 d2.evaluation.evaluator]: [0mInference done 643/1257. 0.1242 s / img. ETA=0:01:18
[32m[04/15 10:19:16 d2.evaluation.evaluator]: [0mInference done 683/1257. 0.1242 s / img. ETA=0:01:13
[32m[04/15 10:19:21 d2.evaluation.evaluator]: [0mInference done 723/1257. 0.1241 s / img. ETA=0:01:08
[32m[04/15 10:19:26 d2.evaluation.evaluator]: [0mInference done 764/1257. 0.1239 s / img. ETA=0:01:02
[32m[04/15 10:19:31 d2.evaluation.evaluator]: [0mInference done 804/1257. 0.1239 s / img. ETA=0:00:57
[32m[04/15 10:19:36 d2.evaluation.evaluator]: [0mInference done 843/1257. 0.1240 s / img. ETA=0:00:52
[32m[04/15 10:19:42 d2.evaluation.evaluator]: [0mInference done 879/1257. 0.1240 s / img. ETA=0:00:48
[32m[04/15 10:19:48 d2.evaluation.evaluator]: [0mInference done 883/1257. 0.1239 s / img. ETA=0:00:50
[32m[04/15 10:19:53 d2.evaluation.evaluator]: [0mInference done 919/1257. 0.1241 s / img. ETA=0:00:45
[32m[04/15 10:19:58 d2.evaluation.evaluator]: [0mInference done 955/1257. 0.1243 s / img. ETA=0:00:41
[32m[04/15 10:20:03 d2.evaluation.evaluator]: [0mInference done 993/1257. 0.1243 s / img. ETA=0:00:35
[32m[04/15 10:20:08 d2.evaluation.evaluator]: [0mInference done 1031/1257. 0.1243 s / img. ETA=0:00:30
[32m[04/15 10:20:13 d2.evaluation.evaluator]: [0mInference done 1069/1257. 0.1242 s / img. ETA=0:00:25
[32m[04/15 10:20:18 d2.evaluation.evaluator]: [0mInference done 1106/1257. 0.1242 s / img. ETA=0:00:20
[32m[04/15 10:20:23 d2.evaluation.evaluator]: [0mInference done 1140/1257. 0.1241 s / img. ETA=0:00:15
[32m[04/15 10:20:28 d2.evaluation.evaluator]: [0mInference done 1175/1257. 0.1241 s / img. ETA=0:00:11
[32m[04/15 10:20:34 d2.evaluation.evaluator]: [0mInference done 1213/1257. 0.1240 s / img. ETA=0:00:05
[32m[04/15 10:20:39 d2.evaluation.evaluator]: [0mInference done 1253/1257. 0.1240 s / img. ETA=0:00:00
[32m[04/15 10:20:39 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:50.168235 (0.135917 s / img per device, on 1 devices)
[32m[04/15 10:20:39 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:35 (0.123993 s / img per device, on 1 devices)
[32m[04/15 10:20:39 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 10:20:39 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 10:20:39 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=6.94s).
Accumulating evaluation results...
DONE (t=0.85s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.183
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.370
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.157
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.116
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.093
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.232
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.233
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.167
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.277
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.428
[32m[04/15 10:20:47 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 18.315 | 37.023 | 15.702 | 11.568 | 22.593 | 39.269 |
[32m[04/15 10:20:47 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 26.432 | bicycle       | 7.145 | car            | 39.684 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  13  *  2000  iterations ============
4 channel input
[32m[04/15 10:20:49 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 10:20:50 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.03 seconds.
[5m[31mWARNING[0m [32m[04/15 10:20:50 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 10:20:50 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 10:20:51 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 10:20:51 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 10:20:51 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 10:20:56 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 10:21:07 d2.utils.events]: [0m eta: 0:17:35  iter: 19  total_loss: 0.604  loss_cls: 0.209  loss_box_reg: 0.314  loss_rpn_cls: 0.021  loss_rpn_loc: 0.043  time: 0.5376  data_time: 0.0469  lr: 0.000200  max_mem: 3070M
[32m[04/15 10:21:18 d2.utils.events]: [0m eta: 0:17:31  iter: 39  total_loss: 0.612  loss_cls: 0.194  loss_box_reg: 0.317  loss_rpn_cls: 0.018  loss_rpn_loc: 0.063  time: 0.5392  data_time: 0.0077  lr: 0.000400  max_mem: 3070M
[32m[04/15 10:21:29 d2.utils.events]: [0m eta: 0:17:23  iter: 59  total_loss: 0.642  loss_cls: 0.198  loss_box_reg: 0.341  loss_rpn_cls: 0.017  loss_rpn_loc: 0.063  time: 0.5405  data_time: 0.0084  lr: 0.000599  max_mem: 3070M
[32m[04/15 10:21:40 d2.utils.events]: [0m eta: 0:17:10  iter: 79  total_loss: 0.616  loss_cls: 0.182  loss_box_reg: 0.335  loss_rpn_cls: 0.019  loss_rpn_loc: 0.050  time: 0.5386  data_time: 0.0074  lr: 0.000799  max_mem: 3070M
[32m[04/15 10:21:51 d2.utils.events]: [0m eta: 0:16:58  iter: 99  total_loss: 0.766  loss_cls: 0.222  loss_box_reg: 0.405  loss_rpn_cls: 0.023  loss_rpn_loc: 0.070  time: 0.5377  data_time: 0.0073  lr: 0.000999  max_mem: 3070M
[32m[04/15 10:22:02 d2.utils.events]: [0m eta: 0:16:53  iter: 119  total_loss: 0.617  loss_cls: 0.206  loss_box_reg: 0.353  loss_rpn_cls: 0.018  loss_rpn_loc: 0.054  time: 0.5432  data_time: 0.0073  lr: 0.001199  max_mem: 3070M
[32m[04/15 10:22:13 d2.utils.events]: [0m eta: 0:16:41  iter: 139  total_loss: 0.694  loss_cls: 0.234  loss_box_reg: 0.381  loss_rpn_cls: 0.015  loss_rpn_loc: 0.057  time: 0.5432  data_time: 0.0074  lr: 0.001399  max_mem: 3070M
[32m[04/15 10:22:25 d2.utils.events]: [0m eta: 0:16:36  iter: 159  total_loss: 0.637  loss_cls: 0.213  loss_box_reg: 0.358  loss_rpn_cls: 0.018  loss_rpn_loc: 0.047  time: 0.5459  data_time: 0.0077  lr: 0.001598  max_mem: 3070M
[32m[04/15 10:22:35 d2.utils.events]: [0m eta: 0:16:18  iter: 179  total_loss: 0.732  loss_cls: 0.244  loss_box_reg: 0.361  loss_rpn_cls: 0.020  loss_rpn_loc: 0.059  time: 0.5421  data_time: 0.0067  lr: 0.001798  max_mem: 3070M
[32m[04/15 10:22:46 d2.utils.events]: [0m eta: 0:16:10  iter: 199  total_loss: 0.723  loss_cls: 0.231  loss_box_reg: 0.399  loss_rpn_cls: 0.024  loss_rpn_loc: 0.071  time: 0.5429  data_time: 0.0065  lr: 0.001998  max_mem: 3070M
[32m[04/15 10:22:57 d2.utils.events]: [0m eta: 0:16:02  iter: 219  total_loss: 0.518  loss_cls: 0.148  loss_box_reg: 0.224  loss_rpn_cls: 0.020  loss_rpn_loc: 0.043  time: 0.5433  data_time: 0.0066  lr: 0.002198  max_mem: 3070M
[32m[04/15 10:23:09 d2.utils.events]: [0m eta: 0:15:59  iter: 239  total_loss: 0.551  loss_cls: 0.188  loss_box_reg: 0.300  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5454  data_time: 0.0078  lr: 0.002398  max_mem: 3070M
[32m[04/15 10:23:20 d2.utils.events]: [0m eta: 0:15:50  iter: 259  total_loss: 0.744  loss_cls: 0.229  loss_box_reg: 0.366  loss_rpn_cls: 0.019  loss_rpn_loc: 0.076  time: 0.5465  data_time: 0.0062  lr: 0.002597  max_mem: 3070M
[32m[04/15 10:23:31 d2.utils.events]: [0m eta: 0:15:42  iter: 279  total_loss: 0.683  loss_cls: 0.230  loss_box_reg: 0.380  loss_rpn_cls: 0.026  loss_rpn_loc: 0.051  time: 0.5469  data_time: 0.0075  lr: 0.002797  max_mem: 3070M
[32m[04/15 10:23:42 d2.utils.events]: [0m eta: 0:15:32  iter: 299  total_loss: 0.667  loss_cls: 0.215  loss_box_reg: 0.350  loss_rpn_cls: 0.020  loss_rpn_loc: 0.047  time: 0.5479  data_time: 0.0075  lr: 0.002997  max_mem: 3070M
[32m[04/15 10:23:53 d2.utils.events]: [0m eta: 0:15:21  iter: 319  total_loss: 0.574  loss_cls: 0.186  loss_box_reg: 0.310  loss_rpn_cls: 0.017  loss_rpn_loc: 0.052  time: 0.5474  data_time: 0.0069  lr: 0.003197  max_mem: 3070M
[32m[04/15 10:24:04 d2.utils.events]: [0m eta: 0:15:10  iter: 339  total_loss: 0.686  loss_cls: 0.211  loss_box_reg: 0.397  loss_rpn_cls: 0.015  loss_rpn_loc: 0.067  time: 0.5476  data_time: 0.0076  lr: 0.003397  max_mem: 3070M
[32m[04/15 10:24:16 d2.utils.events]: [0m eta: 0:15:04  iter: 359  total_loss: 0.640  loss_cls: 0.217  loss_box_reg: 0.338  loss_rpn_cls: 0.020  loss_rpn_loc: 0.056  time: 0.5492  data_time: 0.0060  lr: 0.003596  max_mem: 3070M
[32m[04/15 10:24:27 d2.utils.events]: [0m eta: 0:14:53  iter: 379  total_loss: 0.693  loss_cls: 0.207  loss_box_reg: 0.375  loss_rpn_cls: 0.022  loss_rpn_loc: 0.072  time: 0.5495  data_time: 0.0069  lr: 0.003796  max_mem: 3070M
[32m[04/15 10:24:38 d2.utils.events]: [0m eta: 0:14:42  iter: 399  total_loss: 0.611  loss_cls: 0.225  loss_box_reg: 0.320  loss_rpn_cls: 0.019  loss_rpn_loc: 0.050  time: 0.5490  data_time: 0.0073  lr: 0.003996  max_mem: 3070M
[32m[04/15 10:24:49 d2.utils.events]: [0m eta: 0:14:32  iter: 419  total_loss: 0.655  loss_cls: 0.191  loss_box_reg: 0.339  loss_rpn_cls: 0.014  loss_rpn_loc: 0.046  time: 0.5497  data_time: 0.0084  lr: 0.004196  max_mem: 3070M
[32m[04/15 10:25:01 d2.utils.events]: [0m eta: 0:14:21  iter: 439  total_loss: 0.549  loss_cls: 0.183  loss_box_reg: 0.300  loss_rpn_cls: 0.013  loss_rpn_loc: 0.055  time: 0.5500  data_time: 0.0077  lr: 0.004396  max_mem: 3070M
[32m[04/15 10:25:12 d2.utils.events]: [0m eta: 0:14:10  iter: 459  total_loss: 0.592  loss_cls: 0.186  loss_box_reg: 0.313  loss_rpn_cls: 0.025  loss_rpn_loc: 0.064  time: 0.5509  data_time: 0.0075  lr: 0.004595  max_mem: 3070M
[32m[04/15 10:25:23 d2.utils.events]: [0m eta: 0:14:00  iter: 479  total_loss: 0.667  loss_cls: 0.192  loss_box_reg: 0.351  loss_rpn_cls: 0.014  loss_rpn_loc: 0.050  time: 0.5506  data_time: 0.0069  lr: 0.004795  max_mem: 3070M
[32m[04/15 10:25:34 d2.utils.events]: [0m eta: 0:13:49  iter: 499  total_loss: 0.649  loss_cls: 0.222  loss_box_reg: 0.350  loss_rpn_cls: 0.018  loss_rpn_loc: 0.061  time: 0.5505  data_time: 0.0081  lr: 0.004995  max_mem: 3070M
[32m[04/15 10:25:45 d2.utils.events]: [0m eta: 0:13:38  iter: 519  total_loss: 0.535  loss_cls: 0.179  loss_box_reg: 0.305  loss_rpn_cls: 0.016  loss_rpn_loc: 0.050  time: 0.5505  data_time: 0.0073  lr: 0.005195  max_mem: 3070M
[32m[04/15 10:25:56 d2.utils.events]: [0m eta: 0:13:26  iter: 539  total_loss: 0.705  loss_cls: 0.234  loss_box_reg: 0.389  loss_rpn_cls: 0.017  loss_rpn_loc: 0.062  time: 0.5503  data_time: 0.0070  lr: 0.005395  max_mem: 3070M
[32m[04/15 10:26:07 d2.utils.events]: [0m eta: 0:13:15  iter: 559  total_loss: 0.731  loss_cls: 0.230  loss_box_reg: 0.405  loss_rpn_cls: 0.020  loss_rpn_loc: 0.068  time: 0.5501  data_time: 0.0062  lr: 0.005594  max_mem: 3070M
[32m[04/15 10:26:18 d2.utils.events]: [0m eta: 0:13:04  iter: 579  total_loss: 0.615  loss_cls: 0.206  loss_box_reg: 0.348  loss_rpn_cls: 0.012  loss_rpn_loc: 0.051  time: 0.5500  data_time: 0.0075  lr: 0.005794  max_mem: 3070M
[32m[04/15 10:26:29 d2.utils.events]: [0m eta: 0:12:52  iter: 599  total_loss: 0.742  loss_cls: 0.235  loss_box_reg: 0.404  loss_rpn_cls: 0.020  loss_rpn_loc: 0.071  time: 0.5499  data_time: 0.0071  lr: 0.005994  max_mem: 3070M
[32m[04/15 10:26:40 d2.utils.events]: [0m eta: 0:12:41  iter: 619  total_loss: 0.613  loss_cls: 0.209  loss_box_reg: 0.340  loss_rpn_cls: 0.022  loss_rpn_loc: 0.064  time: 0.5498  data_time: 0.0073  lr: 0.006194  max_mem: 3070M
[32m[04/15 10:26:51 d2.utils.events]: [0m eta: 0:12:30  iter: 639  total_loss: 0.661  loss_cls: 0.220  loss_box_reg: 0.361  loss_rpn_cls: 0.022  loss_rpn_loc: 0.052  time: 0.5494  data_time: 0.0077  lr: 0.006394  max_mem: 3070M
[32m[04/15 10:27:02 d2.utils.events]: [0m eta: 0:12:20  iter: 659  total_loss: 0.603  loss_cls: 0.186  loss_box_reg: 0.331  loss_rpn_cls: 0.021  loss_rpn_loc: 0.048  time: 0.5495  data_time: 0.0069  lr: 0.006593  max_mem: 3070M
[32m[04/15 10:27:14 d2.utils.events]: [0m eta: 0:12:08  iter: 679  total_loss: 0.774  loss_cls: 0.234  loss_box_reg: 0.412  loss_rpn_cls: 0.019  loss_rpn_loc: 0.095  time: 0.5495  data_time: 0.0092  lr: 0.006793  max_mem: 3070M
[32m[04/15 10:27:25 d2.utils.events]: [0m eta: 0:11:58  iter: 699  total_loss: 0.609  loss_cls: 0.177  loss_box_reg: 0.317  loss_rpn_cls: 0.024  loss_rpn_loc: 0.056  time: 0.5499  data_time: 0.0084  lr: 0.006993  max_mem: 3070M
[32m[04/15 10:27:36 d2.utils.events]: [0m eta: 0:11:47  iter: 719  total_loss: 0.683  loss_cls: 0.215  loss_box_reg: 0.326  loss_rpn_cls: 0.023  loss_rpn_loc: 0.071  time: 0.5502  data_time: 0.0074  lr: 0.007193  max_mem: 3070M
[32m[04/15 10:27:48 d2.utils.events]: [0m eta: 0:11:36  iter: 739  total_loss: 0.546  loss_cls: 0.181  loss_box_reg: 0.303  loss_rpn_cls: 0.016  loss_rpn_loc: 0.048  time: 0.5507  data_time: 0.0080  lr: 0.007393  max_mem: 3070M
[32m[04/15 10:27:59 d2.utils.events]: [0m eta: 0:11:26  iter: 759  total_loss: 0.687  loss_cls: 0.191  loss_box_reg: 0.343  loss_rpn_cls: 0.024  loss_rpn_loc: 0.047  time: 0.5509  data_time: 0.0079  lr: 0.007592  max_mem: 3070M
[32m[04/15 10:28:11 d2.utils.events]: [0m eta: 0:11:16  iter: 779  total_loss: 0.552  loss_cls: 0.188  loss_box_reg: 0.307  loss_rpn_cls: 0.014  loss_rpn_loc: 0.047  time: 0.5514  data_time: 0.0086  lr: 0.007792  max_mem: 3070M
[32m[04/15 10:28:22 d2.utils.events]: [0m eta: 0:11:05  iter: 799  total_loss: 0.626  loss_cls: 0.213  loss_box_reg: 0.341  loss_rpn_cls: 0.017  loss_rpn_loc: 0.057  time: 0.5517  data_time: 0.0071  lr: 0.007992  max_mem: 3070M
[32m[04/15 10:28:33 d2.utils.events]: [0m eta: 0:10:54  iter: 819  total_loss: 0.670  loss_cls: 0.212  loss_box_reg: 0.366  loss_rpn_cls: 0.020  loss_rpn_loc: 0.096  time: 0.5515  data_time: 0.0067  lr: 0.008192  max_mem: 3070M
[32m[04/15 10:28:44 d2.utils.events]: [0m eta: 0:10:42  iter: 839  total_loss: 0.691  loss_cls: 0.227  loss_box_reg: 0.352  loss_rpn_cls: 0.029  loss_rpn_loc: 0.054  time: 0.5511  data_time: 0.0077  lr: 0.008392  max_mem: 3070M
[32m[04/15 10:28:55 d2.utils.events]: [0m eta: 0:10:31  iter: 859  total_loss: 0.508  loss_cls: 0.192  loss_box_reg: 0.284  loss_rpn_cls: 0.016  loss_rpn_loc: 0.040  time: 0.5514  data_time: 0.0077  lr: 0.008591  max_mem: 3070M
[32m[04/15 10:29:06 d2.utils.events]: [0m eta: 0:10:20  iter: 879  total_loss: 0.743  loss_cls: 0.243  loss_box_reg: 0.409  loss_rpn_cls: 0.020  loss_rpn_loc: 0.064  time: 0.5514  data_time: 0.0067  lr: 0.008791  max_mem: 3070M
[32m[04/15 10:29:18 d2.utils.events]: [0m eta: 0:10:09  iter: 899  total_loss: 0.593  loss_cls: 0.205  loss_box_reg: 0.355  loss_rpn_cls: 0.020  loss_rpn_loc: 0.045  time: 0.5518  data_time: 0.0105  lr: 0.008991  max_mem: 3070M
[32m[04/15 10:29:29 d2.utils.events]: [0m eta: 0:09:59  iter: 919  total_loss: 0.773  loss_cls: 0.275  loss_box_reg: 0.420  loss_rpn_cls: 0.028  loss_rpn_loc: 0.060  time: 0.5520  data_time: 0.0069  lr: 0.009191  max_mem: 3070M
[32m[04/15 10:29:40 d2.utils.events]: [0m eta: 0:09:48  iter: 939  total_loss: 0.555  loss_cls: 0.169  loss_box_reg: 0.294  loss_rpn_cls: 0.018  loss_rpn_loc: 0.048  time: 0.5519  data_time: 0.0074  lr: 0.009391  max_mem: 3070M
[32m[04/15 10:29:51 d2.utils.events]: [0m eta: 0:09:36  iter: 959  total_loss: 0.739  loss_cls: 0.247  loss_box_reg: 0.402  loss_rpn_cls: 0.023  loss_rpn_loc: 0.054  time: 0.5516  data_time: 0.0083  lr: 0.009590  max_mem: 3070M
[32m[04/15 10:30:02 d2.utils.events]: [0m eta: 0:09:25  iter: 979  total_loss: 0.721  loss_cls: 0.250  loss_box_reg: 0.334  loss_rpn_cls: 0.026  loss_rpn_loc: 0.059  time: 0.5516  data_time: 0.0068  lr: 0.009790  max_mem: 3070M
[32m[04/15 10:30:13 d2.utils.events]: [0m eta: 0:09:14  iter: 999  total_loss: 0.824  loss_cls: 0.265  loss_box_reg: 0.449  loss_rpn_cls: 0.022  loss_rpn_loc: 0.074  time: 0.5516  data_time: 0.0068  lr: 0.009990  max_mem: 3070M
[32m[04/15 10:30:24 d2.utils.events]: [0m eta: 0:09:03  iter: 1019  total_loss: 0.722  loss_cls: 0.236  loss_box_reg: 0.404  loss_rpn_cls: 0.019  loss_rpn_loc: 0.065  time: 0.5518  data_time: 0.0115  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:30:36 d2.utils.events]: [0m eta: 0:08:52  iter: 1039  total_loss: 0.724  loss_cls: 0.218  loss_box_reg: 0.359  loss_rpn_cls: 0.021  loss_rpn_loc: 0.061  time: 0.5519  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:30:47 d2.utils.events]: [0m eta: 0:08:42  iter: 1059  total_loss: 0.729  loss_cls: 0.219  loss_box_reg: 0.418  loss_rpn_cls: 0.021  loss_rpn_loc: 0.075  time: 0.5521  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:30:58 d2.utils.events]: [0m eta: 0:08:31  iter: 1079  total_loss: 0.626  loss_cls: 0.204  loss_box_reg: 0.316  loss_rpn_cls: 0.024  loss_rpn_loc: 0.068  time: 0.5521  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:31:10 d2.utils.events]: [0m eta: 0:08:21  iter: 1099  total_loss: 0.666  loss_cls: 0.219  loss_box_reg: 0.348  loss_rpn_cls: 0.017  loss_rpn_loc: 0.065  time: 0.5525  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:31:20 d2.utils.events]: [0m eta: 0:08:09  iter: 1119  total_loss: 0.701  loss_cls: 0.229  loss_box_reg: 0.365  loss_rpn_cls: 0.022  loss_rpn_loc: 0.066  time: 0.5523  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:31:32 d2.utils.events]: [0m eta: 0:07:58  iter: 1139  total_loss: 0.718  loss_cls: 0.206  loss_box_reg: 0.407  loss_rpn_cls: 0.026  loss_rpn_loc: 0.076  time: 0.5525  data_time: 0.0061  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:31:43 d2.utils.events]: [0m eta: 0:07:47  iter: 1159  total_loss: 0.651  loss_cls: 0.201  loss_box_reg: 0.338  loss_rpn_cls: 0.018  loss_rpn_loc: 0.065  time: 0.5526  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:31:54 d2.utils.events]: [0m eta: 0:07:37  iter: 1179  total_loss: 0.529  loss_cls: 0.173  loss_box_reg: 0.301  loss_rpn_cls: 0.018  loss_rpn_loc: 0.048  time: 0.5528  data_time: 0.0109  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:32:06 d2.utils.events]: [0m eta: 0:07:26  iter: 1199  total_loss: 0.801  loss_cls: 0.268  loss_box_reg: 0.375  loss_rpn_cls: 0.035  loss_rpn_loc: 0.070  time: 0.5528  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:32:17 d2.utils.events]: [0m eta: 0:07:15  iter: 1219  total_loss: 0.727  loss_cls: 0.210  loss_box_reg: 0.387  loss_rpn_cls: 0.019  loss_rpn_loc: 0.087  time: 0.5532  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:32:28 d2.utils.events]: [0m eta: 0:07:03  iter: 1239  total_loss: 0.602  loss_cls: 0.190  loss_box_reg: 0.336  loss_rpn_cls: 0.019  loss_rpn_loc: 0.049  time: 0.5532  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:32:40 d2.utils.events]: [0m eta: 0:06:52  iter: 1259  total_loss: 0.770  loss_cls: 0.238  loss_box_reg: 0.409  loss_rpn_cls: 0.026  loss_rpn_loc: 0.084  time: 0.5534  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:32:51 d2.utils.events]: [0m eta: 0:06:41  iter: 1279  total_loss: 0.633  loss_cls: 0.218  loss_box_reg: 0.320  loss_rpn_cls: 0.023  loss_rpn_loc: 0.059  time: 0.5535  data_time: 0.0096  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:33:03 d2.utils.events]: [0m eta: 0:06:30  iter: 1299  total_loss: 0.723  loss_cls: 0.231  loss_box_reg: 0.351  loss_rpn_cls: 0.026  loss_rpn_loc: 0.062  time: 0.5538  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:33:14 d2.utils.events]: [0m eta: 0:06:19  iter: 1319  total_loss: 0.665  loss_cls: 0.206  loss_box_reg: 0.309  loss_rpn_cls: 0.022  loss_rpn_loc: 0.066  time: 0.5536  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:33:25 d2.utils.events]: [0m eta: 0:06:08  iter: 1339  total_loss: 0.660  loss_cls: 0.212  loss_box_reg: 0.308  loss_rpn_cls: 0.025  loss_rpn_loc: 0.055  time: 0.5538  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:33:36 d2.utils.events]: [0m eta: 0:05:57  iter: 1359  total_loss: 0.740  loss_cls: 0.236  loss_box_reg: 0.319  loss_rpn_cls: 0.034  loss_rpn_loc: 0.093  time: 0.5541  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:33:48 d2.utils.events]: [0m eta: 0:05:46  iter: 1379  total_loss: 0.559  loss_cls: 0.199  loss_box_reg: 0.285  loss_rpn_cls: 0.019  loss_rpn_loc: 0.066  time: 0.5542  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:33:59 d2.utils.events]: [0m eta: 0:05:35  iter: 1399  total_loss: 0.724  loss_cls: 0.231  loss_box_reg: 0.380  loss_rpn_cls: 0.024  loss_rpn_loc: 0.076  time: 0.5541  data_time: 0.0041  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:34:10 d2.utils.events]: [0m eta: 0:05:23  iter: 1419  total_loss: 0.636  loss_cls: 0.238  loss_box_reg: 0.316  loss_rpn_cls: 0.024  loss_rpn_loc: 0.042  time: 0.5539  data_time: 0.0041  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:34:21 d2.utils.events]: [0m eta: 0:05:13  iter: 1439  total_loss: 0.634  loss_cls: 0.202  loss_box_reg: 0.365  loss_rpn_cls: 0.027  loss_rpn_loc: 0.056  time: 0.5540  data_time: 0.0041  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:34:32 d2.utils.events]: [0m eta: 0:05:01  iter: 1459  total_loss: 0.648  loss_cls: 0.203  loss_box_reg: 0.313  loss_rpn_cls: 0.027  loss_rpn_loc: 0.057  time: 0.5539  data_time: 0.0040  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:34:43 d2.utils.events]: [0m eta: 0:04:50  iter: 1479  total_loss: 0.719  loss_cls: 0.230  loss_box_reg: 0.387  loss_rpn_cls: 0.026  loss_rpn_loc: 0.073  time: 0.5539  data_time: 0.0042  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:34:54 d2.utils.events]: [0m eta: 0:04:39  iter: 1499  total_loss: 0.674  loss_cls: 0.245  loss_box_reg: 0.331  loss_rpn_cls: 0.031  loss_rpn_loc: 0.061  time: 0.5536  data_time: 0.0039  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:35:05 d2.utils.events]: [0m eta: 0:04:28  iter: 1519  total_loss: 0.621  loss_cls: 0.213  loss_box_reg: 0.343  loss_rpn_cls: 0.021  loss_rpn_loc: 0.063  time: 0.5535  data_time: 0.0042  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:35:16 d2.utils.events]: [0m eta: 0:04:17  iter: 1539  total_loss: 0.670  loss_cls: 0.199  loss_box_reg: 0.353  loss_rpn_cls: 0.024  loss_rpn_loc: 0.053  time: 0.5534  data_time: 0.0042  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:35:26 d2.utils.events]: [0m eta: 0:04:05  iter: 1559  total_loss: 0.685  loss_cls: 0.220  loss_box_reg: 0.376  loss_rpn_cls: 0.014  loss_rpn_loc: 0.076  time: 0.5532  data_time: 0.0043  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:35:37 d2.utils.events]: [0m eta: 0:03:54  iter: 1579  total_loss: 0.600  loss_cls: 0.216  loss_box_reg: 0.297  loss_rpn_cls: 0.027  loss_rpn_loc: 0.055  time: 0.5530  data_time: 0.0041  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:35:48 d2.utils.events]: [0m eta: 0:03:43  iter: 1599  total_loss: 0.747  loss_cls: 0.224  loss_box_reg: 0.382  loss_rpn_cls: 0.021  loss_rpn_loc: 0.082  time: 0.5528  data_time: 0.0040  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:35:59 d2.utils.events]: [0m eta: 0:03:32  iter: 1619  total_loss: 0.593  loss_cls: 0.189  loss_box_reg: 0.321  loss_rpn_cls: 0.026  loss_rpn_loc: 0.073  time: 0.5525  data_time: 0.0041  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:36:09 d2.utils.events]: [0m eta: 0:03:20  iter: 1639  total_loss: 0.721  loss_cls: 0.228  loss_box_reg: 0.392  loss_rpn_cls: 0.025  loss_rpn_loc: 0.082  time: 0.5522  data_time: 0.0041  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:36:20 d2.utils.events]: [0m eta: 0:03:09  iter: 1659  total_loss: 0.642  loss_cls: 0.210  loss_box_reg: 0.334  loss_rpn_cls: 0.029  loss_rpn_loc: 0.067  time: 0.5520  data_time: 0.0041  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:36:31 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.595  loss_cls: 0.209  loss_box_reg: 0.338  loss_rpn_cls: 0.019  loss_rpn_loc: 0.068  time: 0.5519  data_time: 0.0041  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:36:42 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.731  loss_cls: 0.257  loss_box_reg: 0.387  loss_rpn_cls: 0.027  loss_rpn_loc: 0.061  time: 0.5517  data_time: 0.0040  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:36:53 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.780  loss_cls: 0.218  loss_box_reg: 0.408  loss_rpn_cls: 0.018  loss_rpn_loc: 0.077  time: 0.5518  data_time: 0.0040  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:37:04 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.770  loss_cls: 0.245  loss_box_reg: 0.389  loss_rpn_cls: 0.024  loss_rpn_loc: 0.078  time: 0.5516  data_time: 0.0037  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:37:14 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.657  loss_cls: 0.227  loss_box_reg: 0.322  loss_rpn_cls: 0.022  loss_rpn_loc: 0.071  time: 0.5515  data_time: 0.0040  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:37:25 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.653  loss_cls: 0.196  loss_box_reg: 0.350  loss_rpn_cls: 0.029  loss_rpn_loc: 0.058  time: 0.5514  data_time: 0.0040  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:37:36 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.616  loss_cls: 0.196  loss_box_reg: 0.321  loss_rpn_cls: 0.028  loss_rpn_loc: 0.060  time: 0.5512  data_time: 0.0040  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:37:47 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.736  loss_cls: 0.229  loss_box_reg: 0.349  loss_rpn_cls: 0.023  loss_rpn_loc: 0.053  time: 0.5512  data_time: 0.0042  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:37:58 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.540  loss_cls: 0.170  loss_box_reg: 0.303  loss_rpn_cls: 0.022  loss_rpn_loc: 0.037  time: 0.5511  data_time: 0.0040  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:38:08 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.735  loss_cls: 0.239  loss_box_reg: 0.392  loss_rpn_cls: 0.030  loss_rpn_loc: 0.056  time: 0.5508  data_time: 0.0038  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:38:20 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.586  loss_cls: 0.190  loss_box_reg: 0.298  loss_rpn_cls: 0.019  loss_rpn_loc: 0.065  time: 0.5505  data_time: 0.0042  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:38:31 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.651  loss_cls: 0.187  loss_box_reg: 0.376  loss_rpn_cls: 0.019  loss_rpn_loc: 0.070  time: 0.5504  data_time: 0.0043  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:38:41 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.710  loss_cls: 0.232  loss_box_reg: 0.396  loss_rpn_cls: 0.023  loss_rpn_loc: 0.061  time: 0.5502  data_time: 0.0042  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:38:52 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.619  loss_cls: 0.198  loss_box_reg: 0.307  loss_rpn_cls: 0.024  loss_rpn_loc: 0.053  time: 0.5499  data_time: 0.0045  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:39:03 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.651  loss_cls: 0.200  loss_box_reg: 0.364  loss_rpn_cls: 0.022  loss_rpn_loc: 0.066  time: 0.5498  data_time: 0.0040  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:39:13 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.819  loss_cls: 0.253  loss_box_reg: 0.446  loss_rpn_cls: 0.024  loss_rpn_loc: 0.059  time: 0.5495  data_time: 0.0039  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 10:39:30 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 10:39:30 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 10:39:30 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 10:39:30 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.862  loss_cls: 0.278  loss_box_reg: 0.471  loss_rpn_cls: 0.026  loss_rpn_loc: 0.071  time: 0.5493  data_time: 0.0042  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:39:32 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:17 (0.5496 s / it)
[32m[04/15 10:39:32 d2.engine.hooks]: [0mTotal training time: 0:18:34 (0:00:16 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 10:39:38 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 10:39:38 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 10:39:38 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 10:39:40 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1175 s / img. ETA=0:02:28
[32m[04/15 10:39:45 d2.evaluation.evaluator]: [0mInference done 54/1257. 0.1171 s / img. ETA=0:02:23
[32m[04/15 10:39:50 d2.evaluation.evaluator]: [0mInference done 97/1257. 0.1170 s / img. ETA=0:02:18
[32m[04/15 10:39:55 d2.evaluation.evaluator]: [0mInference done 140/1257. 0.1169 s / img. ETA=0:02:12
[32m[04/15 10:40:00 d2.evaluation.evaluator]: [0mInference done 183/1257. 0.1169 s / img. ETA=0:02:07
[32m[04/15 10:40:05 d2.evaluation.evaluator]: [0mInference done 225/1257. 0.1171 s / img. ETA=0:02:02
[32m[04/15 10:40:10 d2.evaluation.evaluator]: [0mInference done 267/1257. 0.1173 s / img. ETA=0:01:58
[32m[04/15 10:40:15 d2.evaluation.evaluator]: [0mInference done 305/1257. 0.1189 s / img. ETA=0:01:55
[32m[04/15 10:40:21 d2.evaluation.evaluator]: [0mInference done 344/1257. 0.1199 s / img. ETA=0:01:51
[32m[04/15 10:40:26 d2.evaluation.evaluator]: [0mInference done 382/1257. 0.1208 s / img. ETA=0:01:47
[32m[04/15 10:40:31 d2.evaluation.evaluator]: [0mInference done 421/1257. 0.1215 s / img. ETA=0:01:43
[32m[04/15 10:40:36 d2.evaluation.evaluator]: [0mInference done 460/1257. 0.1220 s / img. ETA=0:01:39
[32m[04/15 10:40:41 d2.evaluation.evaluator]: [0mInference done 498/1257. 0.1226 s / img. ETA=0:01:35
[32m[04/15 10:40:46 d2.evaluation.evaluator]: [0mInference done 535/1257. 0.1232 s / img. ETA=0:01:30
[32m[04/15 10:40:51 d2.evaluation.evaluator]: [0mInference done 574/1257. 0.1234 s / img. ETA=0:01:26
[32m[04/15 10:40:56 d2.evaluation.evaluator]: [0mInference done 613/1257. 0.1237 s / img. ETA=0:01:21
[32m[04/15 10:41:01 d2.evaluation.evaluator]: [0mInference done 648/1257. 0.1237 s / img. ETA=0:01:17
[32m[04/15 10:41:06 d2.evaluation.evaluator]: [0mInference done 686/1257. 0.1239 s / img. ETA=0:01:12
[32m[04/15 10:41:11 d2.evaluation.evaluator]: [0mInference done 725/1257. 0.1240 s / img. ETA=0:01:08
[32m[04/15 10:41:16 d2.evaluation.evaluator]: [0mInference done 764/1257. 0.1241 s / img. ETA=0:01:03
[32m[04/15 10:41:21 d2.evaluation.evaluator]: [0mInference done 803/1257. 0.1241 s / img. ETA=0:00:58
[32m[04/15 10:41:26 d2.evaluation.evaluator]: [0mInference done 841/1257. 0.1243 s / img. ETA=0:00:53
[32m[04/15 10:41:31 d2.evaluation.evaluator]: [0mInference done 880/1257. 0.1245 s / img. ETA=0:00:48
[32m[04/15 10:41:36 d2.evaluation.evaluator]: [0mInference done 919/1257. 0.1246 s / img. ETA=0:00:43
[32m[04/15 10:41:41 d2.evaluation.evaluator]: [0mInference done 958/1257. 0.1246 s / img. ETA=0:00:38
[32m[04/15 10:41:46 d2.evaluation.evaluator]: [0mInference done 996/1257. 0.1247 s / img. ETA=0:00:33
[32m[04/15 10:41:51 d2.evaluation.evaluator]: [0mInference done 1034/1257. 0.1249 s / img. ETA=0:00:28
[32m[04/15 10:41:57 d2.evaluation.evaluator]: [0mInference done 1073/1257. 0.1248 s / img. ETA=0:00:23
[32m[04/15 10:42:02 d2.evaluation.evaluator]: [0mInference done 1112/1257. 0.1250 s / img. ETA=0:00:18
[32m[04/15 10:42:07 d2.evaluation.evaluator]: [0mInference done 1151/1257. 0.1251 s / img. ETA=0:00:13
[32m[04/15 10:42:12 d2.evaluation.evaluator]: [0mInference done 1190/1257. 0.1251 s / img. ETA=0:00:08
[32m[04/15 10:42:17 d2.evaluation.evaluator]: [0mInference done 1229/1257. 0.1251 s / img. ETA=0:00:03
[32m[04/15 10:42:21 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:41.622868 (0.129092 s / img per device, on 1 devices)
[32m[04/15 10:42:21 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:36 (0.125200 s / img per device, on 1 devices)
[32m[04/15 10:42:21 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 10:42:21 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 10:42:21 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.05s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.74s).
Accumulating evaluation results...
DONE (t=0.76s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.193
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.373
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.174
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.121
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.235
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.432
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.241
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.172
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.284
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.484
[32m[04/15 10:42:28 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.270 | 37.273 | 17.442 | 12.064 | 23.454 | 43.158 |
[32m[04/15 10:42:28 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 29.993 | bicycle       | 5.682 | car            | 41.407 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  14  *  2000  iterations ============
4 channel input
[32m[04/15 10:42:29 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[5m[31mWARNING[0m [32m[04/15 10:42:30 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 10:42:30 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 10:42:31 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 10:42:31 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 10:42:31 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 10:42:33 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 10:42:44 d2.utils.events]: [0m eta: 0:18:05  iter: 19  total_loss: 0.737  loss_cls: 0.247  loss_box_reg: 0.406  loss_rpn_cls: 0.025  loss_rpn_loc: 0.058  time: 0.5504  data_time: 0.0472  lr: 0.000200  max_mem: 3070M
[32m[04/15 10:42:55 d2.utils.events]: [0m eta: 0:17:54  iter: 39  total_loss: 0.602  loss_cls: 0.199  loss_box_reg: 0.334  loss_rpn_cls: 0.019  loss_rpn_loc: 0.053  time: 0.5474  data_time: 0.0077  lr: 0.000400  max_mem: 3070M
[32m[04/15 10:43:07 d2.utils.events]: [0m eta: 0:17:42  iter: 59  total_loss: 0.694  loss_cls: 0.225  loss_box_reg: 0.387  loss_rpn_cls: 0.021  loss_rpn_loc: 0.060  time: 0.5500  data_time: 0.0078  lr: 0.000599  max_mem: 3070M
[32m[04/15 10:43:18 d2.utils.events]: [0m eta: 0:17:30  iter: 79  total_loss: 0.728  loss_cls: 0.239  loss_box_reg: 0.385  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5531  data_time: 0.0082  lr: 0.000799  max_mem: 3070M
[32m[04/15 10:43:29 d2.utils.events]: [0m eta: 0:17:20  iter: 99  total_loss: 0.703  loss_cls: 0.221  loss_box_reg: 0.388  loss_rpn_cls: 0.022  loss_rpn_loc: 0.058  time: 0.5522  data_time: 0.0088  lr: 0.000999  max_mem: 3070M
[32m[04/15 10:43:40 d2.utils.events]: [0m eta: 0:17:09  iter: 119  total_loss: 0.518  loss_cls: 0.183  loss_box_reg: 0.286  loss_rpn_cls: 0.021  loss_rpn_loc: 0.046  time: 0.5523  data_time: 0.0073  lr: 0.001199  max_mem: 3070M
[32m[04/15 10:43:51 d2.utils.events]: [0m eta: 0:17:01  iter: 139  total_loss: 0.678  loss_cls: 0.224  loss_box_reg: 0.366  loss_rpn_cls: 0.029  loss_rpn_loc: 0.065  time: 0.5528  data_time: 0.0077  lr: 0.001399  max_mem: 3070M
[32m[04/15 10:44:03 d2.utils.events]: [0m eta: 0:16:54  iter: 159  total_loss: 0.692  loss_cls: 0.210  loss_box_reg: 0.373  loss_rpn_cls: 0.022  loss_rpn_loc: 0.061  time: 0.5534  data_time: 0.0073  lr: 0.001598  max_mem: 3070M
[32m[04/15 10:44:14 d2.utils.events]: [0m eta: 0:16:46  iter: 179  total_loss: 0.734  loss_cls: 0.231  loss_box_reg: 0.400  loss_rpn_cls: 0.022  loss_rpn_loc: 0.057  time: 0.5549  data_time: 0.0071  lr: 0.001798  max_mem: 3070M
[32m[04/15 10:44:25 d2.utils.events]: [0m eta: 0:16:36  iter: 199  total_loss: 0.684  loss_cls: 0.215  loss_box_reg: 0.352  loss_rpn_cls: 0.020  loss_rpn_loc: 0.080  time: 0.5548  data_time: 0.0074  lr: 0.001998  max_mem: 3070M
[32m[04/15 10:44:36 d2.utils.events]: [0m eta: 0:16:24  iter: 219  total_loss: 0.557  loss_cls: 0.186  loss_box_reg: 0.318  loss_rpn_cls: 0.015  loss_rpn_loc: 0.045  time: 0.5530  data_time: 0.0119  lr: 0.002198  max_mem: 3070M
[32m[04/15 10:44:47 d2.utils.events]: [0m eta: 0:16:12  iter: 239  total_loss: 0.635  loss_cls: 0.200  loss_box_reg: 0.344  loss_rpn_cls: 0.016  loss_rpn_loc: 0.060  time: 0.5529  data_time: 0.0081  lr: 0.002398  max_mem: 3070M
[32m[04/15 10:44:58 d2.utils.events]: [0m eta: 0:16:00  iter: 259  total_loss: 0.645  loss_cls: 0.204  loss_box_reg: 0.388  loss_rpn_cls: 0.016  loss_rpn_loc: 0.051  time: 0.5510  data_time: 0.0066  lr: 0.002597  max_mem: 3070M
[32m[04/15 10:45:09 d2.utils.events]: [0m eta: 0:15:51  iter: 279  total_loss: 0.525  loss_cls: 0.194  loss_box_reg: 0.283  loss_rpn_cls: 0.019  loss_rpn_loc: 0.042  time: 0.5517  data_time: 0.0088  lr: 0.002797  max_mem: 3070M
[32m[04/15 10:45:21 d2.utils.events]: [0m eta: 0:15:40  iter: 299  total_loss: 0.708  loss_cls: 0.213  loss_box_reg: 0.396  loss_rpn_cls: 0.017  loss_rpn_loc: 0.073  time: 0.5520  data_time: 0.0078  lr: 0.002997  max_mem: 3070M
[32m[04/15 10:45:32 d2.utils.events]: [0m eta: 0:15:29  iter: 319  total_loss: 0.623  loss_cls: 0.209  loss_box_reg: 0.356  loss_rpn_cls: 0.014  loss_rpn_loc: 0.045  time: 0.5516  data_time: 0.0066  lr: 0.003197  max_mem: 3070M
[32m[04/15 10:45:43 d2.utils.events]: [0m eta: 0:15:18  iter: 339  total_loss: 0.687  loss_cls: 0.213  loss_box_reg: 0.402  loss_rpn_cls: 0.022  loss_rpn_loc: 0.054  time: 0.5515  data_time: 0.0069  lr: 0.003397  max_mem: 3070M
[32m[04/15 10:45:54 d2.utils.events]: [0m eta: 0:15:08  iter: 359  total_loss: 0.686  loss_cls: 0.215  loss_box_reg: 0.373  loss_rpn_cls: 0.019  loss_rpn_loc: 0.066  time: 0.5524  data_time: 0.0099  lr: 0.003596  max_mem: 3070M
[32m[04/15 10:46:06 d2.utils.events]: [0m eta: 0:14:57  iter: 379  total_loss: 0.653  loss_cls: 0.221  loss_box_reg: 0.369  loss_rpn_cls: 0.021  loss_rpn_loc: 0.058  time: 0.5528  data_time: 0.0072  lr: 0.003796  max_mem: 3070M
[32m[04/15 10:46:17 d2.utils.events]: [0m eta: 0:14:46  iter: 399  total_loss: 0.633  loss_cls: 0.197  loss_box_reg: 0.359  loss_rpn_cls: 0.013  loss_rpn_loc: 0.048  time: 0.5530  data_time: 0.0073  lr: 0.003996  max_mem: 3070M
[32m[04/15 10:46:28 d2.utils.events]: [0m eta: 0:14:38  iter: 419  total_loss: 0.564  loss_cls: 0.178  loss_box_reg: 0.336  loss_rpn_cls: 0.013  loss_rpn_loc: 0.048  time: 0.5533  data_time: 0.0083  lr: 0.004196  max_mem: 3070M
[32m[04/15 10:46:39 d2.utils.events]: [0m eta: 0:14:27  iter: 439  total_loss: 0.555  loss_cls: 0.168  loss_box_reg: 0.288  loss_rpn_cls: 0.019  loss_rpn_loc: 0.058  time: 0.5533  data_time: 0.0066  lr: 0.004396  max_mem: 3070M
[32m[04/15 10:46:51 d2.utils.events]: [0m eta: 0:14:18  iter: 459  total_loss: 0.562  loss_cls: 0.188  loss_box_reg: 0.307  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5543  data_time: 0.0073  lr: 0.004595  max_mem: 3070M
[32m[04/15 10:47:02 d2.utils.events]: [0m eta: 0:14:07  iter: 479  total_loss: 0.635  loss_cls: 0.204  loss_box_reg: 0.351  loss_rpn_cls: 0.022  loss_rpn_loc: 0.059  time: 0.5545  data_time: 0.0079  lr: 0.004795  max_mem: 3070M
[32m[04/15 10:47:13 d2.utils.events]: [0m eta: 0:13:56  iter: 499  total_loss: 0.627  loss_cls: 0.193  loss_box_reg: 0.346  loss_rpn_cls: 0.017  loss_rpn_loc: 0.052  time: 0.5547  data_time: 0.0098  lr: 0.004995  max_mem: 3070M
[32m[04/15 10:47:25 d2.utils.events]: [0m eta: 0:13:45  iter: 519  total_loss: 0.599  loss_cls: 0.205  loss_box_reg: 0.323  loss_rpn_cls: 0.018  loss_rpn_loc: 0.051  time: 0.5551  data_time: 0.0066  lr: 0.005195  max_mem: 3070M
[32m[04/15 10:47:36 d2.utils.events]: [0m eta: 0:13:34  iter: 539  total_loss: 0.705  loss_cls: 0.240  loss_box_reg: 0.372  loss_rpn_cls: 0.019  loss_rpn_loc: 0.061  time: 0.5549  data_time: 0.0068  lr: 0.005395  max_mem: 3070M
[32m[04/15 10:47:47 d2.utils.events]: [0m eta: 0:13:22  iter: 559  total_loss: 0.687  loss_cls: 0.217  loss_box_reg: 0.378  loss_rpn_cls: 0.018  loss_rpn_loc: 0.069  time: 0.5545  data_time: 0.0118  lr: 0.005594  max_mem: 3070M
[32m[04/15 10:47:58 d2.utils.events]: [0m eta: 0:13:10  iter: 579  total_loss: 0.543  loss_cls: 0.198  loss_box_reg: 0.276  loss_rpn_cls: 0.015  loss_rpn_loc: 0.053  time: 0.5541  data_time: 0.0074  lr: 0.005794  max_mem: 3070M
[32m[04/15 10:48:09 d2.utils.events]: [0m eta: 0:12:59  iter: 599  total_loss: 0.637  loss_cls: 0.194  loss_box_reg: 0.350  loss_rpn_cls: 0.011  loss_rpn_loc: 0.052  time: 0.5544  data_time: 0.0089  lr: 0.005994  max_mem: 3070M
[32m[04/15 10:48:21 d2.utils.events]: [0m eta: 0:12:49  iter: 619  total_loss: 0.589  loss_cls: 0.194  loss_box_reg: 0.327  loss_rpn_cls: 0.014  loss_rpn_loc: 0.040  time: 0.5547  data_time: 0.0075  lr: 0.006194  max_mem: 3070M
[32m[04/15 10:48:32 d2.utils.events]: [0m eta: 0:12:40  iter: 639  total_loss: 0.799  loss_cls: 0.238  loss_box_reg: 0.433  loss_rpn_cls: 0.015  loss_rpn_loc: 0.065  time: 0.5555  data_time: 0.0079  lr: 0.006394  max_mem: 3070M
[32m[04/15 10:48:43 d2.utils.events]: [0m eta: 0:12:27  iter: 659  total_loss: 0.675  loss_cls: 0.211  loss_box_reg: 0.311  loss_rpn_cls: 0.022  loss_rpn_loc: 0.061  time: 0.5553  data_time: 0.0078  lr: 0.006593  max_mem: 3070M
[32m[04/15 10:48:55 d2.utils.events]: [0m eta: 0:12:17  iter: 679  total_loss: 0.656  loss_cls: 0.204  loss_box_reg: 0.344  loss_rpn_cls: 0.020  loss_rpn_loc: 0.052  time: 0.5559  data_time: 0.0072  lr: 0.006793  max_mem: 3070M
[32m[04/15 10:49:06 d2.utils.events]: [0m eta: 0:12:06  iter: 699  total_loss: 0.636  loss_cls: 0.197  loss_box_reg: 0.341  loss_rpn_cls: 0.011  loss_rpn_loc: 0.072  time: 0.5558  data_time: 0.0083  lr: 0.006993  max_mem: 3070M
[32m[04/15 10:49:17 d2.utils.events]: [0m eta: 0:11:54  iter: 719  total_loss: 0.647  loss_cls: 0.214  loss_box_reg: 0.329  loss_rpn_cls: 0.019  loss_rpn_loc: 0.064  time: 0.5556  data_time: 0.0071  lr: 0.007193  max_mem: 3070M
[32m[04/15 10:49:29 d2.utils.events]: [0m eta: 0:11:43  iter: 739  total_loss: 0.648  loss_cls: 0.203  loss_box_reg: 0.366  loss_rpn_cls: 0.020  loss_rpn_loc: 0.070  time: 0.5558  data_time: 0.0069  lr: 0.007393  max_mem: 3070M
[32m[04/15 10:49:40 d2.utils.events]: [0m eta: 0:11:33  iter: 759  total_loss: 0.648  loss_cls: 0.202  loss_box_reg: 0.357  loss_rpn_cls: 0.020  loss_rpn_loc: 0.054  time: 0.5564  data_time: 0.0067  lr: 0.007592  max_mem: 3070M
[32m[04/15 10:49:52 d2.utils.events]: [0m eta: 0:11:22  iter: 779  total_loss: 0.636  loss_cls: 0.207  loss_box_reg: 0.331  loss_rpn_cls: 0.023  loss_rpn_loc: 0.050  time: 0.5564  data_time: 0.0078  lr: 0.007792  max_mem: 3070M
[32m[04/15 10:50:02 d2.utils.events]: [0m eta: 0:11:10  iter: 799  total_loss: 0.584  loss_cls: 0.197  loss_box_reg: 0.317  loss_rpn_cls: 0.022  loss_rpn_loc: 0.049  time: 0.5557  data_time: 0.0110  lr: 0.007992  max_mem: 3070M
[32m[04/15 10:50:13 d2.utils.events]: [0m eta: 0:10:59  iter: 819  total_loss: 0.758  loss_cls: 0.252  loss_box_reg: 0.418  loss_rpn_cls: 0.026  loss_rpn_loc: 0.075  time: 0.5559  data_time: 0.0066  lr: 0.008192  max_mem: 3070M
[32m[04/15 10:50:24 d2.utils.events]: [0m eta: 0:10:47  iter: 839  total_loss: 0.782  loss_cls: 0.263  loss_box_reg: 0.408  loss_rpn_cls: 0.025  loss_rpn_loc: 0.055  time: 0.5556  data_time: 0.0110  lr: 0.008392  max_mem: 3070M
[32m[04/15 10:50:36 d2.utils.events]: [0m eta: 0:10:36  iter: 859  total_loss: 0.595  loss_cls: 0.201  loss_box_reg: 0.308  loss_rpn_cls: 0.021  loss_rpn_loc: 0.049  time: 0.5556  data_time: 0.0122  lr: 0.008591  max_mem: 3070M
[32m[04/15 10:50:47 d2.utils.events]: [0m eta: 0:10:25  iter: 879  total_loss: 0.667  loss_cls: 0.184  loss_box_reg: 0.376  loss_rpn_cls: 0.023  loss_rpn_loc: 0.067  time: 0.5558  data_time: 0.0075  lr: 0.008791  max_mem: 3070M
[32m[04/15 10:50:58 d2.utils.events]: [0m eta: 0:10:14  iter: 899  total_loss: 0.712  loss_cls: 0.224  loss_box_reg: 0.404  loss_rpn_cls: 0.018  loss_rpn_loc: 0.070  time: 0.5560  data_time: 0.0073  lr: 0.008991  max_mem: 3070M
[32m[04/15 10:51:09 d2.utils.events]: [0m eta: 0:10:03  iter: 919  total_loss: 0.696  loss_cls: 0.213  loss_box_reg: 0.375  loss_rpn_cls: 0.022  loss_rpn_loc: 0.060  time: 0.5557  data_time: 0.0089  lr: 0.009191  max_mem: 3070M
[32m[04/15 10:51:21 d2.utils.events]: [0m eta: 0:09:52  iter: 939  total_loss: 0.597  loss_cls: 0.188  loss_box_reg: 0.318  loss_rpn_cls: 0.015  loss_rpn_loc: 0.038  time: 0.5560  data_time: 0.0078  lr: 0.009391  max_mem: 3070M
[32m[04/15 10:51:32 d2.utils.events]: [0m eta: 0:09:40  iter: 959  total_loss: 0.694  loss_cls: 0.240  loss_box_reg: 0.350  loss_rpn_cls: 0.022  loss_rpn_loc: 0.065  time: 0.5558  data_time: 0.0066  lr: 0.009590  max_mem: 3070M
[32m[04/15 10:51:43 d2.utils.events]: [0m eta: 0:09:29  iter: 979  total_loss: 0.669  loss_cls: 0.237  loss_box_reg: 0.379  loss_rpn_cls: 0.019  loss_rpn_loc: 0.056  time: 0.5558  data_time: 0.0081  lr: 0.009790  max_mem: 3070M
[32m[04/15 10:51:54 d2.utils.events]: [0m eta: 0:09:18  iter: 999  total_loss: 0.628  loss_cls: 0.202  loss_box_reg: 0.322  loss_rpn_cls: 0.015  loss_rpn_loc: 0.046  time: 0.5559  data_time: 0.0076  lr: 0.009990  max_mem: 3070M
[32m[04/15 10:52:06 d2.utils.events]: [0m eta: 0:09:08  iter: 1019  total_loss: 0.711  loss_cls: 0.241  loss_box_reg: 0.410  loss_rpn_cls: 0.018  loss_rpn_loc: 0.066  time: 0.5559  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:52:17 d2.utils.events]: [0m eta: 0:08:57  iter: 1039  total_loss: 0.692  loss_cls: 0.223  loss_box_reg: 0.376  loss_rpn_cls: 0.031  loss_rpn_loc: 0.065  time: 0.5559  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:52:28 d2.utils.events]: [0m eta: 0:08:46  iter: 1059  total_loss: 0.661  loss_cls: 0.202  loss_box_reg: 0.373  loss_rpn_cls: 0.023  loss_rpn_loc: 0.084  time: 0.5556  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:52:39 d2.utils.events]: [0m eta: 0:08:35  iter: 1079  total_loss: 0.634  loss_cls: 0.206  loss_box_reg: 0.316  loss_rpn_cls: 0.021  loss_rpn_loc: 0.054  time: 0.5559  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:52:50 d2.utils.events]: [0m eta: 0:08:24  iter: 1099  total_loss: 0.604  loss_cls: 0.179  loss_box_reg: 0.326  loss_rpn_cls: 0.027  loss_rpn_loc: 0.042  time: 0.5559  data_time: 0.0111  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:53:02 d2.utils.events]: [0m eta: 0:08:12  iter: 1119  total_loss: 0.731  loss_cls: 0.240  loss_box_reg: 0.389  loss_rpn_cls: 0.020  loss_rpn_loc: 0.069  time: 0.5559  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:53:13 d2.utils.events]: [0m eta: 0:08:01  iter: 1139  total_loss: 0.665  loss_cls: 0.235  loss_box_reg: 0.352  loss_rpn_cls: 0.022  loss_rpn_loc: 0.055  time: 0.5558  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:53:24 d2.utils.events]: [0m eta: 0:07:50  iter: 1159  total_loss: 0.669  loss_cls: 0.221  loss_box_reg: 0.335  loss_rpn_cls: 0.025  loss_rpn_loc: 0.063  time: 0.5560  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:53:35 d2.utils.events]: [0m eta: 0:07:38  iter: 1179  total_loss: 0.722  loss_cls: 0.219  loss_box_reg: 0.373  loss_rpn_cls: 0.021  loss_rpn_loc: 0.058  time: 0.5557  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:53:46 d2.utils.events]: [0m eta: 0:07:27  iter: 1199  total_loss: 0.621  loss_cls: 0.226  loss_box_reg: 0.329  loss_rpn_cls: 0.025  loss_rpn_loc: 0.049  time: 0.5557  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:53:57 d2.utils.events]: [0m eta: 0:07:16  iter: 1219  total_loss: 0.736  loss_cls: 0.211  loss_box_reg: 0.404  loss_rpn_cls: 0.026  loss_rpn_loc: 0.071  time: 0.5557  data_time: 0.0091  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:54:08 d2.utils.events]: [0m eta: 0:07:05  iter: 1239  total_loss: 0.625  loss_cls: 0.215  loss_box_reg: 0.324  loss_rpn_cls: 0.031  loss_rpn_loc: 0.059  time: 0.5556  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:54:20 d2.utils.events]: [0m eta: 0:06:54  iter: 1259  total_loss: 0.599  loss_cls: 0.191  loss_box_reg: 0.350  loss_rpn_cls: 0.026  loss_rpn_loc: 0.070  time: 0.5558  data_time: 0.0096  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:54:31 d2.utils.events]: [0m eta: 0:06:43  iter: 1279  total_loss: 0.722  loss_cls: 0.217  loss_box_reg: 0.383  loss_rpn_cls: 0.027  loss_rpn_loc: 0.064  time: 0.5560  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:54:42 d2.utils.events]: [0m eta: 0:06:31  iter: 1299  total_loss: 0.586  loss_cls: 0.183  loss_box_reg: 0.312  loss_rpn_cls: 0.027  loss_rpn_loc: 0.056  time: 0.5559  data_time: 0.0095  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:54:54 d2.utils.events]: [0m eta: 0:06:20  iter: 1319  total_loss: 0.770  loss_cls: 0.233  loss_box_reg: 0.437  loss_rpn_cls: 0.021  loss_rpn_loc: 0.070  time: 0.5561  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:55:05 d2.utils.events]: [0m eta: 0:06:09  iter: 1339  total_loss: 0.730  loss_cls: 0.234  loss_box_reg: 0.404  loss_rpn_cls: 0.022  loss_rpn_loc: 0.076  time: 0.5559  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:55:16 d2.utils.events]: [0m eta: 0:05:58  iter: 1359  total_loss: 0.635  loss_cls: 0.192  loss_box_reg: 0.337  loss_rpn_cls: 0.030  loss_rpn_loc: 0.059  time: 0.5558  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:55:26 d2.utils.events]: [0m eta: 0:05:46  iter: 1379  total_loss: 0.544  loss_cls: 0.181  loss_box_reg: 0.310  loss_rpn_cls: 0.028  loss_rpn_loc: 0.057  time: 0.5554  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:55:38 d2.utils.events]: [0m eta: 0:05:35  iter: 1399  total_loss: 0.679  loss_cls: 0.225  loss_box_reg: 0.374  loss_rpn_cls: 0.023  loss_rpn_loc: 0.060  time: 0.5553  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:55:49 d2.utils.events]: [0m eta: 0:05:23  iter: 1419  total_loss: 0.625  loss_cls: 0.215  loss_box_reg: 0.338  loss_rpn_cls: 0.021  loss_rpn_loc: 0.050  time: 0.5554  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:56:00 d2.utils.events]: [0m eta: 0:05:12  iter: 1439  total_loss: 0.624  loss_cls: 0.223  loss_box_reg: 0.326  loss_rpn_cls: 0.022  loss_rpn_loc: 0.048  time: 0.5555  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:56:11 d2.utils.events]: [0m eta: 0:05:01  iter: 1459  total_loss: 0.653  loss_cls: 0.221  loss_box_reg: 0.343  loss_rpn_cls: 0.018  loss_rpn_loc: 0.061  time: 0.5555  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:56:23 d2.utils.events]: [0m eta: 0:04:50  iter: 1479  total_loss: 0.695  loss_cls: 0.210  loss_box_reg: 0.325  loss_rpn_cls: 0.020  loss_rpn_loc: 0.071  time: 0.5559  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:56:37 d2.utils.events]: [0m eta: 0:04:39  iter: 1499  total_loss: 0.619  loss_cls: 0.213  loss_box_reg: 0.322  loss_rpn_cls: 0.018  loss_rpn_loc: 0.049  time: 0.5571  data_time: 0.1379  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:56:48 d2.utils.events]: [0m eta: 0:04:28  iter: 1519  total_loss: 0.761  loss_cls: 0.255  loss_box_reg: 0.388  loss_rpn_cls: 0.022  loss_rpn_loc: 0.071  time: 0.5572  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:57:00 d2.utils.events]: [0m eta: 0:04:17  iter: 1539  total_loss: 0.777  loss_cls: 0.248  loss_box_reg: 0.381  loss_rpn_cls: 0.023  loss_rpn_loc: 0.079  time: 0.5574  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:57:11 d2.utils.events]: [0m eta: 0:04:06  iter: 1559  total_loss: 0.647  loss_cls: 0.210  loss_box_reg: 0.352  loss_rpn_cls: 0.018  loss_rpn_loc: 0.062  time: 0.5574  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:57:22 d2.utils.events]: [0m eta: 0:03:55  iter: 1579  total_loss: 0.633  loss_cls: 0.205  loss_box_reg: 0.349  loss_rpn_cls: 0.030  loss_rpn_loc: 0.055  time: 0.5573  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:57:33 d2.utils.events]: [0m eta: 0:03:44  iter: 1599  total_loss: 0.777  loss_cls: 0.271  loss_box_reg: 0.427  loss_rpn_cls: 0.023  loss_rpn_loc: 0.072  time: 0.5572  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:57:44 d2.utils.events]: [0m eta: 0:03:33  iter: 1619  total_loss: 0.736  loss_cls: 0.265  loss_box_reg: 0.372  loss_rpn_cls: 0.027  loss_rpn_loc: 0.072  time: 0.5571  data_time: 0.0087  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:57:56 d2.utils.events]: [0m eta: 0:03:21  iter: 1639  total_loss: 0.747  loss_cls: 0.246  loss_box_reg: 0.402  loss_rpn_cls: 0.024  loss_rpn_loc: 0.063  time: 0.5573  data_time: 0.0093  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:58:07 d2.utils.events]: [0m eta: 0:03:10  iter: 1659  total_loss: 0.700  loss_cls: 0.240  loss_box_reg: 0.370  loss_rpn_cls: 0.033  loss_rpn_loc: 0.073  time: 0.5573  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:58:18 d2.utils.events]: [0m eta: 0:02:59  iter: 1679  total_loss: 0.707  loss_cls: 0.224  loss_box_reg: 0.373  loss_rpn_cls: 0.021  loss_rpn_loc: 0.065  time: 0.5573  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:58:29 d2.utils.events]: [0m eta: 0:02:48  iter: 1699  total_loss: 0.596  loss_cls: 0.194  loss_box_reg: 0.331  loss_rpn_cls: 0.023  loss_rpn_loc: 0.061  time: 0.5571  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:58:40 d2.utils.events]: [0m eta: 0:02:37  iter: 1719  total_loss: 0.688  loss_cls: 0.225  loss_box_reg: 0.344  loss_rpn_cls: 0.024  loss_rpn_loc: 0.054  time: 0.5571  data_time: 0.0092  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:58:52 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.595  loss_cls: 0.208  loss_box_reg: 0.320  loss_rpn_cls: 0.023  loss_rpn_loc: 0.054  time: 0.5572  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:59:03 d2.utils.events]: [0m eta: 0:02:14  iter: 1759  total_loss: 0.530  loss_cls: 0.168  loss_box_reg: 0.274  loss_rpn_cls: 0.021  loss_rpn_loc: 0.050  time: 0.5570  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:59:14 d2.utils.events]: [0m eta: 0:02:03  iter: 1779  total_loss: 0.701  loss_cls: 0.214  loss_box_reg: 0.391  loss_rpn_cls: 0.027  loss_rpn_loc: 0.056  time: 0.5570  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:59:26 d2.utils.events]: [0m eta: 0:01:52  iter: 1799  total_loss: 0.664  loss_cls: 0.225  loss_box_reg: 0.348  loss_rpn_cls: 0.026  loss_rpn_loc: 0.057  time: 0.5570  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:59:37 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.663  loss_cls: 0.222  loss_box_reg: 0.345  loss_rpn_cls: 0.024  loss_rpn_loc: 0.050  time: 0.5570  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:59:48 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.632  loss_cls: 0.196  loss_box_reg: 0.324  loss_rpn_cls: 0.023  loss_rpn_loc: 0.052  time: 0.5569  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 10:59:59 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.670  loss_cls: 0.207  loss_box_reg: 0.342  loss_rpn_cls: 0.019  loss_rpn_loc: 0.069  time: 0.5569  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:00:11 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.647  loss_cls: 0.229  loss_box_reg: 0.331  loss_rpn_cls: 0.030  loss_rpn_loc: 0.053  time: 0.5569  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:00:29 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.749  loss_cls: 0.215  loss_box_reg: 0.398  loss_rpn_cls: 0.023  loss_rpn_loc: 0.062  time: 0.5568  data_time: 0.0459  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:00:40 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.641  loss_cls: 0.221  loss_box_reg: 0.335  loss_rpn_cls: 0.025  loss_rpn_loc: 0.060  time: 0.5566  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:00:51 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.755  loss_cls: 0.234  loss_box_reg: 0.374  loss_rpn_cls: 0.022  loss_rpn_loc: 0.068  time: 0.5565  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:01:02 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.609  loss_cls: 0.200  loss_box_reg: 0.325  loss_rpn_cls: 0.021  loss_rpn_loc: 0.056  time: 0.5567  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:01:18 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.629  loss_cls: 0.209  loss_box_reg: 0.362  loss_rpn_cls: 0.022  loss_rpn_loc: 0.063  time: 0.5566  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 11:02:06 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:02:06 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 11:02:06 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 11:02:06 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.737  loss_cls: 0.210  loss_box_reg: 0.356  loss_rpn_cls: 0.032  loss_rpn_loc: 0.068  time: 0.5565  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:02:14 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:32 (0.5569 s / it)
[32m[04/15 11:02:14 d2.engine.hooks]: [0mTotal training time: 0:19:39 (0:01:07 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 11:02:30 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:02:30 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 11:02:30 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 11:02:33 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1221 s / img. ETA=0:02:35
[32m[04/15 11:02:38 d2.evaluation.evaluator]: [0mInference done 51/1257. 0.1221 s / img. ETA=0:02:30
[32m[04/15 11:02:43 d2.evaluation.evaluator]: [0mInference done 91/1257. 0.1222 s / img. ETA=0:02:26
[32m[04/15 11:02:48 d2.evaluation.evaluator]: [0mInference done 130/1257. 0.1235 s / img. ETA=0:02:22
[32m[04/15 11:02:53 d2.evaluation.evaluator]: [0mInference done 170/1257. 0.1232 s / img. ETA=0:02:17
[32m[04/15 11:02:58 d2.evaluation.evaluator]: [0mInference done 210/1257. 0.1234 s / img. ETA=0:02:12
[32m[04/15 11:03:03 d2.evaluation.evaluator]: [0mInference done 249/1257. 0.1238 s / img. ETA=0:02:08
[32m[04/15 11:03:08 d2.evaluation.evaluator]: [0mInference done 287/1257. 0.1244 s / img. ETA=0:02:04
[32m[04/15 11:03:13 d2.evaluation.evaluator]: [0mInference done 326/1257. 0.1244 s / img. ETA=0:01:59
[32m[04/15 11:03:18 d2.evaluation.evaluator]: [0mInference done 365/1257. 0.1245 s / img. ETA=0:01:54
[32m[04/15 11:03:23 d2.evaluation.evaluator]: [0mInference done 405/1257. 0.1243 s / img. ETA=0:01:48
[32m[04/15 11:03:28 d2.evaluation.evaluator]: [0mInference done 444/1257. 0.1244 s / img. ETA=0:01:44
[32m[04/15 11:03:33 d2.evaluation.evaluator]: [0mInference done 483/1257. 0.1245 s / img. ETA=0:01:39
[32m[04/15 11:03:38 d2.evaluation.evaluator]: [0mInference done 523/1257. 0.1245 s / img. ETA=0:01:33
[32m[04/15 11:03:43 d2.evaluation.evaluator]: [0mInference done 563/1257. 0.1244 s / img. ETA=0:01:28
[32m[04/15 11:03:48 d2.evaluation.evaluator]: [0mInference done 603/1257. 0.1242 s / img. ETA=0:01:23
[32m[04/15 11:03:53 d2.evaluation.evaluator]: [0mInference done 643/1257. 0.1241 s / img. ETA=0:01:18
[32m[04/15 11:03:58 d2.evaluation.evaluator]: [0mInference done 683/1257. 0.1241 s / img. ETA=0:01:13
[32m[04/15 11:04:03 d2.evaluation.evaluator]: [0mInference done 723/1257. 0.1240 s / img. ETA=0:01:08
[32m[04/15 11:04:08 d2.evaluation.evaluator]: [0mInference done 764/1257. 0.1239 s / img. ETA=0:01:02
[32m[04/15 11:04:14 d2.evaluation.evaluator]: [0mInference done 803/1257. 0.1241 s / img. ETA=0:00:57
[32m[04/15 11:04:19 d2.evaluation.evaluator]: [0mInference done 843/1257. 0.1240 s / img. ETA=0:00:52
[32m[04/15 11:04:24 d2.evaluation.evaluator]: [0mInference done 882/1257. 0.1241 s / img. ETA=0:00:47
[32m[04/15 11:04:29 d2.evaluation.evaluator]: [0mInference done 920/1257. 0.1243 s / img. ETA=0:00:43
[32m[04/15 11:04:34 d2.evaluation.evaluator]: [0mInference done 960/1257. 0.1243 s / img. ETA=0:00:37
[32m[04/15 11:04:39 d2.evaluation.evaluator]: [0mInference done 999/1257. 0.1244 s / img. ETA=0:00:32
[32m[04/15 11:04:44 d2.evaluation.evaluator]: [0mInference done 1039/1257. 0.1244 s / img. ETA=0:00:27
[32m[04/15 11:04:49 d2.evaluation.evaluator]: [0mInference done 1078/1257. 0.1245 s / img. ETA=0:00:22
[32m[04/15 11:04:54 d2.evaluation.evaluator]: [0mInference done 1117/1257. 0.1246 s / img. ETA=0:00:17
[32m[04/15 11:04:59 d2.evaluation.evaluator]: [0mInference done 1157/1257. 0.1245 s / img. ETA=0:00:12
[32m[04/15 11:05:04 d2.evaluation.evaluator]: [0mInference done 1194/1257. 0.1248 s / img. ETA=0:00:08
[32m[04/15 11:05:09 d2.evaluation.evaluator]: [0mInference done 1233/1257. 0.1248 s / img. ETA=0:00:03
[32m[04/15 11:05:13 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:40.793471 (0.128429 s / img per device, on 1 devices)
[32m[04/15 11:05:13 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:36 (0.124771 s / img per device, on 1 devices)
[32m[04/15 11:05:13 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 11:05:13 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 11:05:13 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.03s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.57s).
Accumulating evaluation results...
DONE (t=0.88s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.186
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.161
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.110
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.411
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.227
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.229
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.157
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.459
[32m[04/15 11:05:19 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 18.574 | 36.604 | 16.126 | 10.967 | 22.576 | 41.108 |
[32m[04/15 11:05:19 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 25.655 | bicycle       | 6.554 | car            | 42.016 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.072 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  15  *  2000  iterations ============
4 channel input
[32m[04/15 11:05:21 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[5m[31mWARNING[0m [32m[04/15 11:05:22 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:05:22 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 11:05:22 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 11:05:22 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 11:05:22 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 11:05:23 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 11:05:34 d2.utils.events]: [0m eta: 0:18:46  iter: 19  total_loss: 0.758  loss_cls: 0.236  loss_box_reg: 0.426  loss_rpn_cls: 0.026  loss_rpn_loc: 0.071  time: 0.5596  data_time: 0.0258  lr: 0.000200  max_mem: 3070M
[32m[04/15 11:05:45 d2.utils.events]: [0m eta: 0:17:51  iter: 39  total_loss: 0.615  loss_cls: 0.204  loss_box_reg: 0.356  loss_rpn_cls: 0.026  loss_rpn_loc: 0.061  time: 0.5455  data_time: 0.0074  lr: 0.000400  max_mem: 3070M
[32m[04/15 11:05:56 d2.utils.events]: [0m eta: 0:17:54  iter: 59  total_loss: 0.617  loss_cls: 0.219  loss_box_reg: 0.339  loss_rpn_cls: 0.022  loss_rpn_loc: 0.053  time: 0.5457  data_time: 0.0075  lr: 0.000599  max_mem: 3070M
[32m[04/15 11:06:07 d2.utils.events]: [0m eta: 0:17:38  iter: 79  total_loss: 0.704  loss_cls: 0.230  loss_box_reg: 0.405  loss_rpn_cls: 0.019  loss_rpn_loc: 0.068  time: 0.5455  data_time: 0.0088  lr: 0.000799  max_mem: 3070M
[32m[04/15 11:06:18 d2.utils.events]: [0m eta: 0:17:38  iter: 99  total_loss: 0.640  loss_cls: 0.189  loss_box_reg: 0.346  loss_rpn_cls: 0.020  loss_rpn_loc: 0.065  time: 0.5514  data_time: 0.0075  lr: 0.000999  max_mem: 3070M
[32m[04/15 11:06:30 d2.utils.events]: [0m eta: 0:17:35  iter: 119  total_loss: 0.574  loss_cls: 0.192  loss_box_reg: 0.316  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5550  data_time: 0.0066  lr: 0.001199  max_mem: 3070M
[32m[04/15 11:06:41 d2.utils.events]: [0m eta: 0:17:21  iter: 139  total_loss: 0.627  loss_cls: 0.196  loss_box_reg: 0.317  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5541  data_time: 0.0072  lr: 0.001399  max_mem: 3070M
[32m[04/15 11:06:52 d2.utils.events]: [0m eta: 0:17:06  iter: 159  total_loss: 0.585  loss_cls: 0.211  loss_box_reg: 0.333  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5533  data_time: 0.0079  lr: 0.001598  max_mem: 3070M
[32m[04/15 11:07:03 d2.utils.events]: [0m eta: 0:16:52  iter: 179  total_loss: 0.657  loss_cls: 0.207  loss_box_reg: 0.362  loss_rpn_cls: 0.023  loss_rpn_loc: 0.052  time: 0.5536  data_time: 0.0079  lr: 0.001798  max_mem: 3070M
[32m[04/15 11:07:14 d2.utils.events]: [0m eta: 0:16:41  iter: 199  total_loss: 0.628  loss_cls: 0.192  loss_box_reg: 0.325  loss_rpn_cls: 0.017  loss_rpn_loc: 0.057  time: 0.5537  data_time: 0.0068  lr: 0.001998  max_mem: 3070M
[32m[04/15 11:07:25 d2.utils.events]: [0m eta: 0:16:31  iter: 219  total_loss: 0.697  loss_cls: 0.223  loss_box_reg: 0.362  loss_rpn_cls: 0.021  loss_rpn_loc: 0.064  time: 0.5538  data_time: 0.0071  lr: 0.002198  max_mem: 3070M
[32m[04/15 11:07:37 d2.utils.events]: [0m eta: 0:16:27  iter: 239  total_loss: 0.588  loss_cls: 0.206  loss_box_reg: 0.339  loss_rpn_cls: 0.022  loss_rpn_loc: 0.041  time: 0.5568  data_time: 0.0076  lr: 0.002398  max_mem: 3070M
[32m[04/15 11:07:48 d2.utils.events]: [0m eta: 0:16:14  iter: 259  total_loss: 0.616  loss_cls: 0.203  loss_box_reg: 0.326  loss_rpn_cls: 0.016  loss_rpn_loc: 0.044  time: 0.5561  data_time: 0.0067  lr: 0.002597  max_mem: 3070M
[32m[04/15 11:08:00 d2.utils.events]: [0m eta: 0:16:05  iter: 279  total_loss: 0.732  loss_cls: 0.224  loss_box_reg: 0.411  loss_rpn_cls: 0.019  loss_rpn_loc: 0.070  time: 0.5572  data_time: 0.0073  lr: 0.002797  max_mem: 3070M
[32m[04/15 11:08:11 d2.utils.events]: [0m eta: 0:15:54  iter: 299  total_loss: 0.662  loss_cls: 0.214  loss_box_reg: 0.370  loss_rpn_cls: 0.016  loss_rpn_loc: 0.050  time: 0.5576  data_time: 0.0078  lr: 0.002997  max_mem: 3070M
[32m[04/15 11:08:23 d2.utils.events]: [0m eta: 0:15:42  iter: 319  total_loss: 0.541  loss_cls: 0.168  loss_box_reg: 0.329  loss_rpn_cls: 0.014  loss_rpn_loc: 0.044  time: 0.5574  data_time: 0.0067  lr: 0.003197  max_mem: 3070M
[32m[04/15 11:08:34 d2.utils.events]: [0m eta: 0:15:32  iter: 339  total_loss: 0.664  loss_cls: 0.212  loss_box_reg: 0.352  loss_rpn_cls: 0.019  loss_rpn_loc: 0.065  time: 0.5579  data_time: 0.0073  lr: 0.003397  max_mem: 3070M
[32m[04/15 11:08:45 d2.utils.events]: [0m eta: 0:15:22  iter: 359  total_loss: 0.622  loss_cls: 0.196  loss_box_reg: 0.346  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5575  data_time: 0.0070  lr: 0.003596  max_mem: 3070M
[32m[04/15 11:08:56 d2.utils.events]: [0m eta: 0:15:10  iter: 379  total_loss: 0.546  loss_cls: 0.167  loss_box_reg: 0.322  loss_rpn_cls: 0.011  loss_rpn_loc: 0.044  time: 0.5577  data_time: 0.0088  lr: 0.003796  max_mem: 3070M
[32m[04/15 11:09:08 d2.utils.events]: [0m eta: 0:14:58  iter: 399  total_loss: 0.728  loss_cls: 0.229  loss_box_reg: 0.388  loss_rpn_cls: 0.018  loss_rpn_loc: 0.061  time: 0.5577  data_time: 0.0064  lr: 0.003996  max_mem: 3070M
[32m[04/15 11:09:19 d2.utils.events]: [0m eta: 0:14:45  iter: 419  total_loss: 0.638  loss_cls: 0.204  loss_box_reg: 0.346  loss_rpn_cls: 0.022  loss_rpn_loc: 0.057  time: 0.5573  data_time: 0.0078  lr: 0.004196  max_mem: 3070M
[32m[04/15 11:09:30 d2.utils.events]: [0m eta: 0:14:33  iter: 439  total_loss: 0.561  loss_cls: 0.202  loss_box_reg: 0.306  loss_rpn_cls: 0.015  loss_rpn_loc: 0.040  time: 0.5569  data_time: 0.0071  lr: 0.004396  max_mem: 3070M
[32m[04/15 11:09:41 d2.utils.events]: [0m eta: 0:14:23  iter: 459  total_loss: 0.607  loss_cls: 0.188  loss_box_reg: 0.329  loss_rpn_cls: 0.014  loss_rpn_loc: 0.052  time: 0.5573  data_time: 0.0072  lr: 0.004595  max_mem: 3070M
[32m[04/15 11:09:52 d2.utils.events]: [0m eta: 0:14:11  iter: 479  total_loss: 0.722  loss_cls: 0.212  loss_box_reg: 0.395  loss_rpn_cls: 0.021  loss_rpn_loc: 0.080  time: 0.5571  data_time: 0.0070  lr: 0.004795  max_mem: 3070M
[32m[04/15 11:10:03 d2.utils.events]: [0m eta: 0:13:59  iter: 499  total_loss: 0.582  loss_cls: 0.210  loss_box_reg: 0.341  loss_rpn_cls: 0.012  loss_rpn_loc: 0.047  time: 0.5570  data_time: 0.0079  lr: 0.004995  max_mem: 3070M
[32m[04/15 11:10:15 d2.utils.events]: [0m eta: 0:13:49  iter: 519  total_loss: 0.666  loss_cls: 0.209  loss_box_reg: 0.400  loss_rpn_cls: 0.016  loss_rpn_loc: 0.061  time: 0.5571  data_time: 0.0069  lr: 0.005195  max_mem: 3070M
[32m[04/15 11:10:26 d2.utils.events]: [0m eta: 0:13:38  iter: 539  total_loss: 0.572  loss_cls: 0.177  loss_box_reg: 0.298  loss_rpn_cls: 0.018  loss_rpn_loc: 0.043  time: 0.5572  data_time: 0.0065  lr: 0.005395  max_mem: 3070M
[32m[04/15 11:10:37 d2.utils.events]: [0m eta: 0:13:28  iter: 559  total_loss: 0.627  loss_cls: 0.193  loss_box_reg: 0.375  loss_rpn_cls: 0.028  loss_rpn_loc: 0.058  time: 0.5572  data_time: 0.0082  lr: 0.005594  max_mem: 3070M
[32m[04/15 11:10:49 d2.utils.events]: [0m eta: 0:13:16  iter: 579  total_loss: 0.527  loss_cls: 0.165  loss_box_reg: 0.325  loss_rpn_cls: 0.011  loss_rpn_loc: 0.041  time: 0.5573  data_time: 0.0105  lr: 0.005794  max_mem: 3070M
[32m[04/15 11:11:00 d2.utils.events]: [0m eta: 0:13:05  iter: 599  total_loss: 0.594  loss_cls: 0.206  loss_box_reg: 0.333  loss_rpn_cls: 0.022  loss_rpn_loc: 0.052  time: 0.5572  data_time: 0.0067  lr: 0.005994  max_mem: 3070M
[32m[04/15 11:11:11 d2.utils.events]: [0m eta: 0:12:53  iter: 619  total_loss: 0.646  loss_cls: 0.227  loss_box_reg: 0.350  loss_rpn_cls: 0.020  loss_rpn_loc: 0.053  time: 0.5571  data_time: 0.0065  lr: 0.006194  max_mem: 3070M
[32m[04/15 11:11:22 d2.utils.events]: [0m eta: 0:12:42  iter: 639  total_loss: 0.744  loss_cls: 0.224  loss_box_reg: 0.415  loss_rpn_cls: 0.022  loss_rpn_loc: 0.061  time: 0.5571  data_time: 0.0081  lr: 0.006394  max_mem: 3070M
[32m[04/15 11:11:33 d2.utils.events]: [0m eta: 0:12:31  iter: 659  total_loss: 0.687  loss_cls: 0.220  loss_box_reg: 0.327  loss_rpn_cls: 0.021  loss_rpn_loc: 0.049  time: 0.5569  data_time: 0.0094  lr: 0.006593  max_mem: 3070M
[32m[04/15 11:11:44 d2.utils.events]: [0m eta: 0:12:19  iter: 679  total_loss: 0.638  loss_cls: 0.208  loss_box_reg: 0.351  loss_rpn_cls: 0.026  loss_rpn_loc: 0.061  time: 0.5568  data_time: 0.0082  lr: 0.006793  max_mem: 3070M
[32m[04/15 11:11:56 d2.utils.events]: [0m eta: 0:12:08  iter: 699  total_loss: 0.680  loss_cls: 0.230  loss_box_reg: 0.360  loss_rpn_cls: 0.016  loss_rpn_loc: 0.061  time: 0.5568  data_time: 0.0065  lr: 0.006993  max_mem: 3070M
[32m[04/15 11:12:07 d2.utils.events]: [0m eta: 0:11:56  iter: 719  total_loss: 0.568  loss_cls: 0.181  loss_box_reg: 0.320  loss_rpn_cls: 0.019  loss_rpn_loc: 0.044  time: 0.5566  data_time: 0.0124  lr: 0.007193  max_mem: 3070M
[32m[04/15 11:12:18 d2.utils.events]: [0m eta: 0:11:45  iter: 739  total_loss: 0.704  loss_cls: 0.239  loss_box_reg: 0.376  loss_rpn_cls: 0.019  loss_rpn_loc: 0.065  time: 0.5564  data_time: 0.0274  lr: 0.007393  max_mem: 3070M
[32m[04/15 11:12:31 d2.utils.events]: [0m eta: 0:11:34  iter: 759  total_loss: 0.662  loss_cls: 0.208  loss_box_reg: 0.377  loss_rpn_cls: 0.018  loss_rpn_loc: 0.053  time: 0.5585  data_time: 0.1149  lr: 0.007592  max_mem: 3070M
[32m[04/15 11:12:42 d2.utils.events]: [0m eta: 0:11:24  iter: 779  total_loss: 0.650  loss_cls: 0.215  loss_box_reg: 0.330  loss_rpn_cls: 0.026  loss_rpn_loc: 0.052  time: 0.5587  data_time: 0.0079  lr: 0.007792  max_mem: 3070M
[32m[04/15 11:12:54 d2.utils.events]: [0m eta: 0:11:13  iter: 799  total_loss: 0.653  loss_cls: 0.215  loss_box_reg: 0.357  loss_rpn_cls: 0.028  loss_rpn_loc: 0.069  time: 0.5589  data_time: 0.0083  lr: 0.007992  max_mem: 3070M
[32m[04/15 11:13:04 d2.utils.events]: [0m eta: 0:11:01  iter: 819  total_loss: 0.670  loss_cls: 0.194  loss_box_reg: 0.365  loss_rpn_cls: 0.020  loss_rpn_loc: 0.084  time: 0.5585  data_time: 0.0076  lr: 0.008192  max_mem: 3070M
[32m[04/15 11:13:16 d2.utils.events]: [0m eta: 0:10:50  iter: 839  total_loss: 0.614  loss_cls: 0.183  loss_box_reg: 0.329  loss_rpn_cls: 0.016  loss_rpn_loc: 0.068  time: 0.5587  data_time: 0.0082  lr: 0.008392  max_mem: 3070M
[32m[04/15 11:13:27 d2.utils.events]: [0m eta: 0:10:39  iter: 859  total_loss: 0.588  loss_cls: 0.196  loss_box_reg: 0.320  loss_rpn_cls: 0.018  loss_rpn_loc: 0.056  time: 0.5586  data_time: 0.0070  lr: 0.008591  max_mem: 3070M
[32m[04/15 11:13:38 d2.utils.events]: [0m eta: 0:10:28  iter: 879  total_loss: 0.547  loss_cls: 0.167  loss_box_reg: 0.285  loss_rpn_cls: 0.019  loss_rpn_loc: 0.048  time: 0.5585  data_time: 0.0081  lr: 0.008791  max_mem: 3070M
[32m[04/15 11:13:49 d2.utils.events]: [0m eta: 0:10:16  iter: 899  total_loss: 0.707  loss_cls: 0.222  loss_box_reg: 0.398  loss_rpn_cls: 0.022  loss_rpn_loc: 0.049  time: 0.5581  data_time: 0.0076  lr: 0.008991  max_mem: 3070M
[32m[04/15 11:14:01 d2.utils.events]: [0m eta: 0:10:04  iter: 919  total_loss: 0.559  loss_cls: 0.186  loss_box_reg: 0.292  loss_rpn_cls: 0.025  loss_rpn_loc: 0.044  time: 0.5577  data_time: 0.0090  lr: 0.009191  max_mem: 3070M
[32m[04/15 11:14:12 d2.utils.events]: [0m eta: 0:09:53  iter: 939  total_loss: 0.726  loss_cls: 0.234  loss_box_reg: 0.401  loss_rpn_cls: 0.021  loss_rpn_loc: 0.082  time: 0.5582  data_time: 0.0070  lr: 0.009391  max_mem: 3070M
[32m[04/15 11:14:23 d2.utils.events]: [0m eta: 0:09:42  iter: 959  total_loss: 0.662  loss_cls: 0.216  loss_box_reg: 0.336  loss_rpn_cls: 0.023  loss_rpn_loc: 0.080  time: 0.5581  data_time: 0.0089  lr: 0.009590  max_mem: 3070M
[32m[04/15 11:14:35 d2.utils.events]: [0m eta: 0:09:31  iter: 979  total_loss: 0.587  loss_cls: 0.184  loss_box_reg: 0.300  loss_rpn_cls: 0.020  loss_rpn_loc: 0.042  time: 0.5582  data_time: 0.0078  lr: 0.009790  max_mem: 3070M
[32m[04/15 11:14:46 d2.utils.events]: [0m eta: 0:09:19  iter: 999  total_loss: 0.565  loss_cls: 0.185  loss_box_reg: 0.300  loss_rpn_cls: 0.023  loss_rpn_loc: 0.049  time: 0.5578  data_time: 0.0065  lr: 0.009990  max_mem: 3070M
[32m[04/15 11:14:57 d2.utils.events]: [0m eta: 0:09:08  iter: 1019  total_loss: 0.736  loss_cls: 0.233  loss_box_reg: 0.381  loss_rpn_cls: 0.027  loss_rpn_loc: 0.069  time: 0.5578  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:15:08 d2.utils.events]: [0m eta: 0:08:57  iter: 1039  total_loss: 0.765  loss_cls: 0.228  loss_box_reg: 0.376  loss_rpn_cls: 0.031  loss_rpn_loc: 0.064  time: 0.5579  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:15:20 d2.utils.events]: [0m eta: 0:08:46  iter: 1059  total_loss: 0.560  loss_cls: 0.192  loss_box_reg: 0.298  loss_rpn_cls: 0.023  loss_rpn_loc: 0.045  time: 0.5579  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:15:31 d2.utils.events]: [0m eta: 0:08:35  iter: 1079  total_loss: 0.752  loss_cls: 0.235  loss_box_reg: 0.427  loss_rpn_cls: 0.033  loss_rpn_loc: 0.072  time: 0.5581  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:15:43 d2.utils.events]: [0m eta: 0:08:23  iter: 1099  total_loss: 0.706  loss_cls: 0.239  loss_box_reg: 0.350  loss_rpn_cls: 0.026  loss_rpn_loc: 0.067  time: 0.5582  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:15:54 d2.utils.events]: [0m eta: 0:08:12  iter: 1119  total_loss: 0.667  loss_cls: 0.225  loss_box_reg: 0.376  loss_rpn_cls: 0.031  loss_rpn_loc: 0.070  time: 0.5581  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:16:05 d2.utils.events]: [0m eta: 0:08:01  iter: 1139  total_loss: 0.578  loss_cls: 0.209  loss_box_reg: 0.309  loss_rpn_cls: 0.034  loss_rpn_loc: 0.051  time: 0.5580  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:16:16 d2.utils.events]: [0m eta: 0:07:49  iter: 1159  total_loss: 0.742  loss_cls: 0.248  loss_box_reg: 0.411  loss_rpn_cls: 0.032  loss_rpn_loc: 0.064  time: 0.5577  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:16:27 d2.utils.events]: [0m eta: 0:07:38  iter: 1179  total_loss: 0.629  loss_cls: 0.214  loss_box_reg: 0.307  loss_rpn_cls: 0.034  loss_rpn_loc: 0.064  time: 0.5575  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:16:38 d2.utils.events]: [0m eta: 0:07:27  iter: 1199  total_loss: 0.708  loss_cls: 0.204  loss_box_reg: 0.374  loss_rpn_cls: 0.026  loss_rpn_loc: 0.084  time: 0.5577  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:16:49 d2.utils.events]: [0m eta: 0:07:16  iter: 1219  total_loss: 0.617  loss_cls: 0.188  loss_box_reg: 0.354  loss_rpn_cls: 0.018  loss_rpn_loc: 0.058  time: 0.5576  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:17:01 d2.utils.events]: [0m eta: 0:07:04  iter: 1239  total_loss: 0.634  loss_cls: 0.214  loss_box_reg: 0.364  loss_rpn_cls: 0.023  loss_rpn_loc: 0.048  time: 0.5577  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:17:12 d2.utils.events]: [0m eta: 0:06:53  iter: 1259  total_loss: 0.622  loss_cls: 0.243  loss_box_reg: 0.341  loss_rpn_cls: 0.020  loss_rpn_loc: 0.058  time: 0.5578  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:17:24 d2.utils.events]: [0m eta: 0:06:42  iter: 1279  total_loss: 0.638  loss_cls: 0.206  loss_box_reg: 0.351  loss_rpn_cls: 0.019  loss_rpn_loc: 0.049  time: 0.5582  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:17:35 d2.utils.events]: [0m eta: 0:06:31  iter: 1299  total_loss: 0.796  loss_cls: 0.228  loss_box_reg: 0.416  loss_rpn_cls: 0.018  loss_rpn_loc: 0.091  time: 0.5579  data_time: 0.0061  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:17:46 d2.utils.events]: [0m eta: 0:06:20  iter: 1319  total_loss: 0.759  loss_cls: 0.234  loss_box_reg: 0.441  loss_rpn_cls: 0.022  loss_rpn_loc: 0.076  time: 0.5580  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:17:57 d2.utils.events]: [0m eta: 0:06:09  iter: 1339  total_loss: 0.729  loss_cls: 0.256  loss_box_reg: 0.364  loss_rpn_cls: 0.030  loss_rpn_loc: 0.056  time: 0.5580  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:18:08 d2.utils.events]: [0m eta: 0:05:57  iter: 1359  total_loss: 0.682  loss_cls: 0.236  loss_box_reg: 0.364  loss_rpn_cls: 0.026  loss_rpn_loc: 0.048  time: 0.5578  data_time: 0.0061  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:18:19 d2.utils.events]: [0m eta: 0:05:46  iter: 1379  total_loss: 0.606  loss_cls: 0.211  loss_box_reg: 0.307  loss_rpn_cls: 0.024  loss_rpn_loc: 0.060  time: 0.5578  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:18:30 d2.utils.events]: [0m eta: 0:05:35  iter: 1399  total_loss: 0.598  loss_cls: 0.223  loss_box_reg: 0.325  loss_rpn_cls: 0.014  loss_rpn_loc: 0.059  time: 0.5576  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:18:41 d2.utils.events]: [0m eta: 0:05:24  iter: 1419  total_loss: 0.609  loss_cls: 0.207  loss_box_reg: 0.323  loss_rpn_cls: 0.022  loss_rpn_loc: 0.063  time: 0.5574  data_time: 0.0062  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:18:53 d2.utils.events]: [0m eta: 0:05:13  iter: 1439  total_loss: 0.714  loss_cls: 0.239  loss_box_reg: 0.376  loss_rpn_cls: 0.022  loss_rpn_loc: 0.052  time: 0.5574  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:19:04 d2.utils.events]: [0m eta: 0:05:02  iter: 1459  total_loss: 0.704  loss_cls: 0.237  loss_box_reg: 0.338  loss_rpn_cls: 0.022  loss_rpn_loc: 0.055  time: 0.5576  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:19:15 d2.utils.events]: [0m eta: 0:04:51  iter: 1479  total_loss: 0.668  loss_cls: 0.242  loss_box_reg: 0.339  loss_rpn_cls: 0.020  loss_rpn_loc: 0.057  time: 0.5576  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:19:27 d2.utils.events]: [0m eta: 0:04:39  iter: 1499  total_loss: 0.611  loss_cls: 0.201  loss_box_reg: 0.308  loss_rpn_cls: 0.023  loss_rpn_loc: 0.066  time: 0.5576  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:19:38 d2.utils.events]: [0m eta: 0:04:28  iter: 1519  total_loss: 0.643  loss_cls: 0.210  loss_box_reg: 0.336  loss_rpn_cls: 0.031  loss_rpn_loc: 0.059  time: 0.5576  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:19:49 d2.utils.events]: [0m eta: 0:04:17  iter: 1539  total_loss: 0.587  loss_cls: 0.194  loss_box_reg: 0.321  loss_rpn_cls: 0.023  loss_rpn_loc: 0.048  time: 0.5575  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:20:00 d2.utils.events]: [0m eta: 0:04:06  iter: 1559  total_loss: 0.693  loss_cls: 0.231  loss_box_reg: 0.362  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5576  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:20:12 d2.utils.events]: [0m eta: 0:03:55  iter: 1579  total_loss: 0.732  loss_cls: 0.241  loss_box_reg: 0.391  loss_rpn_cls: 0.026  loss_rpn_loc: 0.064  time: 0.5577  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:20:23 d2.utils.events]: [0m eta: 0:03:43  iter: 1599  total_loss: 0.769  loss_cls: 0.258  loss_box_reg: 0.389  loss_rpn_cls: 0.022  loss_rpn_loc: 0.083  time: 0.5576  data_time: 0.0105  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:20:34 d2.utils.events]: [0m eta: 0:03:32  iter: 1619  total_loss: 0.804  loss_cls: 0.276  loss_box_reg: 0.431  loss_rpn_cls: 0.029  loss_rpn_loc: 0.079  time: 0.5575  data_time: 0.0096  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:20:45 d2.utils.events]: [0m eta: 0:03:21  iter: 1639  total_loss: 0.717  loss_cls: 0.211  loss_box_reg: 0.386  loss_rpn_cls: 0.017  loss_rpn_loc: 0.068  time: 0.5575  data_time: 0.0117  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:20:57 d2.utils.events]: [0m eta: 0:03:10  iter: 1659  total_loss: 0.690  loss_cls: 0.219  loss_box_reg: 0.389  loss_rpn_cls: 0.019  loss_rpn_loc: 0.043  time: 0.5578  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:21:08 d2.utils.events]: [0m eta: 0:02:59  iter: 1679  total_loss: 0.654  loss_cls: 0.236  loss_box_reg: 0.338  loss_rpn_cls: 0.027  loss_rpn_loc: 0.057  time: 0.5578  data_time: 0.0089  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:21:19 d2.utils.events]: [0m eta: 0:02:48  iter: 1699  total_loss: 0.626  loss_cls: 0.206  loss_box_reg: 0.316  loss_rpn_cls: 0.030  loss_rpn_loc: 0.069  time: 0.5577  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:21:30 d2.utils.events]: [0m eta: 0:02:37  iter: 1719  total_loss: 0.722  loss_cls: 0.221  loss_box_reg: 0.382  loss_rpn_cls: 0.017  loss_rpn_loc: 0.069  time: 0.5575  data_time: 0.0100  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:21:41 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.602  loss_cls: 0.187  loss_box_reg: 0.342  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5576  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:21:53 d2.utils.events]: [0m eta: 0:02:14  iter: 1759  total_loss: 0.613  loss_cls: 0.209  loss_box_reg: 0.344  loss_rpn_cls: 0.019  loss_rpn_loc: 0.059  time: 0.5577  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:22:04 d2.utils.events]: [0m eta: 0:02:03  iter: 1779  total_loss: 0.675  loss_cls: 0.215  loss_box_reg: 0.372  loss_rpn_cls: 0.021  loss_rpn_loc: 0.071  time: 0.5578  data_time: 0.0142  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:22:15 d2.utils.events]: [0m eta: 0:01:52  iter: 1799  total_loss: 0.586  loss_cls: 0.188  loss_box_reg: 0.316  loss_rpn_cls: 0.036  loss_rpn_loc: 0.063  time: 0.5577  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:22:27 d2.utils.events]: [0m eta: 0:01:41  iter: 1819  total_loss: 0.692  loss_cls: 0.205  loss_box_reg: 0.390  loss_rpn_cls: 0.027  loss_rpn_loc: 0.068  time: 0.5579  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:22:38 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.769  loss_cls: 0.248  loss_box_reg: 0.402  loss_rpn_cls: 0.023  loss_rpn_loc: 0.087  time: 0.5581  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:22:50 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.636  loss_cls: 0.207  loss_box_reg: 0.333  loss_rpn_cls: 0.027  loss_rpn_loc: 0.074  time: 0.5580  data_time: 0.0100  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:23:01 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.574  loss_cls: 0.190  loss_box_reg: 0.318  loss_rpn_cls: 0.023  loss_rpn_loc: 0.064  time: 0.5580  data_time: 0.0060  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:23:17 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.722  loss_cls: 0.258  loss_box_reg: 0.408  loss_rpn_cls: 0.027  loss_rpn_loc: 0.058  time: 0.5580  data_time: 0.0291  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:23:29 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.670  loss_cls: 0.248  loss_box_reg: 0.330  loss_rpn_cls: 0.024  loss_rpn_loc: 0.067  time: 0.5581  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:23:40 d2.utils.events]: [0m eta: 0:00:34  iter: 1939  total_loss: 0.744  loss_cls: 0.205  loss_box_reg: 0.370  loss_rpn_cls: 0.040  loss_rpn_loc: 0.069  time: 0.5581  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:23:55 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.754  loss_cls: 0.234  loss_box_reg: 0.387  loss_rpn_cls: 0.030  loss_rpn_loc: 0.053  time: 0.5577  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:24:06 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.638  loss_cls: 0.207  loss_box_reg: 0.321  loss_rpn_cls: 0.031  loss_rpn_loc: 0.074  time: 0.5575  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 11:24:48 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:24:48 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 11:24:48 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 11:24:48 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.587  loss_cls: 0.184  loss_box_reg: 0.320  loss_rpn_cls: 0.027  loss_rpn_loc: 0.048  time: 0.5575  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:24:55 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:34 (0.5578 s / it)
[32m[04/15 11:24:55 d2.engine.hooks]: [0mTotal training time: 0:19:31 (0:00:57 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 11:25:17 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:25:17 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 11:25:17 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 11:25:19 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1205 s / img. ETA=0:02:33
[32m[04/15 11:25:24 d2.evaluation.evaluator]: [0mInference done 51/1257. 0.1231 s / img. ETA=0:02:32
[32m[04/15 11:25:30 d2.evaluation.evaluator]: [0mInference done 91/1257. 0.1232 s / img. ETA=0:02:28
[32m[04/15 11:25:35 d2.evaluation.evaluator]: [0mInference done 130/1257. 0.1243 s / img. ETA=0:02:24
[32m[04/15 11:25:40 d2.evaluation.evaluator]: [0mInference done 170/1257. 0.1239 s / img. ETA=0:02:19
[32m[04/15 11:25:45 d2.evaluation.evaluator]: [0mInference done 210/1257. 0.1239 s / img. ETA=0:02:13
[32m[04/15 11:25:50 d2.evaluation.evaluator]: [0mInference done 250/1257. 0.1239 s / img. ETA=0:02:08
[32m[04/15 11:25:55 d2.evaluation.evaluator]: [0mInference done 289/1257. 0.1241 s / img. ETA=0:02:04
[32m[04/15 11:26:00 d2.evaluation.evaluator]: [0mInference done 328/1257. 0.1243 s / img. ETA=0:01:59
[32m[04/15 11:26:05 d2.evaluation.evaluator]: [0mInference done 368/1257. 0.1242 s / img. ETA=0:01:53
[32m[04/15 11:26:10 d2.evaluation.evaluator]: [0mInference done 408/1257. 0.1241 s / img. ETA=0:01:48
[32m[04/15 11:26:15 d2.evaluation.evaluator]: [0mInference done 447/1257. 0.1242 s / img. ETA=0:01:43
[32m[04/15 11:26:20 d2.evaluation.evaluator]: [0mInference done 486/1257. 0.1242 s / img. ETA=0:01:38
[32m[04/15 11:26:25 d2.evaluation.evaluator]: [0mInference done 525/1257. 0.1242 s / img. ETA=0:01:33
[32m[04/15 11:26:30 d2.evaluation.evaluator]: [0mInference done 565/1257. 0.1242 s / img. ETA=0:01:28
[32m[04/15 11:26:35 d2.evaluation.evaluator]: [0mInference done 605/1257. 0.1240 s / img. ETA=0:01:23
[32m[04/15 11:26:40 d2.evaluation.evaluator]: [0mInference done 645/1257. 0.1240 s / img. ETA=0:01:18
[32m[04/15 11:26:45 d2.evaluation.evaluator]: [0mInference done 686/1257. 0.1237 s / img. ETA=0:01:12
[32m[04/15 11:26:51 d2.evaluation.evaluator]: [0mInference done 726/1257. 0.1237 s / img. ETA=0:01:07
[32m[04/15 11:26:56 d2.evaluation.evaluator]: [0mInference done 766/1257. 0.1237 s / img. ETA=0:01:02
[32m[04/15 11:27:01 d2.evaluation.evaluator]: [0mInference done 805/1257. 0.1237 s / img. ETA=0:00:57
[32m[04/15 11:27:06 d2.evaluation.evaluator]: [0mInference done 845/1257. 0.1238 s / img. ETA=0:00:52
[32m[04/15 11:27:11 d2.evaluation.evaluator]: [0mInference done 884/1257. 0.1239 s / img. ETA=0:00:47
[32m[04/15 11:27:16 d2.evaluation.evaluator]: [0mInference done 923/1257. 0.1240 s / img. ETA=0:00:42
[32m[04/15 11:27:21 d2.evaluation.evaluator]: [0mInference done 962/1257. 0.1241 s / img. ETA=0:00:37
[32m[04/15 11:27:26 d2.evaluation.evaluator]: [0mInference done 1001/1257. 0.1241 s / img. ETA=0:00:32
[32m[04/15 11:27:31 d2.evaluation.evaluator]: [0mInference done 1041/1257. 0.1241 s / img. ETA=0:00:27
[32m[04/15 11:27:36 d2.evaluation.evaluator]: [0mInference done 1081/1257. 0.1242 s / img. ETA=0:00:22
[32m[04/15 11:27:41 d2.evaluation.evaluator]: [0mInference done 1120/1257. 0.1242 s / img. ETA=0:00:17
[32m[04/15 11:27:46 d2.evaluation.evaluator]: [0mInference done 1159/1257. 0.1243 s / img. ETA=0:00:12
[32m[04/15 11:27:51 d2.evaluation.evaluator]: [0mInference done 1198/1257. 0.1243 s / img. ETA=0:00:07
[32m[04/15 11:27:56 d2.evaluation.evaluator]: [0mInference done 1237/1257. 0.1244 s / img. ETA=0:00:02
[32m[04/15 11:27:59 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:40.555275 (0.128239 s / img per device, on 1 devices)
[32m[04/15 11:27:59 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:35 (0.124415 s / img per device, on 1 devices)
[32m[04/15 11:27:59 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 11:27:59 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 11:27:59 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=4.96s).
Accumulating evaluation results...
DONE (t=0.68s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.191
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.388
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.165
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.122
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.235
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.095
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.176
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.286
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.432
[32m[04/15 11:28:05 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.107 | 38.786 | 16.467 | 12.233 | 23.497 | 38.307 |
[32m[04/15 11:28:05 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 29.308 | bicycle       | 7.120 | car            | 39.738 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.264 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  16  *  2000  iterations ============
4 channel input
[32m[04/15 11:28:06 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 11:28:07 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.16 seconds.
[5m[31mWARNING[0m [32m[04/15 11:28:07 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:28:07 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 11:28:08 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 11:28:08 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 11:28:08 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 11:28:24 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 11:28:36 d2.utils.events]: [0m eta: 0:18:02  iter: 19  total_loss: 0.743  loss_cls: 0.235  loss_box_reg: 0.387  loss_rpn_cls: 0.021  loss_rpn_loc: 0.051  time: 0.5491  data_time: 0.0603  lr: 0.000200  max_mem: 3070M
[32m[04/15 11:28:47 d2.utils.events]: [0m eta: 0:17:32  iter: 39  total_loss: 0.603  loss_cls: 0.203  loss_box_reg: 0.326  loss_rpn_cls: 0.023  loss_rpn_loc: 0.062  time: 0.5432  data_time: 0.0084  lr: 0.000400  max_mem: 3070M
[32m[04/15 11:28:58 d2.utils.events]: [0m eta: 0:17:23  iter: 59  total_loss: 0.704  loss_cls: 0.212  loss_box_reg: 0.371  loss_rpn_cls: 0.022  loss_rpn_loc: 0.048  time: 0.5414  data_time: 0.0071  lr: 0.000599  max_mem: 3070M
[32m[04/15 11:29:09 d2.utils.events]: [0m eta: 0:17:12  iter: 79  total_loss: 0.671  loss_cls: 0.212  loss_box_reg: 0.361  loss_rpn_cls: 0.019  loss_rpn_loc: 0.069  time: 0.5393  data_time: 0.0092  lr: 0.000799  max_mem: 3070M
[32m[04/15 11:29:20 d2.utils.events]: [0m eta: 0:17:06  iter: 99  total_loss: 0.599  loss_cls: 0.191  loss_box_reg: 0.314  loss_rpn_cls: 0.018  loss_rpn_loc: 0.047  time: 0.5411  data_time: 0.0082  lr: 0.000999  max_mem: 3070M
[32m[04/15 11:29:30 d2.utils.events]: [0m eta: 0:16:53  iter: 119  total_loss: 0.661  loss_cls: 0.201  loss_box_reg: 0.385  loss_rpn_cls: 0.019  loss_rpn_loc: 0.067  time: 0.5381  data_time: 0.0077  lr: 0.001199  max_mem: 3070M
[32m[04/15 11:29:42 d2.utils.events]: [0m eta: 0:16:43  iter: 139  total_loss: 0.706  loss_cls: 0.234  loss_box_reg: 0.385  loss_rpn_cls: 0.019  loss_rpn_loc: 0.058  time: 0.5401  data_time: 0.0071  lr: 0.001399  max_mem: 3070M
[32m[04/15 11:29:53 d2.utils.events]: [0m eta: 0:16:38  iter: 159  total_loss: 0.635  loss_cls: 0.193  loss_box_reg: 0.322  loss_rpn_cls: 0.022  loss_rpn_loc: 0.051  time: 0.5433  data_time: 0.0079  lr: 0.001598  max_mem: 3070M
[32m[04/15 11:30:04 d2.utils.events]: [0m eta: 0:16:29  iter: 179  total_loss: 0.601  loss_cls: 0.200  loss_box_reg: 0.341  loss_rpn_cls: 0.020  loss_rpn_loc: 0.062  time: 0.5438  data_time: 0.0076  lr: 0.001798  max_mem: 3070M
[32m[04/15 11:30:15 d2.utils.events]: [0m eta: 0:16:25  iter: 199  total_loss: 0.612  loss_cls: 0.188  loss_box_reg: 0.349  loss_rpn_cls: 0.014  loss_rpn_loc: 0.052  time: 0.5445  data_time: 0.0071  lr: 0.001998  max_mem: 3070M
[32m[04/15 11:30:26 d2.utils.events]: [0m eta: 0:16:04  iter: 219  total_loss: 0.684  loss_cls: 0.231  loss_box_reg: 0.396  loss_rpn_cls: 0.028  loss_rpn_loc: 0.051  time: 0.5427  data_time: 0.0114  lr: 0.002198  max_mem: 3070M
[32m[04/15 11:30:37 d2.utils.events]: [0m eta: 0:15:53  iter: 239  total_loss: 0.625  loss_cls: 0.226  loss_box_reg: 0.341  loss_rpn_cls: 0.022  loss_rpn_loc: 0.052  time: 0.5416  data_time: 0.0083  lr: 0.002398  max_mem: 3070M
[32m[04/15 11:30:48 d2.utils.events]: [0m eta: 0:15:44  iter: 259  total_loss: 0.609  loss_cls: 0.194  loss_box_reg: 0.344  loss_rpn_cls: 0.021  loss_rpn_loc: 0.038  time: 0.5430  data_time: 0.0075  lr: 0.002597  max_mem: 3070M
[32m[04/15 11:30:59 d2.utils.events]: [0m eta: 0:15:35  iter: 279  total_loss: 0.631  loss_cls: 0.208  loss_box_reg: 0.320  loss_rpn_cls: 0.016  loss_rpn_loc: 0.040  time: 0.5435  data_time: 0.0068  lr: 0.002797  max_mem: 3070M
[32m[04/15 11:31:10 d2.utils.events]: [0m eta: 0:15:27  iter: 299  total_loss: 0.646  loss_cls: 0.216  loss_box_reg: 0.380  loss_rpn_cls: 0.021  loss_rpn_loc: 0.052  time: 0.5439  data_time: 0.0084  lr: 0.002997  max_mem: 3070M
[32m[04/15 11:31:21 d2.utils.events]: [0m eta: 0:15:15  iter: 319  total_loss: 0.648  loss_cls: 0.217  loss_box_reg: 0.377  loss_rpn_cls: 0.019  loss_rpn_loc: 0.053  time: 0.5436  data_time: 0.0075  lr: 0.003197  max_mem: 3070M
[32m[04/15 11:31:32 d2.utils.events]: [0m eta: 0:15:05  iter: 339  total_loss: 0.658  loss_cls: 0.220  loss_box_reg: 0.373  loss_rpn_cls: 0.018  loss_rpn_loc: 0.066  time: 0.5435  data_time: 0.0072  lr: 0.003397  max_mem: 3070M
[32m[04/15 11:31:43 d2.utils.events]: [0m eta: 0:15:00  iter: 359  total_loss: 0.539  loss_cls: 0.177  loss_box_reg: 0.310  loss_rpn_cls: 0.020  loss_rpn_loc: 0.051  time: 0.5450  data_time: 0.0074  lr: 0.003596  max_mem: 3070M
[32m[04/15 11:31:55 d2.utils.events]: [0m eta: 0:14:50  iter: 379  total_loss: 0.805  loss_cls: 0.247  loss_box_reg: 0.399  loss_rpn_cls: 0.021  loss_rpn_loc: 0.067  time: 0.5460  data_time: 0.0079  lr: 0.003796  max_mem: 3070M
[32m[04/15 11:32:05 d2.utils.events]: [0m eta: 0:14:38  iter: 399  total_loss: 0.728  loss_cls: 0.237  loss_box_reg: 0.409  loss_rpn_cls: 0.022  loss_rpn_loc: 0.072  time: 0.5453  data_time: 0.0069  lr: 0.003996  max_mem: 3070M
[32m[04/15 11:32:17 d2.utils.events]: [0m eta: 0:14:27  iter: 419  total_loss: 0.582  loss_cls: 0.185  loss_box_reg: 0.316  loss_rpn_cls: 0.015  loss_rpn_loc: 0.051  time: 0.5459  data_time: 0.0067  lr: 0.004196  max_mem: 3070M
[32m[04/15 11:32:28 d2.utils.events]: [0m eta: 0:14:14  iter: 439  total_loss: 0.709  loss_cls: 0.207  loss_box_reg: 0.398  loss_rpn_cls: 0.017  loss_rpn_loc: 0.059  time: 0.5457  data_time: 0.0063  lr: 0.004396  max_mem: 3070M
[32m[04/15 11:32:39 d2.utils.events]: [0m eta: 0:14:04  iter: 459  total_loss: 0.581  loss_cls: 0.223  loss_box_reg: 0.313  loss_rpn_cls: 0.020  loss_rpn_loc: 0.043  time: 0.5459  data_time: 0.0049  lr: 0.004595  max_mem: 3070M
[32m[04/15 11:32:50 d2.utils.events]: [0m eta: 0:13:52  iter: 479  total_loss: 0.651  loss_cls: 0.210  loss_box_reg: 0.329  loss_rpn_cls: 0.018  loss_rpn_loc: 0.062  time: 0.5458  data_time: 0.0060  lr: 0.004795  max_mem: 3070M
[32m[04/15 11:33:01 d2.utils.events]: [0m eta: 0:13:43  iter: 499  total_loss: 0.642  loss_cls: 0.190  loss_box_reg: 0.360  loss_rpn_cls: 0.009  loss_rpn_loc: 0.051  time: 0.5464  data_time: 0.0067  lr: 0.004995  max_mem: 3070M
[32m[04/15 11:33:12 d2.utils.events]: [0m eta: 0:13:32  iter: 519  total_loss: 0.573  loss_cls: 0.189  loss_box_reg: 0.326  loss_rpn_cls: 0.014  loss_rpn_loc: 0.060  time: 0.5467  data_time: 0.0073  lr: 0.005195  max_mem: 3070M
[32m[04/15 11:33:23 d2.utils.events]: [0m eta: 0:13:22  iter: 539  total_loss: 0.488  loss_cls: 0.159  loss_box_reg: 0.296  loss_rpn_cls: 0.010  loss_rpn_loc: 0.028  time: 0.5474  data_time: 0.0072  lr: 0.005395  max_mem: 3070M
[32m[04/15 11:33:35 d2.utils.events]: [0m eta: 0:13:11  iter: 559  total_loss: 0.659  loss_cls: 0.205  loss_box_reg: 0.321  loss_rpn_cls: 0.012  loss_rpn_loc: 0.052  time: 0.5484  data_time: 0.0087  lr: 0.005594  max_mem: 3070M
[32m[04/15 11:33:46 d2.utils.events]: [0m eta: 0:13:00  iter: 579  total_loss: 0.838  loss_cls: 0.239  loss_box_reg: 0.447  loss_rpn_cls: 0.016  loss_rpn_loc: 0.071  time: 0.5489  data_time: 0.0074  lr: 0.005794  max_mem: 3070M
[32m[04/15 11:33:57 d2.utils.events]: [0m eta: 0:12:50  iter: 599  total_loss: 0.549  loss_cls: 0.181  loss_box_reg: 0.294  loss_rpn_cls: 0.022  loss_rpn_loc: 0.039  time: 0.5489  data_time: 0.0067  lr: 0.005994  max_mem: 3070M
[32m[04/15 11:34:09 d2.utils.events]: [0m eta: 0:12:40  iter: 619  total_loss: 0.664  loss_cls: 0.211  loss_box_reg: 0.366  loss_rpn_cls: 0.021  loss_rpn_loc: 0.059  time: 0.5490  data_time: 0.0087  lr: 0.006194  max_mem: 3070M
[32m[04/15 11:34:20 d2.utils.events]: [0m eta: 0:12:28  iter: 639  total_loss: 0.704  loss_cls: 0.231  loss_box_reg: 0.393  loss_rpn_cls: 0.030  loss_rpn_loc: 0.078  time: 0.5488  data_time: 0.0082  lr: 0.006394  max_mem: 3070M
[32m[04/15 11:34:30 d2.utils.events]: [0m eta: 0:12:16  iter: 659  total_loss: 0.620  loss_cls: 0.187  loss_box_reg: 0.314  loss_rpn_cls: 0.023  loss_rpn_loc: 0.060  time: 0.5484  data_time: 0.0069  lr: 0.006593  max_mem: 3070M
[32m[04/15 11:34:42 d2.utils.events]: [0m eta: 0:12:05  iter: 679  total_loss: 0.718  loss_cls: 0.218  loss_box_reg: 0.394  loss_rpn_cls: 0.016  loss_rpn_loc: 0.074  time: 0.5485  data_time: 0.0063  lr: 0.006793  max_mem: 3070M
[32m[04/15 11:34:53 d2.utils.events]: [0m eta: 0:11:55  iter: 699  total_loss: 0.543  loss_cls: 0.158  loss_box_reg: 0.253  loss_rpn_cls: 0.016  loss_rpn_loc: 0.046  time: 0.5490  data_time: 0.0082  lr: 0.006993  max_mem: 3070M
[32m[04/15 11:35:04 d2.utils.events]: [0m eta: 0:11:45  iter: 719  total_loss: 0.717  loss_cls: 0.226  loss_box_reg: 0.401  loss_rpn_cls: 0.024  loss_rpn_loc: 0.067  time: 0.5495  data_time: 0.0069  lr: 0.007193  max_mem: 3070M
[32m[04/15 11:35:15 d2.utils.events]: [0m eta: 0:11:34  iter: 739  total_loss: 0.545  loss_cls: 0.187  loss_box_reg: 0.292  loss_rpn_cls: 0.016  loss_rpn_loc: 0.041  time: 0.5494  data_time: 0.0068  lr: 0.007393  max_mem: 3070M
[32m[04/15 11:35:27 d2.utils.events]: [0m eta: 0:11:23  iter: 759  total_loss: 0.691  loss_cls: 0.223  loss_box_reg: 0.386  loss_rpn_cls: 0.020  loss_rpn_loc: 0.051  time: 0.5499  data_time: 0.0087  lr: 0.007592  max_mem: 3070M
[32m[04/15 11:35:38 d2.utils.events]: [0m eta: 0:11:12  iter: 779  total_loss: 0.751  loss_cls: 0.239  loss_box_reg: 0.410  loss_rpn_cls: 0.020  loss_rpn_loc: 0.064  time: 0.5500  data_time: 0.0083  lr: 0.007792  max_mem: 3070M
[32m[04/15 11:35:49 d2.utils.events]: [0m eta: 0:11:01  iter: 799  total_loss: 0.809  loss_cls: 0.283  loss_box_reg: 0.463  loss_rpn_cls: 0.016  loss_rpn_loc: 0.065  time: 0.5502  data_time: 0.0068  lr: 0.007992  max_mem: 3070M
[32m[04/15 11:36:01 d2.utils.events]: [0m eta: 0:10:51  iter: 819  total_loss: 0.648  loss_cls: 0.216  loss_box_reg: 0.333  loss_rpn_cls: 0.015  loss_rpn_loc: 0.053  time: 0.5507  data_time: 0.0114  lr: 0.008192  max_mem: 3070M
[32m[04/15 11:36:11 d2.utils.events]: [0m eta: 0:10:39  iter: 839  total_loss: 0.615  loss_cls: 0.208  loss_box_reg: 0.326  loss_rpn_cls: 0.021  loss_rpn_loc: 0.054  time: 0.5503  data_time: 0.0076  lr: 0.008392  max_mem: 3070M
[32m[04/15 11:36:23 d2.utils.events]: [0m eta: 0:10:29  iter: 859  total_loss: 0.613  loss_cls: 0.204  loss_box_reg: 0.343  loss_rpn_cls: 0.020  loss_rpn_loc: 0.056  time: 0.5503  data_time: 0.0082  lr: 0.008591  max_mem: 3070M
[32m[04/15 11:36:34 d2.utils.events]: [0m eta: 0:10:18  iter: 879  total_loss: 0.743  loss_cls: 0.259  loss_box_reg: 0.363  loss_rpn_cls: 0.019  loss_rpn_loc: 0.066  time: 0.5506  data_time: 0.0071  lr: 0.008791  max_mem: 3070M
[32m[04/15 11:36:45 d2.utils.events]: [0m eta: 0:10:08  iter: 899  total_loss: 0.658  loss_cls: 0.214  loss_box_reg: 0.360  loss_rpn_cls: 0.019  loss_rpn_loc: 0.066  time: 0.5509  data_time: 0.0084  lr: 0.008991  max_mem: 3070M
[32m[04/15 11:36:56 d2.utils.events]: [0m eta: 0:09:57  iter: 919  total_loss: 0.638  loss_cls: 0.195  loss_box_reg: 0.340  loss_rpn_cls: 0.019  loss_rpn_loc: 0.060  time: 0.5510  data_time: 0.0064  lr: 0.009191  max_mem: 3070M
[32m[04/15 11:37:08 d2.utils.events]: [0m eta: 0:09:46  iter: 939  total_loss: 0.578  loss_cls: 0.177  loss_box_reg: 0.280  loss_rpn_cls: 0.022  loss_rpn_loc: 0.045  time: 0.5515  data_time: 0.0075  lr: 0.009391  max_mem: 3070M
[32m[04/15 11:37:19 d2.utils.events]: [0m eta: 0:09:35  iter: 959  total_loss: 0.731  loss_cls: 0.224  loss_box_reg: 0.361  loss_rpn_cls: 0.023  loss_rpn_loc: 0.082  time: 0.5517  data_time: 0.0087  lr: 0.009590  max_mem: 3070M
[32m[04/15 11:37:31 d2.utils.events]: [0m eta: 0:09:24  iter: 979  total_loss: 0.589  loss_cls: 0.185  loss_box_reg: 0.333  loss_rpn_cls: 0.018  loss_rpn_loc: 0.050  time: 0.5521  data_time: 0.0068  lr: 0.009790  max_mem: 3070M
[32m[04/15 11:37:42 d2.utils.events]: [0m eta: 0:09:14  iter: 999  total_loss: 0.701  loss_cls: 0.198  loss_box_reg: 0.331  loss_rpn_cls: 0.022  loss_rpn_loc: 0.062  time: 0.5525  data_time: 0.0087  lr: 0.009990  max_mem: 3070M
[32m[04/15 11:37:53 d2.utils.events]: [0m eta: 0:09:03  iter: 1019  total_loss: 0.640  loss_cls: 0.196  loss_box_reg: 0.358  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5524  data_time: 0.0097  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:38:05 d2.utils.events]: [0m eta: 0:08:52  iter: 1039  total_loss: 0.784  loss_cls: 0.234  loss_box_reg: 0.420  loss_rpn_cls: 0.029  loss_rpn_loc: 0.092  time: 0.5527  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:38:16 d2.utils.events]: [0m eta: 0:08:42  iter: 1059  total_loss: 0.734  loss_cls: 0.222  loss_box_reg: 0.400  loss_rpn_cls: 0.033  loss_rpn_loc: 0.070  time: 0.5531  data_time: 0.0062  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:38:28 d2.utils.events]: [0m eta: 0:08:31  iter: 1079  total_loss: 0.667  loss_cls: 0.204  loss_box_reg: 0.368  loss_rpn_cls: 0.018  loss_rpn_loc: 0.061  time: 0.5535  data_time: 0.0090  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:38:39 d2.utils.events]: [0m eta: 0:08:20  iter: 1099  total_loss: 0.585  loss_cls: 0.183  loss_box_reg: 0.317  loss_rpn_cls: 0.019  loss_rpn_loc: 0.050  time: 0.5533  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:38:50 d2.utils.events]: [0m eta: 0:08:10  iter: 1119  total_loss: 0.682  loss_cls: 0.230  loss_box_reg: 0.382  loss_rpn_cls: 0.016  loss_rpn_loc: 0.059  time: 0.5538  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:39:02 d2.utils.events]: [0m eta: 0:07:59  iter: 1139  total_loss: 0.725  loss_cls: 0.244  loss_box_reg: 0.363  loss_rpn_cls: 0.020  loss_rpn_loc: 0.068  time: 0.5538  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:39:13 d2.utils.events]: [0m eta: 0:07:48  iter: 1159  total_loss: 0.586  loss_cls: 0.204  loss_box_reg: 0.333  loss_rpn_cls: 0.019  loss_rpn_loc: 0.043  time: 0.5537  data_time: 0.0091  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:39:24 d2.utils.events]: [0m eta: 0:07:37  iter: 1179  total_loss: 0.783  loss_cls: 0.247  loss_box_reg: 0.442  loss_rpn_cls: 0.018  loss_rpn_loc: 0.076  time: 0.5538  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:39:35 d2.utils.events]: [0m eta: 0:07:26  iter: 1199  total_loss: 0.697  loss_cls: 0.205  loss_box_reg: 0.348  loss_rpn_cls: 0.017  loss_rpn_loc: 0.063  time: 0.5539  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:39:46 d2.utils.events]: [0m eta: 0:07:15  iter: 1219  total_loss: 0.654  loss_cls: 0.207  loss_box_reg: 0.346  loss_rpn_cls: 0.024  loss_rpn_loc: 0.060  time: 0.5538  data_time: 0.0090  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:39:58 d2.utils.events]: [0m eta: 0:07:04  iter: 1239  total_loss: 0.582  loss_cls: 0.170  loss_box_reg: 0.319  loss_rpn_cls: 0.020  loss_rpn_loc: 0.039  time: 0.5540  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:40:09 d2.utils.events]: [0m eta: 0:06:52  iter: 1259  total_loss: 0.653  loss_cls: 0.210  loss_box_reg: 0.350  loss_rpn_cls: 0.022  loss_rpn_loc: 0.076  time: 0.5540  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:40:20 d2.utils.events]: [0m eta: 0:06:41  iter: 1279  total_loss: 0.670  loss_cls: 0.218  loss_box_reg: 0.330  loss_rpn_cls: 0.033  loss_rpn_loc: 0.063  time: 0.5538  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:40:31 d2.utils.events]: [0m eta: 0:06:30  iter: 1299  total_loss: 0.599  loss_cls: 0.204  loss_box_reg: 0.329  loss_rpn_cls: 0.025  loss_rpn_loc: 0.050  time: 0.5538  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:40:42 d2.utils.events]: [0m eta: 0:06:19  iter: 1319  total_loss: 0.660  loss_cls: 0.199  loss_box_reg: 0.363  loss_rpn_cls: 0.019  loss_rpn_loc: 0.074  time: 0.5537  data_time: 0.0061  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:40:54 d2.utils.events]: [0m eta: 0:06:09  iter: 1339  total_loss: 0.741  loss_cls: 0.220  loss_box_reg: 0.406  loss_rpn_cls: 0.024  loss_rpn_loc: 0.053  time: 0.5542  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:41:05 d2.utils.events]: [0m eta: 0:05:58  iter: 1359  total_loss: 0.645  loss_cls: 0.218  loss_box_reg: 0.330  loss_rpn_cls: 0.021  loss_rpn_loc: 0.067  time: 0.5544  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:41:17 d2.utils.events]: [0m eta: 0:05:46  iter: 1379  total_loss: 0.639  loss_cls: 0.202  loss_box_reg: 0.315  loss_rpn_cls: 0.021  loss_rpn_loc: 0.049  time: 0.5545  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:41:27 d2.utils.events]: [0m eta: 0:05:35  iter: 1399  total_loss: 0.720  loss_cls: 0.248  loss_box_reg: 0.368  loss_rpn_cls: 0.021  loss_rpn_loc: 0.063  time: 0.5543  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:41:39 d2.utils.events]: [0m eta: 0:05:24  iter: 1419  total_loss: 0.662  loss_cls: 0.222  loss_box_reg: 0.370  loss_rpn_cls: 0.024  loss_rpn_loc: 0.063  time: 0.5542  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:41:50 d2.utils.events]: [0m eta: 0:05:13  iter: 1439  total_loss: 0.729  loss_cls: 0.227  loss_box_reg: 0.387  loss_rpn_cls: 0.023  loss_rpn_loc: 0.053  time: 0.5543  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:42:01 d2.utils.events]: [0m eta: 0:05:02  iter: 1459  total_loss: 0.688  loss_cls: 0.246  loss_box_reg: 0.352  loss_rpn_cls: 0.026  loss_rpn_loc: 0.069  time: 0.5543  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:42:12 d2.utils.events]: [0m eta: 0:04:51  iter: 1479  total_loss: 0.530  loss_cls: 0.188  loss_box_reg: 0.287  loss_rpn_cls: 0.026  loss_rpn_loc: 0.055  time: 0.5542  data_time: 0.0092  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:42:23 d2.utils.events]: [0m eta: 0:04:40  iter: 1499  total_loss: 0.640  loss_cls: 0.215  loss_box_reg: 0.311  loss_rpn_cls: 0.032  loss_rpn_loc: 0.056  time: 0.5543  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:42:34 d2.utils.events]: [0m eta: 0:04:28  iter: 1519  total_loss: 0.794  loss_cls: 0.260  loss_box_reg: 0.420  loss_rpn_cls: 0.025  loss_rpn_loc: 0.069  time: 0.5543  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:42:46 d2.utils.events]: [0m eta: 0:04:17  iter: 1539  total_loss: 0.760  loss_cls: 0.229  loss_box_reg: 0.410  loss_rpn_cls: 0.032  loss_rpn_loc: 0.077  time: 0.5546  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:42:58 d2.utils.events]: [0m eta: 0:04:06  iter: 1559  total_loss: 0.662  loss_cls: 0.210  loss_box_reg: 0.370  loss_rpn_cls: 0.021  loss_rpn_loc: 0.063  time: 0.5549  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:43:09 d2.utils.events]: [0m eta: 0:03:55  iter: 1579  total_loss: 0.576  loss_cls: 0.203  loss_box_reg: 0.315  loss_rpn_cls: 0.024  loss_rpn_loc: 0.042  time: 0.5549  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:43:21 d2.utils.events]: [0m eta: 0:03:44  iter: 1599  total_loss: 0.815  loss_cls: 0.281  loss_box_reg: 0.397  loss_rpn_cls: 0.029  loss_rpn_loc: 0.096  time: 0.5552  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:43:32 d2.utils.events]: [0m eta: 0:03:33  iter: 1619  total_loss: 0.733  loss_cls: 0.269  loss_box_reg: 0.391  loss_rpn_cls: 0.019  loss_rpn_loc: 0.065  time: 0.5551  data_time: 0.0101  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:43:43 d2.utils.events]: [0m eta: 0:03:22  iter: 1639  total_loss: 0.757  loss_cls: 0.248  loss_box_reg: 0.414  loss_rpn_cls: 0.019  loss_rpn_loc: 0.062  time: 0.5555  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:43:55 d2.utils.events]: [0m eta: 0:03:11  iter: 1659  total_loss: 0.556  loss_cls: 0.191  loss_box_reg: 0.279  loss_rpn_cls: 0.030  loss_rpn_loc: 0.038  time: 0.5554  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:44:07 d2.utils.events]: [0m eta: 0:03:00  iter: 1679  total_loss: 0.781  loss_cls: 0.238  loss_box_reg: 0.411  loss_rpn_cls: 0.022  loss_rpn_loc: 0.075  time: 0.5563  data_time: 0.1154  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:44:19 d2.utils.events]: [0m eta: 0:02:48  iter: 1699  total_loss: 0.779  loss_cls: 0.258  loss_box_reg: 0.427  loss_rpn_cls: 0.033  loss_rpn_loc: 0.070  time: 0.5563  data_time: 0.0091  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:44:30 d2.utils.events]: [0m eta: 0:02:37  iter: 1719  total_loss: 0.613  loss_cls: 0.203  loss_box_reg: 0.343  loss_rpn_cls: 0.030  loss_rpn_loc: 0.053  time: 0.5562  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:44:41 d2.utils.events]: [0m eta: 0:02:26  iter: 1739  total_loss: 0.667  loss_cls: 0.210  loss_box_reg: 0.350  loss_rpn_cls: 0.024  loss_rpn_loc: 0.068  time: 0.5564  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:44:52 d2.utils.events]: [0m eta: 0:02:15  iter: 1759  total_loss: 0.590  loss_cls: 0.190  loss_box_reg: 0.268  loss_rpn_cls: 0.023  loss_rpn_loc: 0.054  time: 0.5561  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:45:04 d2.utils.events]: [0m eta: 0:02:04  iter: 1779  total_loss: 0.514  loss_cls: 0.177  loss_box_reg: 0.285  loss_rpn_cls: 0.028  loss_rpn_loc: 0.034  time: 0.5563  data_time: 0.0093  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:45:15 d2.utils.events]: [0m eta: 0:01:52  iter: 1799  total_loss: 0.616  loss_cls: 0.204  loss_box_reg: 0.350  loss_rpn_cls: 0.026  loss_rpn_loc: 0.059  time: 0.5564  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:45:26 d2.utils.events]: [0m eta: 0:01:41  iter: 1819  total_loss: 0.664  loss_cls: 0.206  loss_box_reg: 0.334  loss_rpn_cls: 0.026  loss_rpn_loc: 0.068  time: 0.5565  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:45:38 d2.utils.events]: [0m eta: 0:01:30  iter: 1839  total_loss: 0.786  loss_cls: 0.252  loss_box_reg: 0.365  loss_rpn_cls: 0.026  loss_rpn_loc: 0.084  time: 0.5566  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:45:49 d2.utils.events]: [0m eta: 0:01:19  iter: 1859  total_loss: 0.780  loss_cls: 0.236  loss_box_reg: 0.410  loss_rpn_cls: 0.028  loss_rpn_loc: 0.072  time: 0.5565  data_time: 0.0087  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:46:00 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.709  loss_cls: 0.229  loss_box_reg: 0.358  loss_rpn_cls: 0.027  loss_rpn_loc: 0.060  time: 0.5563  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:46:12 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.729  loss_cls: 0.238  loss_box_reg: 0.391  loss_rpn_cls: 0.025  loss_rpn_loc: 0.064  time: 0.5561  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:46:26 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.594  loss_cls: 0.183  loss_box_reg: 0.313  loss_rpn_cls: 0.023  loss_rpn_loc: 0.050  time: 0.5559  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:46:37 d2.utils.events]: [0m eta: 0:00:34  iter: 1939  total_loss: 0.730  loss_cls: 0.250  loss_box_reg: 0.393  loss_rpn_cls: 0.028  loss_rpn_loc: 0.074  time: 0.5560  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:46:49 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.669  loss_cls: 0.226  loss_box_reg: 0.323  loss_rpn_cls: 0.034  loss_rpn_loc: 0.061  time: 0.5560  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:47:00 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.731  loss_cls: 0.239  loss_box_reg: 0.378  loss_rpn_cls: 0.026  loss_rpn_loc: 0.063  time: 0.5559  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 11:47:53 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:47:53 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 11:47:53 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 11:47:53 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.659  loss_cls: 0.201  loss_box_reg: 0.368  loss_rpn_cls: 0.023  loss_rpn_loc: 0.073  time: 0.5557  data_time: 0.0093  lr: 0.010000  max_mem: 3070M
[32m[04/15 11:47:59 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:30 (0.5560 s / it)
[32m[04/15 11:47:59 d2.engine.hooks]: [0mTotal training time: 0:19:32 (0:01:02 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 11:48:27 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:48:27 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 11:48:27 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 11:48:29 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1232 s / img. ETA=0:02:37
[32m[04/15 11:48:34 d2.evaluation.evaluator]: [0mInference done 50/1257. 0.1250 s / img. ETA=0:02:35
[32m[04/15 11:48:39 d2.evaluation.evaluator]: [0mInference done 89/1257. 0.1251 s / img. ETA=0:02:31
[32m[04/15 11:48:44 d2.evaluation.evaluator]: [0mInference done 129/1257. 0.1240 s / img. ETA=0:02:24
[32m[04/15 11:48:49 d2.evaluation.evaluator]: [0mInference done 169/1257. 0.1235 s / img. ETA=0:02:18
[32m[04/15 11:48:54 d2.evaluation.evaluator]: [0mInference done 209/1257. 0.1236 s / img. ETA=0:02:13
[32m[04/15 11:49:00 d2.evaluation.evaluator]: [0mInference done 250/1257. 0.1234 s / img. ETA=0:02:07
[32m[04/15 11:49:05 d2.evaluation.evaluator]: [0mInference done 288/1257. 0.1235 s / img. ETA=0:02:03
[32m[04/15 11:49:10 d2.evaluation.evaluator]: [0mInference done 327/1257. 0.1238 s / img. ETA=0:01:59
[32m[04/15 11:49:15 d2.evaluation.evaluator]: [0mInference done 366/1257. 0.1241 s / img. ETA=0:01:54
[32m[04/15 11:49:20 d2.evaluation.evaluator]: [0mInference done 407/1257. 0.1238 s / img. ETA=0:01:48
[32m[04/15 11:49:25 d2.evaluation.evaluator]: [0mInference done 447/1257. 0.1239 s / img. ETA=0:01:43
[32m[04/15 11:49:30 d2.evaluation.evaluator]: [0mInference done 487/1257. 0.1238 s / img. ETA=0:01:38
[32m[04/15 11:49:35 d2.evaluation.evaluator]: [0mInference done 527/1257. 0.1237 s / img. ETA=0:01:33
[32m[04/15 11:49:40 d2.evaluation.evaluator]: [0mInference done 568/1257. 0.1236 s / img. ETA=0:01:27
[32m[04/15 11:49:45 d2.evaluation.evaluator]: [0mInference done 608/1257. 0.1235 s / img. ETA=0:01:22
[32m[04/15 11:49:50 d2.evaluation.evaluator]: [0mInference done 649/1257. 0.1234 s / img. ETA=0:01:17
[32m[04/15 11:49:55 d2.evaluation.evaluator]: [0mInference done 689/1257. 0.1233 s / img. ETA=0:01:12
[32m[04/15 11:50:00 d2.evaluation.evaluator]: [0mInference done 729/1257. 0.1232 s / img. ETA=0:01:07
[32m[04/15 11:50:06 d2.evaluation.evaluator]: [0mInference done 769/1257. 0.1232 s / img. ETA=0:01:02
[32m[04/15 11:50:11 d2.evaluation.evaluator]: [0mInference done 809/1257. 0.1232 s / img. ETA=0:00:56
[32m[04/15 11:50:16 d2.evaluation.evaluator]: [0mInference done 848/1257. 0.1233 s / img. ETA=0:00:52
[32m[04/15 11:50:21 d2.evaluation.evaluator]: [0mInference done 888/1257. 0.1234 s / img. ETA=0:00:46
[32m[04/15 11:50:26 d2.evaluation.evaluator]: [0mInference done 928/1257. 0.1234 s / img. ETA=0:00:41
[32m[04/15 11:50:31 d2.evaluation.evaluator]: [0mInference done 968/1257. 0.1234 s / img. ETA=0:00:36
[32m[04/15 11:50:36 d2.evaluation.evaluator]: [0mInference done 1008/1257. 0.1234 s / img. ETA=0:00:31
[32m[04/15 11:50:41 d2.evaluation.evaluator]: [0mInference done 1048/1257. 0.1234 s / img. ETA=0:00:26
[32m[04/15 11:50:46 d2.evaluation.evaluator]: [0mInference done 1086/1257. 0.1236 s / img. ETA=0:00:21
[32m[04/15 11:50:51 d2.evaluation.evaluator]: [0mInference done 1125/1257. 0.1236 s / img. ETA=0:00:16
[32m[04/15 11:50:56 d2.evaluation.evaluator]: [0mInference done 1164/1257. 0.1236 s / img. ETA=0:00:11
[32m[04/15 11:51:01 d2.evaluation.evaluator]: [0mInference done 1204/1257. 0.1237 s / img. ETA=0:00:06
[32m[04/15 11:51:06 d2.evaluation.evaluator]: [0mInference done 1242/1257. 0.1238 s / img. ETA=0:00:01
[32m[04/15 11:51:08 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:39.797280 (0.127634 s / img per device, on 1 devices)
[32m[04/15 11:51:08 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:34 (0.123796 s / img per device, on 1 devices)
[32m[04/15 11:51:08 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 11:51:08 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 11:51:08 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.03s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.06s).
Accumulating evaluation results...
DONE (t=0.83s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.178
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.347
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.161
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.107
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.356
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.096
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.223
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.150
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.398
[32m[04/15 11:51:15 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 17.797 | 34.711 | 16.051 | 10.746 | 21.941 | 35.641 |
[32m[04/15 11:51:15 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 23.327 | bicycle       | 7.030 | car            | 40.500 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.330 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  17  *  2000  iterations ============
4 channel input
[32m[04/15 11:51:16 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 11:51:18 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.49 seconds.
[5m[31mWARNING[0m [32m[04/15 11:51:18 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:51:18 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 11:51:18 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 11:51:18 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 11:51:18 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 11:51:19 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 11:51:30 d2.utils.events]: [0m eta: 0:18:33  iter: 19  total_loss: 0.731  loss_cls: 0.227  loss_box_reg: 0.407  loss_rpn_cls: 0.019  loss_rpn_loc: 0.056  time: 0.5611  data_time: 0.0402  lr: 0.000200  max_mem: 3070M
[32m[04/15 11:51:41 d2.utils.events]: [0m eta: 0:18:02  iter: 39  total_loss: 0.572  loss_cls: 0.210  loss_box_reg: 0.318  loss_rpn_cls: 0.021  loss_rpn_loc: 0.056  time: 0.5436  data_time: 0.0066  lr: 0.000400  max_mem: 3070M
[32m[04/15 11:51:52 d2.utils.events]: [0m eta: 0:17:35  iter: 59  total_loss: 0.746  loss_cls: 0.240  loss_box_reg: 0.399  loss_rpn_cls: 0.029  loss_rpn_loc: 0.080  time: 0.5424  data_time: 0.0082  lr: 0.000599  max_mem: 3070M
[32m[04/15 11:52:03 d2.utils.events]: [0m eta: 0:17:25  iter: 79  total_loss: 0.695  loss_cls: 0.218  loss_box_reg: 0.356  loss_rpn_cls: 0.024  loss_rpn_loc: 0.057  time: 0.5440  data_time: 0.0085  lr: 0.000799  max_mem: 3070M
[32m[04/15 11:52:14 d2.utils.events]: [0m eta: 0:17:23  iter: 99  total_loss: 0.711  loss_cls: 0.232  loss_box_reg: 0.394  loss_rpn_cls: 0.020  loss_rpn_loc: 0.074  time: 0.5496  data_time: 0.0066  lr: 0.000999  max_mem: 3070M
[32m[04/15 11:52:26 d2.utils.events]: [0m eta: 0:17:19  iter: 119  total_loss: 0.601  loss_cls: 0.192  loss_box_reg: 0.329  loss_rpn_cls: 0.017  loss_rpn_loc: 0.057  time: 0.5514  data_time: 0.0077  lr: 0.001199  max_mem: 3070M
[32m[04/15 11:52:37 d2.utils.events]: [0m eta: 0:17:05  iter: 139  total_loss: 0.688  loss_cls: 0.228  loss_box_reg: 0.380  loss_rpn_cls: 0.024  loss_rpn_loc: 0.057  time: 0.5495  data_time: 0.0076  lr: 0.001399  max_mem: 3070M
[32m[04/15 11:52:47 d2.utils.events]: [0m eta: 0:16:53  iter: 159  total_loss: 0.649  loss_cls: 0.214  loss_box_reg: 0.351  loss_rpn_cls: 0.024  loss_rpn_loc: 0.064  time: 0.5484  data_time: 0.0064  lr: 0.001598  max_mem: 3070M
[32m[04/15 11:52:59 d2.utils.events]: [0m eta: 0:16:44  iter: 179  total_loss: 0.740  loss_cls: 0.217  loss_box_reg: 0.384  loss_rpn_cls: 0.024  loss_rpn_loc: 0.082  time: 0.5497  data_time: 0.0082  lr: 0.001798  max_mem: 3070M
[32m[04/15 11:53:10 d2.utils.events]: [0m eta: 0:16:35  iter: 199  total_loss: 0.652  loss_cls: 0.208  loss_box_reg: 0.336  loss_rpn_cls: 0.014  loss_rpn_loc: 0.043  time: 0.5505  data_time: 0.0083  lr: 0.001998  max_mem: 3070M
[32m[04/15 11:53:21 d2.utils.events]: [0m eta: 0:16:26  iter: 219  total_loss: 0.680  loss_cls: 0.214  loss_box_reg: 0.383  loss_rpn_cls: 0.017  loss_rpn_loc: 0.072  time: 0.5518  data_time: 0.0081  lr: 0.002198  max_mem: 3070M
[32m[04/15 11:53:32 d2.utils.events]: [0m eta: 0:16:15  iter: 239  total_loss: 0.565  loss_cls: 0.183  loss_box_reg: 0.308  loss_rpn_cls: 0.014  loss_rpn_loc: 0.057  time: 0.5515  data_time: 0.0070  lr: 0.002398  max_mem: 3070M
[32m[04/15 11:53:44 d2.utils.events]: [0m eta: 0:16:05  iter: 259  total_loss: 0.593  loss_cls: 0.194  loss_box_reg: 0.328  loss_rpn_cls: 0.012  loss_rpn_loc: 0.058  time: 0.5520  data_time: 0.0112  lr: 0.002597  max_mem: 3070M
[32m[04/15 11:53:55 d2.utils.events]: [0m eta: 0:15:56  iter: 279  total_loss: 0.571  loss_cls: 0.167  loss_box_reg: 0.343  loss_rpn_cls: 0.010  loss_rpn_loc: 0.047  time: 0.5526  data_time: 0.0086  lr: 0.002797  max_mem: 3070M
[32m[04/15 11:54:06 d2.utils.events]: [0m eta: 0:15:44  iter: 299  total_loss: 0.595  loss_cls: 0.198  loss_box_reg: 0.327  loss_rpn_cls: 0.014  loss_rpn_loc: 0.033  time: 0.5526  data_time: 0.0084  lr: 0.002997  max_mem: 3070M
[32m[04/15 11:54:17 d2.utils.events]: [0m eta: 0:15:33  iter: 319  total_loss: 0.635  loss_cls: 0.196  loss_box_reg: 0.367  loss_rpn_cls: 0.016  loss_rpn_loc: 0.042  time: 0.5529  data_time: 0.0071  lr: 0.003197  max_mem: 3070M
[32m[04/15 11:54:29 d2.utils.events]: [0m eta: 0:15:23  iter: 339  total_loss: 0.681  loss_cls: 0.216  loss_box_reg: 0.385  loss_rpn_cls: 0.019  loss_rpn_loc: 0.072  time: 0.5537  data_time: 0.0076  lr: 0.003397  max_mem: 3070M
[32m[04/15 11:54:40 d2.utils.events]: [0m eta: 0:15:10  iter: 359  total_loss: 0.568  loss_cls: 0.185  loss_box_reg: 0.313  loss_rpn_cls: 0.015  loss_rpn_loc: 0.057  time: 0.5532  data_time: 0.0075  lr: 0.003596  max_mem: 3070M
[32m[04/15 11:54:51 d2.utils.events]: [0m eta: 0:15:00  iter: 379  total_loss: 0.710  loss_cls: 0.208  loss_box_reg: 0.384  loss_rpn_cls: 0.020  loss_rpn_loc: 0.074  time: 0.5538  data_time: 0.0130  lr: 0.003796  max_mem: 3070M
[32m[04/15 11:55:03 d2.utils.events]: [0m eta: 0:14:49  iter: 399  total_loss: 0.542  loss_cls: 0.184  loss_box_reg: 0.279  loss_rpn_cls: 0.012  loss_rpn_loc: 0.053  time: 0.5546  data_time: 0.0090  lr: 0.003996  max_mem: 3070M
[32m[04/15 11:55:14 d2.utils.events]: [0m eta: 0:14:38  iter: 419  total_loss: 0.613  loss_cls: 0.184  loss_box_reg: 0.337  loss_rpn_cls: 0.014  loss_rpn_loc: 0.045  time: 0.5546  data_time: 0.0107  lr: 0.004196  max_mem: 3070M
[32m[04/15 11:55:25 d2.utils.events]: [0m eta: 0:14:27  iter: 439  total_loss: 0.589  loss_cls: 0.171  loss_box_reg: 0.341  loss_rpn_cls: 0.019  loss_rpn_loc: 0.047  time: 0.5542  data_time: 0.0067  lr: 0.004396  max_mem: 3070M
[32m[04/15 11:55:36 d2.utils.events]: [0m eta: 0:14:15  iter: 459  total_loss: 0.637  loss_cls: 0.220  loss_box_reg: 0.331  loss_rpn_cls: 0.013  loss_rpn_loc: 0.052  time: 0.5544  data_time: 0.0096  lr: 0.004595  max_mem: 3070M
[32m[04/15 11:55:47 d2.utils.events]: [0m eta: 0:14:04  iter: 479  total_loss: 0.588  loss_cls: 0.180  loss_box_reg: 0.335  loss_rpn_cls: 0.017  loss_rpn_loc: 0.062  time: 0.5546  data_time: 0.0077  lr: 0.004795  max_mem: 3070M
[32m[04/15 11:55:59 d2.utils.events]: [0m eta: 0:13:53  iter: 499  total_loss: 0.474  loss_cls: 0.156  loss_box_reg: 0.268  loss_rpn_cls: 0.010  loss_rpn_loc: 0.037  time: 0.5549  data_time: 0.0063  lr: 0.004995  max_mem: 3070M
[32m[04/15 11:56:10 d2.utils.events]: [0m eta: 0:13:42  iter: 519  total_loss: 0.546  loss_cls: 0.179  loss_box_reg: 0.323  loss_rpn_cls: 0.015  loss_rpn_loc: 0.051  time: 0.5546  data_time: 0.0076  lr: 0.005195  max_mem: 3070M
[32m[04/15 11:56:21 d2.utils.events]: [0m eta: 0:13:30  iter: 539  total_loss: 0.525  loss_cls: 0.150  loss_box_reg: 0.288  loss_rpn_cls: 0.014  loss_rpn_loc: 0.063  time: 0.5541  data_time: 0.0083  lr: 0.005395  max_mem: 3070M
[32m[04/15 11:56:32 d2.utils.events]: [0m eta: 0:13:20  iter: 559  total_loss: 0.591  loss_cls: 0.198  loss_box_reg: 0.324  loss_rpn_cls: 0.014  loss_rpn_loc: 0.040  time: 0.5546  data_time: 0.0080  lr: 0.005594  max_mem: 3070M
[32m[04/15 11:56:44 d2.utils.events]: [0m eta: 0:13:09  iter: 579  total_loss: 0.626  loss_cls: 0.204  loss_box_reg: 0.358  loss_rpn_cls: 0.012  loss_rpn_loc: 0.052  time: 0.5552  data_time: 0.0067  lr: 0.005794  max_mem: 3070M
[32m[04/15 11:56:55 d2.utils.events]: [0m eta: 0:12:58  iter: 599  total_loss: 0.580  loss_cls: 0.212  loss_box_reg: 0.324  loss_rpn_cls: 0.013  loss_rpn_loc: 0.037  time: 0.5550  data_time: 0.0082  lr: 0.005994  max_mem: 3070M
[32m[04/15 11:57:06 d2.utils.events]: [0m eta: 0:12:47  iter: 619  total_loss: 0.539  loss_cls: 0.175  loss_box_reg: 0.289  loss_rpn_cls: 0.027  loss_rpn_loc: 0.044  time: 0.5550  data_time: 0.0069  lr: 0.006194  max_mem: 3070M
[32m[04/15 11:57:17 d2.utils.events]: [0m eta: 0:12:36  iter: 639  total_loss: 0.609  loss_cls: 0.191  loss_box_reg: 0.350  loss_rpn_cls: 0.013  loss_rpn_loc: 0.055  time: 0.5547  data_time: 0.0080  lr: 0.006394  max_mem: 3070M
[32m[04/15 11:57:28 d2.utils.events]: [0m eta: 0:12:25  iter: 659  total_loss: 0.660  loss_cls: 0.212  loss_box_reg: 0.381  loss_rpn_cls: 0.016  loss_rpn_loc: 0.069  time: 0.5548  data_time: 0.0085  lr: 0.006593  max_mem: 3070M
[32m[04/15 11:57:40 d2.utils.events]: [0m eta: 0:12:14  iter: 679  total_loss: 0.633  loss_cls: 0.214  loss_box_reg: 0.364  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5551  data_time: 0.0091  lr: 0.006793  max_mem: 3070M
[32m[04/15 11:57:51 d2.utils.events]: [0m eta: 0:12:03  iter: 699  total_loss: 0.708  loss_cls: 0.204  loss_box_reg: 0.433  loss_rpn_cls: 0.020  loss_rpn_loc: 0.062  time: 0.5551  data_time: 0.0071  lr: 0.006993  max_mem: 3070M
[32m[04/15 11:58:02 d2.utils.events]: [0m eta: 0:11:52  iter: 719  total_loss: 0.590  loss_cls: 0.203  loss_box_reg: 0.324  loss_rpn_cls: 0.018  loss_rpn_loc: 0.039  time: 0.5551  data_time: 0.0088  lr: 0.007193  max_mem: 3070M
[32m[04/15 11:58:13 d2.utils.events]: [0m eta: 0:11:41  iter: 739  total_loss: 0.667  loss_cls: 0.214  loss_box_reg: 0.365  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5551  data_time: 0.0074  lr: 0.007393  max_mem: 3070M
[32m[04/15 11:58:24 d2.utils.events]: [0m eta: 0:11:30  iter: 759  total_loss: 0.672  loss_cls: 0.225  loss_box_reg: 0.385  loss_rpn_cls: 0.031  loss_rpn_loc: 0.065  time: 0.5550  data_time: 0.0074  lr: 0.007592  max_mem: 3070M
[32m[04/15 11:58:36 d2.utils.events]: [0m eta: 0:11:19  iter: 779  total_loss: 0.560  loss_cls: 0.190  loss_box_reg: 0.305  loss_rpn_cls: 0.020  loss_rpn_loc: 0.046  time: 0.5549  data_time: 0.0067  lr: 0.007792  max_mem: 3070M
[32m[04/15 11:58:47 d2.utils.events]: [0m eta: 0:11:08  iter: 799  total_loss: 0.526  loss_cls: 0.153  loss_box_reg: 0.313  loss_rpn_cls: 0.018  loss_rpn_loc: 0.046  time: 0.5551  data_time: 0.0090  lr: 0.007992  max_mem: 3070M
[32m[04/15 11:58:58 d2.utils.events]: [0m eta: 0:10:56  iter: 819  total_loss: 0.709  loss_cls: 0.215  loss_box_reg: 0.372  loss_rpn_cls: 0.021  loss_rpn_loc: 0.063  time: 0.5550  data_time: 0.0078  lr: 0.008192  max_mem: 3070M
[32m[04/15 11:59:09 d2.utils.events]: [0m eta: 0:10:45  iter: 839  total_loss: 0.796  loss_cls: 0.254  loss_box_reg: 0.430  loss_rpn_cls: 0.031  loss_rpn_loc: 0.066  time: 0.5549  data_time: 0.0069  lr: 0.008392  max_mem: 3070M
[32m[04/15 11:59:21 d2.utils.events]: [0m eta: 0:10:35  iter: 859  total_loss: 0.568  loss_cls: 0.189  loss_box_reg: 0.304  loss_rpn_cls: 0.021  loss_rpn_loc: 0.063  time: 0.5552  data_time: 0.0084  lr: 0.008591  max_mem: 3070M
[32m[04/15 11:59:32 d2.utils.events]: [0m eta: 0:10:23  iter: 879  total_loss: 0.650  loss_cls: 0.207  loss_box_reg: 0.357  loss_rpn_cls: 0.019  loss_rpn_loc: 0.055  time: 0.5551  data_time: 0.0100  lr: 0.008791  max_mem: 3070M
[32m[04/15 11:59:43 d2.utils.events]: [0m eta: 0:10:13  iter: 899  total_loss: 0.739  loss_cls: 0.223  loss_box_reg: 0.415  loss_rpn_cls: 0.021  loss_rpn_loc: 0.061  time: 0.5557  data_time: 0.0076  lr: 0.008991  max_mem: 3070M
[32m[04/15 11:59:55 d2.utils.events]: [0m eta: 0:10:02  iter: 919  total_loss: 0.669  loss_cls: 0.195  loss_box_reg: 0.343  loss_rpn_cls: 0.019  loss_rpn_loc: 0.051  time: 0.5556  data_time: 0.0072  lr: 0.009191  max_mem: 3070M
[32m[04/15 12:00:08 d2.utils.events]: [0m eta: 0:09:50  iter: 939  total_loss: 0.638  loss_cls: 0.192  loss_box_reg: 0.369  loss_rpn_cls: 0.015  loss_rpn_loc: 0.053  time: 0.5574  data_time: 0.1465  lr: 0.009391  max_mem: 3070M
[32m[04/15 12:00:19 d2.utils.events]: [0m eta: 0:09:39  iter: 959  total_loss: 0.733  loss_cls: 0.232  loss_box_reg: 0.407  loss_rpn_cls: 0.021  loss_rpn_loc: 0.073  time: 0.5573  data_time: 0.0088  lr: 0.009590  max_mem: 3070M
[32m[04/15 12:00:30 d2.utils.events]: [0m eta: 0:09:28  iter: 979  total_loss: 0.554  loss_cls: 0.185  loss_box_reg: 0.289  loss_rpn_cls: 0.020  loss_rpn_loc: 0.051  time: 0.5574  data_time: 0.0069  lr: 0.009790  max_mem: 3070M
[32m[04/15 12:00:41 d2.utils.events]: [0m eta: 0:09:17  iter: 999  total_loss: 0.690  loss_cls: 0.215  loss_box_reg: 0.360  loss_rpn_cls: 0.021  loss_rpn_loc: 0.059  time: 0.5575  data_time: 0.0091  lr: 0.009990  max_mem: 3070M
[32m[04/15 12:00:53 d2.utils.events]: [0m eta: 0:09:05  iter: 1019  total_loss: 0.631  loss_cls: 0.209  loss_box_reg: 0.316  loss_rpn_cls: 0.020  loss_rpn_loc: 0.053  time: 0.5573  data_time: 0.0098  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:01:04 d2.utils.events]: [0m eta: 0:08:54  iter: 1039  total_loss: 0.743  loss_cls: 0.241  loss_box_reg: 0.381  loss_rpn_cls: 0.026  loss_rpn_loc: 0.066  time: 0.5571  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:01:14 d2.utils.events]: [0m eta: 0:08:44  iter: 1059  total_loss: 0.674  loss_cls: 0.214  loss_box_reg: 0.365  loss_rpn_cls: 0.020  loss_rpn_loc: 0.067  time: 0.5567  data_time: 0.0093  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:01:26 d2.utils.events]: [0m eta: 0:08:33  iter: 1079  total_loss: 0.672  loss_cls: 0.212  loss_box_reg: 0.357  loss_rpn_cls: 0.027  loss_rpn_loc: 0.067  time: 0.5567  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:01:37 d2.utils.events]: [0m eta: 0:08:22  iter: 1099  total_loss: 0.721  loss_cls: 0.224  loss_box_reg: 0.387  loss_rpn_cls: 0.024  loss_rpn_loc: 0.065  time: 0.5568  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:01:48 d2.utils.events]: [0m eta: 0:08:10  iter: 1119  total_loss: 0.557  loss_cls: 0.189  loss_box_reg: 0.308  loss_rpn_cls: 0.021  loss_rpn_loc: 0.059  time: 0.5568  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:01:59 d2.utils.events]: [0m eta: 0:07:59  iter: 1139  total_loss: 0.734  loss_cls: 0.226  loss_box_reg: 0.416  loss_rpn_cls: 0.021  loss_rpn_loc: 0.057  time: 0.5567  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:02:11 d2.utils.events]: [0m eta: 0:07:48  iter: 1159  total_loss: 0.675  loss_cls: 0.220  loss_box_reg: 0.317  loss_rpn_cls: 0.020  loss_rpn_loc: 0.067  time: 0.5567  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:02:22 d2.utils.events]: [0m eta: 0:07:37  iter: 1179  total_loss: 0.722  loss_cls: 0.230  loss_box_reg: 0.425  loss_rpn_cls: 0.022  loss_rpn_loc: 0.061  time: 0.5570  data_time: 0.0096  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:02:33 d2.utils.events]: [0m eta: 0:07:26  iter: 1199  total_loss: 0.682  loss_cls: 0.218  loss_box_reg: 0.367  loss_rpn_cls: 0.024  loss_rpn_loc: 0.063  time: 0.5571  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:02:45 d2.utils.events]: [0m eta: 0:07:15  iter: 1219  total_loss: 0.725  loss_cls: 0.228  loss_box_reg: 0.370  loss_rpn_cls: 0.020  loss_rpn_loc: 0.074  time: 0.5576  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:02:57 d2.utils.events]: [0m eta: 0:07:04  iter: 1239  total_loss: 0.695  loss_cls: 0.219  loss_box_reg: 0.354  loss_rpn_cls: 0.026  loss_rpn_loc: 0.055  time: 0.5576  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:03:08 d2.utils.events]: [0m eta: 0:06:53  iter: 1259  total_loss: 0.689  loss_cls: 0.195  loss_box_reg: 0.339  loss_rpn_cls: 0.025  loss_rpn_loc: 0.063  time: 0.5576  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:03:19 d2.utils.events]: [0m eta: 0:06:42  iter: 1279  total_loss: 0.617  loss_cls: 0.214  loss_box_reg: 0.348  loss_rpn_cls: 0.029  loss_rpn_loc: 0.067  time: 0.5576  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:03:30 d2.utils.events]: [0m eta: 0:06:31  iter: 1299  total_loss: 0.712  loss_cls: 0.241  loss_box_reg: 0.410  loss_rpn_cls: 0.015  loss_rpn_loc: 0.062  time: 0.5576  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:03:41 d2.utils.events]: [0m eta: 0:06:20  iter: 1319  total_loss: 0.618  loss_cls: 0.199  loss_box_reg: 0.331  loss_rpn_cls: 0.021  loss_rpn_loc: 0.052  time: 0.5574  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:03:53 d2.utils.events]: [0m eta: 0:06:08  iter: 1339  total_loss: 0.686  loss_cls: 0.200  loss_box_reg: 0.378  loss_rpn_cls: 0.022  loss_rpn_loc: 0.071  time: 0.5573  data_time: 0.0095  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:04:04 d2.utils.events]: [0m eta: 0:05:57  iter: 1359  total_loss: 0.565  loss_cls: 0.201  loss_box_reg: 0.300  loss_rpn_cls: 0.020  loss_rpn_loc: 0.050  time: 0.5571  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:04:15 d2.utils.events]: [0m eta: 0:05:46  iter: 1379  total_loss: 0.711  loss_cls: 0.236  loss_box_reg: 0.376  loss_rpn_cls: 0.023  loss_rpn_loc: 0.060  time: 0.5572  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:04:26 d2.utils.events]: [0m eta: 0:05:35  iter: 1399  total_loss: 0.564  loss_cls: 0.201  loss_box_reg: 0.306  loss_rpn_cls: 0.022  loss_rpn_loc: 0.058  time: 0.5574  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:04:37 d2.utils.events]: [0m eta: 0:05:23  iter: 1419  total_loss: 0.606  loss_cls: 0.192  loss_box_reg: 0.332  loss_rpn_cls: 0.022  loss_rpn_loc: 0.076  time: 0.5571  data_time: 0.0106  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:04:48 d2.utils.events]: [0m eta: 0:05:12  iter: 1439  total_loss: 0.582  loss_cls: 0.181  loss_box_reg: 0.276  loss_rpn_cls: 0.026  loss_rpn_loc: 0.049  time: 0.5569  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:04:59 d2.utils.events]: [0m eta: 0:05:01  iter: 1459  total_loss: 0.672  loss_cls: 0.229  loss_box_reg: 0.368  loss_rpn_cls: 0.022  loss_rpn_loc: 0.069  time: 0.5569  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:05:10 d2.utils.events]: [0m eta: 0:04:50  iter: 1479  total_loss: 0.655  loss_cls: 0.208  loss_box_reg: 0.348  loss_rpn_cls: 0.021  loss_rpn_loc: 0.055  time: 0.5568  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:05:22 d2.utils.events]: [0m eta: 0:04:39  iter: 1499  total_loss: 0.585  loss_cls: 0.176  loss_box_reg: 0.304  loss_rpn_cls: 0.021  loss_rpn_loc: 0.057  time: 0.5569  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:05:33 d2.utils.events]: [0m eta: 0:04:28  iter: 1519  total_loss: 0.693  loss_cls: 0.247  loss_box_reg: 0.358  loss_rpn_cls: 0.037  loss_rpn_loc: 0.068  time: 0.5568  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:05:44 d2.utils.events]: [0m eta: 0:04:17  iter: 1539  total_loss: 0.591  loss_cls: 0.175  loss_box_reg: 0.347  loss_rpn_cls: 0.020  loss_rpn_loc: 0.053  time: 0.5568  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:05:55 d2.utils.events]: [0m eta: 0:04:05  iter: 1559  total_loss: 0.628  loss_cls: 0.203  loss_box_reg: 0.326  loss_rpn_cls: 0.029  loss_rpn_loc: 0.064  time: 0.5567  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:06:06 d2.utils.events]: [0m eta: 0:03:54  iter: 1579  total_loss: 0.553  loss_cls: 0.176  loss_box_reg: 0.320  loss_rpn_cls: 0.022  loss_rpn_loc: 0.055  time: 0.5567  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:06:17 d2.utils.events]: [0m eta: 0:03:43  iter: 1599  total_loss: 0.698  loss_cls: 0.196  loss_box_reg: 0.358  loss_rpn_cls: 0.022  loss_rpn_loc: 0.073  time: 0.5567  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:06:29 d2.utils.events]: [0m eta: 0:03:32  iter: 1619  total_loss: 0.739  loss_cls: 0.241  loss_box_reg: 0.423  loss_rpn_cls: 0.023  loss_rpn_loc: 0.075  time: 0.5566  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:06:40 d2.utils.events]: [0m eta: 0:03:21  iter: 1639  total_loss: 0.685  loss_cls: 0.235  loss_box_reg: 0.386  loss_rpn_cls: 0.023  loss_rpn_loc: 0.065  time: 0.5567  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:06:51 d2.utils.events]: [0m eta: 0:03:10  iter: 1659  total_loss: 0.648  loss_cls: 0.190  loss_box_reg: 0.378  loss_rpn_cls: 0.024  loss_rpn_loc: 0.064  time: 0.5564  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:07:02 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.549  loss_cls: 0.186  loss_box_reg: 0.295  loss_rpn_cls: 0.020  loss_rpn_loc: 0.046  time: 0.5566  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:07:14 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.768  loss_cls: 0.239  loss_box_reg: 0.406  loss_rpn_cls: 0.021  loss_rpn_loc: 0.083  time: 0.5566  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:07:25 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.563  loss_cls: 0.198  loss_box_reg: 0.319  loss_rpn_cls: 0.018  loss_rpn_loc: 0.061  time: 0.5566  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:07:36 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.816  loss_cls: 0.236  loss_box_reg: 0.404  loss_rpn_cls: 0.031  loss_rpn_loc: 0.088  time: 0.5566  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:07:47 d2.utils.events]: [0m eta: 0:02:14  iter: 1759  total_loss: 0.746  loss_cls: 0.228  loss_box_reg: 0.403  loss_rpn_cls: 0.023  loss_rpn_loc: 0.069  time: 0.5566  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:07:58 d2.utils.events]: [0m eta: 0:02:03  iter: 1779  total_loss: 0.680  loss_cls: 0.217  loss_box_reg: 0.349  loss_rpn_cls: 0.032  loss_rpn_loc: 0.071  time: 0.5564  data_time: 0.0093  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:08:09 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.735  loss_cls: 0.241  loss_box_reg: 0.394  loss_rpn_cls: 0.025  loss_rpn_loc: 0.062  time: 0.5563  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:08:20 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.701  loss_cls: 0.216  loss_box_reg: 0.394  loss_rpn_cls: 0.022  loss_rpn_loc: 0.056  time: 0.5562  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:08:31 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.637  loss_cls: 0.212  loss_box_reg: 0.318  loss_rpn_cls: 0.026  loss_rpn_loc: 0.069  time: 0.5561  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:08:43 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.798  loss_cls: 0.257  loss_box_reg: 0.414  loss_rpn_cls: 0.025  loss_rpn_loc: 0.070  time: 0.5561  data_time: 0.0087  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:08:54 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.634  loss_cls: 0.184  loss_box_reg: 0.356  loss_rpn_cls: 0.025  loss_rpn_loc: 0.060  time: 0.5561  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:09:06 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.672  loss_cls: 0.214  loss_box_reg: 0.393  loss_rpn_cls: 0.020  loss_rpn_loc: 0.063  time: 0.5559  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:09:19 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.614  loss_cls: 0.193  loss_box_reg: 0.295  loss_rpn_cls: 0.020  loss_rpn_loc: 0.059  time: 0.5559  data_time: 0.0403  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:09:30 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.676  loss_cls: 0.212  loss_box_reg: 0.364  loss_rpn_cls: 0.022  loss_rpn_loc: 0.058  time: 0.5559  data_time: 0.0094  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:09:41 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.766  loss_cls: 0.233  loss_box_reg: 0.404  loss_rpn_cls: 0.029  loss_rpn_loc: 0.093  time: 0.5558  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:09:52 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.624  loss_cls: 0.191  loss_box_reg: 0.317  loss_rpn_cls: 0.028  loss_rpn_loc: 0.057  time: 0.5557  data_time: 0.0060  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 12:11:44 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 12:11:44 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 12:11:44 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 12:11:44 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.673  loss_cls: 0.210  loss_box_reg: 0.348  loss_rpn_cls: 0.022  loss_rpn_loc: 0.069  time: 0.5557  data_time: 0.0377  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:12:07 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:30 (0.5560 s / it)
[32m[04/15 12:12:07 d2.engine.hooks]: [0mTotal training time: 0:20:46 (0:02:16 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 12:12:33 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 12:12:33 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 12:12:33 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 12:12:39 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1200 s / img. ETA=0:07:51
[32m[04/15 12:12:44 d2.evaluation.evaluator]: [0mInference done 29/1257. 0.1191 s / img. ETA=0:06:34
[32m[04/15 12:12:49 d2.evaluation.evaluator]: [0mInference done 48/1257. 0.1191 s / img. ETA=0:06:00
[32m[04/15 12:12:54 d2.evaluation.evaluator]: [0mInference done 60/1257. 0.1199 s / img. ETA=0:06:31
[32m[04/15 12:13:00 d2.evaluation.evaluator]: [0mInference done 86/1257. 0.1205 s / img. ETA=0:05:34
[32m[04/15 12:13:05 d2.evaluation.evaluator]: [0mInference done 112/1257. 0.1206 s / img. ETA=0:05:01
[32m[04/15 12:13:10 d2.evaluation.evaluator]: [0mInference done 135/1257. 0.1202 s / img. ETA=0:04:48
[32m[04/15 12:13:15 d2.evaluation.evaluator]: [0mInference done 160/1257. 0.1201 s / img. ETA=0:04:32
[32m[04/15 12:13:20 d2.evaluation.evaluator]: [0mInference done 196/1257. 0.1204 s / img. ETA=0:04:01
[32m[04/15 12:13:25 d2.evaluation.evaluator]: [0mInference done 229/1257. 0.1204 s / img. ETA=0:03:42
[32m[04/15 12:13:30 d2.evaluation.evaluator]: [0mInference done 257/1257. 0.1204 s / img. ETA=0:03:33
[32m[04/15 12:13:36 d2.evaluation.evaluator]: [0mInference done 281/1257. 0.1204 s / img. ETA=0:03:28
[32m[04/15 12:13:41 d2.evaluation.evaluator]: [0mInference done 301/1257. 0.1203 s / img. ETA=0:03:27
[32m[04/15 12:13:46 d2.evaluation.evaluator]: [0mInference done 333/1257. 0.1203 s / img. ETA=0:03:15
[32m[04/15 12:13:51 d2.evaluation.evaluator]: [0mInference done 366/1257. 0.1203 s / img. ETA=0:03:04
[32m[04/15 12:13:56 d2.evaluation.evaluator]: [0mInference done 392/1257. 0.1203 s / img. ETA=0:02:58
[32m[04/15 12:14:01 d2.evaluation.evaluator]: [0mInference done 421/1257. 0.1205 s / img. ETA=0:02:50
[32m[04/15 12:14:06 d2.evaluation.evaluator]: [0mInference done 441/1257. 0.1204 s / img. ETA=0:02:47
[32m[04/15 12:14:12 d2.evaluation.evaluator]: [0mInference done 465/1257. 0.1204 s / img. ETA=0:02:43
[32m[04/15 12:14:17 d2.evaluation.evaluator]: [0mInference done 494/1257. 0.1203 s / img. ETA=0:02:36
[32m[04/15 12:14:22 d2.evaluation.evaluator]: [0mInference done 522/1257. 0.1203 s / img. ETA=0:02:29
[32m[04/15 12:14:27 d2.evaluation.evaluator]: [0mInference done 550/1257. 0.1203 s / img. ETA=0:02:23
[32m[04/15 12:14:32 d2.evaluation.evaluator]: [0mInference done 585/1257. 0.1203 s / img. ETA=0:02:13
[32m[04/15 12:14:37 d2.evaluation.evaluator]: [0mInference done 612/1257. 0.1203 s / img. ETA=0:02:08
[32m[04/15 12:14:42 d2.evaluation.evaluator]: [0mInference done 640/1257. 0.1203 s / img. ETA=0:02:02
[32m[04/15 12:14:47 d2.evaluation.evaluator]: [0mInference done 660/1257. 0.1203 s / img. ETA=0:01:59
[32m[04/15 12:14:52 d2.evaluation.evaluator]: [0mInference done 679/1257. 0.1203 s / img. ETA=0:01:56
[32m[04/15 12:14:57 d2.evaluation.evaluator]: [0mInference done 707/1257. 0.1202 s / img. ETA=0:01:50
[32m[04/15 12:15:02 d2.evaluation.evaluator]: [0mInference done 728/1257. 0.1202 s / img. ETA=0:01:46
[32m[04/15 12:15:07 d2.evaluation.evaluator]: [0mInference done 754/1257. 0.1202 s / img. ETA=0:01:41
[32m[04/15 12:15:12 d2.evaluation.evaluator]: [0mInference done 779/1257. 0.1202 s / img. ETA=0:01:36
[32m[04/15 12:15:18 d2.evaluation.evaluator]: [0mInference done 815/1257. 0.1202 s / img. ETA=0:01:27
[32m[04/15 12:15:23 d2.evaluation.evaluator]: [0mInference done 852/1257. 0.1203 s / img. ETA=0:01:19
[32m[04/15 12:15:28 d2.evaluation.evaluator]: [0mInference done 883/1257. 0.1205 s / img. ETA=0:01:12
[32m[04/15 12:15:33 d2.evaluation.evaluator]: [0mInference done 914/1257. 0.1212 s / img. ETA=0:01:06
[32m[04/15 12:15:38 d2.evaluation.evaluator]: [0mInference done 951/1257. 0.1212 s / img. ETA=0:00:58
[32m[04/15 12:15:43 d2.evaluation.evaluator]: [0mInference done 988/1257. 0.1213 s / img. ETA=0:00:51
[32m[04/15 12:15:48 d2.evaluation.evaluator]: [0mInference done 1027/1257. 0.1213 s / img. ETA=0:00:43
[32m[04/15 12:15:53 d2.evaluation.evaluator]: [0mInference done 1067/1257. 0.1214 s / img. ETA=0:00:35
[32m[04/15 12:15:58 d2.evaluation.evaluator]: [0mInference done 1107/1257. 0.1214 s / img. ETA=0:00:27
[32m[04/15 12:16:03 d2.evaluation.evaluator]: [0mInference done 1147/1257. 0.1215 s / img. ETA=0:00:19
[32m[04/15 12:16:08 d2.evaluation.evaluator]: [0mInference done 1187/1257. 0.1215 s / img. ETA=0:00:12
[32m[04/15 12:16:13 d2.evaluation.evaluator]: [0mInference done 1227/1257. 0.1216 s / img. ETA=0:00:05
[32m[04/15 12:16:19 d2.evaluation.evaluator]: [0mInference done 1246/1257. 0.1216 s / img. ETA=0:00:01
[32m[04/15 12:16:24 d2.evaluation.evaluator]: [0mInference done 1253/1257. 0.1216 s / img. ETA=0:00:00
[32m[04/15 12:16:26 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:49.046391 (0.182944 s / img per device, on 1 devices)
[32m[04/15 12:16:26 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:32 (0.121576 s / img per device, on 1 devices)
[32m[04/15 12:16:26 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 12:16:26 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 12:16:26 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=4.88s).
Accumulating evaluation results...
DONE (t=1.28s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.180
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.355
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.107
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.394
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.144
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.267
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.460
[32m[04/15 12:16:33 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 18.005 | 35.490 | 15.914 | 10.701 | 22.230 | 39.369 |
[32m[04/15 12:16:33 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 23.077 | bicycle       | 7.848 | car            | 41.097 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  18  *  2000  iterations ============
4 channel input
[32m[04/15 12:16:35 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 12:16:36 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.64 seconds.
[5m[31mWARNING[0m [32m[04/15 12:16:36 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 12:16:36 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 12:16:37 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 12:16:37 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 12:16:37 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 12:16:46 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 12:16:58 d2.utils.events]: [0m eta: 0:18:45  iter: 19  total_loss: 0.501  loss_cls: 0.154  loss_box_reg: 0.267  loss_rpn_cls: 0.018  loss_rpn_loc: 0.062  time: 0.5520  data_time: 0.0707  lr: 0.000200  max_mem: 3070M
[32m[04/15 12:17:10 d2.utils.events]: [0m eta: 0:17:55  iter: 39  total_loss: 0.605  loss_cls: 0.216  loss_box_reg: 0.317  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5434  data_time: 0.0105  lr: 0.000400  max_mem: 3070M
[32m[04/15 12:17:21 d2.utils.events]: [0m eta: 0:17:25  iter: 59  total_loss: 0.647  loss_cls: 0.209  loss_box_reg: 0.336  loss_rpn_cls: 0.016  loss_rpn_loc: 0.061  time: 0.5335  data_time: 0.0081  lr: 0.000599  max_mem: 3070M
[32m[04/15 12:17:32 d2.utils.events]: [0m eta: 0:17:15  iter: 79  total_loss: 0.708  loss_cls: 0.216  loss_box_reg: 0.388  loss_rpn_cls: 0.019  loss_rpn_loc: 0.059  time: 0.5338  data_time: 0.0062  lr: 0.000799  max_mem: 3070M
[32m[04/15 12:17:45 d2.utils.events]: [0m eta: 0:17:04  iter: 99  total_loss: 0.617  loss_cls: 0.212  loss_box_reg: 0.336  loss_rpn_cls: 0.023  loss_rpn_loc: 0.071  time: 0.5339  data_time: 0.0071  lr: 0.000999  max_mem: 3070M
[32m[04/15 12:17:57 d2.utils.events]: [0m eta: 0:17:01  iter: 119  total_loss: 0.587  loss_cls: 0.186  loss_box_reg: 0.316  loss_rpn_cls: 0.015  loss_rpn_loc: 0.058  time: 0.5371  data_time: 0.0066  lr: 0.001199  max_mem: 3070M
[32m[04/15 12:18:08 d2.utils.events]: [0m eta: 0:16:45  iter: 139  total_loss: 0.665  loss_cls: 0.213  loss_box_reg: 0.319  loss_rpn_cls: 0.027  loss_rpn_loc: 0.052  time: 0.5366  data_time: 0.0071  lr: 0.001399  max_mem: 3070M
[32m[04/15 12:18:19 d2.utils.events]: [0m eta: 0:16:32  iter: 159  total_loss: 0.777  loss_cls: 0.248  loss_box_reg: 0.410  loss_rpn_cls: 0.014  loss_rpn_loc: 0.068  time: 0.5356  data_time: 0.0063  lr: 0.001598  max_mem: 3070M
[32m[04/15 12:18:30 d2.utils.events]: [0m eta: 0:16:21  iter: 179  total_loss: 0.693  loss_cls: 0.203  loss_box_reg: 0.376  loss_rpn_cls: 0.018  loss_rpn_loc: 0.073  time: 0.5363  data_time: 0.0078  lr: 0.001798  max_mem: 3070M
[32m[04/15 12:18:41 d2.utils.events]: [0m eta: 0:16:14  iter: 199  total_loss: 0.663  loss_cls: 0.224  loss_box_reg: 0.379  loss_rpn_cls: 0.018  loss_rpn_loc: 0.051  time: 0.5390  data_time: 0.0065  lr: 0.001998  max_mem: 3070M
[32m[04/15 12:18:52 d2.utils.events]: [0m eta: 0:16:04  iter: 219  total_loss: 0.638  loss_cls: 0.197  loss_box_reg: 0.339  loss_rpn_cls: 0.014  loss_rpn_loc: 0.055  time: 0.5390  data_time: 0.0065  lr: 0.002198  max_mem: 3070M
[32m[04/15 12:19:03 d2.utils.events]: [0m eta: 0:15:54  iter: 239  total_loss: 0.719  loss_cls: 0.223  loss_box_reg: 0.392  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5394  data_time: 0.0101  lr: 0.002398  max_mem: 3070M
[32m[04/15 12:19:24 d2.utils.events]: [0m eta: 0:15:44  iter: 259  total_loss: 0.612  loss_cls: 0.194  loss_box_reg: 0.335  loss_rpn_cls: 0.017  loss_rpn_loc: 0.045  time: 0.5397  data_time: 0.0081  lr: 0.002597  max_mem: 3070M
[32m[04/15 12:19:35 d2.utils.events]: [0m eta: 0:15:34  iter: 279  total_loss: 0.582  loss_cls: 0.187  loss_box_reg: 0.317  loss_rpn_cls: 0.014  loss_rpn_loc: 0.035  time: 0.5396  data_time: 0.0065  lr: 0.002797  max_mem: 3070M
[32m[04/15 12:19:46 d2.utils.events]: [0m eta: 0:15:22  iter: 299  total_loss: 0.612  loss_cls: 0.176  loss_box_reg: 0.309  loss_rpn_cls: 0.014  loss_rpn_loc: 0.072  time: 0.5390  data_time: 0.0072  lr: 0.002997  max_mem: 3070M
[32m[04/15 12:19:57 d2.utils.events]: [0m eta: 0:15:11  iter: 319  total_loss: 0.546  loss_cls: 0.177  loss_box_reg: 0.310  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5378  data_time: 0.0061  lr: 0.003197  max_mem: 3070M
[32m[04/15 12:20:08 d2.utils.events]: [0m eta: 0:14:59  iter: 339  total_loss: 0.640  loss_cls: 0.208  loss_box_reg: 0.368  loss_rpn_cls: 0.015  loss_rpn_loc: 0.059  time: 0.5376  data_time: 0.0059  lr: 0.003397  max_mem: 3070M
[32m[04/15 12:20:19 d2.utils.events]: [0m eta: 0:14:51  iter: 359  total_loss: 0.660  loss_cls: 0.223  loss_box_reg: 0.380  loss_rpn_cls: 0.019  loss_rpn_loc: 0.039  time: 0.5381  data_time: 0.0067  lr: 0.003596  max_mem: 3070M
[32m[04/15 12:20:31 d2.utils.events]: [0m eta: 0:14:41  iter: 379  total_loss: 0.673  loss_cls: 0.209  loss_box_reg: 0.365  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5386  data_time: 0.0078  lr: 0.003796  max_mem: 3070M
[32m[04/15 12:20:42 d2.utils.events]: [0m eta: 0:14:30  iter: 399  total_loss: 0.545  loss_cls: 0.165  loss_box_reg: 0.302  loss_rpn_cls: 0.010  loss_rpn_loc: 0.043  time: 0.5389  data_time: 0.0055  lr: 0.003996  max_mem: 3070M
[32m[04/15 12:20:53 d2.utils.events]: [0m eta: 0:14:20  iter: 419  total_loss: 0.572  loss_cls: 0.171  loss_box_reg: 0.332  loss_rpn_cls: 0.015  loss_rpn_loc: 0.044  time: 0.5403  data_time: 0.0060  lr: 0.004196  max_mem: 3070M
[32m[04/15 12:21:04 d2.utils.events]: [0m eta: 0:14:10  iter: 439  total_loss: 0.578  loss_cls: 0.183  loss_box_reg: 0.325  loss_rpn_cls: 0.021  loss_rpn_loc: 0.057  time: 0.5404  data_time: 0.0082  lr: 0.004396  max_mem: 3070M
[32m[04/15 12:21:15 d2.utils.events]: [0m eta: 0:13:59  iter: 459  total_loss: 0.645  loss_cls: 0.208  loss_box_reg: 0.347  loss_rpn_cls: 0.018  loss_rpn_loc: 0.071  time: 0.5402  data_time: 0.0064  lr: 0.004595  max_mem: 3070M
[32m[04/15 12:21:26 d2.utils.events]: [0m eta: 0:13:47  iter: 479  total_loss: 0.631  loss_cls: 0.196  loss_box_reg: 0.354  loss_rpn_cls: 0.014  loss_rpn_loc: 0.050  time: 0.5400  data_time: 0.0062  lr: 0.004795  max_mem: 3070M
[32m[04/15 12:21:37 d2.utils.events]: [0m eta: 0:13:36  iter: 499  total_loss: 0.634  loss_cls: 0.198  loss_box_reg: 0.360  loss_rpn_cls: 0.016  loss_rpn_loc: 0.061  time: 0.5399  data_time: 0.0069  lr: 0.004995  max_mem: 3070M
[32m[04/15 12:21:48 d2.utils.events]: [0m eta: 0:13:28  iter: 519  total_loss: 0.591  loss_cls: 0.187  loss_box_reg: 0.325  loss_rpn_cls: 0.017  loss_rpn_loc: 0.049  time: 0.5410  data_time: 0.0079  lr: 0.005195  max_mem: 3070M
[32m[04/15 12:22:00 d2.utils.events]: [0m eta: 0:13:18  iter: 539  total_loss: 0.672  loss_cls: 0.200  loss_box_reg: 0.340  loss_rpn_cls: 0.015  loss_rpn_loc: 0.060  time: 0.5414  data_time: 0.0090  lr: 0.005395  max_mem: 3070M
[32m[04/15 12:22:11 d2.utils.events]: [0m eta: 0:13:09  iter: 559  total_loss: 0.653  loss_cls: 0.230  loss_box_reg: 0.363  loss_rpn_cls: 0.024  loss_rpn_loc: 0.046  time: 0.5418  data_time: 0.0088  lr: 0.005594  max_mem: 3070M
[32m[04/15 12:22:23 d2.utils.events]: [0m eta: 0:12:58  iter: 579  total_loss: 0.643  loss_cls: 0.203  loss_box_reg: 0.363  loss_rpn_cls: 0.017  loss_rpn_loc: 0.058  time: 0.5426  data_time: 0.0417  lr: 0.005794  max_mem: 3070M
[32m[04/15 12:22:36 d2.utils.events]: [0m eta: 0:12:46  iter: 599  total_loss: 0.653  loss_cls: 0.229  loss_box_reg: 0.316  loss_rpn_cls: 0.022  loss_rpn_loc: 0.071  time: 0.5423  data_time: 0.0081  lr: 0.005994  max_mem: 3070M
[32m[04/15 12:22:47 d2.utils.events]: [0m eta: 0:12:34  iter: 619  total_loss: 0.710  loss_cls: 0.226  loss_box_reg: 0.352  loss_rpn_cls: 0.028  loss_rpn_loc: 0.055  time: 0.5423  data_time: 0.0063  lr: 0.006194  max_mem: 3070M
[32m[04/15 12:22:58 d2.utils.events]: [0m eta: 0:12:25  iter: 639  total_loss: 0.643  loss_cls: 0.204  loss_box_reg: 0.351  loss_rpn_cls: 0.029  loss_rpn_loc: 0.059  time: 0.5430  data_time: 0.0082  lr: 0.006394  max_mem: 3070M
[32m[04/15 12:23:10 d2.utils.events]: [0m eta: 0:12:15  iter: 659  total_loss: 0.644  loss_cls: 0.180  loss_box_reg: 0.360  loss_rpn_cls: 0.011  loss_rpn_loc: 0.059  time: 0.5438  data_time: 0.0067  lr: 0.006593  max_mem: 3070M
[32m[04/15 12:23:21 d2.utils.events]: [0m eta: 0:12:03  iter: 679  total_loss: 0.629  loss_cls: 0.209  loss_box_reg: 0.357  loss_rpn_cls: 0.021  loss_rpn_loc: 0.067  time: 0.5434  data_time: 0.0060  lr: 0.006793  max_mem: 3070M
[32m[04/15 12:23:32 d2.utils.events]: [0m eta: 0:11:53  iter: 699  total_loss: 0.706  loss_cls: 0.205  loss_box_reg: 0.405  loss_rpn_cls: 0.022  loss_rpn_loc: 0.050  time: 0.5436  data_time: 0.0072  lr: 0.006993  max_mem: 3070M
[32m[04/15 12:23:44 d2.utils.events]: [0m eta: 0:11:42  iter: 719  total_loss: 0.619  loss_cls: 0.201  loss_box_reg: 0.328  loss_rpn_cls: 0.018  loss_rpn_loc: 0.049  time: 0.5442  data_time: 0.0229  lr: 0.007193  max_mem: 3070M
[32m[04/15 12:23:55 d2.utils.events]: [0m eta: 0:11:32  iter: 739  total_loss: 0.628  loss_cls: 0.199  loss_box_reg: 0.333  loss_rpn_cls: 0.016  loss_rpn_loc: 0.040  time: 0.5445  data_time: 0.0064  lr: 0.007393  max_mem: 3070M
[32m[04/15 12:24:06 d2.utils.events]: [0m eta: 0:11:21  iter: 759  total_loss: 0.574  loss_cls: 0.185  loss_box_reg: 0.326  loss_rpn_cls: 0.012  loss_rpn_loc: 0.055  time: 0.5449  data_time: 0.0082  lr: 0.007592  max_mem: 3070M
[32m[04/15 12:24:17 d2.utils.events]: [0m eta: 0:11:10  iter: 779  total_loss: 0.637  loss_cls: 0.199  loss_box_reg: 0.391  loss_rpn_cls: 0.017  loss_rpn_loc: 0.042  time: 0.5451  data_time: 0.0070  lr: 0.007792  max_mem: 3070M
[32m[04/15 12:24:33 d2.utils.events]: [0m eta: 0:10:58  iter: 799  total_loss: 0.654  loss_cls: 0.213  loss_box_reg: 0.340  loss_rpn_cls: 0.027  loss_rpn_loc: 0.041  time: 0.5452  data_time: 0.0505  lr: 0.007992  max_mem: 3070M
[32m[04/15 12:24:44 d2.utils.events]: [0m eta: 0:10:48  iter: 819  total_loss: 0.751  loss_cls: 0.253  loss_box_reg: 0.416  loss_rpn_cls: 0.023  loss_rpn_loc: 0.079  time: 0.5453  data_time: 0.0060  lr: 0.008192  max_mem: 3070M
[32m[04/15 12:24:55 d2.utils.events]: [0m eta: 0:10:37  iter: 839  total_loss: 0.727  loss_cls: 0.239  loss_box_reg: 0.412  loss_rpn_cls: 0.018  loss_rpn_loc: 0.046  time: 0.5454  data_time: 0.0069  lr: 0.008392  max_mem: 3070M
[32m[04/15 12:25:07 d2.utils.events]: [0m eta: 0:10:26  iter: 859  total_loss: 0.560  loss_cls: 0.192  loss_box_reg: 0.294  loss_rpn_cls: 0.014  loss_rpn_loc: 0.044  time: 0.5461  data_time: 0.0064  lr: 0.008591  max_mem: 3070M
[32m[04/15 12:25:17 d2.utils.events]: [0m eta: 0:10:15  iter: 879  total_loss: 0.574  loss_cls: 0.194  loss_box_reg: 0.301  loss_rpn_cls: 0.019  loss_rpn_loc: 0.045  time: 0.5456  data_time: 0.0078  lr: 0.008791  max_mem: 3070M
[32m[04/15 12:25:28 d2.utils.events]: [0m eta: 0:10:04  iter: 899  total_loss: 0.721  loss_cls: 0.207  loss_box_reg: 0.379  loss_rpn_cls: 0.016  loss_rpn_loc: 0.070  time: 0.5456  data_time: 0.0070  lr: 0.008991  max_mem: 3070M
[32m[04/15 12:25:39 d2.utils.events]: [0m eta: 0:09:53  iter: 919  total_loss: 0.684  loss_cls: 0.221  loss_box_reg: 0.336  loss_rpn_cls: 0.015  loss_rpn_loc: 0.070  time: 0.5453  data_time: 0.0044  lr: 0.009191  max_mem: 3070M
[32m[04/15 12:25:50 d2.utils.events]: [0m eta: 0:09:41  iter: 939  total_loss: 0.658  loss_cls: 0.204  loss_box_reg: 0.343  loss_rpn_cls: 0.023  loss_rpn_loc: 0.053  time: 0.5448  data_time: 0.0040  lr: 0.009391  max_mem: 3070M
[32m[04/15 12:26:00 d2.utils.events]: [0m eta: 0:09:30  iter: 959  total_loss: 0.638  loss_cls: 0.216  loss_box_reg: 0.369  loss_rpn_cls: 0.019  loss_rpn_loc: 0.063  time: 0.5445  data_time: 0.0069  lr: 0.009590  max_mem: 3070M
[32m[04/15 12:26:12 d2.utils.events]: [0m eta: 0:09:19  iter: 979  total_loss: 0.606  loss_cls: 0.197  loss_box_reg: 0.312  loss_rpn_cls: 0.017  loss_rpn_loc: 0.064  time: 0.5448  data_time: 0.0061  lr: 0.009790  max_mem: 3070M
[32m[04/15 12:26:25 d2.utils.events]: [0m eta: 0:09:08  iter: 999  total_loss: 0.789  loss_cls: 0.259  loss_box_reg: 0.419  loss_rpn_cls: 0.027  loss_rpn_loc: 0.073  time: 0.5446  data_time: 0.0076  lr: 0.009990  max_mem: 3070M
[32m[04/15 12:26:36 d2.utils.events]: [0m eta: 0:08:57  iter: 1019  total_loss: 0.647  loss_cls: 0.208  loss_box_reg: 0.353  loss_rpn_cls: 0.022  loss_rpn_loc: 0.068  time: 0.5452  data_time: 0.0096  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:26:48 d2.utils.events]: [0m eta: 0:08:47  iter: 1039  total_loss: 0.766  loss_cls: 0.250  loss_box_reg: 0.406  loss_rpn_cls: 0.025  loss_rpn_loc: 0.054  time: 0.5458  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:26:59 d2.utils.events]: [0m eta: 0:08:36  iter: 1059  total_loss: 0.738  loss_cls: 0.238  loss_box_reg: 0.393  loss_rpn_cls: 0.019  loss_rpn_loc: 0.061  time: 0.5457  data_time: 0.0059  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:27:12 d2.utils.events]: [0m eta: 0:08:25  iter: 1079  total_loss: 0.619  loss_cls: 0.179  loss_box_reg: 0.323  loss_rpn_cls: 0.015  loss_rpn_loc: 0.048  time: 0.5455  data_time: 0.0093  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:27:23 d2.utils.events]: [0m eta: 0:08:14  iter: 1099  total_loss: 0.625  loss_cls: 0.206  loss_box_reg: 0.309  loss_rpn_cls: 0.022  loss_rpn_loc: 0.051  time: 0.5454  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:27:34 d2.utils.events]: [0m eta: 0:08:03  iter: 1119  total_loss: 0.611  loss_cls: 0.189  loss_box_reg: 0.362  loss_rpn_cls: 0.025  loss_rpn_loc: 0.054  time: 0.5453  data_time: 0.0060  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:28:08 d2.utils.events]: [0m eta: 0:07:51  iter: 1139  total_loss: 0.675  loss_cls: 0.223  loss_box_reg: 0.351  loss_rpn_cls: 0.015  loss_rpn_loc: 0.046  time: 0.5450  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:28:19 d2.utils.events]: [0m eta: 0:07:41  iter: 1159  total_loss: 0.550  loss_cls: 0.186  loss_box_reg: 0.306  loss_rpn_cls: 0.014  loss_rpn_loc: 0.045  time: 0.5450  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:28:30 d2.utils.events]: [0m eta: 0:07:30  iter: 1179  total_loss: 0.766  loss_cls: 0.253  loss_box_reg: 0.427  loss_rpn_cls: 0.021  loss_rpn_loc: 0.062  time: 0.5452  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:28:41 d2.utils.events]: [0m eta: 0:07:19  iter: 1199  total_loss: 0.570  loss_cls: 0.192  loss_box_reg: 0.297  loss_rpn_cls: 0.020  loss_rpn_loc: 0.041  time: 0.5453  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:28:53 d2.utils.events]: [0m eta: 0:07:08  iter: 1219  total_loss: 0.631  loss_cls: 0.196  loss_box_reg: 0.384  loss_rpn_cls: 0.015  loss_rpn_loc: 0.058  time: 0.5452  data_time: 0.0162  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:29:04 d2.utils.events]: [0m eta: 0:06:57  iter: 1239  total_loss: 0.683  loss_cls: 0.229  loss_box_reg: 0.386  loss_rpn_cls: 0.016  loss_rpn_loc: 0.069  time: 0.5451  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:29:15 d2.utils.events]: [0m eta: 0:06:46  iter: 1259  total_loss: 0.734  loss_cls: 0.224  loss_box_reg: 0.381  loss_rpn_cls: 0.021  loss_rpn_loc: 0.067  time: 0.5452  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:29:26 d2.utils.events]: [0m eta: 0:06:35  iter: 1279  total_loss: 0.655  loss_cls: 0.206  loss_box_reg: 0.387  loss_rpn_cls: 0.025  loss_rpn_loc: 0.059  time: 0.5454  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:29:37 d2.utils.events]: [0m eta: 0:06:25  iter: 1299  total_loss: 0.593  loss_cls: 0.177  loss_box_reg: 0.316  loss_rpn_cls: 0.018  loss_rpn_loc: 0.072  time: 0.5458  data_time: 0.0091  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:29:48 d2.utils.events]: [0m eta: 0:06:14  iter: 1319  total_loss: 0.635  loss_cls: 0.207  loss_box_reg: 0.333  loss_rpn_cls: 0.018  loss_rpn_loc: 0.057  time: 0.5456  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:29:59 d2.utils.events]: [0m eta: 0:06:03  iter: 1339  total_loss: 0.624  loss_cls: 0.206  loss_box_reg: 0.359  loss_rpn_cls: 0.024  loss_rpn_loc: 0.060  time: 0.5457  data_time: 0.0061  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:30:11 d2.utils.events]: [0m eta: 0:05:52  iter: 1359  total_loss: 0.762  loss_cls: 0.239  loss_box_reg: 0.399  loss_rpn_cls: 0.026  loss_rpn_loc: 0.078  time: 0.5459  data_time: 0.0089  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:30:21 d2.utils.events]: [0m eta: 0:05:41  iter: 1379  total_loss: 0.527  loss_cls: 0.167  loss_box_reg: 0.276  loss_rpn_cls: 0.017  loss_rpn_loc: 0.066  time: 0.5454  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:30:32 d2.utils.events]: [0m eta: 0:05:30  iter: 1399  total_loss: 0.621  loss_cls: 0.203  loss_box_reg: 0.368  loss_rpn_cls: 0.023  loss_rpn_loc: 0.043  time: 0.5454  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:30:43 d2.utils.events]: [0m eta: 0:05:18  iter: 1419  total_loss: 0.674  loss_cls: 0.232  loss_box_reg: 0.359  loss_rpn_cls: 0.020  loss_rpn_loc: 0.061  time: 0.5455  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:30:55 d2.utils.events]: [0m eta: 0:05:08  iter: 1439  total_loss: 0.792  loss_cls: 0.246  loss_box_reg: 0.413  loss_rpn_cls: 0.023  loss_rpn_loc: 0.066  time: 0.5457  data_time: 0.0111  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:31:06 d2.utils.events]: [0m eta: 0:04:57  iter: 1459  total_loss: 0.676  loss_cls: 0.218  loss_box_reg: 0.368  loss_rpn_cls: 0.028  loss_rpn_loc: 0.055  time: 0.5457  data_time: 0.0092  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:31:17 d2.utils.events]: [0m eta: 0:04:46  iter: 1479  total_loss: 0.496  loss_cls: 0.189  loss_box_reg: 0.293  loss_rpn_cls: 0.020  loss_rpn_loc: 0.046  time: 0.5458  data_time: 0.0061  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:31:28 d2.utils.events]: [0m eta: 0:04:35  iter: 1499  total_loss: 0.618  loss_cls: 0.212  loss_box_reg: 0.344  loss_rpn_cls: 0.018  loss_rpn_loc: 0.057  time: 0.5461  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:31:40 d2.utils.events]: [0m eta: 0:04:24  iter: 1519  total_loss: 0.589  loss_cls: 0.195  loss_box_reg: 0.338  loss_rpn_cls: 0.018  loss_rpn_loc: 0.051  time: 0.5464  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:31:51 d2.utils.events]: [0m eta: 0:04:13  iter: 1539  total_loss: 0.648  loss_cls: 0.207  loss_box_reg: 0.349  loss_rpn_cls: 0.017  loss_rpn_loc: 0.057  time: 0.5465  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:32:03 d2.utils.events]: [0m eta: 0:04:02  iter: 1559  total_loss: 0.674  loss_cls: 0.217  loss_box_reg: 0.344  loss_rpn_cls: 0.026  loss_rpn_loc: 0.054  time: 0.5466  data_time: 0.0099  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:32:14 d2.utils.events]: [0m eta: 0:03:51  iter: 1579  total_loss: 0.715  loss_cls: 0.233  loss_box_reg: 0.400  loss_rpn_cls: 0.022  loss_rpn_loc: 0.059  time: 0.5466  data_time: 0.0150  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:32:25 d2.utils.events]: [0m eta: 0:03:40  iter: 1599  total_loss: 0.614  loss_cls: 0.196  loss_box_reg: 0.337  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5469  data_time: 0.0394  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:32:36 d2.utils.events]: [0m eta: 0:03:29  iter: 1619  total_loss: 0.643  loss_cls: 0.187  loss_box_reg: 0.311  loss_rpn_cls: 0.023  loss_rpn_loc: 0.054  time: 0.5468  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:32:47 d2.utils.events]: [0m eta: 0:03:18  iter: 1639  total_loss: 0.579  loss_cls: 0.188  loss_box_reg: 0.318  loss_rpn_cls: 0.022  loss_rpn_loc: 0.052  time: 0.5468  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:32:58 d2.utils.events]: [0m eta: 0:03:07  iter: 1659  total_loss: 0.560  loss_cls: 0.179  loss_box_reg: 0.295  loss_rpn_cls: 0.025  loss_rpn_loc: 0.049  time: 0.5468  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:33:09 d2.utils.events]: [0m eta: 0:02:56  iter: 1679  total_loss: 0.633  loss_cls: 0.197  loss_box_reg: 0.337  loss_rpn_cls: 0.017  loss_rpn_loc: 0.052  time: 0.5469  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:33:20 d2.utils.events]: [0m eta: 0:02:45  iter: 1699  total_loss: 0.641  loss_cls: 0.186  loss_box_reg: 0.318  loss_rpn_cls: 0.018  loss_rpn_loc: 0.060  time: 0.5468  data_time: 0.0062  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:33:31 d2.utils.events]: [0m eta: 0:02:34  iter: 1719  total_loss: 0.690  loss_cls: 0.213  loss_box_reg: 0.367  loss_rpn_cls: 0.020  loss_rpn_loc: 0.065  time: 0.5468  data_time: 0.0092  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:33:43 d2.utils.events]: [0m eta: 0:02:23  iter: 1739  total_loss: 0.747  loss_cls: 0.231  loss_box_reg: 0.427  loss_rpn_cls: 0.030  loss_rpn_loc: 0.068  time: 0.5470  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:33:54 d2.utils.events]: [0m eta: 0:02:12  iter: 1759  total_loss: 0.646  loss_cls: 0.207  loss_box_reg: 0.354  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5472  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:34:19 d2.utils.events]: [0m eta: 0:02:01  iter: 1779  total_loss: 0.599  loss_cls: 0.194  loss_box_reg: 0.325  loss_rpn_cls: 0.019  loss_rpn_loc: 0.061  time: 0.5474  data_time: 0.0696  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:34:35 d2.utils.events]: [0m eta: 0:01:50  iter: 1799  total_loss: 0.673  loss_cls: 0.208  loss_box_reg: 0.339  loss_rpn_cls: 0.018  loss_rpn_loc: 0.063  time: 0.5492  data_time: 0.2242  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:34:46 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.740  loss_cls: 0.242  loss_box_reg: 0.393  loss_rpn_cls: 0.020  loss_rpn_loc: 0.069  time: 0.5489  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:34:57 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.607  loss_cls: 0.226  loss_box_reg: 0.332  loss_rpn_cls: 0.013  loss_rpn_loc: 0.050  time: 0.5488  data_time: 0.0153  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:35:07 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.646  loss_cls: 0.194  loss_box_reg: 0.348  loss_rpn_cls: 0.022  loss_rpn_loc: 0.056  time: 0.5486  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:35:18 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.638  loss_cls: 0.197  loss_box_reg: 0.343  loss_rpn_cls: 0.025  loss_rpn_loc: 0.061  time: 0.5485  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:35:59 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.709  loss_cls: 0.218  loss_box_reg: 0.358  loss_rpn_cls: 0.033  loss_rpn_loc: 0.068  time: 0.5483  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:36:10 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.702  loss_cls: 0.223  loss_box_reg: 0.379  loss_rpn_cls: 0.026  loss_rpn_loc: 0.067  time: 0.5482  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:36:25 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.702  loss_cls: 0.202  loss_box_reg: 0.372  loss_rpn_cls: 0.020  loss_rpn_loc: 0.066  time: 0.5481  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:36:36 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.773  loss_cls: 0.252  loss_box_reg: 0.396  loss_rpn_cls: 0.023  loss_rpn_loc: 0.082  time: 0.5480  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:36:47 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.689  loss_cls: 0.208  loss_box_reg: 0.352  loss_rpn_cls: 0.020  loss_rpn_loc: 0.069  time: 0.5478  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 12:38:13 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 12:38:13 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 12:38:13 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 12:38:13 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.714  loss_cls: 0.253  loss_box_reg: 0.379  loss_rpn_cls: 0.021  loss_rpn_loc: 0.061  time: 0.5478  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:38:14 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:14 (0.5481 s / it)
[32m[04/15 12:38:14 d2.engine.hooks]: [0mTotal training time: 0:21:26 (0:03:11 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 12:38:54 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 12:38:54 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 12:38:54 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 12:38:58 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1225 s / img. ETA=0:04:17
[32m[04/15 12:39:03 d2.evaluation.evaluator]: [0mInference done 32/1257. 0.1202 s / img. ETA=0:04:44
[32m[04/15 12:39:08 d2.evaluation.evaluator]: [0mInference done 53/1257. 0.1201 s / img. ETA=0:04:48
[32m[04/15 12:39:13 d2.evaluation.evaluator]: [0mInference done 71/1257. 0.1201 s / img. ETA=0:04:58
[32m[04/15 12:39:19 d2.evaluation.evaluator]: [0mInference done 90/1257. 0.1199 s / img. ETA=0:05:09
[32m[04/15 12:39:24 d2.evaluation.evaluator]: [0mInference done 108/1257. 0.1196 s / img. ETA=0:05:07
[32m[04/15 12:39:29 d2.evaluation.evaluator]: [0mInference done 138/1257. 0.1198 s / img. ETA=0:04:34
[32m[04/15 12:39:34 d2.evaluation.evaluator]: [0mInference done 169/1257. 0.1202 s / img. ETA=0:04:09
[32m[04/15 12:39:39 d2.evaluation.evaluator]: [0mInference done 198/1257. 0.1203 s / img. ETA=0:03:53
[32m[04/15 12:39:44 d2.evaluation.evaluator]: [0mInference done 226/1257. 0.1207 s / img. ETA=0:03:42
[32m[04/15 12:39:49 d2.evaluation.evaluator]: [0mInference done 259/1257. 0.1207 s / img. ETA=0:03:27
[32m[04/15 12:39:54 d2.evaluation.evaluator]: [0mInference done 288/1257. 0.1208 s / img. ETA=0:03:17
[32m[04/15 12:39:59 d2.evaluation.evaluator]: [0mInference done 306/1257. 0.1208 s / img. ETA=0:03:18
[32m[04/15 12:40:05 d2.evaluation.evaluator]: [0mInference done 332/1257. 0.1208 s / img. ETA=0:03:12
[32m[04/15 12:40:10 d2.evaluation.evaluator]: [0mInference done 358/1257. 0.1207 s / img. ETA=0:03:06
[32m[04/15 12:40:15 d2.evaluation.evaluator]: [0mInference done 382/1257. 0.1208 s / img. ETA=0:03:01
[32m[04/15 12:40:20 d2.evaluation.evaluator]: [0mInference done 390/1257. 0.1209 s / img. ETA=0:03:07
[32m[04/15 12:40:25 d2.evaluation.evaluator]: [0mInference done 422/1257. 0.1211 s / img. ETA=0:02:57
[32m[04/15 12:40:31 d2.evaluation.evaluator]: [0mInference done 450/1257. 0.1210 s / img. ETA=0:02:50
[32m[04/15 12:40:36 d2.evaluation.evaluator]: [0mInference done 472/1257. 0.1209 s / img. ETA=0:02:46
[32m[04/15 12:40:41 d2.evaluation.evaluator]: [0mInference done 495/1257. 0.1209 s / img. ETA=0:02:42
[32m[04/15 12:40:46 d2.evaluation.evaluator]: [0mInference done 519/1257. 0.1208 s / img. ETA=0:02:37
[32m[04/15 12:40:52 d2.evaluation.evaluator]: [0mInference done 547/1257. 0.1209 s / img. ETA=0:02:30
[32m[04/15 12:40:57 d2.evaluation.evaluator]: [0mInference done 563/1257. 0.1208 s / img. ETA=0:02:29
[32m[04/15 12:41:02 d2.evaluation.evaluator]: [0mInference done 586/1257. 0.1209 s / img. ETA=0:02:24
[32m[04/15 12:41:07 d2.evaluation.evaluator]: [0mInference done 606/1257. 0.1208 s / img. ETA=0:02:21
[32m[04/15 12:41:12 d2.evaluation.evaluator]: [0mInference done 635/1257. 0.1208 s / img. ETA=0:02:13
[32m[04/15 12:41:17 d2.evaluation.evaluator]: [0mInference done 664/1257. 0.1208 s / img. ETA=0:02:06
[32m[04/15 12:41:22 d2.evaluation.evaluator]: [0mInference done 691/1257. 0.1207 s / img. ETA=0:02:00
[32m[04/15 12:41:27 d2.evaluation.evaluator]: [0mInference done 723/1257. 0.1208 s / img. ETA=0:01:52
[32m[04/15 12:41:32 d2.evaluation.evaluator]: [0mInference done 758/1257. 0.1209 s / img. ETA=0:01:43
[32m[04/15 12:41:38 d2.evaluation.evaluator]: [0mInference done 790/1257. 0.1209 s / img. ETA=0:01:35
[32m[04/15 12:41:43 d2.evaluation.evaluator]: [0mInference done 820/1257. 0.1210 s / img. ETA=0:01:28
[32m[04/15 12:41:48 d2.evaluation.evaluator]: [0mInference done 852/1257. 0.1211 s / img. ETA=0:01:21
[32m[04/15 12:41:53 d2.evaluation.evaluator]: [0mInference done 882/1257. 0.1211 s / img. ETA=0:01:15
[32m[04/15 12:41:58 d2.evaluation.evaluator]: [0mInference done 923/1257. 0.1211 s / img. ETA=0:01:05
[32m[04/15 12:42:03 d2.evaluation.evaluator]: [0mInference done 963/1257. 0.1212 s / img. ETA=0:00:57
[32m[04/15 12:42:08 d2.evaluation.evaluator]: [0mInference done 998/1257. 0.1218 s / img. ETA=0:00:49
[32m[04/15 12:42:13 d2.evaluation.evaluator]: [0mInference done 1038/1257. 0.1218 s / img. ETA=0:00:41
[32m[04/15 12:42:18 d2.evaluation.evaluator]: [0mInference done 1073/1257. 0.1219 s / img. ETA=0:00:34
[32m[04/15 12:42:23 d2.evaluation.evaluator]: [0mInference done 1097/1257. 0.1219 s / img. ETA=0:00:30
[32m[04/15 12:42:28 d2.evaluation.evaluator]: [0mInference done 1120/1257. 0.1219 s / img. ETA=0:00:26
[32m[04/15 12:42:34 d2.evaluation.evaluator]: [0mInference done 1136/1257. 0.1218 s / img. ETA=0:00:23
[32m[04/15 12:42:39 d2.evaluation.evaluator]: [0mInference done 1158/1257. 0.1218 s / img. ETA=0:00:19
[32m[04/15 12:42:44 d2.evaluation.evaluator]: [0mInference done 1182/1257. 0.1218 s / img. ETA=0:00:14
[32m[04/15 12:42:49 d2.evaluation.evaluator]: [0mInference done 1204/1257. 0.1217 s / img. ETA=0:00:10
[32m[04/15 12:42:55 d2.evaluation.evaluator]: [0mInference done 1231/1257. 0.1217 s / img. ETA=0:00:05
[32m[04/15 12:42:59 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:01.937266 (0.193241 s / img per device, on 1 devices)
[32m[04/15 12:42:59 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:32 (0.121733 s / img per device, on 1 devices)
[32m[04/15 12:42:59 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 12:42:59 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 12:42:59 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=4.76s).
Accumulating evaluation results...
DONE (t=0.64s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.188
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.175
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.115
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.233
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.404
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.227
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.153
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.275
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.444
[32m[04/15 12:43:04 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 18.841 | 36.335 | 17.544 | 11.452 | 23.340 | 40.425 |
[32m[04/15 12:43:04 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 27.502 | bicycle       | 6.320 | car            | 41.541 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  19  *  2000  iterations ============
4 channel input
[32m[04/15 12:43:06 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 12:43:09 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 2.97 seconds.
[5m[31mWARNING[0m [32m[04/15 12:43:09 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 12:43:09 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 12:43:10 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 12:43:10 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 12:43:10 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 12:43:24 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 12:43:35 d2.utils.events]: [0m eta: 0:17:00  iter: 19  total_loss: 0.648  loss_cls: 0.189  loss_box_reg: 0.370  loss_rpn_cls: 0.017  loss_rpn_loc: 0.067  time: 0.5200  data_time: 0.0642  lr: 0.000200  max_mem: 3070M
[32m[04/15 12:43:48 d2.utils.events]: [0m eta: 0:16:58  iter: 39  total_loss: 0.638  loss_cls: 0.209  loss_box_reg: 0.353  loss_rpn_cls: 0.014  loss_rpn_loc: 0.058  time: 0.5199  data_time: 0.0074  lr: 0.000400  max_mem: 3070M
[32m[04/15 12:44:00 d2.utils.events]: [0m eta: 0:17:06  iter: 59  total_loss: 0.621  loss_cls: 0.203  loss_box_reg: 0.362  loss_rpn_cls: 0.016  loss_rpn_loc: 0.046  time: 0.5287  data_time: 0.0096  lr: 0.000599  max_mem: 3070M
[32m[04/15 12:44:13 d2.utils.events]: [0m eta: 0:17:04  iter: 79  total_loss: 0.587  loss_cls: 0.184  loss_box_reg: 0.324  loss_rpn_cls: 0.022  loss_rpn_loc: 0.050  time: 0.5313  data_time: 0.0073  lr: 0.000799  max_mem: 3070M
[32m[04/15 12:44:24 d2.utils.events]: [0m eta: 0:16:58  iter: 99  total_loss: 0.532  loss_cls: 0.183  loss_box_reg: 0.283  loss_rpn_cls: 0.020  loss_rpn_loc: 0.055  time: 0.5340  data_time: 0.0093  lr: 0.000999  max_mem: 3070M
[32m[04/15 12:44:36 d2.utils.events]: [0m eta: 0:16:48  iter: 119  total_loss: 0.674  loss_cls: 0.222  loss_box_reg: 0.335  loss_rpn_cls: 0.022  loss_rpn_loc: 0.059  time: 0.5343  data_time: 0.0069  lr: 0.001199  max_mem: 3070M
[32m[04/15 12:44:47 d2.utils.events]: [0m eta: 0:16:35  iter: 139  total_loss: 0.717  loss_cls: 0.231  loss_box_reg: 0.399  loss_rpn_cls: 0.023  loss_rpn_loc: 0.060  time: 0.5340  data_time: 0.0076  lr: 0.001399  max_mem: 3070M
[32m[04/15 12:44:59 d2.utils.events]: [0m eta: 0:16:25  iter: 159  total_loss: 0.702  loss_cls: 0.223  loss_box_reg: 0.366  loss_rpn_cls: 0.019  loss_rpn_loc: 0.067  time: 0.5342  data_time: 0.0075  lr: 0.001598  max_mem: 3070M
[32m[04/15 12:45:10 d2.utils.events]: [0m eta: 0:16:15  iter: 179  total_loss: 0.682  loss_cls: 0.230  loss_box_reg: 0.372  loss_rpn_cls: 0.014  loss_rpn_loc: 0.072  time: 0.5350  data_time: 0.0082  lr: 0.001798  max_mem: 3070M
[32m[04/15 12:45:22 d2.utils.events]: [0m eta: 0:16:07  iter: 199  total_loss: 0.686  loss_cls: 0.219  loss_box_reg: 0.394  loss_rpn_cls: 0.016  loss_rpn_loc: 0.055  time: 0.5366  data_time: 0.0067  lr: 0.001998  max_mem: 3070M
[32m[04/15 12:45:33 d2.utils.events]: [0m eta: 0:15:54  iter: 219  total_loss: 0.729  loss_cls: 0.220  loss_box_reg: 0.395  loss_rpn_cls: 0.026  loss_rpn_loc: 0.072  time: 0.5358  data_time: 0.0097  lr: 0.002198  max_mem: 3070M
[32m[04/15 12:45:44 d2.utils.events]: [0m eta: 0:15:46  iter: 239  total_loss: 0.689  loss_cls: 0.183  loss_box_reg: 0.389  loss_rpn_cls: 0.014  loss_rpn_loc: 0.073  time: 0.5380  data_time: 0.0094  lr: 0.002398  max_mem: 3070M
[32m[04/15 12:45:55 d2.utils.events]: [0m eta: 0:15:38  iter: 259  total_loss: 0.703  loss_cls: 0.225  loss_box_reg: 0.381  loss_rpn_cls: 0.020  loss_rpn_loc: 0.055  time: 0.5382  data_time: 0.0061  lr: 0.002597  max_mem: 3070M
[32m[04/15 12:46:08 d2.utils.events]: [0m eta: 0:15:30  iter: 279  total_loss: 0.578  loss_cls: 0.185  loss_box_reg: 0.339  loss_rpn_cls: 0.015  loss_rpn_loc: 0.033  time: 0.5412  data_time: 0.0309  lr: 0.002797  max_mem: 3070M
[32m[04/15 12:46:20 d2.utils.events]: [0m eta: 0:15:24  iter: 299  total_loss: 0.749  loss_cls: 0.241  loss_box_reg: 0.417  loss_rpn_cls: 0.017  loss_rpn_loc: 0.057  time: 0.5416  data_time: 0.0089  lr: 0.002997  max_mem: 3070M
[32m[04/15 12:46:32 d2.utils.events]: [0m eta: 0:15:10  iter: 319  total_loss: 0.594  loss_cls: 0.178  loss_box_reg: 0.331  loss_rpn_cls: 0.020  loss_rpn_loc: 0.044  time: 0.5411  data_time: 0.0065  lr: 0.003197  max_mem: 3070M
[32m[04/15 12:46:43 d2.utils.events]: [0m eta: 0:14:56  iter: 339  total_loss: 0.574  loss_cls: 0.193  loss_box_reg: 0.317  loss_rpn_cls: 0.014  loss_rpn_loc: 0.052  time: 0.5400  data_time: 0.0073  lr: 0.003397  max_mem: 3070M
[32m[04/15 12:46:55 d2.utils.events]: [0m eta: 0:14:52  iter: 359  total_loss: 0.743  loss_cls: 0.227  loss_box_reg: 0.418  loss_rpn_cls: 0.016  loss_rpn_loc: 0.062  time: 0.5417  data_time: 0.0098  lr: 0.003596  max_mem: 3070M
[32m[04/15 12:47:07 d2.utils.events]: [0m eta: 0:14:41  iter: 379  total_loss: 0.658  loss_cls: 0.210  loss_box_reg: 0.353  loss_rpn_cls: 0.023  loss_rpn_loc: 0.050  time: 0.5412  data_time: 0.0071  lr: 0.003796  max_mem: 3070M
[32m[04/15 12:47:20 d2.utils.events]: [0m eta: 0:14:23  iter: 399  total_loss: 0.511  loss_cls: 0.168  loss_box_reg: 0.278  loss_rpn_cls: 0.019  loss_rpn_loc: 0.052  time: 0.5399  data_time: 0.0080  lr: 0.003996  max_mem: 3070M
[32m[04/15 12:47:33 d2.utils.events]: [0m eta: 0:14:14  iter: 419  total_loss: 0.603  loss_cls: 0.194  loss_box_reg: 0.322  loss_rpn_cls: 0.016  loss_rpn_loc: 0.048  time: 0.5401  data_time: 0.0086  lr: 0.004196  max_mem: 3070M
[32m[04/15 12:47:44 d2.utils.events]: [0m eta: 0:14:08  iter: 439  total_loss: 0.572  loss_cls: 0.190  loss_box_reg: 0.316  loss_rpn_cls: 0.012  loss_rpn_loc: 0.043  time: 0.5410  data_time: 0.0115  lr: 0.004396  max_mem: 3070M
[32m[04/15 12:47:56 d2.utils.events]: [0m eta: 0:13:56  iter: 459  total_loss: 0.666  loss_cls: 0.214  loss_box_reg: 0.370  loss_rpn_cls: 0.017  loss_rpn_loc: 0.063  time: 0.5408  data_time: 0.0084  lr: 0.004595  max_mem: 3070M
[32m[04/15 12:48:07 d2.utils.events]: [0m eta: 0:13:43  iter: 479  total_loss: 0.613  loss_cls: 0.208  loss_box_reg: 0.336  loss_rpn_cls: 0.014  loss_rpn_loc: 0.073  time: 0.5411  data_time: 0.0432  lr: 0.004795  max_mem: 3070M
[32m[04/15 12:48:24 d2.utils.events]: [0m eta: 0:13:36  iter: 499  total_loss: 0.629  loss_cls: 0.185  loss_box_reg: 0.373  loss_rpn_cls: 0.012  loss_rpn_loc: 0.054  time: 0.5500  data_time: 0.2796  lr: 0.004995  max_mem: 3070M
[32m[04/15 12:48:35 d2.utils.events]: [0m eta: 0:13:26  iter: 519  total_loss: 0.594  loss_cls: 0.197  loss_box_reg: 0.321  loss_rpn_cls: 0.011  loss_rpn_loc: 0.047  time: 0.5500  data_time: 0.0072  lr: 0.005195  max_mem: 3070M
[32m[04/15 12:48:46 d2.utils.events]: [0m eta: 0:13:15  iter: 539  total_loss: 0.584  loss_cls: 0.185  loss_box_reg: 0.316  loss_rpn_cls: 0.018  loss_rpn_loc: 0.054  time: 0.5497  data_time: 0.0071  lr: 0.005395  max_mem: 3070M
[32m[04/15 12:48:57 d2.utils.events]: [0m eta: 0:13:04  iter: 559  total_loss: 0.594  loss_cls: 0.204  loss_box_reg: 0.333  loss_rpn_cls: 0.018  loss_rpn_loc: 0.042  time: 0.5493  data_time: 0.0079  lr: 0.005594  max_mem: 3070M
[32m[04/15 12:49:13 d2.utils.events]: [0m eta: 0:12:53  iter: 579  total_loss: 0.617  loss_cls: 0.190  loss_box_reg: 0.297  loss_rpn_cls: 0.020  loss_rpn_loc: 0.042  time: 0.5491  data_time: 0.0403  lr: 0.005794  max_mem: 3070M
[32m[04/15 12:49:24 d2.utils.events]: [0m eta: 0:12:42  iter: 599  total_loss: 0.586  loss_cls: 0.187  loss_box_reg: 0.310  loss_rpn_cls: 0.018  loss_rpn_loc: 0.045  time: 0.5489  data_time: 0.0221  lr: 0.005994  max_mem: 3070M
[32m[04/15 12:49:35 d2.utils.events]: [0m eta: 0:12:31  iter: 619  total_loss: 0.621  loss_cls: 0.198  loss_box_reg: 0.337  loss_rpn_cls: 0.015  loss_rpn_loc: 0.046  time: 0.5486  data_time: 0.0086  lr: 0.006194  max_mem: 3070M
[32m[04/15 12:49:49 d2.utils.events]: [0m eta: 0:12:21  iter: 639  total_loss: 0.623  loss_cls: 0.201  loss_box_reg: 0.366  loss_rpn_cls: 0.015  loss_rpn_loc: 0.046  time: 0.5487  data_time: 0.0141  lr: 0.006394  max_mem: 3070M
[32m[04/15 12:50:01 d2.utils.events]: [0m eta: 0:12:10  iter: 659  total_loss: 0.700  loss_cls: 0.217  loss_box_reg: 0.363  loss_rpn_cls: 0.018  loss_rpn_loc: 0.062  time: 0.5485  data_time: 0.0060  lr: 0.006593  max_mem: 3070M
[32m[04/15 12:50:12 d2.utils.events]: [0m eta: 0:11:59  iter: 679  total_loss: 0.606  loss_cls: 0.206  loss_box_reg: 0.298  loss_rpn_cls: 0.016  loss_rpn_loc: 0.050  time: 0.5488  data_time: 0.0096  lr: 0.006793  max_mem: 3070M
[32m[04/15 12:50:30 d2.utils.events]: [0m eta: 0:11:48  iter: 699  total_loss: 0.626  loss_cls: 0.187  loss_box_reg: 0.326  loss_rpn_cls: 0.012  loss_rpn_loc: 0.072  time: 0.5490  data_time: 0.0465  lr: 0.006993  max_mem: 3070M
[32m[04/15 12:50:42 d2.utils.events]: [0m eta: 0:11:38  iter: 719  total_loss: 0.764  loss_cls: 0.223  loss_box_reg: 0.424  loss_rpn_cls: 0.013  loss_rpn_loc: 0.060  time: 0.5493  data_time: 0.0337  lr: 0.007193  max_mem: 3070M
[32m[04/15 12:50:54 d2.utils.events]: [0m eta: 0:11:29  iter: 739  total_loss: 0.669  loss_cls: 0.200  loss_box_reg: 0.380  loss_rpn_cls: 0.012  loss_rpn_loc: 0.061  time: 0.5507  data_time: 0.0669  lr: 0.007393  max_mem: 3070M
[32m[04/15 12:51:06 d2.utils.events]: [0m eta: 0:11:18  iter: 759  total_loss: 0.723  loss_cls: 0.226  loss_box_reg: 0.403  loss_rpn_cls: 0.016  loss_rpn_loc: 0.064  time: 0.5509  data_time: 0.0591  lr: 0.007592  max_mem: 3070M
[32m[04/15 12:51:19 d2.utils.events]: [0m eta: 0:11:06  iter: 779  total_loss: 0.648  loss_cls: 0.212  loss_box_reg: 0.353  loss_rpn_cls: 0.018  loss_rpn_loc: 0.046  time: 0.5507  data_time: 0.0433  lr: 0.007792  max_mem: 3070M
[32m[04/15 12:51:30 d2.utils.events]: [0m eta: 0:10:54  iter: 799  total_loss: 0.693  loss_cls: 0.207  loss_box_reg: 0.378  loss_rpn_cls: 0.017  loss_rpn_loc: 0.074  time: 0.5504  data_time: 0.0091  lr: 0.007992  max_mem: 3070M
[32m[04/15 12:51:42 d2.utils.events]: [0m eta: 0:10:43  iter: 819  total_loss: 0.766  loss_cls: 0.233  loss_box_reg: 0.401  loss_rpn_cls: 0.017  loss_rpn_loc: 0.057  time: 0.5501  data_time: 0.0093  lr: 0.008192  max_mem: 3070M
[32m[04/15 12:51:54 d2.utils.events]: [0m eta: 0:10:32  iter: 839  total_loss: 0.641  loss_cls: 0.209  loss_box_reg: 0.337  loss_rpn_cls: 0.015  loss_rpn_loc: 0.070  time: 0.5496  data_time: 0.0125  lr: 0.008392  max_mem: 3070M
[32m[04/15 12:52:05 d2.utils.events]: [0m eta: 0:10:20  iter: 859  total_loss: 0.683  loss_cls: 0.250  loss_box_reg: 0.412  loss_rpn_cls: 0.021  loss_rpn_loc: 0.052  time: 0.5492  data_time: 0.0069  lr: 0.008591  max_mem: 3070M
[32m[04/15 12:52:17 d2.utils.events]: [0m eta: 0:10:10  iter: 879  total_loss: 0.570  loss_cls: 0.187  loss_box_reg: 0.275  loss_rpn_cls: 0.017  loss_rpn_loc: 0.052  time: 0.5492  data_time: 0.0082  lr: 0.008791  max_mem: 3070M
[32m[04/15 12:52:29 d2.utils.events]: [0m eta: 0:09:59  iter: 899  total_loss: 0.718  loss_cls: 0.231  loss_box_reg: 0.361  loss_rpn_cls: 0.025  loss_rpn_loc: 0.063  time: 0.5493  data_time: 0.0082  lr: 0.008991  max_mem: 3070M
[32m[04/15 12:52:40 d2.utils.events]: [0m eta: 0:09:48  iter: 919  total_loss: 0.598  loss_cls: 0.201  loss_box_reg: 0.314  loss_rpn_cls: 0.022  loss_rpn_loc: 0.051  time: 0.5488  data_time: 0.0084  lr: 0.009191  max_mem: 3070M
[32m[04/15 12:52:52 d2.utils.events]: [0m eta: 0:09:38  iter: 939  total_loss: 0.654  loss_cls: 0.234  loss_box_reg: 0.370  loss_rpn_cls: 0.020  loss_rpn_loc: 0.053  time: 0.5492  data_time: 0.0072  lr: 0.009391  max_mem: 3070M
[32m[04/15 12:53:04 d2.utils.events]: [0m eta: 0:09:26  iter: 959  total_loss: 0.589  loss_cls: 0.166  loss_box_reg: 0.347  loss_rpn_cls: 0.020  loss_rpn_loc: 0.056  time: 0.5488  data_time: 0.0075  lr: 0.009590  max_mem: 3070M
[32m[04/15 12:53:16 d2.utils.events]: [0m eta: 0:09:16  iter: 979  total_loss: 0.788  loss_cls: 0.239  loss_box_reg: 0.429  loss_rpn_cls: 0.021  loss_rpn_loc: 0.061  time: 0.5489  data_time: 0.0077  lr: 0.009790  max_mem: 3070M
[32m[04/15 12:53:27 d2.utils.events]: [0m eta: 0:09:05  iter: 999  total_loss: 0.777  loss_cls: 0.244  loss_box_reg: 0.410  loss_rpn_cls: 0.020  loss_rpn_loc: 0.068  time: 0.5487  data_time: 0.0075  lr: 0.009990  max_mem: 3070M
[32m[04/15 12:53:46 d2.utils.events]: [0m eta: 0:08:54  iter: 1019  total_loss: 0.569  loss_cls: 0.189  loss_box_reg: 0.308  loss_rpn_cls: 0.013  loss_rpn_loc: 0.059  time: 0.5486  data_time: 0.0099  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:53:59 d2.utils.events]: [0m eta: 0:08:45  iter: 1039  total_loss: 0.685  loss_cls: 0.213  loss_box_reg: 0.388  loss_rpn_cls: 0.018  loss_rpn_loc: 0.059  time: 0.5485  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:54:10 d2.utils.events]: [0m eta: 0:08:34  iter: 1059  total_loss: 0.620  loss_cls: 0.202  loss_box_reg: 0.343  loss_rpn_cls: 0.019  loss_rpn_loc: 0.060  time: 0.5485  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:54:22 d2.utils.events]: [0m eta: 0:08:23  iter: 1079  total_loss: 0.611  loss_cls: 0.199  loss_box_reg: 0.309  loss_rpn_cls: 0.021  loss_rpn_loc: 0.068  time: 0.5485  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:54:35 d2.utils.events]: [0m eta: 0:08:12  iter: 1099  total_loss: 0.629  loss_cls: 0.215  loss_box_reg: 0.340  loss_rpn_cls: 0.020  loss_rpn_loc: 0.049  time: 0.5487  data_time: 0.0090  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:54:46 d2.utils.events]: [0m eta: 0:08:02  iter: 1119  total_loss: 0.726  loss_cls: 0.229  loss_box_reg: 0.380  loss_rpn_cls: 0.017  loss_rpn_loc: 0.055  time: 0.5485  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:55:00 d2.utils.events]: [0m eta: 0:07:51  iter: 1139  total_loss: 0.722  loss_cls: 0.238  loss_box_reg: 0.383  loss_rpn_cls: 0.017  loss_rpn_loc: 0.073  time: 0.5480  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:55:11 d2.utils.events]: [0m eta: 0:07:40  iter: 1159  total_loss: 0.652  loss_cls: 0.209  loss_box_reg: 0.343  loss_rpn_cls: 0.024  loss_rpn_loc: 0.047  time: 0.5479  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:55:22 d2.utils.events]: [0m eta: 0:07:30  iter: 1179  total_loss: 0.696  loss_cls: 0.235  loss_box_reg: 0.340  loss_rpn_cls: 0.026  loss_rpn_loc: 0.059  time: 0.5481  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:55:34 d2.utils.events]: [0m eta: 0:07:18  iter: 1199  total_loss: 0.600  loss_cls: 0.177  loss_box_reg: 0.334  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5483  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:55:45 d2.utils.events]: [0m eta: 0:07:08  iter: 1219  total_loss: 0.696  loss_cls: 0.238  loss_box_reg: 0.367  loss_rpn_cls: 0.025  loss_rpn_loc: 0.061  time: 0.5483  data_time: 0.0091  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:55:57 d2.utils.events]: [0m eta: 0:06:57  iter: 1239  total_loss: 0.609  loss_cls: 0.194  loss_box_reg: 0.354  loss_rpn_cls: 0.020  loss_rpn_loc: 0.049  time: 0.5483  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:56:09 d2.utils.events]: [0m eta: 0:06:45  iter: 1259  total_loss: 0.558  loss_cls: 0.184  loss_box_reg: 0.290  loss_rpn_cls: 0.024  loss_rpn_loc: 0.050  time: 0.5481  data_time: 0.0096  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:56:21 d2.utils.events]: [0m eta: 0:06:34  iter: 1279  total_loss: 0.660  loss_cls: 0.204  loss_box_reg: 0.344  loss_rpn_cls: 0.019  loss_rpn_loc: 0.067  time: 0.5480  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:56:33 d2.utils.events]: [0m eta: 0:06:23  iter: 1299  total_loss: 0.605  loss_cls: 0.199  loss_box_reg: 0.322  loss_rpn_cls: 0.025  loss_rpn_loc: 0.053  time: 0.5477  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:56:44 d2.utils.events]: [0m eta: 0:06:12  iter: 1319  total_loss: 0.693  loss_cls: 0.211  loss_box_reg: 0.374  loss_rpn_cls: 0.019  loss_rpn_loc: 0.071  time: 0.5479  data_time: 0.0159  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:56:56 d2.utils.events]: [0m eta: 0:06:01  iter: 1339  total_loss: 0.699  loss_cls: 0.238  loss_box_reg: 0.371  loss_rpn_cls: 0.020  loss_rpn_loc: 0.071  time: 0.5479  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:57:08 d2.utils.events]: [0m eta: 0:05:50  iter: 1359  total_loss: 0.707  loss_cls: 0.221  loss_box_reg: 0.395  loss_rpn_cls: 0.018  loss_rpn_loc: 0.061  time: 0.5480  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:57:21 d2.utils.events]: [0m eta: 0:05:39  iter: 1379  total_loss: 0.461  loss_cls: 0.164  loss_box_reg: 0.273  loss_rpn_cls: 0.012  loss_rpn_loc: 0.042  time: 0.5480  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:57:32 d2.utils.events]: [0m eta: 0:05:29  iter: 1399  total_loss: 0.511  loss_cls: 0.166  loss_box_reg: 0.281  loss_rpn_cls: 0.015  loss_rpn_loc: 0.045  time: 0.5482  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:57:44 d2.utils.events]: [0m eta: 0:05:18  iter: 1419  total_loss: 0.602  loss_cls: 0.194  loss_box_reg: 0.332  loss_rpn_cls: 0.018  loss_rpn_loc: 0.065  time: 0.5480  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:57:56 d2.utils.events]: [0m eta: 0:05:08  iter: 1439  total_loss: 0.633  loss_cls: 0.194  loss_box_reg: 0.330  loss_rpn_cls: 0.023  loss_rpn_loc: 0.056  time: 0.5482  data_time: 0.0094  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:58:07 d2.utils.events]: [0m eta: 0:04:57  iter: 1459  total_loss: 0.592  loss_cls: 0.191  loss_box_reg: 0.351  loss_rpn_cls: 0.024  loss_rpn_loc: 0.055  time: 0.5483  data_time: 0.0060  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:58:18 d2.utils.events]: [0m eta: 0:04:46  iter: 1479  total_loss: 0.596  loss_cls: 0.181  loss_box_reg: 0.318  loss_rpn_cls: 0.015  loss_rpn_loc: 0.054  time: 0.5483  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:58:29 d2.utils.events]: [0m eta: 0:04:35  iter: 1499  total_loss: 0.690  loss_cls: 0.223  loss_box_reg: 0.393  loss_rpn_cls: 0.014  loss_rpn_loc: 0.059  time: 0.5482  data_time: 0.0120  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:58:40 d2.utils.events]: [0m eta: 0:04:23  iter: 1519  total_loss: 0.851  loss_cls: 0.249  loss_box_reg: 0.449  loss_rpn_cls: 0.024  loss_rpn_loc: 0.114  time: 0.5480  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:58:53 d2.utils.events]: [0m eta: 0:04:12  iter: 1539  total_loss: 0.659  loss_cls: 0.223  loss_box_reg: 0.357  loss_rpn_cls: 0.022  loss_rpn_loc: 0.066  time: 0.5479  data_time: 0.0202  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:59:05 d2.utils.events]: [0m eta: 0:04:02  iter: 1559  total_loss: 0.579  loss_cls: 0.174  loss_box_reg: 0.314  loss_rpn_cls: 0.021  loss_rpn_loc: 0.042  time: 0.5482  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:59:16 d2.utils.events]: [0m eta: 0:03:51  iter: 1579  total_loss: 0.623  loss_cls: 0.206  loss_box_reg: 0.312  loss_rpn_cls: 0.023  loss_rpn_loc: 0.072  time: 0.5483  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:59:31 d2.utils.events]: [0m eta: 0:03:40  iter: 1599  total_loss: 0.617  loss_cls: 0.192  loss_box_reg: 0.362  loss_rpn_cls: 0.030  loss_rpn_loc: 0.062  time: 0.5481  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:59:42 d2.utils.events]: [0m eta: 0:03:29  iter: 1619  total_loss: 0.608  loss_cls: 0.206  loss_box_reg: 0.338  loss_rpn_cls: 0.016  loss_rpn_loc: 0.061  time: 0.5482  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 12:59:53 d2.utils.events]: [0m eta: 0:03:18  iter: 1639  total_loss: 0.681  loss_cls: 0.227  loss_box_reg: 0.404  loss_rpn_cls: 0.022  loss_rpn_loc: 0.062  time: 0.5480  data_time: 0.0062  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:00:08 d2.utils.events]: [0m eta: 0:03:07  iter: 1659  total_loss: 0.657  loss_cls: 0.230  loss_box_reg: 0.297  loss_rpn_cls: 0.019  loss_rpn_loc: 0.079  time: 0.5478  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:00:20 d2.utils.events]: [0m eta: 0:02:56  iter: 1679  total_loss: 0.679  loss_cls: 0.218  loss_box_reg: 0.342  loss_rpn_cls: 0.022  loss_rpn_loc: 0.074  time: 0.5479  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:00:34 d2.utils.events]: [0m eta: 0:02:45  iter: 1699  total_loss: 0.622  loss_cls: 0.224  loss_box_reg: 0.345  loss_rpn_cls: 0.022  loss_rpn_loc: 0.055  time: 0.5477  data_time: 0.0255  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:00:48 d2.utils.events]: [0m eta: 0:02:34  iter: 1719  total_loss: 0.724  loss_cls: 0.241  loss_box_reg: 0.367  loss_rpn_cls: 0.025  loss_rpn_loc: 0.074  time: 0.5478  data_time: 0.0732  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:00:59 d2.utils.events]: [0m eta: 0:02:22  iter: 1739  total_loss: 0.648  loss_cls: 0.208  loss_box_reg: 0.365  loss_rpn_cls: 0.020  loss_rpn_loc: 0.067  time: 0.5477  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:01:11 d2.utils.events]: [0m eta: 0:02:11  iter: 1759  total_loss: 0.563  loss_cls: 0.172  loss_box_reg: 0.328  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5476  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:01:24 d2.utils.events]: [0m eta: 0:02:01  iter: 1779  total_loss: 0.559  loss_cls: 0.191  loss_box_reg: 0.318  loss_rpn_cls: 0.016  loss_rpn_loc: 0.050  time: 0.5476  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:02:02 d2.utils.events]: [0m eta: 0:01:50  iter: 1799  total_loss: 0.716  loss_cls: 0.208  loss_box_reg: 0.372  loss_rpn_cls: 0.029  loss_rpn_loc: 0.071  time: 0.5474  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:02:14 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.603  loss_cls: 0.186  loss_box_reg: 0.314  loss_rpn_cls: 0.026  loss_rpn_loc: 0.063  time: 0.5476  data_time: 0.0714  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:02:26 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.714  loss_cls: 0.206  loss_box_reg: 0.401  loss_rpn_cls: 0.019  loss_rpn_loc: 0.062  time: 0.5474  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:02:41 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.700  loss_cls: 0.229  loss_box_reg: 0.362  loss_rpn_cls: 0.023  loss_rpn_loc: 0.074  time: 0.5472  data_time: 0.0087  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:02:52 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.731  loss_cls: 0.219  loss_box_reg: 0.408  loss_rpn_cls: 0.018  loss_rpn_loc: 0.078  time: 0.5470  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:03:05 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.721  loss_cls: 0.204  loss_box_reg: 0.384  loss_rpn_cls: 0.025  loss_rpn_loc: 0.080  time: 0.5467  data_time: 0.0054  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:03:20 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.698  loss_cls: 0.221  loss_box_reg: 0.387  loss_rpn_cls: 0.033  loss_rpn_loc: 0.074  time: 0.5469  data_time: 0.0426  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:03:32 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.720  loss_cls: 0.200  loss_box_reg: 0.375  loss_rpn_cls: 0.026  loss_rpn_loc: 0.080  time: 0.5475  data_time: 0.0994  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:03:48 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.618  loss_cls: 0.205  loss_box_reg: 0.349  loss_rpn_cls: 0.020  loss_rpn_loc: 0.063  time: 0.5473  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:03:59 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.654  loss_cls: 0.211  loss_box_reg: 0.375  loss_rpn_cls: 0.020  loss_rpn_loc: 0.049  time: 0.5473  data_time: 0.0369  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 13:05:12 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:05:12 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 13:05:12 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 13:05:12 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.747  loss_cls: 0.220  loss_box_reg: 0.405  loss_rpn_cls: 0.023  loss_rpn_loc: 0.079  time: 0.5492  data_time: 0.2563  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:05:17 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:17 (0.5495 s / it)
[32m[04/15 13:05:17 d2.engine.hooks]: [0mTotal training time: 0:21:50 (0:03:33 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 13:05:40 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:05:40 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 13:05:40 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 13:05:44 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1198 s / img. ETA=0:04:07
[32m[04/15 13:05:49 d2.evaluation.evaluator]: [0mInference done 36/1257. 0.1202 s / img. ETA=0:04:08
[32m[04/15 13:05:54 d2.evaluation.evaluator]: [0mInference done 52/1257. 0.1205 s / img. ETA=0:04:51
[32m[04/15 13:05:59 d2.evaluation.evaluator]: [0mInference done 63/1257. 0.1209 s / img. ETA=0:05:42
[32m[04/15 13:06:05 d2.evaluation.evaluator]: [0mInference done 81/1257. 0.1211 s / img. ETA=0:05:42
[32m[04/15 13:06:10 d2.evaluation.evaluator]: [0mInference done 103/1257. 0.1210 s / img. ETA=0:05:20
[32m[04/15 13:06:15 d2.evaluation.evaluator]: [0mInference done 124/1257. 0.1208 s / img. ETA=0:05:06
[32m[04/15 13:06:20 d2.evaluation.evaluator]: [0mInference done 145/1257. 0.1205 s / img. ETA=0:04:56
[32m[04/15 13:06:25 d2.evaluation.evaluator]: [0mInference done 179/1257. 0.1205 s / img. ETA=0:04:22
[32m[04/15 13:06:30 d2.evaluation.evaluator]: [0mInference done 207/1257. 0.1205 s / img. ETA=0:04:06
[32m[04/15 13:06:35 d2.evaluation.evaluator]: [0mInference done 229/1257. 0.1206 s / img. ETA=0:04:01
[32m[04/15 13:06:40 d2.evaluation.evaluator]: [0mInference done 255/1257. 0.1207 s / img. ETA=0:03:51
[32m[04/15 13:06:45 d2.evaluation.evaluator]: [0mInference done 276/1257. 0.1209 s / img. ETA=0:03:46
[32m[04/15 13:06:51 d2.evaluation.evaluator]: [0mInference done 295/1257. 0.1210 s / img. ETA=0:03:46
[32m[04/15 13:06:56 d2.evaluation.evaluator]: [0mInference done 310/1257. 0.1208 s / img. ETA=0:03:47
[32m[04/15 13:07:01 d2.evaluation.evaluator]: [0mInference done 341/1257. 0.1208 s / img. ETA=0:03:33
[32m[04/15 13:07:06 d2.evaluation.evaluator]: [0mInference done 369/1257. 0.1207 s / img. ETA=0:03:23
[32m[04/15 13:07:11 d2.evaluation.evaluator]: [0mInference done 390/1257. 0.1206 s / img. ETA=0:03:19
[32m[04/15 13:07:16 d2.evaluation.evaluator]: [0mInference done 412/1257. 0.1206 s / img. ETA=0:03:14
[32m[04/15 13:07:21 d2.evaluation.evaluator]: [0mInference done 430/1257. 0.1206 s / img. ETA=0:03:12
[32m[04/15 13:07:28 d2.evaluation.evaluator]: [0mInference done 439/1257. 0.1206 s / img. ETA=0:03:18
[32m[04/15 13:07:33 d2.evaluation.evaluator]: [0mInference done 459/1257. 0.1205 s / img. ETA=0:03:14
[32m[04/15 13:07:38 d2.evaluation.evaluator]: [0mInference done 488/1257. 0.1205 s / img. ETA=0:03:03
[32m[04/15 13:07:43 d2.evaluation.evaluator]: [0mInference done 507/1257. 0.1205 s / img. ETA=0:03:00
[32m[04/15 13:07:48 d2.evaluation.evaluator]: [0mInference done 526/1257. 0.1205 s / img. ETA=0:02:56
[32m[04/15 13:07:54 d2.evaluation.evaluator]: [0mInference done 553/1257. 0.1215 s / img. ETA=0:02:48
[32m[04/15 13:07:59 d2.evaluation.evaluator]: [0mInference done 574/1257. 0.1216 s / img. ETA=0:02:43
[32m[04/15 13:08:04 d2.evaluation.evaluator]: [0mInference done 599/1257. 0.1216 s / img. ETA=0:02:37
[32m[04/15 13:08:09 d2.evaluation.evaluator]: [0mInference done 605/1257. 0.1216 s / img. ETA=0:02:39
[32m[04/15 13:08:15 d2.evaluation.evaluator]: [0mInference done 625/1257. 0.1215 s / img. ETA=0:02:35
[32m[04/15 13:08:20 d2.evaluation.evaluator]: [0mInference done 640/1257. 0.1215 s / img. ETA=0:02:32
[32m[04/15 13:08:25 d2.evaluation.evaluator]: [0mInference done 655/1257. 0.1214 s / img. ETA=0:02:30
[32m[04/15 13:08:30 d2.evaluation.evaluator]: [0mInference done 678/1257. 0.1214 s / img. ETA=0:02:24
[32m[04/15 13:08:35 d2.evaluation.evaluator]: [0mInference done 700/1257. 0.1215 s / img. ETA=0:02:18
[32m[04/15 13:08:40 d2.evaluation.evaluator]: [0mInference done 711/1257. 0.1214 s / img. ETA=0:02:17
[32m[04/15 13:08:45 d2.evaluation.evaluator]: [0mInference done 730/1257. 0.1214 s / img. ETA=0:02:13
[32m[04/15 13:08:51 d2.evaluation.evaluator]: [0mInference done 743/1257. 0.1213 s / img. ETA=0:02:11
[32m[04/15 13:08:56 d2.evaluation.evaluator]: [0mInference done 772/1257. 0.1213 s / img. ETA=0:02:02
[32m[04/15 13:09:01 d2.evaluation.evaluator]: [0mInference done 798/1257. 0.1213 s / img. ETA=0:01:55
[32m[04/15 13:09:07 d2.evaluation.evaluator]: [0mInference done 827/1257. 0.1213 s / img. ETA=0:01:47
[32m[04/15 13:09:12 d2.evaluation.evaluator]: [0mInference done 852/1257. 0.1213 s / img. ETA=0:01:40
[32m[04/15 13:09:17 d2.evaluation.evaluator]: [0mInference done 881/1257. 0.1214 s / img. ETA=0:01:32
[32m[04/15 13:09:22 d2.evaluation.evaluator]: [0mInference done 912/1257. 0.1214 s / img. ETA=0:01:23
[32m[04/15 13:09:27 d2.evaluation.evaluator]: [0mInference done 941/1257. 0.1214 s / img. ETA=0:01:15
[32m[04/15 13:09:32 d2.evaluation.evaluator]: [0mInference done 982/1257. 0.1214 s / img. ETA=0:01:04
[32m[04/15 13:09:37 d2.evaluation.evaluator]: [0mInference done 1022/1257. 0.1214 s / img. ETA=0:00:54
[32m[04/15 13:09:42 d2.evaluation.evaluator]: [0mInference done 1062/1257. 0.1215 s / img. ETA=0:00:44
[32m[04/15 13:09:48 d2.evaluation.evaluator]: [0mInference done 1103/1257. 0.1215 s / img. ETA=0:00:34
[32m[04/15 13:09:53 d2.evaluation.evaluator]: [0mInference done 1143/1257. 0.1216 s / img. ETA=0:00:25
[32m[04/15 13:09:58 d2.evaluation.evaluator]: [0mInference done 1183/1257. 0.1216 s / img. ETA=0:00:16
[32m[04/15 13:10:03 d2.evaluation.evaluator]: [0mInference done 1224/1257. 0.1216 s / img. ETA=0:00:07
[32m[04/15 13:10:07 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:24.678147 (0.211404 s / img per device, on 1 devices)
[32m[04/15 13:10:07 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:32 (0.121615 s / img per device, on 1 devices)
[32m[04/15 13:10:07 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 13:10:07 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 13:10:07 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.02s).
Accumulating evaluation results...
DONE (t=0.68s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.185
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.157
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.116
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.388
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.232
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.234
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.171
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.274
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.420
[32m[04/15 13:10:13 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 18.479 | 36.681 | 15.662 | 11.602 | 22.272 | 38.832 |
[32m[04/15 13:10:13 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 26.560 | bicycle       | 5.075 | car            | 42.279 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  20  *  2000  iterations ============
4 channel input
[32m[04/15 13:10:15 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 13:10:16 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.17 seconds.
[5m[31mWARNING[0m [32m[04/15 13:10:16 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:10:16 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 13:10:17 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 13:10:17 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 13:10:17 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 13:10:17 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 13:10:28 d2.utils.events]: [0m eta: 0:17:17  iter: 19  total_loss: 0.704  loss_cls: 0.210  loss_box_reg: 0.368  loss_rpn_cls: 0.021  loss_rpn_loc: 0.068  time: 0.5354  data_time: 0.0595  lr: 0.000200  max_mem: 3070M
[32m[04/15 13:10:44 d2.utils.events]: [0m eta: 0:17:21  iter: 39  total_loss: 0.744  loss_cls: 0.238  loss_box_reg: 0.395  loss_rpn_cls: 0.022  loss_rpn_loc: 0.065  time: 0.5365  data_time: 0.0063  lr: 0.000400  max_mem: 3070M
[32m[04/15 13:10:55 d2.utils.events]: [0m eta: 0:17:20  iter: 59  total_loss: 0.605  loss_cls: 0.214  loss_box_reg: 0.316  loss_rpn_cls: 0.020  loss_rpn_loc: 0.047  time: 0.5397  data_time: 0.0127  lr: 0.000599  max_mem: 3070M
[32m[04/15 13:11:06 d2.utils.events]: [0m eta: 0:17:10  iter: 79  total_loss: 0.633  loss_cls: 0.184  loss_box_reg: 0.318  loss_rpn_cls: 0.017  loss_rpn_loc: 0.058  time: 0.5447  data_time: 0.0626  lr: 0.000799  max_mem: 3070M
[32m[04/15 13:11:18 d2.utils.events]: [0m eta: 0:17:00  iter: 99  total_loss: 0.632  loss_cls: 0.226  loss_box_reg: 0.340  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5453  data_time: 0.0085  lr: 0.000999  max_mem: 3070M
[32m[04/15 13:11:29 d2.utils.events]: [0m eta: 0:16:49  iter: 119  total_loss: 0.614  loss_cls: 0.210  loss_box_reg: 0.322  loss_rpn_cls: 0.020  loss_rpn_loc: 0.059  time: 0.5451  data_time: 0.0187  lr: 0.001199  max_mem: 3070M
[32m[04/15 13:11:40 d2.utils.events]: [0m eta: 0:16:39  iter: 139  total_loss: 0.541  loss_cls: 0.174  loss_box_reg: 0.302  loss_rpn_cls: 0.015  loss_rpn_loc: 0.049  time: 0.5424  data_time: 0.0244  lr: 0.001399  max_mem: 3070M
[32m[04/15 13:11:56 d2.utils.events]: [0m eta: 0:16:27  iter: 159  total_loss: 0.617  loss_cls: 0.204  loss_box_reg: 0.351  loss_rpn_cls: 0.020  loss_rpn_loc: 0.044  time: 0.5419  data_time: 0.0065  lr: 0.001598  max_mem: 3070M
[32m[04/15 13:12:07 d2.utils.events]: [0m eta: 0:16:19  iter: 179  total_loss: 0.726  loss_cls: 0.220  loss_box_reg: 0.357  loss_rpn_cls: 0.022  loss_rpn_loc: 0.070  time: 0.5423  data_time: 0.0073  lr: 0.001798  max_mem: 3070M
[32m[04/15 13:12:18 d2.utils.events]: [0m eta: 0:16:14  iter: 199  total_loss: 0.591  loss_cls: 0.194  loss_box_reg: 0.333  loss_rpn_cls: 0.019  loss_rpn_loc: 0.046  time: 0.5424  data_time: 0.0061  lr: 0.001998  max_mem: 3070M
[32m[04/15 13:12:31 d2.utils.events]: [0m eta: 0:16:04  iter: 219  total_loss: 0.610  loss_cls: 0.185  loss_box_reg: 0.326  loss_rpn_cls: 0.015  loss_rpn_loc: 0.056  time: 0.5473  data_time: 0.0983  lr: 0.002198  max_mem: 3070M
[32m[04/15 13:12:54 d2.utils.events]: [0m eta: 0:15:52  iter: 239  total_loss: 0.685  loss_cls: 0.204  loss_box_reg: 0.367  loss_rpn_cls: 0.018  loss_rpn_loc: 0.057  time: 0.5548  data_time: 0.1486  lr: 0.002398  max_mem: 3070M
[32m[04/15 13:13:11 d2.utils.events]: [0m eta: 0:15:40  iter: 259  total_loss: 0.530  loss_cls: 0.162  loss_box_reg: 0.310  loss_rpn_cls: 0.012  loss_rpn_loc: 0.042  time: 0.5529  data_time: 0.0072  lr: 0.002597  max_mem: 3070M
[32m[04/15 13:13:25 d2.utils.events]: [0m eta: 0:15:31  iter: 279  total_loss: 0.615  loss_cls: 0.186  loss_box_reg: 0.334  loss_rpn_cls: 0.013  loss_rpn_loc: 0.047  time: 0.5529  data_time: 0.0074  lr: 0.002797  max_mem: 3070M
[32m[04/15 13:13:36 d2.utils.events]: [0m eta: 0:15:21  iter: 299  total_loss: 0.522  loss_cls: 0.166  loss_box_reg: 0.256  loss_rpn_cls: 0.013  loss_rpn_loc: 0.041  time: 0.5526  data_time: 0.0088  lr: 0.002997  max_mem: 3070M
[32m[04/15 13:13:49 d2.utils.events]: [0m eta: 0:15:11  iter: 319  total_loss: 0.629  loss_cls: 0.206  loss_box_reg: 0.349  loss_rpn_cls: 0.012  loss_rpn_loc: 0.044  time: 0.5529  data_time: 0.0078  lr: 0.003197  max_mem: 3070M
[32m[04/15 13:14:01 d2.utils.events]: [0m eta: 0:15:00  iter: 339  total_loss: 0.674  loss_cls: 0.244  loss_box_reg: 0.383  loss_rpn_cls: 0.020  loss_rpn_loc: 0.052  time: 0.5527  data_time: 0.0281  lr: 0.003397  max_mem: 3070M
[32m[04/15 13:14:17 d2.utils.events]: [0m eta: 0:14:51  iter: 359  total_loss: 0.543  loss_cls: 0.173  loss_box_reg: 0.306  loss_rpn_cls: 0.017  loss_rpn_loc: 0.062  time: 0.5527  data_time: 0.0081  lr: 0.003596  max_mem: 3070M
[32m[04/15 13:14:28 d2.utils.events]: [0m eta: 0:14:41  iter: 379  total_loss: 0.603  loss_cls: 0.200  loss_box_reg: 0.325  loss_rpn_cls: 0.012  loss_rpn_loc: 0.054  time: 0.5523  data_time: 0.0143  lr: 0.003796  max_mem: 3070M
[32m[04/15 13:14:39 d2.utils.events]: [0m eta: 0:14:31  iter: 399  total_loss: 0.605  loss_cls: 0.189  loss_box_reg: 0.322  loss_rpn_cls: 0.010  loss_rpn_loc: 0.055  time: 0.5522  data_time: 0.0071  lr: 0.003996  max_mem: 3070M
[32m[04/15 13:14:53 d2.utils.events]: [0m eta: 0:14:18  iter: 419  total_loss: 0.586  loss_cls: 0.195  loss_box_reg: 0.326  loss_rpn_cls: 0.014  loss_rpn_loc: 0.054  time: 0.5508  data_time: 0.0126  lr: 0.004196  max_mem: 3070M
[32m[04/15 13:15:04 d2.utils.events]: [0m eta: 0:14:08  iter: 439  total_loss: 0.557  loss_cls: 0.192  loss_box_reg: 0.308  loss_rpn_cls: 0.011  loss_rpn_loc: 0.045  time: 0.5506  data_time: 0.0073  lr: 0.004396  max_mem: 3070M
[32m[04/15 13:15:16 d2.utils.events]: [0m eta: 0:13:57  iter: 459  total_loss: 0.674  loss_cls: 0.231  loss_box_reg: 0.369  loss_rpn_cls: 0.011  loss_rpn_loc: 0.048  time: 0.5508  data_time: 0.0066  lr: 0.004595  max_mem: 3070M
[32m[04/15 13:15:28 d2.utils.events]: [0m eta: 0:13:47  iter: 479  total_loss: 0.636  loss_cls: 0.211  loss_box_reg: 0.366  loss_rpn_cls: 0.018  loss_rpn_loc: 0.058  time: 0.5506  data_time: 0.0068  lr: 0.004795  max_mem: 3070M
[32m[04/15 13:15:46 d2.utils.events]: [0m eta: 0:13:36  iter: 499  total_loss: 0.698  loss_cls: 0.219  loss_box_reg: 0.395  loss_rpn_cls: 0.020  loss_rpn_loc: 0.056  time: 0.5520  data_time: 0.0822  lr: 0.004995  max_mem: 3070M
[32m[04/15 13:16:00 d2.utils.events]: [0m eta: 0:13:25  iter: 519  total_loss: 0.715  loss_cls: 0.219  loss_box_reg: 0.362  loss_rpn_cls: 0.015  loss_rpn_loc: 0.064  time: 0.5518  data_time: 0.0068  lr: 0.005195  max_mem: 3070M
[32m[04/15 13:16:12 d2.utils.events]: [0m eta: 0:13:15  iter: 539  total_loss: 0.646  loss_cls: 0.210  loss_box_reg: 0.340  loss_rpn_cls: 0.016  loss_rpn_loc: 0.058  time: 0.5518  data_time: 0.0242  lr: 0.005395  max_mem: 3070M
[32m[04/15 13:16:24 d2.utils.events]: [0m eta: 0:13:04  iter: 559  total_loss: 0.804  loss_cls: 0.236  loss_box_reg: 0.425  loss_rpn_cls: 0.024  loss_rpn_loc: 0.078  time: 0.5512  data_time: 0.0136  lr: 0.005594  max_mem: 3070M
[32m[04/15 13:16:36 d2.utils.events]: [0m eta: 0:12:54  iter: 579  total_loss: 0.615  loss_cls: 0.170  loss_box_reg: 0.381  loss_rpn_cls: 0.022  loss_rpn_loc: 0.055  time: 0.5514  data_time: 0.0067  lr: 0.005794  max_mem: 3070M
[32m[04/15 13:16:47 d2.utils.events]: [0m eta: 0:12:44  iter: 599  total_loss: 0.673  loss_cls: 0.208  loss_box_reg: 0.364  loss_rpn_cls: 0.017  loss_rpn_loc: 0.058  time: 0.5516  data_time: 0.0437  lr: 0.005994  max_mem: 3070M
[32m[04/15 13:16:58 d2.utils.events]: [0m eta: 0:12:32  iter: 619  total_loss: 0.667  loss_cls: 0.230  loss_box_reg: 0.329  loss_rpn_cls: 0.020  loss_rpn_loc: 0.048  time: 0.5510  data_time: 0.0098  lr: 0.006194  max_mem: 3070M
[32m[04/15 13:17:12 d2.utils.events]: [0m eta: 0:12:21  iter: 639  total_loss: 0.724  loss_cls: 0.222  loss_box_reg: 0.377  loss_rpn_cls: 0.023  loss_rpn_loc: 0.079  time: 0.5523  data_time: 0.1114  lr: 0.006394  max_mem: 3070M
[32m[04/15 13:17:27 d2.utils.events]: [0m eta: 0:12:10  iter: 659  total_loss: 0.530  loss_cls: 0.154  loss_box_reg: 0.298  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5531  data_time: 0.0712  lr: 0.006593  max_mem: 3070M
[32m[04/15 13:17:41 d2.utils.events]: [0m eta: 0:11:58  iter: 679  total_loss: 0.533  loss_cls: 0.165  loss_box_reg: 0.322  loss_rpn_cls: 0.010  loss_rpn_loc: 0.037  time: 0.5521  data_time: 0.0081  lr: 0.006793  max_mem: 3070M
[32m[04/15 13:17:59 d2.utils.events]: [0m eta: 0:11:46  iter: 699  total_loss: 0.570  loss_cls: 0.189  loss_box_reg: 0.286  loss_rpn_cls: 0.013  loss_rpn_loc: 0.044  time: 0.5514  data_time: 0.0084  lr: 0.006993  max_mem: 3070M
[32m[04/15 13:18:09 d2.utils.events]: [0m eta: 0:11:35  iter: 719  total_loss: 0.635  loss_cls: 0.214  loss_box_reg: 0.351  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5510  data_time: 0.0058  lr: 0.007193  max_mem: 3070M
[32m[04/15 13:18:21 d2.utils.events]: [0m eta: 0:11:24  iter: 739  total_loss: 0.716  loss_cls: 0.243  loss_box_reg: 0.400  loss_rpn_cls: 0.017  loss_rpn_loc: 0.061  time: 0.5505  data_time: 0.0046  lr: 0.007393  max_mem: 3070M
[32m[04/15 13:18:31 d2.utils.events]: [0m eta: 0:11:13  iter: 759  total_loss: 0.659  loss_cls: 0.194  loss_box_reg: 0.352  loss_rpn_cls: 0.021  loss_rpn_loc: 0.075  time: 0.5501  data_time: 0.0098  lr: 0.007592  max_mem: 3070M
[32m[04/15 13:18:42 d2.utils.events]: [0m eta: 0:11:03  iter: 779  total_loss: 0.564  loss_cls: 0.179  loss_box_reg: 0.333  loss_rpn_cls: 0.016  loss_rpn_loc: 0.054  time: 0.5500  data_time: 0.0064  lr: 0.007792  max_mem: 3070M
[32m[04/15 13:18:59 d2.utils.events]: [0m eta: 0:10:51  iter: 799  total_loss: 0.554  loss_cls: 0.184  loss_box_reg: 0.295  loss_rpn_cls: 0.019  loss_rpn_loc: 0.051  time: 0.5492  data_time: 0.0075  lr: 0.007992  max_mem: 3070M
[32m[04/15 13:19:10 d2.utils.events]: [0m eta: 0:10:40  iter: 819  total_loss: 0.599  loss_cls: 0.190  loss_box_reg: 0.329  loss_rpn_cls: 0.012  loss_rpn_loc: 0.070  time: 0.5488  data_time: 0.0072  lr: 0.008192  max_mem: 3070M
[32m[04/15 13:19:21 d2.utils.events]: [0m eta: 0:10:30  iter: 839  total_loss: 0.832  loss_cls: 0.247  loss_box_reg: 0.428  loss_rpn_cls: 0.021  loss_rpn_loc: 0.079  time: 0.5488  data_time: 0.0091  lr: 0.008392  max_mem: 3070M
[32m[04/15 13:19:33 d2.utils.events]: [0m eta: 0:10:20  iter: 859  total_loss: 0.651  loss_cls: 0.185  loss_box_reg: 0.344  loss_rpn_cls: 0.012  loss_rpn_loc: 0.065  time: 0.5491  data_time: 0.0068  lr: 0.008591  max_mem: 3070M
[32m[04/15 13:19:44 d2.utils.events]: [0m eta: 0:10:09  iter: 879  total_loss: 0.649  loss_cls: 0.200  loss_box_reg: 0.377  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5493  data_time: 0.0081  lr: 0.008791  max_mem: 3070M
[32m[04/15 13:19:55 d2.utils.events]: [0m eta: 0:09:59  iter: 899  total_loss: 0.718  loss_cls: 0.244  loss_box_reg: 0.367  loss_rpn_cls: 0.023  loss_rpn_loc: 0.049  time: 0.5496  data_time: 0.0080  lr: 0.008991  max_mem: 3070M
[32m[04/15 13:20:09 d2.utils.events]: [0m eta: 0:09:48  iter: 919  total_loss: 0.587  loss_cls: 0.185  loss_box_reg: 0.311  loss_rpn_cls: 0.022  loss_rpn_loc: 0.053  time: 0.5527  data_time: 0.1804  lr: 0.009191  max_mem: 3070M
[32m[04/15 13:20:20 d2.utils.events]: [0m eta: 0:09:37  iter: 939  total_loss: 0.679  loss_cls: 0.223  loss_box_reg: 0.382  loss_rpn_cls: 0.023  loss_rpn_loc: 0.068  time: 0.5523  data_time: 0.0070  lr: 0.009391  max_mem: 3070M
[32m[04/15 13:20:32 d2.utils.events]: [0m eta: 0:09:27  iter: 959  total_loss: 0.624  loss_cls: 0.203  loss_box_reg: 0.331  loss_rpn_cls: 0.023  loss_rpn_loc: 0.062  time: 0.5522  data_time: 0.0077  lr: 0.009590  max_mem: 3070M
[32m[04/15 13:20:43 d2.utils.events]: [0m eta: 0:09:16  iter: 979  total_loss: 0.545  loss_cls: 0.173  loss_box_reg: 0.314  loss_rpn_cls: 0.015  loss_rpn_loc: 0.042  time: 0.5525  data_time: 0.0104  lr: 0.009790  max_mem: 3070M
[32m[04/15 13:20:54 d2.utils.events]: [0m eta: 0:09:05  iter: 999  total_loss: 0.645  loss_cls: 0.208  loss_box_reg: 0.363  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5526  data_time: 0.0076  lr: 0.009990  max_mem: 3070M
[32m[04/15 13:21:05 d2.utils.events]: [0m eta: 0:08:54  iter: 1019  total_loss: 0.737  loss_cls: 0.241  loss_box_reg: 0.381  loss_rpn_cls: 0.025  loss_rpn_loc: 0.073  time: 0.5522  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:21:16 d2.utils.events]: [0m eta: 0:08:44  iter: 1039  total_loss: 0.713  loss_cls: 0.233  loss_box_reg: 0.424  loss_rpn_cls: 0.026  loss_rpn_loc: 0.054  time: 0.5521  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:21:28 d2.utils.events]: [0m eta: 0:08:33  iter: 1059  total_loss: 0.590  loss_cls: 0.194  loss_box_reg: 0.337  loss_rpn_cls: 0.017  loss_rpn_loc: 0.067  time: 0.5522  data_time: 0.0214  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:21:39 d2.utils.events]: [0m eta: 0:08:22  iter: 1079  total_loss: 0.628  loss_cls: 0.198  loss_box_reg: 0.352  loss_rpn_cls: 0.021  loss_rpn_loc: 0.053  time: 0.5521  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:21:50 d2.utils.events]: [0m eta: 0:08:11  iter: 1099  total_loss: 0.705  loss_cls: 0.231  loss_box_reg: 0.378  loss_rpn_cls: 0.020  loss_rpn_loc: 0.067  time: 0.5519  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:22:01 d2.utils.events]: [0m eta: 0:08:00  iter: 1119  total_loss: 0.596  loss_cls: 0.167  loss_box_reg: 0.374  loss_rpn_cls: 0.013  loss_rpn_loc: 0.057  time: 0.5518  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:22:12 d2.utils.events]: [0m eta: 0:07:50  iter: 1139  total_loss: 0.631  loss_cls: 0.180  loss_box_reg: 0.335  loss_rpn_cls: 0.018  loss_rpn_loc: 0.056  time: 0.5516  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:22:24 d2.utils.events]: [0m eta: 0:07:39  iter: 1159  total_loss: 0.654  loss_cls: 0.208  loss_box_reg: 0.373  loss_rpn_cls: 0.018  loss_rpn_loc: 0.050  time: 0.5516  data_time: 0.0270  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:22:35 d2.utils.events]: [0m eta: 0:07:28  iter: 1179  total_loss: 0.583  loss_cls: 0.181  loss_box_reg: 0.311  loss_rpn_cls: 0.026  loss_rpn_loc: 0.067  time: 0.5517  data_time: 0.0092  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:22:47 d2.utils.events]: [0m eta: 0:07:17  iter: 1199  total_loss: 0.555  loss_cls: 0.179  loss_box_reg: 0.292  loss_rpn_cls: 0.015  loss_rpn_loc: 0.043  time: 0.5516  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:22:58 d2.utils.events]: [0m eta: 0:07:06  iter: 1219  total_loss: 0.693  loss_cls: 0.216  loss_box_reg: 0.379  loss_rpn_cls: 0.027  loss_rpn_loc: 0.073  time: 0.5517  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:23:09 d2.utils.events]: [0m eta: 0:06:56  iter: 1239  total_loss: 0.793  loss_cls: 0.254  loss_box_reg: 0.431  loss_rpn_cls: 0.034  loss_rpn_loc: 0.070  time: 0.5517  data_time: 0.0095  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:23:20 d2.utils.events]: [0m eta: 0:06:45  iter: 1259  total_loss: 0.539  loss_cls: 0.183  loss_box_reg: 0.313  loss_rpn_cls: 0.015  loss_rpn_loc: 0.041  time: 0.5514  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:23:31 d2.utils.events]: [0m eta: 0:06:34  iter: 1279  total_loss: 0.557  loss_cls: 0.189  loss_box_reg: 0.306  loss_rpn_cls: 0.026  loss_rpn_loc: 0.060  time: 0.5514  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:23:46 d2.utils.events]: [0m eta: 0:06:23  iter: 1299  total_loss: 0.715  loss_cls: 0.255  loss_box_reg: 0.379  loss_rpn_cls: 0.026  loss_rpn_loc: 0.074  time: 0.5513  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:23:57 d2.utils.events]: [0m eta: 0:06:12  iter: 1319  total_loss: 0.648  loss_cls: 0.193  loss_box_reg: 0.355  loss_rpn_cls: 0.019  loss_rpn_loc: 0.056  time: 0.5514  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:24:08 d2.utils.events]: [0m eta: 0:06:01  iter: 1339  total_loss: 0.577  loss_cls: 0.175  loss_box_reg: 0.327  loss_rpn_cls: 0.016  loss_rpn_loc: 0.066  time: 0.5511  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:24:19 d2.utils.events]: [0m eta: 0:05:50  iter: 1359  total_loss: 0.729  loss_cls: 0.216  loss_box_reg: 0.379  loss_rpn_cls: 0.020  loss_rpn_loc: 0.051  time: 0.5512  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:24:31 d2.utils.events]: [0m eta: 0:05:39  iter: 1379  total_loss: 0.676  loss_cls: 0.225  loss_box_reg: 0.357  loss_rpn_cls: 0.023  loss_rpn_loc: 0.051  time: 0.5515  data_time: 0.0238  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:24:42 d2.utils.events]: [0m eta: 0:05:28  iter: 1399  total_loss: 0.608  loss_cls: 0.196  loss_box_reg: 0.329  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5516  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:24:53 d2.utils.events]: [0m eta: 0:05:18  iter: 1419  total_loss: 0.637  loss_cls: 0.223  loss_box_reg: 0.322  loss_rpn_cls: 0.024  loss_rpn_loc: 0.044  time: 0.5516  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:25:05 d2.utils.events]: [0m eta: 0:05:07  iter: 1439  total_loss: 0.669  loss_cls: 0.211  loss_box_reg: 0.359  loss_rpn_cls: 0.022  loss_rpn_loc: 0.069  time: 0.5515  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:25:16 d2.utils.events]: [0m eta: 0:04:56  iter: 1459  total_loss: 0.630  loss_cls: 0.205  loss_box_reg: 0.347  loss_rpn_cls: 0.019  loss_rpn_loc: 0.078  time: 0.5516  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:25:27 d2.utils.events]: [0m eta: 0:04:45  iter: 1479  total_loss: 0.763  loss_cls: 0.235  loss_box_reg: 0.413  loss_rpn_cls: 0.019  loss_rpn_loc: 0.081  time: 0.5517  data_time: 0.0302  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:25:39 d2.utils.events]: [0m eta: 0:04:34  iter: 1499  total_loss: 0.664  loss_cls: 0.217  loss_box_reg: 0.378  loss_rpn_cls: 0.022  loss_rpn_loc: 0.070  time: 0.5519  data_time: 0.0360  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:25:50 d2.utils.events]: [0m eta: 0:04:23  iter: 1519  total_loss: 0.726  loss_cls: 0.233  loss_box_reg: 0.399  loss_rpn_cls: 0.017  loss_rpn_loc: 0.059  time: 0.5520  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:26:01 d2.utils.events]: [0m eta: 0:04:12  iter: 1539  total_loss: 0.743  loss_cls: 0.221  loss_box_reg: 0.392  loss_rpn_cls: 0.020  loss_rpn_loc: 0.071  time: 0.5520  data_time: 0.0089  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:26:12 d2.utils.events]: [0m eta: 0:04:01  iter: 1559  total_loss: 0.637  loss_cls: 0.235  loss_box_reg: 0.330  loss_rpn_cls: 0.021  loss_rpn_loc: 0.065  time: 0.5517  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:26:24 d2.utils.events]: [0m eta: 0:03:50  iter: 1579  total_loss: 0.625  loss_cls: 0.194  loss_box_reg: 0.346  loss_rpn_cls: 0.025  loss_rpn_loc: 0.055  time: 0.5520  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:26:36 d2.utils.events]: [0m eta: 0:03:40  iter: 1599  total_loss: 0.734  loss_cls: 0.213  loss_box_reg: 0.391  loss_rpn_cls: 0.017  loss_rpn_loc: 0.069  time: 0.5522  data_time: 0.0262  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:26:47 d2.utils.events]: [0m eta: 0:03:29  iter: 1619  total_loss: 0.717  loss_cls: 0.234  loss_box_reg: 0.377  loss_rpn_cls: 0.017  loss_rpn_loc: 0.068  time: 0.5523  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:26:58 d2.utils.events]: [0m eta: 0:03:18  iter: 1639  total_loss: 0.647  loss_cls: 0.222  loss_box_reg: 0.328  loss_rpn_cls: 0.023  loss_rpn_loc: 0.064  time: 0.5525  data_time: 0.0096  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:27:30 d2.utils.events]: [0m eta: 0:03:07  iter: 1659  total_loss: 0.706  loss_cls: 0.205  loss_box_reg: 0.384  loss_rpn_cls: 0.023  loss_rpn_loc: 0.053  time: 0.5525  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:28:23 d2.utils.events]: [0m eta: 0:02:57  iter: 1679  total_loss: 0.704  loss_cls: 0.214  loss_box_reg: 0.385  loss_rpn_cls: 0.028  loss_rpn_loc: 0.067  time: 0.5536  data_time: 0.1626  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:28:36 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.591  loss_cls: 0.179  loss_box_reg: 0.289  loss_rpn_cls: 0.024  loss_rpn_loc: 0.054  time: 0.5542  data_time: 0.1426  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:28:46 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.763  loss_cls: 0.246  loss_box_reg: 0.379  loss_rpn_cls: 0.025  loss_rpn_loc: 0.071  time: 0.5539  data_time: 0.0198  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:29:28 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.679  loss_cls: 0.210  loss_box_reg: 0.386  loss_rpn_cls: 0.025  loss_rpn_loc: 0.059  time: 0.5540  data_time: 0.0468  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:29:56 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.595  loss_cls: 0.173  loss_box_reg: 0.327  loss_rpn_cls: 0.021  loss_rpn_loc: 0.059  time: 0.5537  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:30:07 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.545  loss_cls: 0.165  loss_box_reg: 0.309  loss_rpn_cls: 0.016  loss_rpn_loc: 0.038  time: 0.5535  data_time: 0.0060  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:30:18 d2.utils.events]: [0m eta: 0:01:50  iter: 1799  total_loss: 0.709  loss_cls: 0.232  loss_box_reg: 0.367  loss_rpn_cls: 0.024  loss_rpn_loc: 0.059  time: 0.5532  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:30:29 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.606  loss_cls: 0.195  loss_box_reg: 0.309  loss_rpn_cls: 0.015  loss_rpn_loc: 0.057  time: 0.5531  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:30:40 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.797  loss_cls: 0.248  loss_box_reg: 0.449  loss_rpn_cls: 0.017  loss_rpn_loc: 0.078  time: 0.5530  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:30:51 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.692  loss_cls: 0.200  loss_box_reg: 0.360  loss_rpn_cls: 0.017  loss_rpn_loc: 0.065  time: 0.5527  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:31:01 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.682  loss_cls: 0.210  loss_box_reg: 0.350  loss_rpn_cls: 0.019  loss_rpn_loc: 0.061  time: 0.5525  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:31:13 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.803  loss_cls: 0.235  loss_box_reg: 0.430  loss_rpn_cls: 0.026  loss_rpn_loc: 0.065  time: 0.5526  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:31:24 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.616  loss_cls: 0.223  loss_box_reg: 0.348  loss_rpn_cls: 0.018  loss_rpn_loc: 0.053  time: 0.5525  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:31:35 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.552  loss_cls: 0.166  loss_box_reg: 0.289  loss_rpn_cls: 0.020  loss_rpn_loc: 0.068  time: 0.5525  data_time: 0.0057  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:31:51 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.724  loss_cls: 0.230  loss_box_reg: 0.397  loss_rpn_cls: 0.020  loss_rpn_loc: 0.054  time: 0.5525  data_time: 0.0097  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:32:02 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.582  loss_cls: 0.181  loss_box_reg: 0.350  loss_rpn_cls: 0.017  loss_rpn_loc: 0.040  time: 0.5526  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 13:32:54 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:32:54 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 13:32:54 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 13:32:54 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.719  loss_cls: 0.223  loss_box_reg: 0.383  loss_rpn_cls: 0.032  loss_rpn_loc: 0.074  time: 0.5523  data_time: 0.0062  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:33:10 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:23 (0.5526 s / it)
[32m[04/15 13:33:10 d2.engine.hooks]: [0mTotal training time: 0:22:51 (0:04:27 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 13:33:40 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:33:46 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 13:33:47 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 13:33:49 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1194 s / img. ETA=0:02:37
[32m[04/15 13:33:54 d2.evaluation.evaluator]: [0mInference done 41/1257. 0.1198 s / img. ETA=0:03:27
[32m[04/15 13:33:59 d2.evaluation.evaluator]: [0mInference done 66/1257. 0.1195 s / img. ETA=0:03:38
[32m[04/15 13:34:04 d2.evaluation.evaluator]: [0mInference done 95/1257. 0.1195 s / img. ETA=0:03:29
[32m[04/15 13:34:09 d2.evaluation.evaluator]: [0mInference done 121/1257. 0.1193 s / img. ETA=0:03:28
[32m[04/15 13:34:14 d2.evaluation.evaluator]: [0mInference done 147/1257. 0.1193 s / img. ETA=0:03:26
[32m[04/15 13:34:20 d2.evaluation.evaluator]: [0mInference done 177/1257. 0.1197 s / img. ETA=0:03:18
[32m[04/15 13:34:25 d2.evaluation.evaluator]: [0mInference done 212/1257. 0.1203 s / img. ETA=0:03:05
[32m[04/15 13:34:30 d2.evaluation.evaluator]: [0mInference done 250/1257. 0.1206 s / img. ETA=0:02:52
[32m[04/15 13:34:35 d2.evaluation.evaluator]: [0mInference done 286/1257. 0.1208 s / img. ETA=0:02:42
[32m[04/15 13:34:40 d2.evaluation.evaluator]: [0mInference done 317/1257. 0.1208 s / img. ETA=0:02:37
[32m[04/15 13:34:45 d2.evaluation.evaluator]: [0mInference done 345/1257. 0.1208 s / img. ETA=0:02:33
[32m[04/15 13:34:50 d2.evaluation.evaluator]: [0mInference done 380/1257. 0.1209 s / img. ETA=0:02:25
[32m[04/15 13:34:55 d2.evaluation.evaluator]: [0mInference done 407/1257. 0.1210 s / img. ETA=0:02:22
[32m[04/15 13:35:00 d2.evaluation.evaluator]: [0mInference done 435/1257. 0.1210 s / img. ETA=0:02:18
[32m[04/15 13:35:05 d2.evaluation.evaluator]: [0mInference done 459/1257. 0.1210 s / img. ETA=0:02:16
[32m[04/15 13:35:10 d2.evaluation.evaluator]: [0mInference done 481/1257. 0.1210 s / img. ETA=0:02:14
[32m[04/15 13:35:16 d2.evaluation.evaluator]: [0mInference done 507/1257. 0.1209 s / img. ETA=0:02:10
[32m[04/15 13:35:21 d2.evaluation.evaluator]: [0mInference done 529/1257. 0.1211 s / img. ETA=0:02:08
[32m[04/15 13:35:26 d2.evaluation.evaluator]: [0mInference done 556/1257. 0.1210 s / img. ETA=0:02:04
[32m[04/15 13:35:31 d2.evaluation.evaluator]: [0mInference done 578/1257. 0.1211 s / img. ETA=0:02:01
[32m[04/15 13:35:36 d2.evaluation.evaluator]: [0mInference done 606/1257. 0.1211 s / img. ETA=0:01:56
[32m[04/15 13:35:41 d2.evaluation.evaluator]: [0mInference done 636/1257. 0.1211 s / img. ETA=0:01:51
[32m[04/15 13:35:46 d2.evaluation.evaluator]: [0mInference done 653/1257. 0.1211 s / img. ETA=0:01:50
[32m[04/15 13:35:53 d2.evaluation.evaluator]: [0mInference done 664/1257. 0.1212 s / img. ETA=0:01:52
[32m[04/15 13:35:58 d2.evaluation.evaluator]: [0mInference done 685/1257. 0.1213 s / img. ETA=0:01:49
[32m[04/15 13:36:03 d2.evaluation.evaluator]: [0mInference done 712/1257. 0.1215 s / img. ETA=0:01:44
[32m[04/15 13:36:08 d2.evaluation.evaluator]: [0mInference done 745/1257. 0.1217 s / img. ETA=0:01:37
[32m[04/15 13:36:13 d2.evaluation.evaluator]: [0mInference done 783/1257. 0.1219 s / img. ETA=0:01:28
[32m[04/15 13:36:19 d2.evaluation.evaluator]: [0mInference done 821/1257. 0.1219 s / img. ETA=0:01:20
[32m[04/15 13:36:24 d2.evaluation.evaluator]: [0mInference done 854/1257. 0.1220 s / img. ETA=0:01:13
[32m[04/15 13:36:29 d2.evaluation.evaluator]: [0mInference done 888/1257. 0.1220 s / img. ETA=0:01:07
[32m[04/15 13:36:34 d2.evaluation.evaluator]: [0mInference done 923/1257. 0.1220 s / img. ETA=0:01:00
[32m[04/15 13:36:39 d2.evaluation.evaluator]: [0mInference done 959/1257. 0.1220 s / img. ETA=0:00:53
[32m[04/15 13:36:44 d2.evaluation.evaluator]: [0mInference done 997/1257. 0.1221 s / img. ETA=0:00:46
[32m[04/15 13:36:49 d2.evaluation.evaluator]: [0mInference done 1036/1257. 0.1221 s / img. ETA=0:00:38
[32m[04/15 13:36:54 d2.evaluation.evaluator]: [0mInference done 1074/1257. 0.1220 s / img. ETA=0:00:31
[32m[04/15 13:36:59 d2.evaluation.evaluator]: [0mInference done 1103/1257. 0.1220 s / img. ETA=0:00:26
[32m[04/15 13:37:04 d2.evaluation.evaluator]: [0mInference done 1133/1257. 0.1221 s / img. ETA=0:00:21
[32m[04/15 13:37:09 d2.evaluation.evaluator]: [0mInference done 1162/1257. 0.1221 s / img. ETA=0:00:16
[32m[04/15 13:37:14 d2.evaluation.evaluator]: [0mInference done 1194/1257. 0.1222 s / img. ETA=0:00:10
[32m[04/15 13:37:20 d2.evaluation.evaluator]: [0mInference done 1228/1257. 0.1222 s / img. ETA=0:00:05
[32m[04/15 13:37:24 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:36.404157 (0.172847 s / img per device, on 1 devices)
[32m[04/15 13:37:24 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:32 (0.122157 s / img per device, on 1 devices)
[32m[04/15 13:37:26 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 13:37:26 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 13:37:27 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.05s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=4.50s).
Accumulating evaluation results...
DONE (t=0.64s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.185
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.379
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.157
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.116
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.230
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.101
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.240
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.242
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.162
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.291
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.419
[32m[04/15 13:37:32 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 18.548 | 37.937 | 15.738 | 11.573 | 22.976 | 37.886 |
[32m[04/15 13:37:32 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 26.142 | bicycle       | 6.590 | car            | 40.509 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.950 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  21  *  2000  iterations ============
4 channel input
[32m[04/15 13:37:33 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 13:37:34 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.26 seconds.
[5m[31mWARNING[0m [32m[04/15 13:37:34 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:37:35 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 13:37:35 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 13:37:36 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 13:37:36 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 13:37:42 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 13:37:54 d2.utils.events]: [0m eta: 0:18:23  iter: 19  total_loss: 0.620  loss_cls: 0.203  loss_box_reg: 0.311  loss_rpn_cls: 0.026  loss_rpn_loc: 0.068  time: 0.5454  data_time: 0.0705  lr: 0.000200  max_mem: 3070M
[32m[04/15 13:38:05 d2.utils.events]: [0m eta: 0:17:41  iter: 39  total_loss: 0.624  loss_cls: 0.186  loss_box_reg: 0.320  loss_rpn_cls: 0.022  loss_rpn_loc: 0.066  time: 0.5442  data_time: 0.0080  lr: 0.000400  max_mem: 3070M
[32m[04/15 13:38:17 d2.utils.events]: [0m eta: 0:17:30  iter: 59  total_loss: 0.638  loss_cls: 0.199  loss_box_reg: 0.360  loss_rpn_cls: 0.017  loss_rpn_loc: 0.056  time: 0.5424  data_time: 0.0082  lr: 0.000599  max_mem: 3070M
[32m[04/15 13:38:27 d2.utils.events]: [0m eta: 0:17:17  iter: 79  total_loss: 0.575  loss_cls: 0.174  loss_box_reg: 0.312  loss_rpn_cls: 0.025  loss_rpn_loc: 0.055  time: 0.5392  data_time: 0.0061  lr: 0.000799  max_mem: 3070M
[32m[04/15 13:38:39 d2.utils.events]: [0m eta: 0:17:00  iter: 99  total_loss: 0.472  loss_cls: 0.167  loss_box_reg: 0.261  loss_rpn_cls: 0.022  loss_rpn_loc: 0.043  time: 0.5365  data_time: 0.0080  lr: 0.000999  max_mem: 3070M
[32m[04/15 13:38:49 d2.utils.events]: [0m eta: 0:16:49  iter: 119  total_loss: 0.740  loss_cls: 0.230  loss_box_reg: 0.410  loss_rpn_cls: 0.020  loss_rpn_loc: 0.064  time: 0.5358  data_time: 0.0080  lr: 0.001199  max_mem: 3070M
[32m[04/15 13:39:00 d2.utils.events]: [0m eta: 0:16:39  iter: 139  total_loss: 0.592  loss_cls: 0.168  loss_box_reg: 0.346  loss_rpn_cls: 0.017  loss_rpn_loc: 0.067  time: 0.5366  data_time: 0.0080  lr: 0.001399  max_mem: 3070M
[32m[04/15 13:39:11 d2.utils.events]: [0m eta: 0:16:27  iter: 159  total_loss: 0.545  loss_cls: 0.175  loss_box_reg: 0.305  loss_rpn_cls: 0.021  loss_rpn_loc: 0.040  time: 0.5358  data_time: 0.0068  lr: 0.001598  max_mem: 3070M
[32m[04/15 13:39:22 d2.utils.events]: [0m eta: 0:16:15  iter: 179  total_loss: 0.607  loss_cls: 0.196  loss_box_reg: 0.363  loss_rpn_cls: 0.015  loss_rpn_loc: 0.052  time: 0.5347  data_time: 0.0069  lr: 0.001798  max_mem: 3070M
[32m[04/15 13:39:35 d2.utils.events]: [0m eta: 0:16:05  iter: 199  total_loss: 0.613  loss_cls: 0.171  loss_box_reg: 0.326  loss_rpn_cls: 0.011  loss_rpn_loc: 0.053  time: 0.5333  data_time: 0.0070  lr: 0.001998  max_mem: 3070M
[32m[04/15 13:39:46 d2.utils.events]: [0m eta: 0:15:56  iter: 219  total_loss: 0.702  loss_cls: 0.229  loss_box_reg: 0.348  loss_rpn_cls: 0.019  loss_rpn_loc: 0.070  time: 0.5346  data_time: 0.0068  lr: 0.002198  max_mem: 3070M
[32m[04/15 13:39:57 d2.utils.events]: [0m eta: 0:15:51  iter: 239  total_loss: 0.665  loss_cls: 0.190  loss_box_reg: 0.325  loss_rpn_cls: 0.016  loss_rpn_loc: 0.060  time: 0.5370  data_time: 0.0070  lr: 0.002398  max_mem: 3070M
[32m[04/15 13:40:15 d2.utils.events]: [0m eta: 0:15:40  iter: 259  total_loss: 0.680  loss_cls: 0.227  loss_box_reg: 0.382  loss_rpn_cls: 0.016  loss_rpn_loc: 0.064  time: 0.5368  data_time: 0.0063  lr: 0.002597  max_mem: 3070M
[32m[04/15 13:40:27 d2.utils.events]: [0m eta: 0:15:30  iter: 279  total_loss: 0.696  loss_cls: 0.221  loss_box_reg: 0.382  loss_rpn_cls: 0.019  loss_rpn_loc: 0.059  time: 0.5374  data_time: 0.0099  lr: 0.002797  max_mem: 3070M
[32m[04/15 13:40:38 d2.utils.events]: [0m eta: 0:15:20  iter: 299  total_loss: 0.583  loss_cls: 0.178  loss_box_reg: 0.309  loss_rpn_cls: 0.010  loss_rpn_loc: 0.038  time: 0.5387  data_time: 0.0090  lr: 0.002997  max_mem: 3070M
[32m[04/15 13:40:49 d2.utils.events]: [0m eta: 0:15:11  iter: 319  total_loss: 0.549  loss_cls: 0.177  loss_box_reg: 0.292  loss_rpn_cls: 0.011  loss_rpn_loc: 0.033  time: 0.5387  data_time: 0.0073  lr: 0.003197  max_mem: 3070M
[32m[04/15 13:41:00 d2.utils.events]: [0m eta: 0:15:00  iter: 339  total_loss: 0.682  loss_cls: 0.213  loss_box_reg: 0.405  loss_rpn_cls: 0.012  loss_rpn_loc: 0.063  time: 0.5392  data_time: 0.0067  lr: 0.003397  max_mem: 3070M
[32m[04/15 13:41:11 d2.utils.events]: [0m eta: 0:14:50  iter: 359  total_loss: 0.652  loss_cls: 0.211  loss_box_reg: 0.342  loss_rpn_cls: 0.013  loss_rpn_loc: 0.046  time: 0.5400  data_time: 0.0077  lr: 0.003596  max_mem: 3070M
[32m[04/15 13:41:22 d2.utils.events]: [0m eta: 0:14:39  iter: 379  total_loss: 0.616  loss_cls: 0.200  loss_box_reg: 0.329  loss_rpn_cls: 0.014  loss_rpn_loc: 0.055  time: 0.5400  data_time: 0.0070  lr: 0.003796  max_mem: 3070M
[32m[04/15 13:41:33 d2.utils.events]: [0m eta: 0:14:29  iter: 399  total_loss: 0.669  loss_cls: 0.200  loss_box_reg: 0.367  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5411  data_time: 0.0063  lr: 0.003996  max_mem: 3070M
[32m[04/15 13:41:45 d2.utils.events]: [0m eta: 0:14:18  iter: 419  total_loss: 0.616  loss_cls: 0.184  loss_box_reg: 0.350  loss_rpn_cls: 0.013  loss_rpn_loc: 0.038  time: 0.5416  data_time: 0.0071  lr: 0.004196  max_mem: 3070M
[32m[04/15 13:41:56 d2.utils.events]: [0m eta: 0:14:08  iter: 439  total_loss: 0.571  loss_cls: 0.181  loss_box_reg: 0.309  loss_rpn_cls: 0.013  loss_rpn_loc: 0.045  time: 0.5417  data_time: 0.0078  lr: 0.004396  max_mem: 3070M
[32m[04/15 13:42:08 d2.utils.events]: [0m eta: 0:13:59  iter: 459  total_loss: 0.610  loss_cls: 0.188  loss_box_reg: 0.350  loss_rpn_cls: 0.011  loss_rpn_loc: 0.043  time: 0.5428  data_time: 0.0068  lr: 0.004595  max_mem: 3070M
[32m[04/15 13:42:19 d2.utils.events]: [0m eta: 0:13:48  iter: 479  total_loss: 0.614  loss_cls: 0.194  loss_box_reg: 0.333  loss_rpn_cls: 0.016  loss_rpn_loc: 0.041  time: 0.5435  data_time: 0.0078  lr: 0.004795  max_mem: 3070M
[32m[04/15 13:42:30 d2.utils.events]: [0m eta: 0:13:37  iter: 499  total_loss: 0.715  loss_cls: 0.231  loss_box_reg: 0.391  loss_rpn_cls: 0.017  loss_rpn_loc: 0.061  time: 0.5433  data_time: 0.0072  lr: 0.004995  max_mem: 3070M
[32m[04/15 13:42:41 d2.utils.events]: [0m eta: 0:13:26  iter: 519  total_loss: 0.552  loss_cls: 0.170  loss_box_reg: 0.309  loss_rpn_cls: 0.016  loss_rpn_loc: 0.048  time: 0.5436  data_time: 0.0072  lr: 0.005195  max_mem: 3070M
[32m[04/15 13:42:53 d2.utils.events]: [0m eta: 0:13:16  iter: 539  total_loss: 0.528  loss_cls: 0.177  loss_box_reg: 0.305  loss_rpn_cls: 0.013  loss_rpn_loc: 0.049  time: 0.5444  data_time: 0.0063  lr: 0.005395  max_mem: 3070M
[32m[04/15 13:43:04 d2.utils.events]: [0m eta: 0:13:06  iter: 559  total_loss: 0.720  loss_cls: 0.232  loss_box_reg: 0.411  loss_rpn_cls: 0.012  loss_rpn_loc: 0.062  time: 0.5448  data_time: 0.0094  lr: 0.005594  max_mem: 3070M
[32m[04/15 13:43:15 d2.utils.events]: [0m eta: 0:12:55  iter: 579  total_loss: 0.661  loss_cls: 0.226  loss_box_reg: 0.344  loss_rpn_cls: 0.017  loss_rpn_loc: 0.051  time: 0.5452  data_time: 0.0068  lr: 0.005794  max_mem: 3070M
[32m[04/15 13:43:26 d2.utils.events]: [0m eta: 0:12:44  iter: 599  total_loss: 0.605  loss_cls: 0.183  loss_box_reg: 0.328  loss_rpn_cls: 0.012  loss_rpn_loc: 0.057  time: 0.5449  data_time: 0.0066  lr: 0.005994  max_mem: 3070M
[32m[04/15 13:43:38 d2.utils.events]: [0m eta: 0:12:34  iter: 619  total_loss: 0.584  loss_cls: 0.181  loss_box_reg: 0.324  loss_rpn_cls: 0.013  loss_rpn_loc: 0.067  time: 0.5456  data_time: 0.0103  lr: 0.006194  max_mem: 3070M
[32m[04/15 13:43:49 d2.utils.events]: [0m eta: 0:12:25  iter: 639  total_loss: 0.648  loss_cls: 0.194  loss_box_reg: 0.372  loss_rpn_cls: 0.012  loss_rpn_loc: 0.067  time: 0.5461  data_time: 0.0113  lr: 0.006394  max_mem: 3070M
[32m[04/15 13:44:01 d2.utils.events]: [0m eta: 0:12:15  iter: 659  total_loss: 0.696  loss_cls: 0.241  loss_box_reg: 0.368  loss_rpn_cls: 0.019  loss_rpn_loc: 0.054  time: 0.5464  data_time: 0.0067  lr: 0.006593  max_mem: 3070M
[32m[04/15 13:44:12 d2.utils.events]: [0m eta: 0:12:04  iter: 679  total_loss: 0.518  loss_cls: 0.148  loss_box_reg: 0.279  loss_rpn_cls: 0.014  loss_rpn_loc: 0.042  time: 0.5467  data_time: 0.0061  lr: 0.006793  max_mem: 3070M
[32m[04/15 13:44:23 d2.utils.events]: [0m eta: 0:11:53  iter: 699  total_loss: 0.580  loss_cls: 0.174  loss_box_reg: 0.308  loss_rpn_cls: 0.014  loss_rpn_loc: 0.041  time: 0.5469  data_time: 0.0083  lr: 0.006993  max_mem: 3070M
[32m[04/15 13:44:34 d2.utils.events]: [0m eta: 0:11:42  iter: 719  total_loss: 0.630  loss_cls: 0.212  loss_box_reg: 0.344  loss_rpn_cls: 0.024  loss_rpn_loc: 0.053  time: 0.5471  data_time: 0.0076  lr: 0.007193  max_mem: 3070M
[32m[04/15 13:44:46 d2.utils.events]: [0m eta: 0:11:32  iter: 739  total_loss: 0.627  loss_cls: 0.194  loss_box_reg: 0.332  loss_rpn_cls: 0.022  loss_rpn_loc: 0.048  time: 0.5477  data_time: 0.0076  lr: 0.007393  max_mem: 3070M
[32m[04/15 13:44:59 d2.utils.events]: [0m eta: 0:11:20  iter: 759  total_loss: 0.560  loss_cls: 0.190  loss_box_reg: 0.287  loss_rpn_cls: 0.018  loss_rpn_loc: 0.046  time: 0.5473  data_time: 0.0080  lr: 0.007592  max_mem: 3070M
[32m[04/15 13:45:10 d2.utils.events]: [0m eta: 0:11:09  iter: 779  total_loss: 0.637  loss_cls: 0.208  loss_box_reg: 0.370  loss_rpn_cls: 0.019  loss_rpn_loc: 0.052  time: 0.5475  data_time: 0.0075  lr: 0.007792  max_mem: 3070M
[32m[04/15 13:45:22 d2.utils.events]: [0m eta: 0:10:58  iter: 799  total_loss: 0.644  loss_cls: 0.197  loss_box_reg: 0.330  loss_rpn_cls: 0.013  loss_rpn_loc: 0.050  time: 0.5476  data_time: 0.0065  lr: 0.007992  max_mem: 3070M
[32m[04/15 13:45:33 d2.utils.events]: [0m eta: 0:10:48  iter: 819  total_loss: 0.572  loss_cls: 0.170  loss_box_reg: 0.339  loss_rpn_cls: 0.015  loss_rpn_loc: 0.048  time: 0.5475  data_time: 0.0066  lr: 0.008192  max_mem: 3070M
[32m[04/15 13:45:44 d2.utils.events]: [0m eta: 0:10:38  iter: 839  total_loss: 0.528  loss_cls: 0.165  loss_box_reg: 0.308  loss_rpn_cls: 0.015  loss_rpn_loc: 0.040  time: 0.5481  data_time: 0.0067  lr: 0.008392  max_mem: 3070M
[32m[04/15 13:45:56 d2.utils.events]: [0m eta: 0:10:27  iter: 859  total_loss: 0.512  loss_cls: 0.170  loss_box_reg: 0.286  loss_rpn_cls: 0.019  loss_rpn_loc: 0.047  time: 0.5483  data_time: 0.0076  lr: 0.008591  max_mem: 3070M
[32m[04/15 13:46:07 d2.utils.events]: [0m eta: 0:10:17  iter: 879  total_loss: 0.578  loss_cls: 0.184  loss_box_reg: 0.308  loss_rpn_cls: 0.023  loss_rpn_loc: 0.056  time: 0.5485  data_time: 0.0065  lr: 0.008791  max_mem: 3070M
[32m[04/15 13:46:18 d2.utils.events]: [0m eta: 0:10:06  iter: 899  total_loss: 0.622  loss_cls: 0.187  loss_box_reg: 0.349  loss_rpn_cls: 0.013  loss_rpn_loc: 0.040  time: 0.5488  data_time: 0.0071  lr: 0.008991  max_mem: 3070M
[32m[04/15 13:46:30 d2.utils.events]: [0m eta: 0:09:57  iter: 919  total_loss: 0.583  loss_cls: 0.175  loss_box_reg: 0.302  loss_rpn_cls: 0.016  loss_rpn_loc: 0.062  time: 0.5494  data_time: 0.0086  lr: 0.009191  max_mem: 3070M
[32m[04/15 13:46:44 d2.utils.events]: [0m eta: 0:09:46  iter: 939  total_loss: 0.651  loss_cls: 0.222  loss_box_reg: 0.369  loss_rpn_cls: 0.016  loss_rpn_loc: 0.066  time: 0.5495  data_time: 0.0072  lr: 0.009391  max_mem: 3070M
[32m[04/15 13:46:56 d2.utils.events]: [0m eta: 0:09:35  iter: 959  total_loss: 0.652  loss_cls: 0.208  loss_box_reg: 0.349  loss_rpn_cls: 0.017  loss_rpn_loc: 0.080  time: 0.5497  data_time: 0.0301  lr: 0.009590  max_mem: 3070M
[32m[04/15 13:47:07 d2.utils.events]: [0m eta: 0:09:24  iter: 979  total_loss: 0.669  loss_cls: 0.214  loss_box_reg: 0.367  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5496  data_time: 0.0070  lr: 0.009790  max_mem: 3070M
[32m[04/15 13:47:17 d2.utils.events]: [0m eta: 0:09:12  iter: 999  total_loss: 0.579  loss_cls: 0.211  loss_box_reg: 0.295  loss_rpn_cls: 0.029  loss_rpn_loc: 0.056  time: 0.5494  data_time: 0.0214  lr: 0.009990  max_mem: 3070M
[32m[04/15 13:47:29 d2.utils.events]: [0m eta: 0:09:01  iter: 1019  total_loss: 0.568  loss_cls: 0.179  loss_box_reg: 0.343  loss_rpn_cls: 0.014  loss_rpn_loc: 0.047  time: 0.5498  data_time: 0.0138  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:47:40 d2.utils.events]: [0m eta: 0:08:50  iter: 1039  total_loss: 0.665  loss_cls: 0.206  loss_box_reg: 0.348  loss_rpn_cls: 0.015  loss_rpn_loc: 0.061  time: 0.5500  data_time: 0.0266  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:47:54 d2.utils.events]: [0m eta: 0:08:39  iter: 1059  total_loss: 0.599  loss_cls: 0.203  loss_box_reg: 0.347  loss_rpn_cls: 0.023  loss_rpn_loc: 0.068  time: 0.5499  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:48:05 d2.utils.events]: [0m eta: 0:08:29  iter: 1079  total_loss: 0.547  loss_cls: 0.200  loss_box_reg: 0.289  loss_rpn_cls: 0.021  loss_rpn_loc: 0.070  time: 0.5499  data_time: 0.0242  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:48:16 d2.utils.events]: [0m eta: 0:08:18  iter: 1099  total_loss: 0.638  loss_cls: 0.199  loss_box_reg: 0.352  loss_rpn_cls: 0.026  loss_rpn_loc: 0.047  time: 0.5498  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:48:28 d2.utils.events]: [0m eta: 0:08:08  iter: 1119  total_loss: 0.549  loss_cls: 0.157  loss_box_reg: 0.304  loss_rpn_cls: 0.018  loss_rpn_loc: 0.054  time: 0.5501  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:48:39 d2.utils.events]: [0m eta: 0:07:57  iter: 1139  total_loss: 0.661  loss_cls: 0.194  loss_box_reg: 0.370  loss_rpn_cls: 0.027  loss_rpn_loc: 0.063  time: 0.5502  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:48:52 d2.utils.events]: [0m eta: 0:07:46  iter: 1159  total_loss: 0.641  loss_cls: 0.190  loss_box_reg: 0.298  loss_rpn_cls: 0.025  loss_rpn_loc: 0.058  time: 0.5500  data_time: 0.0136  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:49:03 d2.utils.events]: [0m eta: 0:07:36  iter: 1179  total_loss: 0.634  loss_cls: 0.220  loss_box_reg: 0.340  loss_rpn_cls: 0.022  loss_rpn_loc: 0.050  time: 0.5500  data_time: 0.0119  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:49:14 d2.utils.events]: [0m eta: 0:07:25  iter: 1199  total_loss: 0.658  loss_cls: 0.205  loss_box_reg: 0.368  loss_rpn_cls: 0.014  loss_rpn_loc: 0.052  time: 0.5500  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:49:26 d2.utils.events]: [0m eta: 0:07:15  iter: 1219  total_loss: 0.763  loss_cls: 0.256  loss_box_reg: 0.404  loss_rpn_cls: 0.020  loss_rpn_loc: 0.074  time: 0.5504  data_time: 0.0103  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:49:37 d2.utils.events]: [0m eta: 0:07:03  iter: 1239  total_loss: 0.586  loss_cls: 0.185  loss_box_reg: 0.295  loss_rpn_cls: 0.017  loss_rpn_loc: 0.050  time: 0.5503  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:49:48 d2.utils.events]: [0m eta: 0:06:52  iter: 1259  total_loss: 0.627  loss_cls: 0.205  loss_box_reg: 0.357  loss_rpn_cls: 0.020  loss_rpn_loc: 0.051  time: 0.5506  data_time: 0.0392  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:49:59 d2.utils.events]: [0m eta: 0:06:41  iter: 1279  total_loss: 0.648  loss_cls: 0.197  loss_box_reg: 0.355  loss_rpn_cls: 0.028  loss_rpn_loc: 0.068  time: 0.5504  data_time: 0.0225  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:50:10 d2.utils.events]: [0m eta: 0:06:30  iter: 1299  total_loss: 0.644  loss_cls: 0.210  loss_box_reg: 0.338  loss_rpn_cls: 0.027  loss_rpn_loc: 0.068  time: 0.5504  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:50:22 d2.utils.events]: [0m eta: 0:06:19  iter: 1319  total_loss: 0.614  loss_cls: 0.214  loss_box_reg: 0.341  loss_rpn_cls: 0.025  loss_rpn_loc: 0.070  time: 0.5504  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:50:33 d2.utils.events]: [0m eta: 0:06:08  iter: 1339  total_loss: 0.463  loss_cls: 0.161  loss_box_reg: 0.268  loss_rpn_cls: 0.024  loss_rpn_loc: 0.052  time: 0.5504  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:50:44 d2.utils.events]: [0m eta: 0:05:57  iter: 1359  total_loss: 0.554  loss_cls: 0.197  loss_box_reg: 0.274  loss_rpn_cls: 0.021  loss_rpn_loc: 0.074  time: 0.5504  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:50:56 d2.utils.events]: [0m eta: 0:05:46  iter: 1379  total_loss: 0.708  loss_cls: 0.210  loss_box_reg: 0.396  loss_rpn_cls: 0.019  loss_rpn_loc: 0.064  time: 0.5505  data_time: 0.0096  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:51:07 d2.utils.events]: [0m eta: 0:05:35  iter: 1399  total_loss: 0.624  loss_cls: 0.186  loss_box_reg: 0.300  loss_rpn_cls: 0.025  loss_rpn_loc: 0.055  time: 0.5507  data_time: 0.0339  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:51:19 d2.utils.events]: [0m eta: 0:05:24  iter: 1419  total_loss: 0.454  loss_cls: 0.142  loss_box_reg: 0.256  loss_rpn_cls: 0.021  loss_rpn_loc: 0.043  time: 0.5507  data_time: 0.0146  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:51:31 d2.utils.events]: [0m eta: 0:05:13  iter: 1439  total_loss: 0.673  loss_cls: 0.223  loss_box_reg: 0.356  loss_rpn_cls: 0.022  loss_rpn_loc: 0.050  time: 0.5508  data_time: 0.0560  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:51:42 d2.utils.events]: [0m eta: 0:05:01  iter: 1459  total_loss: 0.649  loss_cls: 0.205  loss_box_reg: 0.376  loss_rpn_cls: 0.020  loss_rpn_loc: 0.057  time: 0.5510  data_time: 0.0127  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:51:53 d2.utils.events]: [0m eta: 0:04:50  iter: 1479  total_loss: 0.786  loss_cls: 0.264  loss_box_reg: 0.401  loss_rpn_cls: 0.030  loss_rpn_loc: 0.071  time: 0.5509  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:52:05 d2.utils.events]: [0m eta: 0:04:39  iter: 1499  total_loss: 0.676  loss_cls: 0.209  loss_box_reg: 0.342  loss_rpn_cls: 0.023  loss_rpn_loc: 0.078  time: 0.5515  data_time: 0.0899  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:52:21 d2.utils.events]: [0m eta: 0:04:28  iter: 1519  total_loss: 0.602  loss_cls: 0.182  loss_box_reg: 0.347  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5520  data_time: 0.0375  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:52:32 d2.utils.events]: [0m eta: 0:04:16  iter: 1539  total_loss: 0.634  loss_cls: 0.194  loss_box_reg: 0.344  loss_rpn_cls: 0.024  loss_rpn_loc: 0.049  time: 0.5519  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:52:44 d2.utils.events]: [0m eta: 0:04:05  iter: 1559  total_loss: 0.719  loss_cls: 0.221  loss_box_reg: 0.392  loss_rpn_cls: 0.020  loss_rpn_loc: 0.059  time: 0.5522  data_time: 0.0805  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:52:57 d2.utils.events]: [0m eta: 0:03:54  iter: 1579  total_loss: 0.577  loss_cls: 0.191  loss_box_reg: 0.314  loss_rpn_cls: 0.034  loss_rpn_loc: 0.051  time: 0.5534  data_time: 0.1837  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:53:20 d2.utils.events]: [0m eta: 0:03:43  iter: 1599  total_loss: 0.650  loss_cls: 0.195  loss_box_reg: 0.363  loss_rpn_cls: 0.026  loss_rpn_loc: 0.070  time: 0.5532  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:53:33 d2.utils.events]: [0m eta: 0:03:31  iter: 1619  total_loss: 0.669  loss_cls: 0.207  loss_box_reg: 0.366  loss_rpn_cls: 0.019  loss_rpn_loc: 0.060  time: 0.5529  data_time: 0.0104  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:53:50 d2.utils.events]: [0m eta: 0:03:20  iter: 1639  total_loss: 0.681  loss_cls: 0.225  loss_box_reg: 0.373  loss_rpn_cls: 0.029  loss_rpn_loc: 0.065  time: 0.5528  data_time: 0.0181  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:54:05 d2.utils.events]: [0m eta: 0:03:09  iter: 1659  total_loss: 0.675  loss_cls: 0.223  loss_box_reg: 0.364  loss_rpn_cls: 0.022  loss_rpn_loc: 0.061  time: 0.5551  data_time: 0.3030  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:54:16 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.673  loss_cls: 0.215  loss_box_reg: 0.374  loss_rpn_cls: 0.019  loss_rpn_loc: 0.061  time: 0.5551  data_time: 0.0440  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:54:29 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.632  loss_cls: 0.204  loss_box_reg: 0.333  loss_rpn_cls: 0.021  loss_rpn_loc: 0.059  time: 0.5549  data_time: 0.0157  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:54:40 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.554  loss_cls: 0.188  loss_box_reg: 0.318  loss_rpn_cls: 0.020  loss_rpn_loc: 0.054  time: 0.5548  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:55:25 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.610  loss_cls: 0.198  loss_box_reg: 0.351  loss_rpn_cls: 0.028  loss_rpn_loc: 0.052  time: 0.5548  data_time: 0.0623  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:55:37 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.753  loss_cls: 0.218  loss_box_reg: 0.423  loss_rpn_cls: 0.022  loss_rpn_loc: 0.050  time: 0.5546  data_time: 0.0169  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:55:48 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.679  loss_cls: 0.216  loss_box_reg: 0.374  loss_rpn_cls: 0.017  loss_rpn_loc: 0.057  time: 0.5544  data_time: 0.0303  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:55:59 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.574  loss_cls: 0.185  loss_box_reg: 0.293  loss_rpn_cls: 0.021  loss_rpn_loc: 0.063  time: 0.5545  data_time: 0.1093  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:56:14 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.714  loss_cls: 0.221  loss_box_reg: 0.393  loss_rpn_cls: 0.021  loss_rpn_loc: 0.056  time: 0.5542  data_time: 0.0107  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:56:26 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.623  loss_cls: 0.206  loss_box_reg: 0.321  loss_rpn_cls: 0.022  loss_rpn_loc: 0.052  time: 0.5540  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:56:37 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.751  loss_cls: 0.232  loss_box_reg: 0.410  loss_rpn_cls: 0.028  loss_rpn_loc: 0.063  time: 0.5537  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:56:52 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.733  loss_cls: 0.218  loss_box_reg: 0.377  loss_rpn_cls: 0.026  loss_rpn_loc: 0.074  time: 0.5535  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:57:03 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.610  loss_cls: 0.211  loss_box_reg: 0.351  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5534  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:57:14 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.643  loss_cls: 0.196  loss_box_reg: 0.362  loss_rpn_cls: 0.018  loss_rpn_loc: 0.070  time: 0.5530  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:57:28 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.686  loss_cls: 0.216  loss_box_reg: 0.365  loss_rpn_cls: 0.022  loss_rpn_loc: 0.057  time: 0.5528  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:57:40 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.645  loss_cls: 0.195  loss_box_reg: 0.320  loss_rpn_cls: 0.016  loss_rpn_loc: 0.064  time: 0.5527  data_time: 0.0107  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:57:51 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.548  loss_cls: 0.191  loss_box_reg: 0.298  loss_rpn_cls: 0.016  loss_rpn_loc: 0.041  time: 0.5525  data_time: 0.0214  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 13:59:04 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:59:04 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 13:59:04 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 13:59:04 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.579  loss_cls: 0.196  loss_box_reg: 0.296  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5524  data_time: 0.0087  lr: 0.010000  max_mem: 3070M
[32m[04/15 13:59:12 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:23 (0.5527 s / it)
[32m[04/15 13:59:12 d2.engine.hooks]: [0mTotal training time: 0:21:27 (0:03:04 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 13:59:30 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:59:30 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 13:59:30 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 13:59:33 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1206 s / img. ETA=0:04:23
[32m[04/15 13:59:38 d2.evaluation.evaluator]: [0mInference done 37/1257. 0.1213 s / img. ETA=0:03:59
[32m[04/15 13:59:43 d2.evaluation.evaluator]: [0mInference done 63/1257. 0.1213 s / img. ETA=0:03:55
[32m[04/15 13:59:48 d2.evaluation.evaluator]: [0mInference done 84/1257. 0.1213 s / img. ETA=0:04:06
[32m[04/15 13:59:53 d2.evaluation.evaluator]: [0mInference done 111/1257. 0.1212 s / img. ETA=0:03:53
[32m[04/15 13:59:58 d2.evaluation.evaluator]: [0mInference done 133/1257. 0.1212 s / img. ETA=0:03:54
[32m[04/15 14:00:04 d2.evaluation.evaluator]: [0mInference done 161/1257. 0.1214 s / img. ETA=0:03:44
[32m[04/15 14:00:09 d2.evaluation.evaluator]: [0mInference done 196/1257. 0.1219 s / img. ETA=0:03:25
[32m[04/15 14:00:14 d2.evaluation.evaluator]: [0mInference done 223/1257. 0.1218 s / img. ETA=0:03:20
[32m[04/15 14:00:19 d2.evaluation.evaluator]: [0mInference done 258/1257. 0.1221 s / img. ETA=0:03:06
[32m[04/15 14:00:24 d2.evaluation.evaluator]: [0mInference done 293/1257. 0.1221 s / img. ETA=0:02:55
[32m[04/15 14:00:29 d2.evaluation.evaluator]: [0mInference done 318/1257. 0.1220 s / img. ETA=0:02:52
[32m[04/15 14:00:34 d2.evaluation.evaluator]: [0mInference done 340/1257. 0.1220 s / img. ETA=0:02:51
[32m[04/15 14:00:39 d2.evaluation.evaluator]: [0mInference done 371/1257. 0.1220 s / img. ETA=0:02:43
[32m[04/15 14:00:44 d2.evaluation.evaluator]: [0mInference done 398/1257. 0.1222 s / img. ETA=0:02:39
[32m[04/15 14:00:49 d2.evaluation.evaluator]: [0mInference done 428/1257. 0.1221 s / img. ETA=0:02:32
[32m[04/15 14:00:55 d2.evaluation.evaluator]: [0mInference done 456/1257. 0.1219 s / img. ETA=0:02:27
[32m[04/15 14:01:00 d2.evaluation.evaluator]: [0mInference done 490/1257. 0.1219 s / img. ETA=0:02:19
[32m[04/15 14:01:05 d2.evaluation.evaluator]: [0mInference done 514/1257. 0.1219 s / img. ETA=0:02:16
[32m[04/15 14:01:10 d2.evaluation.evaluator]: [0mInference done 534/1257. 0.1219 s / img. ETA=0:02:14
[32m[04/15 14:01:15 d2.evaluation.evaluator]: [0mInference done 554/1257. 0.1219 s / img. ETA=0:02:12
[32m[04/15 14:01:21 d2.evaluation.evaluator]: [0mInference done 578/1257. 0.1219 s / img. ETA=0:02:09
[32m[04/15 14:01:26 d2.evaluation.evaluator]: [0mInference done 600/1257. 0.1219 s / img. ETA=0:02:06
[32m[04/15 14:01:31 d2.evaluation.evaluator]: [0mInference done 617/1257. 0.1218 s / img. ETA=0:02:04
[32m[04/15 14:01:36 d2.evaluation.evaluator]: [0mInference done 639/1257. 0.1218 s / img. ETA=0:02:01
[32m[04/15 14:01:41 d2.evaluation.evaluator]: [0mInference done 660/1257. 0.1218 s / img. ETA=0:01:58
[32m[04/15 14:01:47 d2.evaluation.evaluator]: [0mInference done 684/1257. 0.1218 s / img. ETA=0:01:53
[32m[04/15 14:01:52 d2.evaluation.evaluator]: [0mInference done 713/1257. 0.1218 s / img. ETA=0:01:47
[32m[04/15 14:01:57 d2.evaluation.evaluator]: [0mInference done 748/1257. 0.1218 s / img. ETA=0:01:39
[32m[04/15 14:02:02 d2.evaluation.evaluator]: [0mInference done 778/1257. 0.1218 s / img. ETA=0:01:33
[32m[04/15 14:02:07 d2.evaluation.evaluator]: [0mInference done 811/1257. 0.1219 s / img. ETA=0:01:26
[32m[04/15 14:02:12 d2.evaluation.evaluator]: [0mInference done 833/1257. 0.1219 s / img. ETA=0:01:22
[32m[04/15 14:02:17 d2.evaluation.evaluator]: [0mInference done 864/1257. 0.1219 s / img. ETA=0:01:15
[32m[04/15 14:02:22 d2.evaluation.evaluator]: [0mInference done 892/1257. 0.1219 s / img. ETA=0:01:10
[32m[04/15 14:02:27 d2.evaluation.evaluator]: [0mInference done 920/1257. 0.1219 s / img. ETA=0:01:04
[32m[04/15 14:02:33 d2.evaluation.evaluator]: [0mInference done 945/1257. 0.1219 s / img. ETA=0:01:00
[32m[04/15 14:02:38 d2.evaluation.evaluator]: [0mInference done 976/1257. 0.1219 s / img. ETA=0:00:53
[32m[04/15 14:02:43 d2.evaluation.evaluator]: [0mInference done 1011/1257. 0.1220 s / img. ETA=0:00:46
[32m[04/15 14:02:48 d2.evaluation.evaluator]: [0mInference done 1047/1257. 0.1220 s / img. ETA=0:00:39
[32m[04/15 14:02:53 d2.evaluation.evaluator]: [0mInference done 1083/1257. 0.1221 s / img. ETA=0:00:32
[32m[04/15 14:02:58 d2.evaluation.evaluator]: [0mInference done 1118/1257. 0.1221 s / img. ETA=0:00:25
[32m[04/15 14:03:03 d2.evaluation.evaluator]: [0mInference done 1149/1257. 0.1221 s / img. ETA=0:00:19
[32m[04/15 14:03:08 d2.evaluation.evaluator]: [0mInference done 1183/1257. 0.1221 s / img. ETA=0:00:13
[32m[04/15 14:03:13 d2.evaluation.evaluator]: [0mInference done 1207/1257. 0.1222 s / img. ETA=0:00:09
[32m[04/15 14:03:19 d2.evaluation.evaluator]: [0mInference done 1234/1257. 0.1222 s / img. ETA=0:00:04
[32m[04/15 14:03:24 d2.evaluation.evaluator]: [0mInference done 1255/1257. 0.1222 s / img. ETA=0:00:00
[32m[04/15 14:03:25 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:53.054042 (0.186145 s / img per device, on 1 devices)
[32m[04/15 14:03:25 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:32 (0.122204 s / img per device, on 1 devices)
[32m[04/15 14:03:25 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 14:03:25 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 14:03:25 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.57s).
Accumulating evaluation results...
DONE (t=0.65s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.193
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.396
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.161
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.119
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.238
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.397
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.239
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.242
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.174
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.284
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.458
[32m[04/15 14:03:31 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.318 | 39.642 | 16.070 | 11.924 | 23.794 | 39.675 |
[32m[04/15 14:03:31 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 27.875 | bicycle       | 7.983 | car            | 41.414 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  22  *  2000  iterations ============
4 channel input
[32m[04/15 14:03:32 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 14:03:34 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.24 seconds.
[5m[31mWARNING[0m [32m[04/15 14:03:34 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 14:03:34 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 14:03:35 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 14:03:35 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 14:03:35 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 14:03:44 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 14:03:55 d2.utils.events]: [0m eta: 0:18:33  iter: 19  total_loss: 0.686  loss_cls: 0.220  loss_box_reg: 0.364  loss_rpn_cls: 0.018  loss_rpn_loc: 0.072  time: 0.5488  data_time: 0.0553  lr: 0.000200  max_mem: 3070M
[32m[04/15 14:04:07 d2.utils.events]: [0m eta: 0:17:42  iter: 39  total_loss: 0.608  loss_cls: 0.229  loss_box_reg: 0.319  loss_rpn_cls: 0.018  loss_rpn_loc: 0.056  time: 0.5359  data_time: 0.0123  lr: 0.000400  max_mem: 3070M
[32m[04/15 14:04:19 d2.utils.events]: [0m eta: 0:17:42  iter: 59  total_loss: 0.683  loss_cls: 0.217  loss_box_reg: 0.359  loss_rpn_cls: 0.022  loss_rpn_loc: 0.058  time: 0.5429  data_time: 0.0071  lr: 0.000599  max_mem: 3070M
[32m[04/15 14:04:30 d2.utils.events]: [0m eta: 0:17:47  iter: 79  total_loss: 0.551  loss_cls: 0.181  loss_box_reg: 0.306  loss_rpn_cls: 0.012  loss_rpn_loc: 0.039  time: 0.5470  data_time: 0.0066  lr: 0.000799  max_mem: 3070M
[32m[04/15 14:04:41 d2.utils.events]: [0m eta: 0:17:20  iter: 99  total_loss: 0.549  loss_cls: 0.195  loss_box_reg: 0.324  loss_rpn_cls: 0.020  loss_rpn_loc: 0.042  time: 0.5433  data_time: 0.0234  lr: 0.000999  max_mem: 3070M
[32m[04/15 14:04:52 d2.utils.events]: [0m eta: 0:17:09  iter: 119  total_loss: 0.598  loss_cls: 0.182  loss_box_reg: 0.340  loss_rpn_cls: 0.015  loss_rpn_loc: 0.061  time: 0.5419  data_time: 0.0065  lr: 0.001199  max_mem: 3070M
[32m[04/15 14:05:03 d2.utils.events]: [0m eta: 0:16:59  iter: 139  total_loss: 0.682  loss_cls: 0.227  loss_box_reg: 0.373  loss_rpn_cls: 0.017  loss_rpn_loc: 0.065  time: 0.5434  data_time: 0.0077  lr: 0.001399  max_mem: 3070M
[32m[04/15 14:05:14 d2.utils.events]: [0m eta: 0:16:48  iter: 159  total_loss: 0.562  loss_cls: 0.182  loss_box_reg: 0.317  loss_rpn_cls: 0.021  loss_rpn_loc: 0.050  time: 0.5445  data_time: 0.0079  lr: 0.001598  max_mem: 3070M
[32m[04/15 14:05:26 d2.utils.events]: [0m eta: 0:16:38  iter: 179  total_loss: 0.549  loss_cls: 0.168  loss_box_reg: 0.327  loss_rpn_cls: 0.020  loss_rpn_loc: 0.034  time: 0.5463  data_time: 0.0125  lr: 0.001798  max_mem: 3070M
[32m[04/15 14:05:38 d2.utils.events]: [0m eta: 0:16:27  iter: 199  total_loss: 0.628  loss_cls: 0.204  loss_box_reg: 0.341  loss_rpn_cls: 0.016  loss_rpn_loc: 0.058  time: 0.5464  data_time: 0.0064  lr: 0.001998  max_mem: 3070M
[32m[04/15 14:05:51 d2.utils.events]: [0m eta: 0:16:15  iter: 219  total_loss: 0.577  loss_cls: 0.193  loss_box_reg: 0.313  loss_rpn_cls: 0.015  loss_rpn_loc: 0.049  time: 0.5457  data_time: 0.0252  lr: 0.002198  max_mem: 3070M
[32m[04/15 14:06:04 d2.utils.events]: [0m eta: 0:16:04  iter: 239  total_loss: 0.598  loss_cls: 0.199  loss_box_reg: 0.322  loss_rpn_cls: 0.015  loss_rpn_loc: 0.047  time: 0.5459  data_time: 0.0095  lr: 0.002398  max_mem: 3070M
[32m[04/15 14:06:16 d2.utils.events]: [0m eta: 0:15:53  iter: 259  total_loss: 0.592  loss_cls: 0.184  loss_box_reg: 0.310  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5484  data_time: 0.1010  lr: 0.002597  max_mem: 3070M
[32m[04/15 14:06:36 d2.utils.events]: [0m eta: 0:15:42  iter: 279  total_loss: 0.623  loss_cls: 0.233  loss_box_reg: 0.332  loss_rpn_cls: 0.019  loss_rpn_loc: 0.064  time: 0.5505  data_time: 0.1093  lr: 0.002797  max_mem: 3070M
[32m[04/15 14:06:47 d2.utils.events]: [0m eta: 0:15:30  iter: 299  total_loss: 0.708  loss_cls: 0.234  loss_box_reg: 0.396  loss_rpn_cls: 0.021  loss_rpn_loc: 0.054  time: 0.5477  data_time: 0.0137  lr: 0.002997  max_mem: 3070M
[32m[04/15 14:07:01 d2.utils.events]: [0m eta: 0:15:17  iter: 319  total_loss: 0.631  loss_cls: 0.211  loss_box_reg: 0.316  loss_rpn_cls: 0.017  loss_rpn_loc: 0.052  time: 0.5475  data_time: 0.0568  lr: 0.003197  max_mem: 3070M
[32m[04/15 14:07:18 d2.utils.events]: [0m eta: 0:15:01  iter: 339  total_loss: 0.657  loss_cls: 0.217  loss_box_reg: 0.372  loss_rpn_cls: 0.023  loss_rpn_loc: 0.052  time: 0.5466  data_time: 0.0068  lr: 0.003397  max_mem: 3070M
[32m[04/15 14:07:31 d2.utils.events]: [0m eta: 0:14:51  iter: 359  total_loss: 0.705  loss_cls: 0.215  loss_box_reg: 0.339  loss_rpn_cls: 0.017  loss_rpn_loc: 0.071  time: 0.5459  data_time: 0.0134  lr: 0.003596  max_mem: 3070M
[32m[04/15 14:07:43 d2.utils.events]: [0m eta: 0:14:39  iter: 379  total_loss: 0.636  loss_cls: 0.189  loss_box_reg: 0.348  loss_rpn_cls: 0.018  loss_rpn_loc: 0.049  time: 0.5457  data_time: 0.0077  lr: 0.003796  max_mem: 3070M
[32m[04/15 14:07:56 d2.utils.events]: [0m eta: 0:14:27  iter: 399  total_loss: 0.673  loss_cls: 0.195  loss_box_reg: 0.373  loss_rpn_cls: 0.020  loss_rpn_loc: 0.084  time: 0.5451  data_time: 0.0146  lr: 0.003996  max_mem: 3070M
[32m[04/15 14:08:11 d2.utils.events]: [0m eta: 0:14:19  iter: 419  total_loss: 0.610  loss_cls: 0.195  loss_box_reg: 0.342  loss_rpn_cls: 0.016  loss_rpn_loc: 0.044  time: 0.5540  data_time: 0.2159  lr: 0.004196  max_mem: 3070M
[32m[04/15 14:08:28 d2.utils.events]: [0m eta: 0:14:10  iter: 439  total_loss: 0.667  loss_cls: 0.220  loss_box_reg: 0.335  loss_rpn_cls: 0.014  loss_rpn_loc: 0.048  time: 0.5534  data_time: 0.0094  lr: 0.004396  max_mem: 3070M
[32m[04/15 14:08:43 d2.utils.events]: [0m eta: 0:13:59  iter: 459  total_loss: 0.608  loss_cls: 0.201  loss_box_reg: 0.317  loss_rpn_cls: 0.014  loss_rpn_loc: 0.065  time: 0.5532  data_time: 0.0134  lr: 0.004595  max_mem: 3070M
[32m[04/15 14:08:56 d2.utils.events]: [0m eta: 0:13:49  iter: 479  total_loss: 0.660  loss_cls: 0.205  loss_box_reg: 0.369  loss_rpn_cls: 0.014  loss_rpn_loc: 0.055  time: 0.5536  data_time: 0.0325  lr: 0.004795  max_mem: 3070M
[32m[04/15 14:09:08 d2.utils.events]: [0m eta: 0:13:35  iter: 499  total_loss: 0.512  loss_cls: 0.176  loss_box_reg: 0.288  loss_rpn_cls: 0.012  loss_rpn_loc: 0.046  time: 0.5521  data_time: 0.0066  lr: 0.004995  max_mem: 3070M
[32m[04/15 14:09:20 d2.utils.events]: [0m eta: 0:13:24  iter: 519  total_loss: 0.670  loss_cls: 0.239  loss_box_reg: 0.375  loss_rpn_cls: 0.016  loss_rpn_loc: 0.064  time: 0.5515  data_time: 0.0069  lr: 0.005195  max_mem: 3070M
[32m[04/15 14:09:32 d2.utils.events]: [0m eta: 0:13:15  iter: 539  total_loss: 0.582  loss_cls: 0.180  loss_box_reg: 0.326  loss_rpn_cls: 0.017  loss_rpn_loc: 0.055  time: 0.5511  data_time: 0.0184  lr: 0.005395  max_mem: 3070M
[32m[04/15 14:09:43 d2.utils.events]: [0m eta: 0:13:05  iter: 559  total_loss: 0.614  loss_cls: 0.195  loss_box_reg: 0.350  loss_rpn_cls: 0.017  loss_rpn_loc: 0.063  time: 0.5510  data_time: 0.0349  lr: 0.005594  max_mem: 3070M
[32m[04/15 14:09:54 d2.utils.events]: [0m eta: 0:12:53  iter: 579  total_loss: 0.632  loss_cls: 0.208  loss_box_reg: 0.353  loss_rpn_cls: 0.016  loss_rpn_loc: 0.056  time: 0.5502  data_time: 0.0111  lr: 0.005794  max_mem: 3070M
[32m[04/15 14:10:05 d2.utils.events]: [0m eta: 0:12:41  iter: 599  total_loss: 0.525  loss_cls: 0.153  loss_box_reg: 0.289  loss_rpn_cls: 0.012  loss_rpn_loc: 0.039  time: 0.5493  data_time: 0.0070  lr: 0.005994  max_mem: 3070M
[32m[04/15 14:10:17 d2.utils.events]: [0m eta: 0:12:32  iter: 619  total_loss: 0.589  loss_cls: 0.192  loss_box_reg: 0.323  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5499  data_time: 0.0554  lr: 0.006194  max_mem: 3070M
[32m[04/15 14:10:32 d2.utils.events]: [0m eta: 0:12:21  iter: 639  total_loss: 0.517  loss_cls: 0.156  loss_box_reg: 0.273  loss_rpn_cls: 0.014  loss_rpn_loc: 0.055  time: 0.5538  data_time: 0.2079  lr: 0.006394  max_mem: 3070M
[32m[04/15 14:10:51 d2.utils.events]: [0m eta: 0:12:07  iter: 659  total_loss: 0.656  loss_cls: 0.205  loss_box_reg: 0.352  loss_rpn_cls: 0.016  loss_rpn_loc: 0.057  time: 0.5526  data_time: 0.0152  lr: 0.006593  max_mem: 3070M
[32m[04/15 14:11:05 d2.utils.events]: [0m eta: 0:11:58  iter: 679  total_loss: 0.626  loss_cls: 0.191  loss_box_reg: 0.358  loss_rpn_cls: 0.015  loss_rpn_loc: 0.072  time: 0.5528  data_time: 0.0322  lr: 0.006793  max_mem: 3070M
[32m[04/15 14:11:16 d2.utils.events]: [0m eta: 0:11:47  iter: 699  total_loss: 0.672  loss_cls: 0.197  loss_box_reg: 0.361  loss_rpn_cls: 0.011  loss_rpn_loc: 0.042  time: 0.5519  data_time: 0.0061  lr: 0.006993  max_mem: 3070M
[32m[04/15 14:11:30 d2.utils.events]: [0m eta: 0:11:35  iter: 719  total_loss: 0.597  loss_cls: 0.209  loss_box_reg: 0.332  loss_rpn_cls: 0.015  loss_rpn_loc: 0.043  time: 0.5508  data_time: 0.0040  lr: 0.007193  max_mem: 3070M
[32m[04/15 14:11:41 d2.utils.events]: [0m eta: 0:11:24  iter: 739  total_loss: 0.602  loss_cls: 0.189  loss_box_reg: 0.346  loss_rpn_cls: 0.018  loss_rpn_loc: 0.057  time: 0.5501  data_time: 0.0048  lr: 0.007393  max_mem: 3070M
[32m[04/15 14:11:51 d2.utils.events]: [0m eta: 0:11:12  iter: 759  total_loss: 0.594  loss_cls: 0.162  loss_box_reg: 0.351  loss_rpn_cls: 0.016  loss_rpn_loc: 0.048  time: 0.5493  data_time: 0.0314  lr: 0.007592  max_mem: 3070M
[32m[04/15 14:12:06 d2.utils.events]: [0m eta: 0:11:02  iter: 779  total_loss: 0.693  loss_cls: 0.211  loss_box_reg: 0.332  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5493  data_time: 0.0069  lr: 0.007792  max_mem: 3070M
[32m[04/15 14:12:17 d2.utils.events]: [0m eta: 0:10:51  iter: 799  total_loss: 0.688  loss_cls: 0.207  loss_box_reg: 0.400  loss_rpn_cls: 0.013  loss_rpn_loc: 0.067  time: 0.5488  data_time: 0.0086  lr: 0.007992  max_mem: 3070M
[32m[04/15 14:12:29 d2.utils.events]: [0m eta: 0:10:40  iter: 819  total_loss: 0.574  loss_cls: 0.190  loss_box_reg: 0.329  loss_rpn_cls: 0.013  loss_rpn_loc: 0.040  time: 0.5484  data_time: 0.0073  lr: 0.008192  max_mem: 3070M
[32m[04/15 14:12:41 d2.utils.events]: [0m eta: 0:10:29  iter: 839  total_loss: 0.564  loss_cls: 0.178  loss_box_reg: 0.320  loss_rpn_cls: 0.014  loss_rpn_loc: 0.061  time: 0.5483  data_time: 0.0073  lr: 0.008392  max_mem: 3070M
[32m[04/15 14:12:52 d2.utils.events]: [0m eta: 0:10:18  iter: 859  total_loss: 0.648  loss_cls: 0.211  loss_box_reg: 0.354  loss_rpn_cls: 0.020  loss_rpn_loc: 0.051  time: 0.5483  data_time: 0.0065  lr: 0.008591  max_mem: 3070M
[32m[04/15 14:13:03 d2.utils.events]: [0m eta: 0:10:07  iter: 879  total_loss: 0.679  loss_cls: 0.208  loss_box_reg: 0.352  loss_rpn_cls: 0.018  loss_rpn_loc: 0.058  time: 0.5480  data_time: 0.0069  lr: 0.008791  max_mem: 3070M
[32m[04/15 14:13:14 d2.utils.events]: [0m eta: 0:09:56  iter: 899  total_loss: 0.613  loss_cls: 0.220  loss_box_reg: 0.338  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5479  data_time: 0.0073  lr: 0.008991  max_mem: 3070M
[32m[04/15 14:13:25 d2.utils.events]: [0m eta: 0:09:45  iter: 919  total_loss: 0.719  loss_cls: 0.240  loss_box_reg: 0.388  loss_rpn_cls: 0.017  loss_rpn_loc: 0.050  time: 0.5476  data_time: 0.0067  lr: 0.009191  max_mem: 3070M
[32m[04/15 14:13:36 d2.utils.events]: [0m eta: 0:09:35  iter: 939  total_loss: 0.608  loss_cls: 0.190  loss_box_reg: 0.310  loss_rpn_cls: 0.017  loss_rpn_loc: 0.047  time: 0.5476  data_time: 0.0065  lr: 0.009391  max_mem: 3070M
[32m[04/15 14:13:48 d2.utils.events]: [0m eta: 0:09:24  iter: 959  total_loss: 0.581  loss_cls: 0.189  loss_box_reg: 0.314  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5476  data_time: 0.0074  lr: 0.009590  max_mem: 3070M
[32m[04/15 14:13:59 d2.utils.events]: [0m eta: 0:09:14  iter: 979  total_loss: 0.620  loss_cls: 0.193  loss_box_reg: 0.350  loss_rpn_cls: 0.020  loss_rpn_loc: 0.067  time: 0.5477  data_time: 0.0168  lr: 0.009790  max_mem: 3070M
[32m[04/15 14:14:11 d2.utils.events]: [0m eta: 0:09:03  iter: 999  total_loss: 0.623  loss_cls: 0.195  loss_box_reg: 0.305  loss_rpn_cls: 0.015  loss_rpn_loc: 0.071  time: 0.5475  data_time: 0.0085  lr: 0.009990  max_mem: 3070M
[32m[04/15 14:14:22 d2.utils.events]: [0m eta: 0:08:52  iter: 1019  total_loss: 0.511  loss_cls: 0.171  loss_box_reg: 0.282  loss_rpn_cls: 0.014  loss_rpn_loc: 0.040  time: 0.5477  data_time: 0.0151  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:14:35 d2.utils.events]: [0m eta: 0:08:41  iter: 1039  total_loss: 0.783  loss_cls: 0.247  loss_box_reg: 0.392  loss_rpn_cls: 0.028  loss_rpn_loc: 0.079  time: 0.5474  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:14:47 d2.utils.events]: [0m eta: 0:08:31  iter: 1059  total_loss: 0.624  loss_cls: 0.191  loss_box_reg: 0.322  loss_rpn_cls: 0.020  loss_rpn_loc: 0.064  time: 0.5476  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:14:59 d2.utils.events]: [0m eta: 0:08:19  iter: 1079  total_loss: 0.619  loss_cls: 0.191  loss_box_reg: 0.340  loss_rpn_cls: 0.020  loss_rpn_loc: 0.055  time: 0.5475  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:15:10 d2.utils.events]: [0m eta: 0:08:09  iter: 1099  total_loss: 0.699  loss_cls: 0.227  loss_box_reg: 0.344  loss_rpn_cls: 0.019  loss_rpn_loc: 0.085  time: 0.5473  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:15:21 d2.utils.events]: [0m eta: 0:07:57  iter: 1119  total_loss: 0.638  loss_cls: 0.191  loss_box_reg: 0.355  loss_rpn_cls: 0.017  loss_rpn_loc: 0.065  time: 0.5470  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:15:32 d2.utils.events]: [0m eta: 0:07:47  iter: 1139  total_loss: 0.633  loss_cls: 0.203  loss_box_reg: 0.360  loss_rpn_cls: 0.019  loss_rpn_loc: 0.069  time: 0.5471  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:15:44 d2.utils.events]: [0m eta: 0:07:36  iter: 1159  total_loss: 0.803  loss_cls: 0.266  loss_box_reg: 0.429  loss_rpn_cls: 0.021  loss_rpn_loc: 0.068  time: 0.5472  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:15:55 d2.utils.events]: [0m eta: 0:07:26  iter: 1179  total_loss: 0.623  loss_cls: 0.193  loss_box_reg: 0.327  loss_rpn_cls: 0.013  loss_rpn_loc: 0.066  time: 0.5472  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:16:09 d2.utils.events]: [0m eta: 0:07:14  iter: 1199  total_loss: 0.707  loss_cls: 0.229  loss_box_reg: 0.352  loss_rpn_cls: 0.027  loss_rpn_loc: 0.080  time: 0.5471  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:16:21 d2.utils.events]: [0m eta: 0:07:04  iter: 1219  total_loss: 0.576  loss_cls: 0.175  loss_box_reg: 0.343  loss_rpn_cls: 0.022  loss_rpn_loc: 0.049  time: 0.5475  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:16:33 d2.utils.events]: [0m eta: 0:06:53  iter: 1239  total_loss: 0.517  loss_cls: 0.181  loss_box_reg: 0.294  loss_rpn_cls: 0.018  loss_rpn_loc: 0.050  time: 0.5471  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:16:44 d2.utils.events]: [0m eta: 0:06:42  iter: 1259  total_loss: 0.811  loss_cls: 0.241  loss_box_reg: 0.435  loss_rpn_cls: 0.028  loss_rpn_loc: 0.076  time: 0.5470  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:16:55 d2.utils.events]: [0m eta: 0:06:31  iter: 1279  total_loss: 0.538  loss_cls: 0.178  loss_box_reg: 0.289  loss_rpn_cls: 0.015  loss_rpn_loc: 0.046  time: 0.5473  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:17:07 d2.utils.events]: [0m eta: 0:06:22  iter: 1299  total_loss: 0.630  loss_cls: 0.191  loss_box_reg: 0.337  loss_rpn_cls: 0.023  loss_rpn_loc: 0.067  time: 0.5475  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:17:18 d2.utils.events]: [0m eta: 0:06:11  iter: 1319  total_loss: 0.618  loss_cls: 0.192  loss_box_reg: 0.335  loss_rpn_cls: 0.026  loss_rpn_loc: 0.060  time: 0.5473  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:17:30 d2.utils.events]: [0m eta: 0:06:00  iter: 1339  total_loss: 0.699  loss_cls: 0.235  loss_box_reg: 0.358  loss_rpn_cls: 0.024  loss_rpn_loc: 0.057  time: 0.5475  data_time: 0.0094  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:17:41 d2.utils.events]: [0m eta: 0:05:50  iter: 1359  total_loss: 0.547  loss_cls: 0.151  loss_box_reg: 0.290  loss_rpn_cls: 0.019  loss_rpn_loc: 0.059  time: 0.5477  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:17:52 d2.utils.events]: [0m eta: 0:05:38  iter: 1379  total_loss: 0.678  loss_cls: 0.231  loss_box_reg: 0.382  loss_rpn_cls: 0.028  loss_rpn_loc: 0.064  time: 0.5476  data_time: 0.0247  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:18:04 d2.utils.events]: [0m eta: 0:05:28  iter: 1399  total_loss: 0.742  loss_cls: 0.218  loss_box_reg: 0.389  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5476  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:18:16 d2.utils.events]: [0m eta: 0:05:17  iter: 1419  total_loss: 0.639  loss_cls: 0.211  loss_box_reg: 0.333  loss_rpn_cls: 0.023  loss_rpn_loc: 0.052  time: 0.5480  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:18:28 d2.utils.events]: [0m eta: 0:05:06  iter: 1439  total_loss: 0.644  loss_cls: 0.229  loss_box_reg: 0.359  loss_rpn_cls: 0.028  loss_rpn_loc: 0.053  time: 0.5480  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:18:40 d2.utils.events]: [0m eta: 0:04:55  iter: 1459  total_loss: 0.649  loss_cls: 0.212  loss_box_reg: 0.367  loss_rpn_cls: 0.016  loss_rpn_loc: 0.048  time: 0.5480  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:18:53 d2.utils.events]: [0m eta: 0:04:44  iter: 1479  total_loss: 0.627  loss_cls: 0.198  loss_box_reg: 0.346  loss_rpn_cls: 0.023  loss_rpn_loc: 0.048  time: 0.5479  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:19:04 d2.utils.events]: [0m eta: 0:04:34  iter: 1499  total_loss: 0.660  loss_cls: 0.201  loss_box_reg: 0.365  loss_rpn_cls: 0.020  loss_rpn_loc: 0.070  time: 0.5481  data_time: 0.0438  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:19:15 d2.utils.events]: [0m eta: 0:04:23  iter: 1519  total_loss: 0.667  loss_cls: 0.203  loss_box_reg: 0.337  loss_rpn_cls: 0.025  loss_rpn_loc: 0.062  time: 0.5480  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:19:47 d2.utils.events]: [0m eta: 0:04:12  iter: 1539  total_loss: 0.576  loss_cls: 0.190  loss_box_reg: 0.303  loss_rpn_cls: 0.023  loss_rpn_loc: 0.055  time: 0.5479  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:20:13 d2.utils.events]: [0m eta: 0:04:01  iter: 1559  total_loss: 0.666  loss_cls: 0.215  loss_box_reg: 0.358  loss_rpn_cls: 0.025  loss_rpn_loc: 0.054  time: 0.5476  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:20:24 d2.utils.events]: [0m eta: 0:03:50  iter: 1579  total_loss: 0.597  loss_cls: 0.183  loss_box_reg: 0.321  loss_rpn_cls: 0.018  loss_rpn_loc: 0.058  time: 0.5472  data_time: 0.0047  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:20:35 d2.utils.events]: [0m eta: 0:03:38  iter: 1599  total_loss: 0.591  loss_cls: 0.192  loss_box_reg: 0.321  loss_rpn_cls: 0.020  loss_rpn_loc: 0.064  time: 0.5469  data_time: 0.0042  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:20:46 d2.utils.events]: [0m eta: 0:03:27  iter: 1619  total_loss: 0.668  loss_cls: 0.206  loss_box_reg: 0.350  loss_rpn_cls: 0.024  loss_rpn_loc: 0.060  time: 0.5465  data_time: 0.0041  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:20:59 d2.utils.events]: [0m eta: 0:03:16  iter: 1639  total_loss: 0.637  loss_cls: 0.200  loss_box_reg: 0.333  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5462  data_time: 0.0040  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:21:17 d2.utils.events]: [0m eta: 0:03:05  iter: 1659  total_loss: 0.635  loss_cls: 0.218  loss_box_reg: 0.342  loss_rpn_cls: 0.019  loss_rpn_loc: 0.050  time: 0.5461  data_time: 0.0273  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:21:30 d2.utils.events]: [0m eta: 0:02:54  iter: 1679  total_loss: 0.749  loss_cls: 0.236  loss_box_reg: 0.417  loss_rpn_cls: 0.021  loss_rpn_loc: 0.066  time: 0.5461  data_time: 0.0123  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:21:42 d2.utils.events]: [0m eta: 0:02:44  iter: 1699  total_loss: 0.675  loss_cls: 0.211  loss_box_reg: 0.370  loss_rpn_cls: 0.018  loss_rpn_loc: 0.060  time: 0.5462  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:22:10 d2.utils.events]: [0m eta: 0:02:33  iter: 1719  total_loss: 0.735  loss_cls: 0.224  loss_box_reg: 0.397  loss_rpn_cls: 0.023  loss_rpn_loc: 0.059  time: 0.5460  data_time: 0.0116  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:22:22 d2.utils.events]: [0m eta: 0:02:22  iter: 1739  total_loss: 0.683  loss_cls: 0.211  loss_box_reg: 0.357  loss_rpn_cls: 0.022  loss_rpn_loc: 0.053  time: 0.5459  data_time: 0.0351  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:22:34 d2.utils.events]: [0m eta: 0:02:12  iter: 1759  total_loss: 0.733  loss_cls: 0.220  loss_box_reg: 0.390  loss_rpn_cls: 0.018  loss_rpn_loc: 0.064  time: 0.5461  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:22:45 d2.utils.events]: [0m eta: 0:02:00  iter: 1779  total_loss: 0.707  loss_cls: 0.225  loss_box_reg: 0.394  loss_rpn_cls: 0.024  loss_rpn_loc: 0.062  time: 0.5459  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:23:02 d2.utils.events]: [0m eta: 0:01:49  iter: 1799  total_loss: 0.644  loss_cls: 0.201  loss_box_reg: 0.323  loss_rpn_cls: 0.027  loss_rpn_loc: 0.069  time: 0.5457  data_time: 0.0217  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:23:17 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.731  loss_cls: 0.234  loss_box_reg: 0.391  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5457  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:23:31 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.618  loss_cls: 0.199  loss_box_reg: 0.336  loss_rpn_cls: 0.022  loss_rpn_loc: 0.055  time: 0.5455  data_time: 0.0095  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:23:42 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.611  loss_cls: 0.176  loss_box_reg: 0.348  loss_rpn_cls: 0.021  loss_rpn_loc: 0.047  time: 0.5457  data_time: 0.0405  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:23:56 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.734  loss_cls: 0.245  loss_box_reg: 0.371  loss_rpn_cls: 0.020  loss_rpn_loc: 0.077  time: 0.5456  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:24:10 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.609  loss_cls: 0.196  loss_box_reg: 0.337  loss_rpn_cls: 0.016  loss_rpn_loc: 0.054  time: 0.5457  data_time: 0.0190  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:24:22 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.674  loss_cls: 0.206  loss_box_reg: 0.368  loss_rpn_cls: 0.020  loss_rpn_loc: 0.047  time: 0.5463  data_time: 0.0724  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:24:38 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.711  loss_cls: 0.235  loss_box_reg: 0.384  loss_rpn_cls: 0.015  loss_rpn_loc: 0.075  time: 0.5460  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:24:49 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.632  loss_cls: 0.190  loss_box_reg: 0.357  loss_rpn_cls: 0.016  loss_rpn_loc: 0.051  time: 0.5460  data_time: 0.0198  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:25:06 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.670  loss_cls: 0.212  loss_box_reg: 0.350  loss_rpn_cls: 0.015  loss_rpn_loc: 0.062  time: 0.5460  data_time: 0.0131  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 14:25:43 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 14:25:43 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 14:25:43 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 14:25:43 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.617  loss_cls: 0.187  loss_box_reg: 0.347  loss_rpn_cls: 0.016  loss_rpn_loc: 0.060  time: 0.5458  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:26:02 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:10 (0.5461 s / it)
[32m[04/15 14:26:02 d2.engine.hooks]: [0mTotal training time: 0:22:16 (0:04:06 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 14:26:15 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 14:26:15 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 14:26:15 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 14:26:18 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1228 s / img. ETA=0:05:15
[32m[04/15 14:26:23 d2.evaluation.evaluator]: [0mInference done 27/1257. 0.1213 s / img. ETA=0:06:16
[32m[04/15 14:26:28 d2.evaluation.evaluator]: [0mInference done 43/1257. 0.1208 s / img. ETA=0:06:20
[32m[04/15 14:26:33 d2.evaluation.evaluator]: [0mInference done 68/1257. 0.1211 s / img. ETA=0:05:19
[32m[04/15 14:26:39 d2.evaluation.evaluator]: [0mInference done 100/1257. 0.1209 s / img. ETA=0:04:29
[32m[04/15 14:26:44 d2.evaluation.evaluator]: [0mInference done 129/1257. 0.1214 s / img. ETA=0:04:07
[32m[04/15 14:26:49 d2.evaluation.evaluator]: [0mInference done 156/1257. 0.1213 s / img. ETA=0:03:55
[32m[04/15 14:26:54 d2.evaluation.evaluator]: [0mInference done 183/1257. 0.1212 s / img. ETA=0:03:46
[32m[04/15 14:26:59 d2.evaluation.evaluator]: [0mInference done 203/1257. 0.1210 s / img. ETA=0:03:46
[32m[04/15 14:27:04 d2.evaluation.evaluator]: [0mInference done 218/1257. 0.1209 s / img. ETA=0:03:52
[32m[04/15 14:27:09 d2.evaluation.evaluator]: [0mInference done 236/1257. 0.1208 s / img. ETA=0:03:53
[32m[04/15 14:27:14 d2.evaluation.evaluator]: [0mInference done 264/1257. 0.1209 s / img. ETA=0:03:41
[32m[04/15 14:27:19 d2.evaluation.evaluator]: [0mInference done 297/1257. 0.1209 s / img. ETA=0:03:26
[32m[04/15 14:27:24 d2.evaluation.evaluator]: [0mInference done 322/1257. 0.1209 s / img. ETA=0:03:20
[32m[04/15 14:27:30 d2.evaluation.evaluator]: [0mInference done 347/1257. 0.1209 s / img. ETA=0:03:14
[32m[04/15 14:27:35 d2.evaluation.evaluator]: [0mInference done 374/1257. 0.1210 s / img. ETA=0:03:06
[32m[04/15 14:27:40 d2.evaluation.evaluator]: [0mInference done 412/1257. 0.1211 s / img. ETA=0:02:52
[32m[04/15 14:27:45 d2.evaluation.evaluator]: [0mInference done 436/1257. 0.1211 s / img. ETA=0:02:48
[32m[04/15 14:27:50 d2.evaluation.evaluator]: [0mInference done 457/1257. 0.1210 s / img. ETA=0:02:45
[32m[04/15 14:27:55 d2.evaluation.evaluator]: [0mInference done 473/1257. 0.1210 s / img. ETA=0:02:45
[32m[04/15 14:28:00 d2.evaluation.evaluator]: [0mInference done 494/1257. 0.1211 s / img. ETA=0:02:41
[32m[04/15 14:28:05 d2.evaluation.evaluator]: [0mInference done 515/1257. 0.1212 s / img. ETA=0:02:38
[32m[04/15 14:28:10 d2.evaluation.evaluator]: [0mInference done 537/1257. 0.1212 s / img. ETA=0:02:34
[32m[04/15 14:28:16 d2.evaluation.evaluator]: [0mInference done 553/1257. 0.1212 s / img. ETA=0:02:33
[32m[04/15 14:28:21 d2.evaluation.evaluator]: [0mInference done 576/1257. 0.1213 s / img. ETA=0:02:28
[32m[04/15 14:28:26 d2.evaluation.evaluator]: [0mInference done 600/1257. 0.1213 s / img. ETA=0:02:22
[32m[04/15 14:28:31 d2.evaluation.evaluator]: [0mInference done 621/1257. 0.1214 s / img. ETA=0:02:18
[32m[04/15 14:28:36 d2.evaluation.evaluator]: [0mInference done 639/1257. 0.1214 s / img. ETA=0:02:16
[32m[04/15 14:28:41 d2.evaluation.evaluator]: [0mInference done 660/1257. 0.1213 s / img. ETA=0:02:12
[32m[04/15 14:28:47 d2.evaluation.evaluator]: [0mInference done 678/1257. 0.1213 s / img. ETA=0:02:09
[32m[04/15 14:28:52 d2.evaluation.evaluator]: [0mInference done 698/1257. 0.1214 s / img. ETA=0:02:05
[32m[04/15 14:28:57 d2.evaluation.evaluator]: [0mInference done 722/1257. 0.1214 s / img. ETA=0:01:59
[32m[04/15 14:29:02 d2.evaluation.evaluator]: [0mInference done 744/1257. 0.1214 s / img. ETA=0:01:54
[32m[04/15 14:29:07 d2.evaluation.evaluator]: [0mInference done 769/1257. 0.1214 s / img. ETA=0:01:48
[32m[04/15 14:29:12 d2.evaluation.evaluator]: [0mInference done 797/1257. 0.1215 s / img. ETA=0:01:42
[32m[04/15 14:29:17 d2.evaluation.evaluator]: [0mInference done 827/1257. 0.1216 s / img. ETA=0:01:34
[32m[04/15 14:29:23 d2.evaluation.evaluator]: [0mInference done 857/1257. 0.1216 s / img. ETA=0:01:27
[32m[04/15 14:29:28 d2.evaluation.evaluator]: [0mInference done 884/1257. 0.1216 s / img. ETA=0:01:21
[32m[04/15 14:29:33 d2.evaluation.evaluator]: [0mInference done 914/1257. 0.1216 s / img. ETA=0:01:14
[32m[04/15 14:29:38 d2.evaluation.evaluator]: [0mInference done 940/1257. 0.1216 s / img. ETA=0:01:08
[32m[04/15 14:29:43 d2.evaluation.evaluator]: [0mInference done 965/1257. 0.1216 s / img. ETA=0:01:02
[32m[04/15 14:29:48 d2.evaluation.evaluator]: [0mInference done 986/1257. 0.1215 s / img. ETA=0:00:58
[32m[04/15 14:29:54 d2.evaluation.evaluator]: [0mInference done 1008/1257. 0.1215 s / img. ETA=0:00:53
[32m[04/15 14:29:59 d2.evaluation.evaluator]: [0mInference done 1032/1257. 0.1214 s / img. ETA=0:00:48
[32m[04/15 14:30:04 d2.evaluation.evaluator]: [0mInference done 1050/1257. 0.1214 s / img. ETA=0:00:45
[32m[04/15 14:30:10 d2.evaluation.evaluator]: [0mInference done 1072/1257. 0.1213 s / img. ETA=0:00:40
[32m[04/15 14:30:15 d2.evaluation.evaluator]: [0mInference done 1095/1257. 0.1213 s / img. ETA=0:00:35
[32m[04/15 14:30:20 d2.evaluation.evaluator]: [0mInference done 1116/1257. 0.1212 s / img. ETA=0:00:30
[32m[04/15 14:30:25 d2.evaluation.evaluator]: [0mInference done 1147/1257. 0.1212 s / img. ETA=0:00:23
[32m[04/15 14:30:30 d2.evaluation.evaluator]: [0mInference done 1173/1257. 0.1212 s / img. ETA=0:00:18
[32m[04/15 14:30:35 d2.evaluation.evaluator]: [0mInference done 1203/1257. 0.1212 s / img. ETA=0:00:11
[32m[04/15 14:30:40 d2.evaluation.evaluator]: [0mInference done 1234/1257. 0.1213 s / img. ETA=0:00:04
[32m[04/15 14:30:45 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:28.480521 (0.214441 s / img per device, on 1 devices)
[32m[04/15 14:30:45 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:31 (0.121244 s / img per device, on 1 devices)
[32m[04/15 14:30:45 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 14:30:45 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 14:30:45 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=4.98s).
Accumulating evaluation results...
DONE (t=0.67s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.176
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.347
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.110
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.360
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.097
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.228
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.151
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.271
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396
[32m[04/15 14:30:51 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 17.644 | 34.727 | 15.392 | 10.992 | 21.729 | 36.022 |
[32m[04/15 14:30:51 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 21.319 | bicycle       | 7.446 | car            | 41.648 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.165 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  23  *  2000  iterations ============
4 channel input
[32m[04/15 14:30:52 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 14:30:54 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 2.08 seconds.
[5m[31mWARNING[0m [32m[04/15 14:30:54 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 14:30:54 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 14:30:55 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 14:30:55 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 14:30:55 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 14:31:03 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 14:31:14 d2.utils.events]: [0m eta: 0:17:42  iter: 19  total_loss: 0.558  loss_cls: 0.170  loss_box_reg: 0.324  loss_rpn_cls: 0.013  loss_rpn_loc: 0.046  time: 0.5372  data_time: 0.0297  lr: 0.000200  max_mem: 3070M
[32m[04/15 14:31:28 d2.utils.events]: [0m eta: 0:17:40  iter: 39  total_loss: 0.658  loss_cls: 0.196  loss_box_reg: 0.362  loss_rpn_cls: 0.019  loss_rpn_loc: 0.051  time: 0.5414  data_time: 0.0083  lr: 0.000400  max_mem: 3070M
[32m[04/15 14:31:39 d2.utils.events]: [0m eta: 0:17:30  iter: 59  total_loss: 0.624  loss_cls: 0.202  loss_box_reg: 0.364  loss_rpn_cls: 0.020  loss_rpn_loc: 0.055  time: 0.5404  data_time: 0.0060  lr: 0.000599  max_mem: 3070M
[32m[04/15 14:31:51 d2.utils.events]: [0m eta: 0:17:22  iter: 79  total_loss: 0.586  loss_cls: 0.184  loss_box_reg: 0.327  loss_rpn_cls: 0.011  loss_rpn_loc: 0.052  time: 0.5411  data_time: 0.0065  lr: 0.000799  max_mem: 3070M
[32m[04/15 14:32:06 d2.utils.events]: [0m eta: 0:17:08  iter: 99  total_loss: 0.558  loss_cls: 0.165  loss_box_reg: 0.305  loss_rpn_cls: 0.014  loss_rpn_loc: 0.038  time: 0.5390  data_time: 0.0068  lr: 0.000999  max_mem: 3070M
[32m[04/15 14:32:16 d2.utils.events]: [0m eta: 0:16:57  iter: 119  total_loss: 0.631  loss_cls: 0.180  loss_box_reg: 0.359  loss_rpn_cls: 0.016  loss_rpn_loc: 0.055  time: 0.5358  data_time: 0.0093  lr: 0.001199  max_mem: 3070M
[32m[04/15 14:32:27 d2.utils.events]: [0m eta: 0:16:45  iter: 139  total_loss: 0.494  loss_cls: 0.159  loss_box_reg: 0.293  loss_rpn_cls: 0.012  loss_rpn_loc: 0.042  time: 0.5329  data_time: 0.0069  lr: 0.001399  max_mem: 3070M
[32m[04/15 14:32:41 d2.utils.events]: [0m eta: 0:16:33  iter: 159  total_loss: 0.613  loss_cls: 0.208  loss_box_reg: 0.329  loss_rpn_cls: 0.019  loss_rpn_loc: 0.059  time: 0.5317  data_time: 0.0071  lr: 0.001598  max_mem: 3070M
[32m[04/15 14:32:52 d2.utils.events]: [0m eta: 0:16:24  iter: 179  total_loss: 0.648  loss_cls: 0.214  loss_box_reg: 0.354  loss_rpn_cls: 0.013  loss_rpn_loc: 0.059  time: 0.5329  data_time: 0.0074  lr: 0.001798  max_mem: 3070M
[32m[04/15 14:33:03 d2.utils.events]: [0m eta: 0:16:09  iter: 199  total_loss: 0.607  loss_cls: 0.194  loss_box_reg: 0.328  loss_rpn_cls: 0.015  loss_rpn_loc: 0.067  time: 0.5327  data_time: 0.0092  lr: 0.001998  max_mem: 3070M
[32m[04/15 14:33:19 d2.utils.events]: [0m eta: 0:16:02  iter: 219  total_loss: 0.616  loss_cls: 0.199  loss_box_reg: 0.349  loss_rpn_cls: 0.015  loss_rpn_loc: 0.056  time: 0.5336  data_time: 0.0085  lr: 0.002198  max_mem: 3070M
[32m[04/15 14:33:30 d2.utils.events]: [0m eta: 0:15:52  iter: 239  total_loss: 0.700  loss_cls: 0.211  loss_box_reg: 0.402  loss_rpn_cls: 0.016  loss_rpn_loc: 0.065  time: 0.5351  data_time: 0.0153  lr: 0.002398  max_mem: 3070M
[32m[04/15 14:33:40 d2.utils.events]: [0m eta: 0:15:38  iter: 259  total_loss: 0.600  loss_cls: 0.196  loss_box_reg: 0.342  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5341  data_time: 0.0078  lr: 0.002597  max_mem: 3070M
[32m[04/15 14:33:51 d2.utils.events]: [0m eta: 0:15:33  iter: 279  total_loss: 0.590  loss_cls: 0.186  loss_box_reg: 0.320  loss_rpn_cls: 0.018  loss_rpn_loc: 0.042  time: 0.5355  data_time: 0.0073  lr: 0.002797  max_mem: 3070M
[32m[04/15 14:34:03 d2.utils.events]: [0m eta: 0:15:23  iter: 299  total_loss: 0.533  loss_cls: 0.177  loss_box_reg: 0.310  loss_rpn_cls: 0.018  loss_rpn_loc: 0.045  time: 0.5362  data_time: 0.0060  lr: 0.002997  max_mem: 3070M
[32m[04/15 14:34:13 d2.utils.events]: [0m eta: 0:15:11  iter: 319  total_loss: 0.522  loss_cls: 0.163  loss_box_reg: 0.303  loss_rpn_cls: 0.013  loss_rpn_loc: 0.040  time: 0.5359  data_time: 0.0218  lr: 0.003197  max_mem: 3070M
[32m[04/15 14:34:24 d2.utils.events]: [0m eta: 0:15:01  iter: 339  total_loss: 0.613  loss_cls: 0.176  loss_box_reg: 0.336  loss_rpn_cls: 0.012  loss_rpn_loc: 0.062  time: 0.5365  data_time: 0.0111  lr: 0.003397  max_mem: 3070M
[32m[04/15 14:34:35 d2.utils.events]: [0m eta: 0:14:51  iter: 359  total_loss: 0.637  loss_cls: 0.216  loss_box_reg: 0.332  loss_rpn_cls: 0.014  loss_rpn_loc: 0.058  time: 0.5371  data_time: 0.0084  lr: 0.003596  max_mem: 3070M
[32m[04/15 14:34:47 d2.utils.events]: [0m eta: 0:14:43  iter: 379  total_loss: 0.640  loss_cls: 0.196  loss_box_reg: 0.364  loss_rpn_cls: 0.019  loss_rpn_loc: 0.046  time: 0.5379  data_time: 0.0067  lr: 0.003796  max_mem: 3070M
[32m[04/15 14:34:58 d2.utils.events]: [0m eta: 0:14:33  iter: 399  total_loss: 0.621  loss_cls: 0.228  loss_box_reg: 0.344  loss_rpn_cls: 0.017  loss_rpn_loc: 0.051  time: 0.5385  data_time: 0.0074  lr: 0.003996  max_mem: 3070M
[32m[04/15 14:35:09 d2.utils.events]: [0m eta: 0:14:24  iter: 419  total_loss: 0.580  loss_cls: 0.181  loss_box_reg: 0.338  loss_rpn_cls: 0.009  loss_rpn_loc: 0.046  time: 0.5389  data_time: 0.0114  lr: 0.004196  max_mem: 3070M
[32m[04/15 14:35:20 d2.utils.events]: [0m eta: 0:14:14  iter: 439  total_loss: 0.557  loss_cls: 0.182  loss_box_reg: 0.295  loss_rpn_cls: 0.015  loss_rpn_loc: 0.042  time: 0.5395  data_time: 0.0073  lr: 0.004396  max_mem: 3070M
[32m[04/15 14:35:31 d2.utils.events]: [0m eta: 0:14:05  iter: 459  total_loss: 0.527  loss_cls: 0.171  loss_box_reg: 0.316  loss_rpn_cls: 0.015  loss_rpn_loc: 0.041  time: 0.5403  data_time: 0.0071  lr: 0.004595  max_mem: 3070M
[32m[04/15 14:35:42 d2.utils.events]: [0m eta: 0:13:54  iter: 479  total_loss: 0.580  loss_cls: 0.188  loss_box_reg: 0.312  loss_rpn_cls: 0.017  loss_rpn_loc: 0.058  time: 0.5406  data_time: 0.0068  lr: 0.004795  max_mem: 3070M
[32m[04/15 14:35:54 d2.utils.events]: [0m eta: 0:13:43  iter: 499  total_loss: 0.614  loss_cls: 0.192  loss_box_reg: 0.356  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5413  data_time: 0.0070  lr: 0.004995  max_mem: 3070M
[32m[04/15 14:36:05 d2.utils.events]: [0m eta: 0:13:32  iter: 519  total_loss: 0.603  loss_cls: 0.191  loss_box_reg: 0.368  loss_rpn_cls: 0.009  loss_rpn_loc: 0.058  time: 0.5413  data_time: 0.0133  lr: 0.005195  max_mem: 3070M
[32m[04/15 14:36:16 d2.utils.events]: [0m eta: 0:13:23  iter: 539  total_loss: 0.614  loss_cls: 0.192  loss_box_reg: 0.351  loss_rpn_cls: 0.015  loss_rpn_loc: 0.067  time: 0.5423  data_time: 0.0063  lr: 0.005395  max_mem: 3070M
[32m[04/15 14:36:27 d2.utils.events]: [0m eta: 0:13:11  iter: 559  total_loss: 0.714  loss_cls: 0.212  loss_box_reg: 0.352  loss_rpn_cls: 0.015  loss_rpn_loc: 0.057  time: 0.5426  data_time: 0.0080  lr: 0.005594  max_mem: 3070M
[32m[04/15 14:36:39 d2.utils.events]: [0m eta: 0:13:00  iter: 579  total_loss: 0.602  loss_cls: 0.205  loss_box_reg: 0.327  loss_rpn_cls: 0.021  loss_rpn_loc: 0.048  time: 0.5428  data_time: 0.0333  lr: 0.005794  max_mem: 3070M
[32m[04/15 14:36:50 d2.utils.events]: [0m eta: 0:12:49  iter: 599  total_loss: 0.643  loss_cls: 0.217  loss_box_reg: 0.337  loss_rpn_cls: 0.018  loss_rpn_loc: 0.057  time: 0.5427  data_time: 0.0086  lr: 0.005994  max_mem: 3070M
[32m[04/15 14:37:02 d2.utils.events]: [0m eta: 0:12:38  iter: 619  total_loss: 0.681  loss_cls: 0.207  loss_box_reg: 0.389  loss_rpn_cls: 0.012  loss_rpn_loc: 0.053  time: 0.5443  data_time: 0.0559  lr: 0.006194  max_mem: 3070M
[32m[04/15 14:37:13 d2.utils.events]: [0m eta: 0:12:28  iter: 639  total_loss: 0.680  loss_cls: 0.202  loss_box_reg: 0.395  loss_rpn_cls: 0.012  loss_rpn_loc: 0.057  time: 0.5449  data_time: 0.0245  lr: 0.006394  max_mem: 3070M
[32m[04/15 14:37:29 d2.utils.events]: [0m eta: 0:12:17  iter: 659  total_loss: 0.592  loss_cls: 0.195  loss_box_reg: 0.315  loss_rpn_cls: 0.017  loss_rpn_loc: 0.062  time: 0.5518  data_time: 0.2911  lr: 0.006593  max_mem: 3070M
[32m[04/15 14:37:42 d2.utils.events]: [0m eta: 0:12:06  iter: 679  total_loss: 0.592  loss_cls: 0.187  loss_box_reg: 0.318  loss_rpn_cls: 0.019  loss_rpn_loc: 0.070  time: 0.5548  data_time: 0.1806  lr: 0.006793  max_mem: 3070M
[32m[04/15 14:37:53 d2.utils.events]: [0m eta: 0:11:55  iter: 699  total_loss: 0.706  loss_cls: 0.211  loss_box_reg: 0.397  loss_rpn_cls: 0.019  loss_rpn_loc: 0.072  time: 0.5542  data_time: 0.0108  lr: 0.006993  max_mem: 3070M
[32m[04/15 14:38:04 d2.utils.events]: [0m eta: 0:11:44  iter: 719  total_loss: 0.649  loss_cls: 0.201  loss_box_reg: 0.361  loss_rpn_cls: 0.017  loss_rpn_loc: 0.056  time: 0.5540  data_time: 0.0266  lr: 0.007193  max_mem: 3070M
[32m[04/15 14:38:15 d2.utils.events]: [0m eta: 0:11:33  iter: 739  total_loss: 0.705  loss_cls: 0.221  loss_box_reg: 0.369  loss_rpn_cls: 0.014  loss_rpn_loc: 0.070  time: 0.5537  data_time: 0.0076  lr: 0.007393  max_mem: 3070M
[32m[04/15 14:38:30 d2.utils.events]: [0m eta: 0:11:21  iter: 759  total_loss: 0.634  loss_cls: 0.213  loss_box_reg: 0.375  loss_rpn_cls: 0.017  loss_rpn_loc: 0.071  time: 0.5532  data_time: 0.0072  lr: 0.007592  max_mem: 3070M
[32m[04/15 14:38:52 d2.utils.events]: [0m eta: 0:11:10  iter: 779  total_loss: 0.651  loss_cls: 0.219  loss_box_reg: 0.364  loss_rpn_cls: 0.016  loss_rpn_loc: 0.055  time: 0.5535  data_time: 0.0295  lr: 0.007792  max_mem: 3070M
[32m[04/15 14:39:03 d2.utils.events]: [0m eta: 0:10:59  iter: 799  total_loss: 0.640  loss_cls: 0.200  loss_box_reg: 0.353  loss_rpn_cls: 0.020  loss_rpn_loc: 0.056  time: 0.5531  data_time: 0.0078  lr: 0.007992  max_mem: 3070M
[32m[04/15 14:39:14 d2.utils.events]: [0m eta: 0:10:48  iter: 819  total_loss: 0.733  loss_cls: 0.218  loss_box_reg: 0.373  loss_rpn_cls: 0.022  loss_rpn_loc: 0.065  time: 0.5538  data_time: 0.0705  lr: 0.008192  max_mem: 3070M
[32m[04/15 14:39:25 d2.utils.events]: [0m eta: 0:10:37  iter: 839  total_loss: 0.465  loss_cls: 0.146  loss_box_reg: 0.263  loss_rpn_cls: 0.013  loss_rpn_loc: 0.039  time: 0.5531  data_time: 0.0139  lr: 0.008392  max_mem: 3070M
[32m[04/15 14:39:36 d2.utils.events]: [0m eta: 0:10:26  iter: 859  total_loss: 0.518  loss_cls: 0.157  loss_box_reg: 0.327  loss_rpn_cls: 0.010  loss_rpn_loc: 0.047  time: 0.5528  data_time: 0.0067  lr: 0.008591  max_mem: 3070M
[32m[04/15 14:39:47 d2.utils.events]: [0m eta: 0:10:15  iter: 879  total_loss: 0.608  loss_cls: 0.200  loss_box_reg: 0.340  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5528  data_time: 0.0418  lr: 0.008791  max_mem: 3070M
[32m[04/15 14:39:59 d2.utils.events]: [0m eta: 0:10:04  iter: 899  total_loss: 0.675  loss_cls: 0.205  loss_box_reg: 0.336  loss_rpn_cls: 0.030  loss_rpn_loc: 0.065  time: 0.5526  data_time: 0.0306  lr: 0.008991  max_mem: 3070M
[32m[04/15 14:40:09 d2.utils.events]: [0m eta: 0:09:53  iter: 919  total_loss: 0.790  loss_cls: 0.246  loss_box_reg: 0.407  loss_rpn_cls: 0.024  loss_rpn_loc: 0.073  time: 0.5522  data_time: 0.0064  lr: 0.009191  max_mem: 3070M
[32m[04/15 14:40:21 d2.utils.events]: [0m eta: 0:09:42  iter: 939  total_loss: 0.585  loss_cls: 0.179  loss_box_reg: 0.291  loss_rpn_cls: 0.015  loss_rpn_loc: 0.052  time: 0.5524  data_time: 0.0077  lr: 0.009391  max_mem: 3070M
[32m[04/15 14:40:32 d2.utils.events]: [0m eta: 0:09:31  iter: 959  total_loss: 0.691  loss_cls: 0.223  loss_box_reg: 0.382  loss_rpn_cls: 0.018  loss_rpn_loc: 0.053  time: 0.5523  data_time: 0.0102  lr: 0.009590  max_mem: 3070M
[32m[04/15 14:40:43 d2.utils.events]: [0m eta: 0:09:20  iter: 979  total_loss: 0.771  loss_cls: 0.228  loss_box_reg: 0.419  loss_rpn_cls: 0.021  loss_rpn_loc: 0.061  time: 0.5525  data_time: 0.0072  lr: 0.009790  max_mem: 3070M
[32m[04/15 14:40:54 d2.utils.events]: [0m eta: 0:09:09  iter: 999  total_loss: 0.572  loss_cls: 0.161  loss_box_reg: 0.302  loss_rpn_cls: 0.017  loss_rpn_loc: 0.057  time: 0.5522  data_time: 0.0075  lr: 0.009990  max_mem: 3070M
[32m[04/15 14:41:06 d2.utils.events]: [0m eta: 0:08:58  iter: 1019  total_loss: 0.669  loss_cls: 0.209  loss_box_reg: 0.357  loss_rpn_cls: 0.018  loss_rpn_loc: 0.053  time: 0.5523  data_time: 0.0169  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:41:17 d2.utils.events]: [0m eta: 0:08:47  iter: 1039  total_loss: 0.659  loss_cls: 0.212  loss_box_reg: 0.361  loss_rpn_cls: 0.021  loss_rpn_loc: 0.056  time: 0.5522  data_time: 0.0300  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:41:28 d2.utils.events]: [0m eta: 0:08:36  iter: 1059  total_loss: 0.656  loss_cls: 0.210  loss_box_reg: 0.362  loss_rpn_cls: 0.018  loss_rpn_loc: 0.061  time: 0.5521  data_time: 0.0221  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:41:40 d2.utils.events]: [0m eta: 0:08:26  iter: 1079  total_loss: 0.653  loss_cls: 0.198  loss_box_reg: 0.372  loss_rpn_cls: 0.019  loss_rpn_loc: 0.050  time: 0.5527  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:41:52 d2.utils.events]: [0m eta: 0:08:15  iter: 1099  total_loss: 0.574  loss_cls: 0.184  loss_box_reg: 0.344  loss_rpn_cls: 0.019  loss_rpn_loc: 0.042  time: 0.5528  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:42:03 d2.utils.events]: [0m eta: 0:08:04  iter: 1119  total_loss: 0.660  loss_cls: 0.194  loss_box_reg: 0.364  loss_rpn_cls: 0.015  loss_rpn_loc: 0.076  time: 0.5527  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:42:15 d2.utils.events]: [0m eta: 0:07:54  iter: 1139  total_loss: 0.549  loss_cls: 0.187  loss_box_reg: 0.308  loss_rpn_cls: 0.017  loss_rpn_loc: 0.059  time: 0.5527  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:42:26 d2.utils.events]: [0m eta: 0:07:43  iter: 1159  total_loss: 0.664  loss_cls: 0.196  loss_box_reg: 0.390  loss_rpn_cls: 0.023  loss_rpn_loc: 0.069  time: 0.5528  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:42:37 d2.utils.events]: [0m eta: 0:07:32  iter: 1179  total_loss: 0.563  loss_cls: 0.203  loss_box_reg: 0.298  loss_rpn_cls: 0.015  loss_rpn_loc: 0.052  time: 0.5528  data_time: 0.0092  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:42:49 d2.utils.events]: [0m eta: 0:07:22  iter: 1199  total_loss: 0.564  loss_cls: 0.178  loss_box_reg: 0.324  loss_rpn_cls: 0.014  loss_rpn_loc: 0.045  time: 0.5531  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:43:00 d2.utils.events]: [0m eta: 0:07:11  iter: 1219  total_loss: 0.716  loss_cls: 0.216  loss_box_reg: 0.383  loss_rpn_cls: 0.014  loss_rpn_loc: 0.089  time: 0.5531  data_time: 0.0122  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:43:10 d2.utils.events]: [0m eta: 0:06:59  iter: 1239  total_loss: 0.637  loss_cls: 0.218  loss_box_reg: 0.369  loss_rpn_cls: 0.021  loss_rpn_loc: 0.067  time: 0.5526  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:43:22 d2.utils.events]: [0m eta: 0:06:49  iter: 1259  total_loss: 0.495  loss_cls: 0.159  loss_box_reg: 0.303  loss_rpn_cls: 0.017  loss_rpn_loc: 0.042  time: 0.5527  data_time: 0.0063  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:43:32 d2.utils.events]: [0m eta: 0:06:37  iter: 1279  total_loss: 0.569  loss_cls: 0.165  loss_box_reg: 0.321  loss_rpn_cls: 0.020  loss_rpn_loc: 0.045  time: 0.5525  data_time: 0.0094  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:43:45 d2.utils.events]: [0m eta: 0:06:26  iter: 1299  total_loss: 0.624  loss_cls: 0.211  loss_box_reg: 0.305  loss_rpn_cls: 0.027  loss_rpn_loc: 0.051  time: 0.5528  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:43:56 d2.utils.events]: [0m eta: 0:06:16  iter: 1319  total_loss: 0.633  loss_cls: 0.225  loss_box_reg: 0.339  loss_rpn_cls: 0.025  loss_rpn_loc: 0.057  time: 0.5526  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:44:07 d2.utils.events]: [0m eta: 0:06:05  iter: 1339  total_loss: 0.637  loss_cls: 0.191  loss_box_reg: 0.334  loss_rpn_cls: 0.016  loss_rpn_loc: 0.059  time: 0.5524  data_time: 0.0191  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:44:18 d2.utils.events]: [0m eta: 0:05:54  iter: 1359  total_loss: 0.620  loss_cls: 0.210  loss_box_reg: 0.354  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5525  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:44:29 d2.utils.events]: [0m eta: 0:05:42  iter: 1379  total_loss: 0.671  loss_cls: 0.209  loss_box_reg: 0.372  loss_rpn_cls: 0.020  loss_rpn_loc: 0.059  time: 0.5523  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:44:40 d2.utils.events]: [0m eta: 0:05:31  iter: 1399  total_loss: 0.620  loss_cls: 0.176  loss_box_reg: 0.358  loss_rpn_cls: 0.026  loss_rpn_loc: 0.070  time: 0.5522  data_time: 0.0089  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:44:51 d2.utils.events]: [0m eta: 0:05:20  iter: 1419  total_loss: 0.643  loss_cls: 0.209  loss_box_reg: 0.323  loss_rpn_cls: 0.020  loss_rpn_loc: 0.062  time: 0.5521  data_time: 0.0089  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:45:02 d2.utils.events]: [0m eta: 0:05:09  iter: 1439  total_loss: 0.684  loss_cls: 0.220  loss_box_reg: 0.382  loss_rpn_cls: 0.018  loss_rpn_loc: 0.060  time: 0.5522  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:45:13 d2.utils.events]: [0m eta: 0:04:58  iter: 1459  total_loss: 0.737  loss_cls: 0.240  loss_box_reg: 0.427  loss_rpn_cls: 0.029  loss_rpn_loc: 0.063  time: 0.5520  data_time: 0.0096  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:45:24 d2.utils.events]: [0m eta: 0:04:46  iter: 1479  total_loss: 0.653  loss_cls: 0.210  loss_box_reg: 0.353  loss_rpn_cls: 0.021  loss_rpn_loc: 0.061  time: 0.5518  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:45:35 d2.utils.events]: [0m eta: 0:04:35  iter: 1499  total_loss: 0.639  loss_cls: 0.211  loss_box_reg: 0.347  loss_rpn_cls: 0.026  loss_rpn_loc: 0.046  time: 0.5519  data_time: 0.0102  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:45:46 d2.utils.events]: [0m eta: 0:04:24  iter: 1519  total_loss: 0.598  loss_cls: 0.200  loss_box_reg: 0.304  loss_rpn_cls: 0.031  loss_rpn_loc: 0.048  time: 0.5517  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:45:57 d2.utils.events]: [0m eta: 0:04:13  iter: 1539  total_loss: 0.703  loss_cls: 0.211  loss_box_reg: 0.381  loss_rpn_cls: 0.026  loss_rpn_loc: 0.076  time: 0.5517  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:46:08 d2.utils.events]: [0m eta: 0:04:02  iter: 1559  total_loss: 0.559  loss_cls: 0.170  loss_box_reg: 0.306  loss_rpn_cls: 0.023  loss_rpn_loc: 0.068  time: 0.5517  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:46:20 d2.utils.events]: [0m eta: 0:03:51  iter: 1579  total_loss: 0.601  loss_cls: 0.197  loss_box_reg: 0.321  loss_rpn_cls: 0.027  loss_rpn_loc: 0.059  time: 0.5519  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:46:31 d2.utils.events]: [0m eta: 0:03:40  iter: 1599  total_loss: 0.634  loss_cls: 0.198  loss_box_reg: 0.359  loss_rpn_cls: 0.019  loss_rpn_loc: 0.069  time: 0.5519  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:46:42 d2.utils.events]: [0m eta: 0:03:29  iter: 1619  total_loss: 0.541  loss_cls: 0.164  loss_box_reg: 0.285  loss_rpn_cls: 0.019  loss_rpn_loc: 0.059  time: 0.5518  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:46:53 d2.utils.events]: [0m eta: 0:03:18  iter: 1639  total_loss: 0.688  loss_cls: 0.217  loss_box_reg: 0.370  loss_rpn_cls: 0.017  loss_rpn_loc: 0.053  time: 0.5519  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:47:05 d2.utils.events]: [0m eta: 0:03:07  iter: 1659  total_loss: 0.609  loss_cls: 0.181  loss_box_reg: 0.372  loss_rpn_cls: 0.019  loss_rpn_loc: 0.042  time: 0.5520  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:47:16 d2.utils.events]: [0m eta: 0:02:56  iter: 1679  total_loss: 0.682  loss_cls: 0.217  loss_box_reg: 0.334  loss_rpn_cls: 0.027  loss_rpn_loc: 0.062  time: 0.5520  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:47:27 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.554  loss_cls: 0.175  loss_box_reg: 0.306  loss_rpn_cls: 0.025  loss_rpn_loc: 0.064  time: 0.5522  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:47:42 d2.utils.events]: [0m eta: 0:02:34  iter: 1719  total_loss: 0.679  loss_cls: 0.228  loss_box_reg: 0.384  loss_rpn_cls: 0.025  loss_rpn_loc: 0.050  time: 0.5519  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:47:53 d2.utils.events]: [0m eta: 0:02:23  iter: 1739  total_loss: 0.652  loss_cls: 0.190  loss_box_reg: 0.379  loss_rpn_cls: 0.017  loss_rpn_loc: 0.059  time: 0.5517  data_time: 0.0236  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:48:04 d2.utils.events]: [0m eta: 0:02:12  iter: 1759  total_loss: 0.685  loss_cls: 0.203  loss_box_reg: 0.360  loss_rpn_cls: 0.035  loss_rpn_loc: 0.093  time: 0.5517  data_time: 0.0062  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:48:14 d2.utils.events]: [0m eta: 0:02:01  iter: 1779  total_loss: 0.686  loss_cls: 0.213  loss_box_reg: 0.350  loss_rpn_cls: 0.022  loss_rpn_loc: 0.072  time: 0.5515  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:48:32 d2.utils.events]: [0m eta: 0:01:50  iter: 1799  total_loss: 0.719  loss_cls: 0.242  loss_box_reg: 0.389  loss_rpn_cls: 0.022  loss_rpn_loc: 0.061  time: 0.5515  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:48:43 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.585  loss_cls: 0.181  loss_box_reg: 0.332  loss_rpn_cls: 0.012  loss_rpn_loc: 0.060  time: 0.5512  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:48:54 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.473  loss_cls: 0.147  loss_box_reg: 0.256  loss_rpn_cls: 0.013  loss_rpn_loc: 0.044  time: 0.5512  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:49:05 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.734  loss_cls: 0.234  loss_box_reg: 0.421  loss_rpn_cls: 0.027  loss_rpn_loc: 0.063  time: 0.5513  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:49:17 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.631  loss_cls: 0.207  loss_box_reg: 0.323  loss_rpn_cls: 0.021  loss_rpn_loc: 0.056  time: 0.5514  data_time: 0.0123  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:49:27 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.719  loss_cls: 0.205  loss_box_reg: 0.359  loss_rpn_cls: 0.015  loss_rpn_loc: 0.069  time: 0.5512  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:49:39 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.727  loss_cls: 0.224  loss_box_reg: 0.391  loss_rpn_cls: 0.023  loss_rpn_loc: 0.076  time: 0.5512  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:49:50 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.639  loss_cls: 0.199  loss_box_reg: 0.348  loss_rpn_cls: 0.020  loss_rpn_loc: 0.056  time: 0.5512  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:50:01 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.661  loss_cls: 0.180  loss_box_reg: 0.370  loss_rpn_cls: 0.021  loss_rpn_loc: 0.064  time: 0.5513  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:50:12 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.770  loss_cls: 0.208  loss_box_reg: 0.397  loss_rpn_cls: 0.020  loss_rpn_loc: 0.071  time: 0.5513  data_time: 0.0071  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 14:50:56 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 14:50:56 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 14:50:56 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 14:50:56 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.678  loss_cls: 0.220  loss_box_reg: 0.379  loss_rpn_cls: 0.023  loss_rpn_loc: 0.081  time: 0.5512  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 14:50:59 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:21 (0.5515 s / it)
[32m[04/15 14:50:59 d2.engine.hooks]: [0mTotal training time: 0:19:53 (0:01:32 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 14:51:11 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 14:51:11 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 14:51:12 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 14:51:14 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1233 s / img. ETA=0:02:36
[32m[04/15 14:51:19 d2.evaluation.evaluator]: [0mInference done 53/1257. 0.1194 s / img. ETA=0:02:26
[32m[04/15 14:51:25 d2.evaluation.evaluator]: [0mInference done 87/1257. 0.1212 s / img. ETA=0:02:39
[32m[04/15 14:51:30 d2.evaluation.evaluator]: [0mInference done 119/1257. 0.1213 s / img. ETA=0:02:42
[32m[04/15 14:51:35 d2.evaluation.evaluator]: [0mInference done 146/1257. 0.1210 s / img. ETA=0:02:47
[32m[04/15 14:51:40 d2.evaluation.evaluator]: [0mInference done 184/1257. 0.1211 s / img. ETA=0:02:38
[32m[04/15 14:51:45 d2.evaluation.evaluator]: [0mInference done 223/1257. 0.1213 s / img. ETA=0:02:28
[32m[04/15 14:51:50 d2.evaluation.evaluator]: [0mInference done 263/1257. 0.1214 s / img. ETA=0:02:20
[32m[04/15 14:51:55 d2.evaluation.evaluator]: [0mInference done 303/1257. 0.1215 s / img. ETA=0:02:12
[32m[04/15 14:52:00 d2.evaluation.evaluator]: [0mInference done 342/1257. 0.1215 s / img. ETA=0:02:06
[32m[04/15 14:52:05 d2.evaluation.evaluator]: [0mInference done 382/1257. 0.1216 s / img. ETA=0:01:59
[32m[04/15 14:52:10 d2.evaluation.evaluator]: [0mInference done 418/1257. 0.1216 s / img. ETA=0:01:55
[32m[04/15 14:52:16 d2.evaluation.evaluator]: [0mInference done 448/1257. 0.1215 s / img. ETA=0:01:53
[32m[04/15 14:52:21 d2.evaluation.evaluator]: [0mInference done 476/1257. 0.1214 s / img. ETA=0:01:51
[32m[04/15 14:52:26 d2.evaluation.evaluator]: [0mInference done 509/1257. 0.1215 s / img. ETA=0:01:47
[32m[04/15 14:52:31 d2.evaluation.evaluator]: [0mInference done 541/1257. 0.1215 s / img. ETA=0:01:43
[32m[04/15 14:52:36 d2.evaluation.evaluator]: [0mInference done 569/1257. 0.1216 s / img. ETA=0:01:40
[32m[04/15 14:52:41 d2.evaluation.evaluator]: [0mInference done 598/1257. 0.1217 s / img. ETA=0:01:37
[32m[04/15 14:52:46 d2.evaluation.evaluator]: [0mInference done 628/1257. 0.1217 s / img. ETA=0:01:33
[32m[04/15 14:52:51 d2.evaluation.evaluator]: [0mInference done 657/1257. 0.1216 s / img. ETA=0:01:29
[32m[04/15 14:52:56 d2.evaluation.evaluator]: [0mInference done 685/1257. 0.1216 s / img. ETA=0:01:26
[32m[04/15 14:53:01 d2.evaluation.evaluator]: [0mInference done 718/1257. 0.1216 s / img. ETA=0:01:21
[32m[04/15 14:53:07 d2.evaluation.evaluator]: [0mInference done 749/1257. 0.1217 s / img. ETA=0:01:17
[32m[04/15 14:53:12 d2.evaluation.evaluator]: [0mInference done 788/1257. 0.1218 s / img. ETA=0:01:10
[32m[04/15 14:53:17 d2.evaluation.evaluator]: [0mInference done 828/1257. 0.1218 s / img. ETA=0:01:04
[32m[04/15 14:53:22 d2.evaluation.evaluator]: [0mInference done 868/1257. 0.1218 s / img. ETA=0:00:57
[32m[04/15 14:53:27 d2.evaluation.evaluator]: [0mInference done 895/1257. 0.1219 s / img. ETA=0:00:54
[32m[04/15 14:53:33 d2.evaluation.evaluator]: [0mInference done 915/1257. 0.1219 s / img. ETA=0:00:52
[32m[04/15 14:53:38 d2.evaluation.evaluator]: [0mInference done 937/1257. 0.1219 s / img. ETA=0:00:49
[32m[04/15 14:53:43 d2.evaluation.evaluator]: [0mInference done 974/1257. 0.1223 s / img. ETA=0:00:43
[32m[04/15 14:53:48 d2.evaluation.evaluator]: [0mInference done 1011/1257. 0.1223 s / img. ETA=0:00:37
[32m[04/15 14:53:53 d2.evaluation.evaluator]: [0mInference done 1050/1257. 0.1224 s / img. ETA=0:00:31
[32m[04/15 14:53:58 d2.evaluation.evaluator]: [0mInference done 1087/1257. 0.1224 s / img. ETA=0:00:25
[32m[04/15 14:54:03 d2.evaluation.evaluator]: [0mInference done 1119/1257. 0.1224 s / img. ETA=0:00:21
[32m[04/15 14:54:09 d2.evaluation.evaluator]: [0mInference done 1157/1257. 0.1224 s / img. ETA=0:00:15
[32m[04/15 14:54:14 d2.evaluation.evaluator]: [0mInference done 1189/1257. 0.1224 s / img. ETA=0:00:10
[32m[04/15 14:54:19 d2.evaluation.evaluator]: [0mInference done 1225/1257. 0.1224 s / img. ETA=0:00:04
[32m[04/15 14:54:23 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:09.669980 (0.151494 s / img per device, on 1 devices)
[32m[04/15 14:54:23 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:33 (0.122477 s / img per device, on 1 devices)
[32m[04/15 14:54:24 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 14:54:24 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 14:54:24 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.03s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=4.81s).
Accumulating evaluation results...
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.193
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.391
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.166
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.120
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.236
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.406
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.112
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.255
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.166
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.304
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.443
[32m[04/15 14:54:31 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.328 | 39.092 | 16.603 | 12.023 | 23.612 | 40.581 |
[32m[04/15 14:54:31 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 27.150 | bicycle       | 7.227 | car            | 41.464 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 1.471 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  24  *  2000  iterations ============
4 channel input
[32m[04/15 14:54:32 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 14:54:33 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.28 seconds.
[5m[31mWARNING[0m [32m[04/15 14:54:33 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 14:54:33 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 14:54:34 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 14:54:34 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 14:54:34 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 14:54:36 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 14:54:48 d2.utils.events]: [0m eta: 0:16:44  iter: 19  total_loss: 0.601  loss_cls: 0.200  loss_box_reg: 0.320  loss_rpn_cls: 0.020  loss_rpn_loc: 0.066  time: 0.5169  data_time: 0.0515  lr: 0.000200  max_mem: 3070M
[32m[04/15 14:54:58 d2.utils.events]: [0m eta: 0:17:11  iter: 39  total_loss: 0.695  loss_cls: 0.204  loss_box_reg: 0.377  loss_rpn_cls: 0.033  loss_rpn_loc: 0.062  time: 0.5257  data_time: 0.0061  lr: 0.000400  max_mem: 3070M
[32m[04/15 14:55:09 d2.utils.events]: [0m eta: 0:17:01  iter: 59  total_loss: 0.627  loss_cls: 0.190  loss_box_reg: 0.354  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5267  data_time: 0.0083  lr: 0.000599  max_mem: 3070M
[32m[04/15 14:55:20 d2.utils.events]: [0m eta: 0:17:08  iter: 79  total_loss: 0.620  loss_cls: 0.192  loss_box_reg: 0.316  loss_rpn_cls: 0.021  loss_rpn_loc: 0.047  time: 0.5322  data_time: 0.0067  lr: 0.000799  max_mem: 3070M
[32m[04/15 14:55:31 d2.utils.events]: [0m eta: 0:17:09  iter: 99  total_loss: 0.692  loss_cls: 0.224  loss_box_reg: 0.386  loss_rpn_cls: 0.020  loss_rpn_loc: 0.056  time: 0.5354  data_time: 0.0074  lr: 0.000999  max_mem: 3070M
[32m[04/15 14:55:42 d2.utils.events]: [0m eta: 0:17:00  iter: 119  total_loss: 0.576  loss_cls: 0.184  loss_box_reg: 0.314  loss_rpn_cls: 0.020  loss_rpn_loc: 0.053  time: 0.5367  data_time: 0.0063  lr: 0.001199  max_mem: 3070M
[32m[04/15 14:55:54 d2.utils.events]: [0m eta: 0:16:54  iter: 139  total_loss: 0.611  loss_cls: 0.202  loss_box_reg: 0.328  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5388  data_time: 0.0075  lr: 0.001399  max_mem: 3070M
[32m[04/15 14:56:05 d2.utils.events]: [0m eta: 0:16:43  iter: 159  total_loss: 0.632  loss_cls: 0.180  loss_box_reg: 0.341  loss_rpn_cls: 0.019  loss_rpn_loc: 0.055  time: 0.5390  data_time: 0.0069  lr: 0.001598  max_mem: 3070M
[32m[04/15 14:56:16 d2.utils.events]: [0m eta: 0:16:36  iter: 179  total_loss: 0.527  loss_cls: 0.184  loss_box_reg: 0.315  loss_rpn_cls: 0.019  loss_rpn_loc: 0.048  time: 0.5428  data_time: 0.0328  lr: 0.001798  max_mem: 3070M
[32m[04/15 14:56:27 d2.utils.events]: [0m eta: 0:16:25  iter: 199  total_loss: 0.618  loss_cls: 0.211  loss_box_reg: 0.345  loss_rpn_cls: 0.020  loss_rpn_loc: 0.052  time: 0.5428  data_time: 0.0060  lr: 0.001998  max_mem: 3070M
[32m[04/15 14:56:39 d2.utils.events]: [0m eta: 0:16:19  iter: 219  total_loss: 0.564  loss_cls: 0.179  loss_box_reg: 0.324  loss_rpn_cls: 0.012  loss_rpn_loc: 0.046  time: 0.5450  data_time: 0.0070  lr: 0.002198  max_mem: 3070M
[32m[04/15 14:56:50 d2.utils.events]: [0m eta: 0:16:09  iter: 239  total_loss: 0.658  loss_cls: 0.199  loss_box_reg: 0.354  loss_rpn_cls: 0.018  loss_rpn_loc: 0.067  time: 0.5449  data_time: 0.0096  lr: 0.002398  max_mem: 3070M
[32m[04/15 14:57:01 d2.utils.events]: [0m eta: 0:16:00  iter: 259  total_loss: 0.547  loss_cls: 0.196  loss_box_reg: 0.289  loss_rpn_cls: 0.011  loss_rpn_loc: 0.043  time: 0.5453  data_time: 0.0063  lr: 0.002597  max_mem: 3070M
[32m[04/15 14:57:11 d2.utils.events]: [0m eta: 0:15:48  iter: 279  total_loss: 0.572  loss_cls: 0.187  loss_box_reg: 0.334  loss_rpn_cls: 0.013  loss_rpn_loc: 0.045  time: 0.5443  data_time: 0.0083  lr: 0.002797  max_mem: 3070M
[32m[04/15 14:57:22 d2.utils.events]: [0m eta: 0:15:36  iter: 299  total_loss: 0.523  loss_cls: 0.171  loss_box_reg: 0.295  loss_rpn_cls: 0.015  loss_rpn_loc: 0.044  time: 0.5436  data_time: 0.0078  lr: 0.002997  max_mem: 3070M
[32m[04/15 14:57:33 d2.utils.events]: [0m eta: 0:15:25  iter: 319  total_loss: 0.472  loss_cls: 0.158  loss_box_reg: 0.244  loss_rpn_cls: 0.011  loss_rpn_loc: 0.034  time: 0.5441  data_time: 0.0066  lr: 0.003197  max_mem: 3070M
[32m[04/15 14:57:44 d2.utils.events]: [0m eta: 0:15:13  iter: 339  total_loss: 0.592  loss_cls: 0.192  loss_box_reg: 0.351  loss_rpn_cls: 0.013  loss_rpn_loc: 0.061  time: 0.5430  data_time: 0.0073  lr: 0.003397  max_mem: 3070M
[32m[04/15 14:57:55 d2.utils.events]: [0m eta: 0:15:00  iter: 359  total_loss: 0.684  loss_cls: 0.226  loss_box_reg: 0.347  loss_rpn_cls: 0.025  loss_rpn_loc: 0.063  time: 0.5425  data_time: 0.0096  lr: 0.003596  max_mem: 3070M
[32m[04/15 14:58:06 d2.utils.events]: [0m eta: 0:14:50  iter: 379  total_loss: 0.536  loss_cls: 0.183  loss_box_reg: 0.293  loss_rpn_cls: 0.015  loss_rpn_loc: 0.040  time: 0.5436  data_time: 0.0074  lr: 0.003796  max_mem: 3070M
[32m[04/15 14:58:18 d2.utils.events]: [0m eta: 0:14:40  iter: 399  total_loss: 0.691  loss_cls: 0.214  loss_box_reg: 0.400  loss_rpn_cls: 0.015  loss_rpn_loc: 0.060  time: 0.5449  data_time: 0.0130  lr: 0.003996  max_mem: 3070M
[32m[04/15 14:58:29 d2.utils.events]: [0m eta: 0:14:30  iter: 419  total_loss: 0.632  loss_cls: 0.188  loss_box_reg: 0.329  loss_rpn_cls: 0.013  loss_rpn_loc: 0.066  time: 0.5451  data_time: 0.0072  lr: 0.004196  max_mem: 3070M
[32m[04/15 14:58:40 d2.utils.events]: [0m eta: 0:14:19  iter: 439  total_loss: 0.577  loss_cls: 0.183  loss_box_reg: 0.303  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5456  data_time: 0.0070  lr: 0.004396  max_mem: 3070M
[32m[04/15 14:58:51 d2.utils.events]: [0m eta: 0:14:08  iter: 459  total_loss: 0.460  loss_cls: 0.141  loss_box_reg: 0.260  loss_rpn_cls: 0.012  loss_rpn_loc: 0.038  time: 0.5456  data_time: 0.0059  lr: 0.004595  max_mem: 3070M
[32m[04/15 14:59:02 d2.utils.events]: [0m eta: 0:13:56  iter: 479  total_loss: 0.596  loss_cls: 0.191  loss_box_reg: 0.331  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5457  data_time: 0.0065  lr: 0.004795  max_mem: 3070M
[32m[04/15 14:59:14 d2.utils.events]: [0m eta: 0:13:46  iter: 499  total_loss: 0.563  loss_cls: 0.182  loss_box_reg: 0.306  loss_rpn_cls: 0.013  loss_rpn_loc: 0.039  time: 0.5468  data_time: 0.0078  lr: 0.004995  max_mem: 3070M
[32m[04/15 14:59:25 d2.utils.events]: [0m eta: 0:13:36  iter: 519  total_loss: 0.560  loss_cls: 0.164  loss_box_reg: 0.314  loss_rpn_cls: 0.012  loss_rpn_loc: 0.045  time: 0.5471  data_time: 0.0090  lr: 0.005195  max_mem: 3070M
[32m[04/15 14:59:37 d2.utils.events]: [0m eta: 0:13:26  iter: 539  total_loss: 0.584  loss_cls: 0.189  loss_box_reg: 0.341  loss_rpn_cls: 0.015  loss_rpn_loc: 0.058  time: 0.5480  data_time: 0.0064  lr: 0.005395  max_mem: 3070M
[32m[04/15 14:59:48 d2.utils.events]: [0m eta: 0:13:14  iter: 559  total_loss: 0.538  loss_cls: 0.185  loss_box_reg: 0.306  loss_rpn_cls: 0.016  loss_rpn_loc: 0.049  time: 0.5477  data_time: 0.0064  lr: 0.005594  max_mem: 3070M
[32m[04/15 14:59:59 d2.utils.events]: [0m eta: 0:13:03  iter: 579  total_loss: 0.650  loss_cls: 0.191  loss_box_reg: 0.369  loss_rpn_cls: 0.018  loss_rpn_loc: 0.059  time: 0.5481  data_time: 0.0082  lr: 0.005794  max_mem: 3070M
[32m[04/15 15:00:10 d2.utils.events]: [0m eta: 0:12:53  iter: 599  total_loss: 0.583  loss_cls: 0.187  loss_box_reg: 0.312  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5486  data_time: 0.0070  lr: 0.005994  max_mem: 3070M
[32m[04/15 15:00:21 d2.utils.events]: [0m eta: 0:12:42  iter: 619  total_loss: 0.643  loss_cls: 0.206  loss_box_reg: 0.348  loss_rpn_cls: 0.016  loss_rpn_loc: 0.046  time: 0.5487  data_time: 0.0066  lr: 0.006194  max_mem: 3070M
[32m[04/15 15:00:33 d2.utils.events]: [0m eta: 0:12:32  iter: 639  total_loss: 0.722  loss_cls: 0.213  loss_box_reg: 0.407  loss_rpn_cls: 0.013  loss_rpn_loc: 0.074  time: 0.5497  data_time: 0.0074  lr: 0.006394  max_mem: 3070M
[32m[04/15 15:00:45 d2.utils.events]: [0m eta: 0:12:22  iter: 659  total_loss: 0.611  loss_cls: 0.189  loss_box_reg: 0.343  loss_rpn_cls: 0.008  loss_rpn_loc: 0.050  time: 0.5498  data_time: 0.0073  lr: 0.006593  max_mem: 3070M
[32m[04/15 15:00:56 d2.utils.events]: [0m eta: 0:12:11  iter: 679  total_loss: 0.651  loss_cls: 0.210  loss_box_reg: 0.347  loss_rpn_cls: 0.019  loss_rpn_loc: 0.064  time: 0.5500  data_time: 0.0059  lr: 0.006793  max_mem: 3070M
[32m[04/15 15:01:07 d2.utils.events]: [0m eta: 0:12:01  iter: 699  total_loss: 0.527  loss_cls: 0.172  loss_box_reg: 0.321  loss_rpn_cls: 0.013  loss_rpn_loc: 0.048  time: 0.5502  data_time: 0.0078  lr: 0.006993  max_mem: 3070M
[32m[04/15 15:01:18 d2.utils.events]: [0m eta: 0:11:49  iter: 719  total_loss: 0.554  loss_cls: 0.162  loss_box_reg: 0.332  loss_rpn_cls: 0.013  loss_rpn_loc: 0.055  time: 0.5499  data_time: 0.0063  lr: 0.007193  max_mem: 3070M
[32m[04/15 15:01:30 d2.utils.events]: [0m eta: 0:11:39  iter: 739  total_loss: 0.594  loss_cls: 0.197  loss_box_reg: 0.338  loss_rpn_cls: 0.016  loss_rpn_loc: 0.054  time: 0.5504  data_time: 0.0058  lr: 0.007393  max_mem: 3070M
[32m[04/15 15:01:41 d2.utils.events]: [0m eta: 0:11:28  iter: 759  total_loss: 0.632  loss_cls: 0.202  loss_box_reg: 0.348  loss_rpn_cls: 0.020  loss_rpn_loc: 0.074  time: 0.5506  data_time: 0.0076  lr: 0.007592  max_mem: 3070M
[32m[04/15 15:01:53 d2.utils.events]: [0m eta: 0:11:16  iter: 779  total_loss: 0.602  loss_cls: 0.187  loss_box_reg: 0.356  loss_rpn_cls: 0.014  loss_rpn_loc: 0.051  time: 0.5504  data_time: 0.0074  lr: 0.007792  max_mem: 3070M
[32m[04/15 15:02:04 d2.utils.events]: [0m eta: 0:11:04  iter: 799  total_loss: 0.588  loss_cls: 0.183  loss_box_reg: 0.352  loss_rpn_cls: 0.013  loss_rpn_loc: 0.034  time: 0.5503  data_time: 0.0067  lr: 0.007992  max_mem: 3070M
[32m[04/15 15:02:16 d2.utils.events]: [0m eta: 0:10:54  iter: 819  total_loss: 0.612  loss_cls: 0.213  loss_box_reg: 0.329  loss_rpn_cls: 0.018  loss_rpn_loc: 0.054  time: 0.5506  data_time: 0.0060  lr: 0.008192  max_mem: 3070M
[32m[04/15 15:02:27 d2.utils.events]: [0m eta: 0:10:43  iter: 839  total_loss: 0.735  loss_cls: 0.218  loss_box_reg: 0.417  loss_rpn_cls: 0.023  loss_rpn_loc: 0.063  time: 0.5508  data_time: 0.0070  lr: 0.008392  max_mem: 3070M
[32m[04/15 15:02:38 d2.utils.events]: [0m eta: 0:10:32  iter: 859  total_loss: 0.676  loss_cls: 0.215  loss_box_reg: 0.389  loss_rpn_cls: 0.018  loss_rpn_loc: 0.058  time: 0.5507  data_time: 0.0068  lr: 0.008591  max_mem: 3070M
[32m[04/15 15:02:49 d2.utils.events]: [0m eta: 0:10:21  iter: 879  total_loss: 0.612  loss_cls: 0.227  loss_box_reg: 0.359  loss_rpn_cls: 0.013  loss_rpn_loc: 0.047  time: 0.5505  data_time: 0.0069  lr: 0.008791  max_mem: 3070M
[32m[04/15 15:03:01 d2.utils.events]: [0m eta: 0:10:10  iter: 899  total_loss: 0.686  loss_cls: 0.224  loss_box_reg: 0.374  loss_rpn_cls: 0.009  loss_rpn_loc: 0.057  time: 0.5506  data_time: 0.0069  lr: 0.008991  max_mem: 3070M
[32m[04/15 15:03:12 d2.utils.events]: [0m eta: 0:10:00  iter: 919  total_loss: 0.720  loss_cls: 0.219  loss_box_reg: 0.401  loss_rpn_cls: 0.023  loss_rpn_loc: 0.059  time: 0.5505  data_time: 0.0096  lr: 0.009191  max_mem: 3070M
[32m[04/15 15:03:23 d2.utils.events]: [0m eta: 0:09:50  iter: 939  total_loss: 0.581  loss_cls: 0.176  loss_box_reg: 0.338  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5508  data_time: 0.0074  lr: 0.009391  max_mem: 3070M
[32m[04/15 15:03:34 d2.utils.events]: [0m eta: 0:09:39  iter: 959  total_loss: 0.597  loss_cls: 0.190  loss_box_reg: 0.351  loss_rpn_cls: 0.018  loss_rpn_loc: 0.058  time: 0.5511  data_time: 0.0076  lr: 0.009590  max_mem: 3070M
[32m[04/15 15:03:47 d2.utils.events]: [0m eta: 0:09:28  iter: 979  total_loss: 0.654  loss_cls: 0.190  loss_box_reg: 0.336  loss_rpn_cls: 0.018  loss_rpn_loc: 0.043  time: 0.5511  data_time: 0.0081  lr: 0.009790  max_mem: 3070M
[32m[04/15 15:03:58 d2.utils.events]: [0m eta: 0:09:16  iter: 999  total_loss: 0.748  loss_cls: 0.220  loss_box_reg: 0.396  loss_rpn_cls: 0.017  loss_rpn_loc: 0.067  time: 0.5512  data_time: 0.0071  lr: 0.009990  max_mem: 3070M
[32m[04/15 15:04:09 d2.utils.events]: [0m eta: 0:09:06  iter: 1019  total_loss: 0.741  loss_cls: 0.225  loss_box_reg: 0.407  loss_rpn_cls: 0.020  loss_rpn_loc: 0.071  time: 0.5515  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:04:21 d2.utils.events]: [0m eta: 0:08:55  iter: 1039  total_loss: 0.587  loss_cls: 0.187  loss_box_reg: 0.330  loss_rpn_cls: 0.016  loss_rpn_loc: 0.067  time: 0.5514  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:04:32 d2.utils.events]: [0m eta: 0:08:44  iter: 1059  total_loss: 0.741  loss_cls: 0.220  loss_box_reg: 0.416  loss_rpn_cls: 0.022  loss_rpn_loc: 0.069  time: 0.5515  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:04:43 d2.utils.events]: [0m eta: 0:08:33  iter: 1079  total_loss: 0.673  loss_cls: 0.224  loss_box_reg: 0.320  loss_rpn_cls: 0.029  loss_rpn_loc: 0.056  time: 0.5514  data_time: 0.0060  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:04:55 d2.utils.events]: [0m eta: 0:08:22  iter: 1099  total_loss: 0.658  loss_cls: 0.213  loss_box_reg: 0.372  loss_rpn_cls: 0.023  loss_rpn_loc: 0.050  time: 0.5515  data_time: 0.0135  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:05:06 d2.utils.events]: [0m eta: 0:08:11  iter: 1119  total_loss: 0.548  loss_cls: 0.175  loss_box_reg: 0.297  loss_rpn_cls: 0.018  loss_rpn_loc: 0.045  time: 0.5516  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:05:17 d2.utils.events]: [0m eta: 0:08:00  iter: 1139  total_loss: 0.649  loss_cls: 0.208  loss_box_reg: 0.350  loss_rpn_cls: 0.017  loss_rpn_loc: 0.055  time: 0.5517  data_time: 0.0059  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:05:29 d2.utils.events]: [0m eta: 0:07:49  iter: 1159  total_loss: 0.692  loss_cls: 0.223  loss_box_reg: 0.383  loss_rpn_cls: 0.016  loss_rpn_loc: 0.072  time: 0.5519  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:05:40 d2.utils.events]: [0m eta: 0:07:38  iter: 1179  total_loss: 0.546  loss_cls: 0.165  loss_box_reg: 0.286  loss_rpn_cls: 0.020  loss_rpn_loc: 0.049  time: 0.5519  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:05:52 d2.utils.events]: [0m eta: 0:07:27  iter: 1199  total_loss: 0.494  loss_cls: 0.162  loss_box_reg: 0.288  loss_rpn_cls: 0.020  loss_rpn_loc: 0.046  time: 0.5518  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:06:03 d2.utils.events]: [0m eta: 0:07:15  iter: 1219  total_loss: 0.582  loss_cls: 0.184  loss_box_reg: 0.339  loss_rpn_cls: 0.013  loss_rpn_loc: 0.050  time: 0.5517  data_time: 0.0058  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:06:14 d2.utils.events]: [0m eta: 0:07:04  iter: 1239  total_loss: 0.577  loss_cls: 0.176  loss_box_reg: 0.331  loss_rpn_cls: 0.014  loss_rpn_loc: 0.064  time: 0.5518  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:06:26 d2.utils.events]: [0m eta: 0:06:53  iter: 1259  total_loss: 0.677  loss_cls: 0.212  loss_box_reg: 0.397  loss_rpn_cls: 0.014  loss_rpn_loc: 0.059  time: 0.5519  data_time: 0.0087  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:06:37 d2.utils.events]: [0m eta: 0:06:42  iter: 1279  total_loss: 0.740  loss_cls: 0.227  loss_box_reg: 0.389  loss_rpn_cls: 0.019  loss_rpn_loc: 0.069  time: 0.5522  data_time: 0.0265  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:06:48 d2.utils.events]: [0m eta: 0:06:31  iter: 1299  total_loss: 0.648  loss_cls: 0.195  loss_box_reg: 0.325  loss_rpn_cls: 0.012  loss_rpn_loc: 0.056  time: 0.5520  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:07:00 d2.utils.events]: [0m eta: 0:06:20  iter: 1319  total_loss: 0.776  loss_cls: 0.254  loss_box_reg: 0.438  loss_rpn_cls: 0.022  loss_rpn_loc: 0.064  time: 0.5521  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:07:11 d2.utils.events]: [0m eta: 0:06:09  iter: 1339  total_loss: 0.709  loss_cls: 0.245  loss_box_reg: 0.369  loss_rpn_cls: 0.023  loss_rpn_loc: 0.063  time: 0.5521  data_time: 0.0057  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:07:23 d2.utils.events]: [0m eta: 0:05:59  iter: 1359  total_loss: 0.700  loss_cls: 0.218  loss_box_reg: 0.376  loss_rpn_cls: 0.022  loss_rpn_loc: 0.055  time: 0.5525  data_time: 0.0101  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:07:34 d2.utils.events]: [0m eta: 0:05:47  iter: 1379  total_loss: 0.697  loss_cls: 0.209  loss_box_reg: 0.354  loss_rpn_cls: 0.027  loss_rpn_loc: 0.057  time: 0.5526  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:07:45 d2.utils.events]: [0m eta: 0:05:36  iter: 1399  total_loss: 0.590  loss_cls: 0.191  loss_box_reg: 0.284  loss_rpn_cls: 0.020  loss_rpn_loc: 0.070  time: 0.5527  data_time: 0.0065  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:07:57 d2.utils.events]: [0m eta: 0:05:25  iter: 1419  total_loss: 0.669  loss_cls: 0.205  loss_box_reg: 0.363  loss_rpn_cls: 0.016  loss_rpn_loc: 0.055  time: 0.5528  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:08:08 d2.utils.events]: [0m eta: 0:05:14  iter: 1439  total_loss: 0.685  loss_cls: 0.231  loss_box_reg: 0.344  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5530  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:08:20 d2.utils.events]: [0m eta: 0:05:03  iter: 1459  total_loss: 0.644  loss_cls: 0.203  loss_box_reg: 0.354  loss_rpn_cls: 0.019  loss_rpn_loc: 0.052  time: 0.5534  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:08:31 d2.utils.events]: [0m eta: 0:04:52  iter: 1479  total_loss: 0.604  loss_cls: 0.199  loss_box_reg: 0.344  loss_rpn_cls: 0.015  loss_rpn_loc: 0.046  time: 0.5535  data_time: 0.0094  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:08:43 d2.utils.events]: [0m eta: 0:04:41  iter: 1499  total_loss: 0.626  loss_cls: 0.186  loss_box_reg: 0.374  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5538  data_time: 0.0107  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:08:55 d2.utils.events]: [0m eta: 0:04:30  iter: 1519  total_loss: 0.674  loss_cls: 0.197  loss_box_reg: 0.351  loss_rpn_cls: 0.024  loss_rpn_loc: 0.062  time: 0.5538  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:09:06 d2.utils.events]: [0m eta: 0:04:18  iter: 1539  total_loss: 0.657  loss_cls: 0.212  loss_box_reg: 0.353  loss_rpn_cls: 0.018  loss_rpn_loc: 0.060  time: 0.5539  data_time: 0.0372  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:09:32 d2.utils.events]: [0m eta: 0:04:07  iter: 1559  total_loss: 0.698  loss_cls: 0.218  loss_box_reg: 0.363  loss_rpn_cls: 0.024  loss_rpn_loc: 0.060  time: 0.5573  data_time: 0.3434  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:09:43 d2.utils.events]: [0m eta: 0:03:56  iter: 1579  total_loss: 0.583  loss_cls: 0.197  loss_box_reg: 0.342  loss_rpn_cls: 0.016  loss_rpn_loc: 0.046  time: 0.5573  data_time: 0.0148  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:09:54 d2.utils.events]: [0m eta: 0:03:44  iter: 1599  total_loss: 0.675  loss_cls: 0.247  loss_box_reg: 0.364  loss_rpn_cls: 0.018  loss_rpn_loc: 0.061  time: 0.5568  data_time: 0.0171  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:10:13 d2.utils.events]: [0m eta: 0:03:33  iter: 1619  total_loss: 0.669  loss_cls: 0.209  loss_box_reg: 0.351  loss_rpn_cls: 0.019  loss_rpn_loc: 0.058  time: 0.5567  data_time: 0.0213  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:10:24 d2.utils.events]: [0m eta: 0:03:21  iter: 1639  total_loss: 0.595  loss_cls: 0.175  loss_box_reg: 0.338  loss_rpn_cls: 0.023  loss_rpn_loc: 0.066  time: 0.5565  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:10:35 d2.utils.events]: [0m eta: 0:03:10  iter: 1659  total_loss: 0.628  loss_cls: 0.222  loss_box_reg: 0.331  loss_rpn_cls: 0.015  loss_rpn_loc: 0.049  time: 0.5564  data_time: 0.0103  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:10:47 d2.utils.events]: [0m eta: 0:02:59  iter: 1679  total_loss: 0.623  loss_cls: 0.197  loss_box_reg: 0.343  loss_rpn_cls: 0.024  loss_rpn_loc: 0.066  time: 0.5564  data_time: 0.0409  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:11:00 d2.utils.events]: [0m eta: 0:02:48  iter: 1699  total_loss: 0.821  loss_cls: 0.283  loss_box_reg: 0.446  loss_rpn_cls: 0.031  loss_rpn_loc: 0.084  time: 0.5573  data_time: 0.1405  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:11:15 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.455  loss_cls: 0.166  loss_box_reg: 0.265  loss_rpn_cls: 0.015  loss_rpn_loc: 0.044  time: 0.5569  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:11:31 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.661  loss_cls: 0.193  loss_box_reg: 0.342  loss_rpn_cls: 0.023  loss_rpn_loc: 0.068  time: 0.5567  data_time: 0.0205  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:11:54 d2.utils.events]: [0m eta: 0:02:14  iter: 1759  total_loss: 0.704  loss_cls: 0.208  loss_box_reg: 0.388  loss_rpn_cls: 0.022  loss_rpn_loc: 0.064  time: 0.5570  data_time: 0.1284  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:12:06 d2.utils.events]: [0m eta: 0:02:03  iter: 1779  total_loss: 0.538  loss_cls: 0.167  loss_box_reg: 0.287  loss_rpn_cls: 0.013  loss_rpn_loc: 0.061  time: 0.5568  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:12:17 d2.utils.events]: [0m eta: 0:01:52  iter: 1799  total_loss: 0.628  loss_cls: 0.198  loss_box_reg: 0.341  loss_rpn_cls: 0.018  loss_rpn_loc: 0.068  time: 0.5569  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:12:29 d2.utils.events]: [0m eta: 0:01:41  iter: 1819  total_loss: 0.457  loss_cls: 0.145  loss_box_reg: 0.252  loss_rpn_cls: 0.013  loss_rpn_loc: 0.056  time: 0.5569  data_time: 0.0350  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:13:25 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.459  loss_cls: 0.175  loss_box_reg: 0.249  loss_rpn_cls: 0.017  loss_rpn_loc: 0.035  time: 0.5566  data_time: 0.0053  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:13:35 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.699  loss_cls: 0.221  loss_box_reg: 0.396  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5561  data_time: 0.0049  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:14:02 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.700  loss_cls: 0.241  loss_box_reg: 0.375  loss_rpn_cls: 0.015  loss_rpn_loc: 0.062  time: 0.5559  data_time: 0.0067  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:14:13 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.821  loss_cls: 0.253  loss_box_reg: 0.411  loss_rpn_cls: 0.026  loss_rpn_loc: 0.078  time: 0.5557  data_time: 0.0127  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:14:25 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.578  loss_cls: 0.173  loss_box_reg: 0.318  loss_rpn_cls: 0.021  loss_rpn_loc: 0.046  time: 0.5559  data_time: 0.0864  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:14:37 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.789  loss_cls: 0.258  loss_box_reg: 0.413  loss_rpn_cls: 0.029  loss_rpn_loc: 0.078  time: 0.5561  data_time: 0.0397  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:15:04 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.699  loss_cls: 0.218  loss_box_reg: 0.382  loss_rpn_cls: 0.016  loss_rpn_loc: 0.068  time: 0.5557  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:15:15 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.701  loss_cls: 0.207  loss_box_reg: 0.385  loss_rpn_cls: 0.024  loss_rpn_loc: 0.062  time: 0.5554  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 15:15:58 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 15:15:58 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 15:15:58 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 15:15:58 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.631  loss_cls: 0.189  loss_box_reg: 0.353  loss_rpn_cls: 0.022  loss_rpn_loc: 0.049  time: 0.5550  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:16:01 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:28 (0.5553 s / it)
[32m[04/15 15:16:01 d2.engine.hooks]: [0mTotal training time: 0:21:22 (0:02:53 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 15:17:29 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 15:17:29 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 15:17:29 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 15:17:31 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1200 s / img. ETA=0:02:34
[32m[04/15 15:17:36 d2.evaluation.evaluator]: [0mInference done 52/1257. 0.1208 s / img. ETA=0:02:29
[32m[04/15 15:17:41 d2.evaluation.evaluator]: [0mInference done 92/1257. 0.1212 s / img. ETA=0:02:25
[32m[04/15 15:17:46 d2.evaluation.evaluator]: [0mInference done 133/1257. 0.1211 s / img. ETA=0:02:19
[32m[04/15 15:17:51 d2.evaluation.evaluator]: [0mInference done 174/1257. 0.1207 s / img. ETA=0:02:14
[32m[04/15 15:17:56 d2.evaluation.evaluator]: [0mInference done 215/1257. 0.1206 s / img. ETA=0:02:09
[32m[04/15 15:18:01 d2.evaluation.evaluator]: [0mInference done 256/1257. 0.1206 s / img. ETA=0:02:03
[32m[04/15 15:18:06 d2.evaluation.evaluator]: [0mInference done 297/1257. 0.1206 s / img. ETA=0:01:58
[32m[04/15 15:18:11 d2.evaluation.evaluator]: [0mInference done 338/1257. 0.1206 s / img. ETA=0:01:53
[32m[04/15 15:18:17 d2.evaluation.evaluator]: [0mInference done 379/1257. 0.1206 s / img. ETA=0:01:48
[32m[04/15 15:18:22 d2.evaluation.evaluator]: [0mInference done 420/1257. 0.1206 s / img. ETA=0:01:43
[32m[04/15 15:18:27 d2.evaluation.evaluator]: [0mInference done 461/1257. 0.1206 s / img. ETA=0:01:38
[32m[04/15 15:18:32 d2.evaluation.evaluator]: [0mInference done 502/1257. 0.1207 s / img. ETA=0:01:33
[32m[04/15 15:18:37 d2.evaluation.evaluator]: [0mInference done 542/1257. 0.1208 s / img. ETA=0:01:28
[32m[04/15 15:18:42 d2.evaluation.evaluator]: [0mInference done 583/1257. 0.1207 s / img. ETA=0:01:23
[32m[04/15 15:18:47 d2.evaluation.evaluator]: [0mInference done 624/1257. 0.1207 s / img. ETA=0:01:18
[32m[04/15 15:18:52 d2.evaluation.evaluator]: [0mInference done 665/1257. 0.1207 s / img. ETA=0:01:13
[32m[04/15 15:18:57 d2.evaluation.evaluator]: [0mInference done 706/1257. 0.1207 s / img. ETA=0:01:08
[32m[04/15 15:19:02 d2.evaluation.evaluator]: [0mInference done 747/1257. 0.1208 s / img. ETA=0:01:03
[32m[04/15 15:19:07 d2.evaluation.evaluator]: [0mInference done 788/1257. 0.1207 s / img. ETA=0:00:58
[32m[04/15 15:19:12 d2.evaluation.evaluator]: [0mInference done 828/1257. 0.1208 s / img. ETA=0:00:53
[32m[04/15 15:19:17 d2.evaluation.evaluator]: [0mInference done 868/1257. 0.1209 s / img. ETA=0:00:48
[32m[04/15 15:19:22 d2.evaluation.evaluator]: [0mInference done 908/1257. 0.1209 s / img. ETA=0:00:43
[32m[04/15 15:19:27 d2.evaluation.evaluator]: [0mInference done 949/1257. 0.1209 s / img. ETA=0:00:38
[32m[04/15 15:19:32 d2.evaluation.evaluator]: [0mInference done 990/1257. 0.1209 s / img. ETA=0:00:33
[32m[04/15 15:19:38 d2.evaluation.evaluator]: [0mInference done 1031/1257. 0.1209 s / img. ETA=0:00:28
[32m[04/15 15:19:43 d2.evaluation.evaluator]: [0mInference done 1072/1257. 0.1210 s / img. ETA=0:00:22
[32m[04/15 15:19:48 d2.evaluation.evaluator]: [0mInference done 1112/1257. 0.1210 s / img. ETA=0:00:18
[32m[04/15 15:19:53 d2.evaluation.evaluator]: [0mInference done 1153/1257. 0.1210 s / img. ETA=0:00:12
[32m[04/15 15:19:58 d2.evaluation.evaluator]: [0mInference done 1194/1257. 0.1210 s / img. ETA=0:00:07
[32m[04/15 15:20:03 d2.evaluation.evaluator]: [0mInference done 1235/1257. 0.1210 s / img. ETA=0:00:02
[32m[04/15 15:20:06 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:35.480191 (0.124185 s / img per device, on 1 devices)
[32m[04/15 15:20:06 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:31 (0.120947 s / img per device, on 1 devices)
[32m[04/15 15:20:06 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 15:20:06 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 15:20:06 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.63s).
Accumulating evaluation results...
DONE (t=0.63s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.181
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.374
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.104
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.222
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.151
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.430
[32m[04/15 15:20:13 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 18.074 | 37.440 | 15.385 | 10.417 | 22.722 | 39.345 |
[32m[04/15 15:20:13 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 26.278 | bicycle       | 6.027 | car            | 39.833 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.158 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  25  *  2000  iterations ============
4 channel input
[32m[04/15 15:20:14 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 15:20:16 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.67 seconds.
[5m[31mWARNING[0m [32m[04/15 15:20:16 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 15:20:16 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 15:20:17 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 15:20:17 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 15:20:17 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 15:20:17 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 15:20:30 d2.utils.events]: [0m eta: 0:18:14  iter: 19  total_loss: 0.562  loss_cls: 0.187  loss_box_reg: 0.313  loss_rpn_cls: 0.017  loss_rpn_loc: 0.042  time: 0.5527  data_time: 0.1033  lr: 0.000200  max_mem: 3070M
[32m[04/15 15:20:40 d2.utils.events]: [0m eta: 0:17:33  iter: 39  total_loss: 0.605  loss_cls: 0.202  loss_box_reg: 0.346  loss_rpn_cls: 0.020  loss_rpn_loc: 0.055  time: 0.5392  data_time: 0.0063  lr: 0.000400  max_mem: 3070M
[32m[04/15 15:20:51 d2.utils.events]: [0m eta: 0:17:32  iter: 59  total_loss: 0.536  loss_cls: 0.170  loss_box_reg: 0.283  loss_rpn_cls: 0.015  loss_rpn_loc: 0.038  time: 0.5414  data_time: 0.0078  lr: 0.000599  max_mem: 3070M
[32m[04/15 15:21:02 d2.utils.events]: [0m eta: 0:16:57  iter: 79  total_loss: 0.583  loss_cls: 0.176  loss_box_reg: 0.321  loss_rpn_cls: 0.017  loss_rpn_loc: 0.049  time: 0.5337  data_time: 0.0074  lr: 0.000799  max_mem: 3070M
[32m[04/15 15:21:12 d2.utils.events]: [0m eta: 0:16:45  iter: 99  total_loss: 0.580  loss_cls: 0.194  loss_box_reg: 0.322  loss_rpn_cls: 0.015  loss_rpn_loc: 0.054  time: 0.5309  data_time: 0.0076  lr: 0.000999  max_mem: 3070M
[32m[04/15 15:21:23 d2.utils.events]: [0m eta: 0:16:35  iter: 119  total_loss: 0.659  loss_cls: 0.195  loss_box_reg: 0.357  loss_rpn_cls: 0.015  loss_rpn_loc: 0.062  time: 0.5305  data_time: 0.0089  lr: 0.001199  max_mem: 3070M
[32m[04/15 15:21:36 d2.utils.events]: [0m eta: 0:16:26  iter: 139  total_loss: 0.615  loss_cls: 0.197  loss_box_reg: 0.338  loss_rpn_cls: 0.020  loss_rpn_loc: 0.049  time: 0.5302  data_time: 0.0080  lr: 0.001399  max_mem: 3070M
[32m[04/15 15:21:48 d2.utils.events]: [0m eta: 0:16:15  iter: 159  total_loss: 0.584  loss_cls: 0.189  loss_box_reg: 0.327  loss_rpn_cls: 0.023  loss_rpn_loc: 0.050  time: 0.5296  data_time: 0.0079  lr: 0.001598  max_mem: 3070M
[32m[04/15 15:21:59 d2.utils.events]: [0m eta: 0:16:06  iter: 179  total_loss: 0.616  loss_cls: 0.207  loss_box_reg: 0.355  loss_rpn_cls: 0.013  loss_rpn_loc: 0.059  time: 0.5305  data_time: 0.0081  lr: 0.001798  max_mem: 3070M
[32m[04/15 15:22:10 d2.utils.events]: [0m eta: 0:15:57  iter: 199  total_loss: 0.661  loss_cls: 0.205  loss_box_reg: 0.387  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5319  data_time: 0.0068  lr: 0.001998  max_mem: 3070M
[32m[04/15 15:22:21 d2.utils.events]: [0m eta: 0:15:53  iter: 219  total_loss: 0.595  loss_cls: 0.188  loss_box_reg: 0.344  loss_rpn_cls: 0.021  loss_rpn_loc: 0.054  time: 0.5320  data_time: 0.0072  lr: 0.002198  max_mem: 3070M
[32m[04/15 15:22:35 d2.utils.events]: [0m eta: 0:15:46  iter: 239  total_loss: 0.650  loss_cls: 0.208  loss_box_reg: 0.360  loss_rpn_cls: 0.017  loss_rpn_loc: 0.064  time: 0.5353  data_time: 0.0505  lr: 0.002398  max_mem: 3070M
[32m[04/15 15:22:46 d2.utils.events]: [0m eta: 0:15:36  iter: 259  total_loss: 0.520  loss_cls: 0.169  loss_box_reg: 0.283  loss_rpn_cls: 0.011  loss_rpn_loc: 0.050  time: 0.5352  data_time: 0.0082  lr: 0.002597  max_mem: 3070M
[32m[04/15 15:22:57 d2.utils.events]: [0m eta: 0:15:25  iter: 279  total_loss: 0.570  loss_cls: 0.168  loss_box_reg: 0.331  loss_rpn_cls: 0.015  loss_rpn_loc: 0.051  time: 0.5352  data_time: 0.0067  lr: 0.002797  max_mem: 3070M
[32m[04/15 15:23:09 d2.utils.events]: [0m eta: 0:15:14  iter: 299  total_loss: 0.499  loss_cls: 0.142  loss_box_reg: 0.297  loss_rpn_cls: 0.010  loss_rpn_loc: 0.042  time: 0.5363  data_time: 0.0610  lr: 0.002997  max_mem: 3070M
[32m[04/15 15:23:19 d2.utils.events]: [0m eta: 0:15:03  iter: 319  total_loss: 0.550  loss_cls: 0.164  loss_box_reg: 0.309  loss_rpn_cls: 0.012  loss_rpn_loc: 0.040  time: 0.5354  data_time: 0.0078  lr: 0.003197  max_mem: 3070M
[32m[04/15 15:23:30 d2.utils.events]: [0m eta: 0:14:49  iter: 339  total_loss: 0.654  loss_cls: 0.194  loss_box_reg: 0.384  loss_rpn_cls: 0.014  loss_rpn_loc: 0.061  time: 0.5343  data_time: 0.0082  lr: 0.003397  max_mem: 3070M
[32m[04/15 15:23:41 d2.utils.events]: [0m eta: 0:14:35  iter: 359  total_loss: 0.678  loss_cls: 0.217  loss_box_reg: 0.366  loss_rpn_cls: 0.016  loss_rpn_loc: 0.051  time: 0.5336  data_time: 0.0069  lr: 0.003596  max_mem: 3070M
[32m[04/15 15:23:57 d2.utils.events]: [0m eta: 0:14:21  iter: 379  total_loss: 0.539  loss_cls: 0.155  loss_box_reg: 0.307  loss_rpn_cls: 0.015  loss_rpn_loc: 0.039  time: 0.5329  data_time: 0.0071  lr: 0.003796  max_mem: 3070M
[32m[04/15 15:24:09 d2.utils.events]: [0m eta: 0:14:12  iter: 399  total_loss: 0.579  loss_cls: 0.167  loss_box_reg: 0.328  loss_rpn_cls: 0.014  loss_rpn_loc: 0.049  time: 0.5333  data_time: 0.0095  lr: 0.003996  max_mem: 3070M
[32m[04/15 15:24:20 d2.utils.events]: [0m eta: 0:14:06  iter: 419  total_loss: 0.531  loss_cls: 0.178  loss_box_reg: 0.318  loss_rpn_cls: 0.019  loss_rpn_loc: 0.047  time: 0.5342  data_time: 0.0091  lr: 0.004196  max_mem: 3070M
[32m[04/15 15:24:31 d2.utils.events]: [0m eta: 0:13:56  iter: 439  total_loss: 0.574  loss_cls: 0.199  loss_box_reg: 0.300  loss_rpn_cls: 0.014  loss_rpn_loc: 0.058  time: 0.5340  data_time: 0.0106  lr: 0.004396  max_mem: 3070M
[32m[04/15 15:24:42 d2.utils.events]: [0m eta: 0:13:42  iter: 459  total_loss: 0.665  loss_cls: 0.211  loss_box_reg: 0.364  loss_rpn_cls: 0.015  loss_rpn_loc: 0.056  time: 0.5330  data_time: 0.0076  lr: 0.004595  max_mem: 3070M
[32m[04/15 15:24:53 d2.utils.events]: [0m eta: 0:13:33  iter: 479  total_loss: 0.591  loss_cls: 0.178  loss_box_reg: 0.363  loss_rpn_cls: 0.007  loss_rpn_loc: 0.037  time: 0.5334  data_time: 0.0089  lr: 0.004795  max_mem: 3070M
[32m[04/15 15:25:05 d2.utils.events]: [0m eta: 0:13:21  iter: 499  total_loss: 0.618  loss_cls: 0.183  loss_box_reg: 0.351  loss_rpn_cls: 0.019  loss_rpn_loc: 0.059  time: 0.5334  data_time: 0.0201  lr: 0.004995  max_mem: 3070M
[32m[04/15 15:25:16 d2.utils.events]: [0m eta: 0:13:09  iter: 519  total_loss: 0.646  loss_cls: 0.195  loss_box_reg: 0.353  loss_rpn_cls: 0.016  loss_rpn_loc: 0.063  time: 0.5324  data_time: 0.0085  lr: 0.005195  max_mem: 3070M
[32m[04/15 15:25:27 d2.utils.events]: [0m eta: 0:12:59  iter: 539  total_loss: 0.694  loss_cls: 0.218  loss_box_reg: 0.354  loss_rpn_cls: 0.013  loss_rpn_loc: 0.069  time: 0.5323  data_time: 0.0084  lr: 0.005395  max_mem: 3070M
[32m[04/15 15:25:38 d2.utils.events]: [0m eta: 0:12:51  iter: 559  total_loss: 0.560  loss_cls: 0.166  loss_box_reg: 0.353  loss_rpn_cls: 0.011  loss_rpn_loc: 0.045  time: 0.5332  data_time: 0.0095  lr: 0.005594  max_mem: 3070M
[32m[04/15 15:25:49 d2.utils.events]: [0m eta: 0:12:39  iter: 579  total_loss: 0.703  loss_cls: 0.221  loss_box_reg: 0.384  loss_rpn_cls: 0.016  loss_rpn_loc: 0.056  time: 0.5329  data_time: 0.0060  lr: 0.005794  max_mem: 3070M
[32m[04/15 15:26:01 d2.utils.events]: [0m eta: 0:12:30  iter: 599  total_loss: 0.643  loss_cls: 0.195  loss_box_reg: 0.368  loss_rpn_cls: 0.014  loss_rpn_loc: 0.052  time: 0.5333  data_time: 0.0094  lr: 0.005994  max_mem: 3070M
[32m[04/15 15:26:13 d2.utils.events]: [0m eta: 0:12:20  iter: 619  total_loss: 0.610  loss_cls: 0.210  loss_box_reg: 0.353  loss_rpn_cls: 0.023  loss_rpn_loc: 0.048  time: 0.5334  data_time: 0.0066  lr: 0.006194  max_mem: 3070M
[32m[04/15 15:26:24 d2.utils.events]: [0m eta: 0:12:08  iter: 639  total_loss: 0.642  loss_cls: 0.199  loss_box_reg: 0.350  loss_rpn_cls: 0.013  loss_rpn_loc: 0.052  time: 0.5329  data_time: 0.0086  lr: 0.006394  max_mem: 3070M
[32m[04/15 15:26:35 d2.utils.events]: [0m eta: 0:11:57  iter: 659  total_loss: 0.640  loss_cls: 0.200  loss_box_reg: 0.342  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5328  data_time: 0.0080  lr: 0.006593  max_mem: 3070M
[32m[04/15 15:26:46 d2.utils.events]: [0m eta: 0:11:47  iter: 679  total_loss: 0.673  loss_cls: 0.203  loss_box_reg: 0.378  loss_rpn_cls: 0.019  loss_rpn_loc: 0.049  time: 0.5332  data_time: 0.0081  lr: 0.006793  max_mem: 3070M
[32m[04/15 15:26:57 d2.utils.events]: [0m eta: 0:11:37  iter: 699  total_loss: 0.678  loss_cls: 0.211  loss_box_reg: 0.380  loss_rpn_cls: 0.023  loss_rpn_loc: 0.074  time: 0.5334  data_time: 0.0091  lr: 0.006993  max_mem: 3070M
[32m[04/15 15:27:08 d2.utils.events]: [0m eta: 0:11:26  iter: 719  total_loss: 0.645  loss_cls: 0.201  loss_box_reg: 0.371  loss_rpn_cls: 0.014  loss_rpn_loc: 0.053  time: 0.5335  data_time: 0.0090  lr: 0.007193  max_mem: 3070M
[32m[04/15 15:27:19 d2.utils.events]: [0m eta: 0:11:16  iter: 739  total_loss: 0.663  loss_cls: 0.205  loss_box_reg: 0.357  loss_rpn_cls: 0.016  loss_rpn_loc: 0.049  time: 0.5336  data_time: 0.0080  lr: 0.007393  max_mem: 3070M
[32m[04/15 15:27:30 d2.utils.events]: [0m eta: 0:11:04  iter: 759  total_loss: 0.613  loss_cls: 0.171  loss_box_reg: 0.353  loss_rpn_cls: 0.014  loss_rpn_loc: 0.056  time: 0.5331  data_time: 0.0072  lr: 0.007592  max_mem: 3070M
[32m[04/15 15:27:41 d2.utils.events]: [0m eta: 0:10:53  iter: 779  total_loss: 0.593  loss_cls: 0.177  loss_box_reg: 0.348  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5332  data_time: 0.0314  lr: 0.007792  max_mem: 3070M
[32m[04/15 15:27:53 d2.utils.events]: [0m eta: 0:10:42  iter: 799  total_loss: 0.674  loss_cls: 0.212  loss_box_reg: 0.386  loss_rpn_cls: 0.021  loss_rpn_loc: 0.076  time: 0.5332  data_time: 0.0079  lr: 0.007992  max_mem: 3070M
[32m[04/15 15:28:04 d2.utils.events]: [0m eta: 0:10:33  iter: 819  total_loss: 0.575  loss_cls: 0.181  loss_box_reg: 0.340  loss_rpn_cls: 0.016  loss_rpn_loc: 0.044  time: 0.5336  data_time: 0.0078  lr: 0.008192  max_mem: 3070M
[32m[04/15 15:28:15 d2.utils.events]: [0m eta: 0:10:22  iter: 839  total_loss: 0.542  loss_cls: 0.182  loss_box_reg: 0.300  loss_rpn_cls: 0.019  loss_rpn_loc: 0.046  time: 0.5333  data_time: 0.0078  lr: 0.008392  max_mem: 3070M
[32m[04/15 15:28:26 d2.utils.events]: [0m eta: 0:10:11  iter: 859  total_loss: 0.505  loss_cls: 0.182  loss_box_reg: 0.288  loss_rpn_cls: 0.014  loss_rpn_loc: 0.035  time: 0.5334  data_time: 0.0058  lr: 0.008591  max_mem: 3070M
[32m[04/15 15:28:37 d2.utils.events]: [0m eta: 0:10:00  iter: 879  total_loss: 0.671  loss_cls: 0.198  loss_box_reg: 0.387  loss_rpn_cls: 0.015  loss_rpn_loc: 0.064  time: 0.5330  data_time: 0.0088  lr: 0.008791  max_mem: 3070M
[32m[04/15 15:28:48 d2.utils.events]: [0m eta: 0:09:49  iter: 899  total_loss: 0.593  loss_cls: 0.177  loss_box_reg: 0.324  loss_rpn_cls: 0.018  loss_rpn_loc: 0.053  time: 0.5332  data_time: 0.0078  lr: 0.008991  max_mem: 3070M
[32m[04/15 15:28:59 d2.utils.events]: [0m eta: 0:09:38  iter: 919  total_loss: 0.725  loss_cls: 0.208  loss_box_reg: 0.380  loss_rpn_cls: 0.016  loss_rpn_loc: 0.060  time: 0.5331  data_time: 0.0070  lr: 0.009191  max_mem: 3070M
[32m[04/15 15:29:10 d2.utils.events]: [0m eta: 0:09:27  iter: 939  total_loss: 0.455  loss_cls: 0.139  loss_box_reg: 0.275  loss_rpn_cls: 0.016  loss_rpn_loc: 0.037  time: 0.5328  data_time: 0.0078  lr: 0.009391  max_mem: 3070M
[32m[04/15 15:29:21 d2.utils.events]: [0m eta: 0:09:16  iter: 959  total_loss: 0.580  loss_cls: 0.196  loss_box_reg: 0.306  loss_rpn_cls: 0.017  loss_rpn_loc: 0.049  time: 0.5325  data_time: 0.0069  lr: 0.009590  max_mem: 3070M
[32m[04/15 15:29:32 d2.utils.events]: [0m eta: 0:09:06  iter: 979  total_loss: 0.687  loss_cls: 0.235  loss_box_reg: 0.371  loss_rpn_cls: 0.024  loss_rpn_loc: 0.059  time: 0.5325  data_time: 0.0069  lr: 0.009790  max_mem: 3070M
[32m[04/15 15:29:43 d2.utils.events]: [0m eta: 0:08:55  iter: 999  total_loss: 0.569  loss_cls: 0.197  loss_box_reg: 0.302  loss_rpn_cls: 0.020  loss_rpn_loc: 0.055  time: 0.5326  data_time: 0.0074  lr: 0.009990  max_mem: 3070M
[32m[04/15 15:29:54 d2.utils.events]: [0m eta: 0:08:44  iter: 1019  total_loss: 0.666  loss_cls: 0.222  loss_box_reg: 0.353  loss_rpn_cls: 0.017  loss_rpn_loc: 0.059  time: 0.5325  data_time: 0.0103  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:30:05 d2.utils.events]: [0m eta: 0:08:34  iter: 1039  total_loss: 0.620  loss_cls: 0.210  loss_box_reg: 0.329  loss_rpn_cls: 0.021  loss_rpn_loc: 0.059  time: 0.5330  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:30:17 d2.utils.events]: [0m eta: 0:08:23  iter: 1059  total_loss: 0.672  loss_cls: 0.203  loss_box_reg: 0.378  loss_rpn_cls: 0.016  loss_rpn_loc: 0.063  time: 0.5329  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:30:27 d2.utils.events]: [0m eta: 0:08:12  iter: 1079  total_loss: 0.576  loss_cls: 0.185  loss_box_reg: 0.337  loss_rpn_cls: 0.016  loss_rpn_loc: 0.057  time: 0.5327  data_time: 0.0089  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:30:38 d2.utils.events]: [0m eta: 0:08:02  iter: 1099  total_loss: 0.648  loss_cls: 0.203  loss_box_reg: 0.359  loss_rpn_cls: 0.014  loss_rpn_loc: 0.059  time: 0.5327  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:30:51 d2.utils.events]: [0m eta: 0:07:51  iter: 1119  total_loss: 0.596  loss_cls: 0.199  loss_box_reg: 0.329  loss_rpn_cls: 0.015  loss_rpn_loc: 0.053  time: 0.5329  data_time: 0.0417  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:31:03 d2.utils.events]: [0m eta: 0:07:40  iter: 1139  total_loss: 0.614  loss_cls: 0.182  loss_box_reg: 0.340  loss_rpn_cls: 0.021  loss_rpn_loc: 0.045  time: 0.5329  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:31:14 d2.utils.events]: [0m eta: 0:07:30  iter: 1159  total_loss: 0.548  loss_cls: 0.200  loss_box_reg: 0.286  loss_rpn_cls: 0.019  loss_rpn_loc: 0.045  time: 0.5330  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:31:25 d2.utils.events]: [0m eta: 0:07:19  iter: 1179  total_loss: 0.708  loss_cls: 0.226  loss_box_reg: 0.386  loss_rpn_cls: 0.020  loss_rpn_loc: 0.073  time: 0.5331  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:31:36 d2.utils.events]: [0m eta: 0:07:09  iter: 1199  total_loss: 0.569  loss_cls: 0.168  loss_box_reg: 0.282  loss_rpn_cls: 0.022  loss_rpn_loc: 0.048  time: 0.5333  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:31:47 d2.utils.events]: [0m eta: 0:06:58  iter: 1219  total_loss: 0.683  loss_cls: 0.217  loss_box_reg: 0.345  loss_rpn_cls: 0.014  loss_rpn_loc: 0.068  time: 0.5334  data_time: 0.0087  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:32:00 d2.utils.events]: [0m eta: 0:06:47  iter: 1239  total_loss: 0.604  loss_cls: 0.197  loss_box_reg: 0.324  loss_rpn_cls: 0.022  loss_rpn_loc: 0.063  time: 0.5336  data_time: 0.0093  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:32:12 d2.utils.events]: [0m eta: 0:06:36  iter: 1259  total_loss: 0.647  loss_cls: 0.199  loss_box_reg: 0.343  loss_rpn_cls: 0.022  loss_rpn_loc: 0.069  time: 0.5338  data_time: 0.0068  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:32:22 d2.utils.events]: [0m eta: 0:06:26  iter: 1279  total_loss: 0.647  loss_cls: 0.214  loss_box_reg: 0.354  loss_rpn_cls: 0.017  loss_rpn_loc: 0.055  time: 0.5338  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:32:33 d2.utils.events]: [0m eta: 0:06:15  iter: 1299  total_loss: 0.653  loss_cls: 0.192  loss_box_reg: 0.370  loss_rpn_cls: 0.028  loss_rpn_loc: 0.050  time: 0.5336  data_time: 0.0089  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:32:44 d2.utils.events]: [0m eta: 0:06:04  iter: 1319  total_loss: 0.611  loss_cls: 0.186  loss_box_reg: 0.329  loss_rpn_cls: 0.022  loss_rpn_loc: 0.061  time: 0.5338  data_time: 0.0093  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:32:55 d2.utils.events]: [0m eta: 0:05:54  iter: 1339  total_loss: 0.550  loss_cls: 0.203  loss_box_reg: 0.317  loss_rpn_cls: 0.022  loss_rpn_loc: 0.044  time: 0.5337  data_time: 0.0089  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:33:06 d2.utils.events]: [0m eta: 0:05:43  iter: 1359  total_loss: 0.706  loss_cls: 0.220  loss_box_reg: 0.394  loss_rpn_cls: 0.019  loss_rpn_loc: 0.065  time: 0.5337  data_time: 0.0225  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:33:16 d2.utils.events]: [0m eta: 0:05:33  iter: 1379  total_loss: 0.596  loss_cls: 0.200  loss_box_reg: 0.325  loss_rpn_cls: 0.015  loss_rpn_loc: 0.055  time: 0.5335  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:33:27 d2.utils.events]: [0m eta: 0:05:22  iter: 1399  total_loss: 0.620  loss_cls: 0.183  loss_box_reg: 0.336  loss_rpn_cls: 0.022  loss_rpn_loc: 0.062  time: 0.5337  data_time: 0.0334  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:33:38 d2.utils.events]: [0m eta: 0:05:11  iter: 1419  total_loss: 0.601  loss_cls: 0.176  loss_box_reg: 0.305  loss_rpn_cls: 0.030  loss_rpn_loc: 0.061  time: 0.5339  data_time: 0.0223  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:33:50 d2.utils.events]: [0m eta: 0:05:01  iter: 1439  total_loss: 0.623  loss_cls: 0.214  loss_box_reg: 0.311  loss_rpn_cls: 0.015  loss_rpn_loc: 0.051  time: 0.5341  data_time: 0.0411  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:34:01 d2.utils.events]: [0m eta: 0:04:50  iter: 1459  total_loss: 0.643  loss_cls: 0.227  loss_box_reg: 0.371  loss_rpn_cls: 0.015  loss_rpn_loc: 0.049  time: 0.5343  data_time: 0.0195  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:34:11 d2.utils.events]: [0m eta: 0:04:39  iter: 1479  total_loss: 0.608  loss_cls: 0.183  loss_box_reg: 0.345  loss_rpn_cls: 0.021  loss_rpn_loc: 0.057  time: 0.5341  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:34:22 d2.utils.events]: [0m eta: 0:04:28  iter: 1499  total_loss: 0.624  loss_cls: 0.193  loss_box_reg: 0.327  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5344  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:34:34 d2.utils.events]: [0m eta: 0:04:18  iter: 1519  total_loss: 0.694  loss_cls: 0.203  loss_box_reg: 0.368  loss_rpn_cls: 0.019  loss_rpn_loc: 0.048  time: 0.5346  data_time: 0.0259  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:34:45 d2.utils.events]: [0m eta: 0:04:07  iter: 1539  total_loss: 0.522  loss_cls: 0.162  loss_box_reg: 0.276  loss_rpn_cls: 0.022  loss_rpn_loc: 0.036  time: 0.5345  data_time: 0.0087  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:34:56 d2.utils.events]: [0m eta: 0:03:56  iter: 1559  total_loss: 0.649  loss_cls: 0.212  loss_box_reg: 0.333  loss_rpn_cls: 0.020  loss_rpn_loc: 0.062  time: 0.5346  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:35:06 d2.utils.events]: [0m eta: 0:03:45  iter: 1579  total_loss: 0.732  loss_cls: 0.228  loss_box_reg: 0.399  loss_rpn_cls: 0.024  loss_rpn_loc: 0.066  time: 0.5345  data_time: 0.0094  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:35:17 d2.utils.events]: [0m eta: 0:03:35  iter: 1599  total_loss: 0.563  loss_cls: 0.173  loss_box_reg: 0.294  loss_rpn_cls: 0.017  loss_rpn_loc: 0.057  time: 0.5344  data_time: 0.0227  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:35:28 d2.utils.events]: [0m eta: 0:03:24  iter: 1619  total_loss: 0.647  loss_cls: 0.191  loss_box_reg: 0.379  loss_rpn_cls: 0.013  loss_rpn_loc: 0.049  time: 0.5343  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:35:39 d2.utils.events]: [0m eta: 0:03:13  iter: 1639  total_loss: 0.723  loss_cls: 0.225  loss_box_reg: 0.362  loss_rpn_cls: 0.022  loss_rpn_loc: 0.065  time: 0.5343  data_time: 0.0090  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:35:49 d2.utils.events]: [0m eta: 0:03:03  iter: 1659  total_loss: 0.683  loss_cls: 0.211  loss_box_reg: 0.353  loss_rpn_cls: 0.023  loss_rpn_loc: 0.064  time: 0.5343  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:36:00 d2.utils.events]: [0m eta: 0:02:52  iter: 1679  total_loss: 0.678  loss_cls: 0.221  loss_box_reg: 0.363  loss_rpn_cls: 0.026  loss_rpn_loc: 0.067  time: 0.5344  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:36:11 d2.utils.events]: [0m eta: 0:02:41  iter: 1699  total_loss: 0.574  loss_cls: 0.177  loss_box_reg: 0.304  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5345  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:36:22 d2.utils.events]: [0m eta: 0:02:30  iter: 1719  total_loss: 0.607  loss_cls: 0.209  loss_box_reg: 0.318  loss_rpn_cls: 0.015  loss_rpn_loc: 0.061  time: 0.5342  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:36:33 d2.utils.events]: [0m eta: 0:02:20  iter: 1739  total_loss: 0.585  loss_cls: 0.192  loss_box_reg: 0.330  loss_rpn_cls: 0.021  loss_rpn_loc: 0.056  time: 0.5343  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:36:43 d2.utils.events]: [0m eta: 0:02:09  iter: 1759  total_loss: 0.708  loss_cls: 0.229  loss_box_reg: 0.373  loss_rpn_cls: 0.021  loss_rpn_loc: 0.075  time: 0.5343  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:36:54 d2.utils.events]: [0m eta: 0:01:58  iter: 1779  total_loss: 0.683  loss_cls: 0.227  loss_box_reg: 0.349  loss_rpn_cls: 0.018  loss_rpn_loc: 0.057  time: 0.5343  data_time: 0.0179  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:37:05 d2.utils.events]: [0m eta: 0:01:47  iter: 1799  total_loss: 0.620  loss_cls: 0.219  loss_box_reg: 0.337  loss_rpn_cls: 0.016  loss_rpn_loc: 0.066  time: 0.5344  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:37:16 d2.utils.events]: [0m eta: 0:01:37  iter: 1819  total_loss: 0.736  loss_cls: 0.236  loss_box_reg: 0.395  loss_rpn_cls: 0.014  loss_rpn_loc: 0.064  time: 0.5345  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:37:28 d2.utils.events]: [0m eta: 0:01:26  iter: 1839  total_loss: 0.720  loss_cls: 0.234  loss_box_reg: 0.424  loss_rpn_cls: 0.021  loss_rpn_loc: 0.069  time: 0.5345  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:37:38 d2.utils.events]: [0m eta: 0:01:15  iter: 1859  total_loss: 0.757  loss_cls: 0.222  loss_box_reg: 0.403  loss_rpn_cls: 0.025  loss_rpn_loc: 0.080  time: 0.5345  data_time: 0.0090  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:37:49 d2.utils.events]: [0m eta: 0:01:04  iter: 1879  total_loss: 0.558  loss_cls: 0.193  loss_box_reg: 0.314  loss_rpn_cls: 0.018  loss_rpn_loc: 0.053  time: 0.5344  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:38:00 d2.utils.events]: [0m eta: 0:00:54  iter: 1899  total_loss: 0.764  loss_cls: 0.215  loss_box_reg: 0.408  loss_rpn_cls: 0.022  loss_rpn_loc: 0.065  time: 0.5343  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:38:10 d2.utils.events]: [0m eta: 0:00:43  iter: 1919  total_loss: 0.596  loss_cls: 0.168  loss_box_reg: 0.319  loss_rpn_cls: 0.014  loss_rpn_loc: 0.060  time: 0.5342  data_time: 0.0094  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:38:29 d2.utils.events]: [0m eta: 0:00:32  iter: 1939  total_loss: 0.575  loss_cls: 0.182  loss_box_reg: 0.324  loss_rpn_cls: 0.015  loss_rpn_loc: 0.051  time: 0.5342  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:38:41 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.486  loss_cls: 0.141  loss_box_reg: 0.274  loss_rpn_cls: 0.014  loss_rpn_loc: 0.036  time: 0.5344  data_time: 0.0121  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:38:52 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.728  loss_cls: 0.213  loss_box_reg: 0.413  loss_rpn_cls: 0.019  loss_rpn_loc: 0.060  time: 0.5346  data_time: 0.0575  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 15:39:35 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 15:39:35 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 15:39:36 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 15:39:36 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.678  loss_cls: 0.188  loss_box_reg: 0.360  loss_rpn_cls: 0.022  loss_rpn_loc: 0.066  time: 0.5345  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:39:43 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:17:47 (0.5348 s / it)
[32m[04/15 15:39:43 d2.engine.hooks]: [0mTotal training time: 0:19:23 (0:01:35 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 15:39:53 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 15:39:53 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 15:39:55 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 15:40:05 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1197 s / img. ETA=0:04:35
[32m[04/15 15:40:10 d2.evaluation.evaluator]: [0mInference done 30/1257. 0.1200 s / img. ETA=0:05:21
[32m[04/15 15:40:16 d2.evaluation.evaluator]: [0mInference done 58/1257. 0.1205 s / img. ETA=0:04:23
[32m[04/15 15:40:21 d2.evaluation.evaluator]: [0mInference done 76/1257. 0.1206 s / img. ETA=0:04:39
[32m[04/15 15:40:26 d2.evaluation.evaluator]: [0mInference done 97/1257. 0.1206 s / img. ETA=0:04:35
[32m[04/15 15:40:31 d2.evaluation.evaluator]: [0mInference done 120/1257. 0.1221 s / img. ETA=0:04:26
[32m[04/15 15:40:36 d2.evaluation.evaluator]: [0mInference done 143/1257. 0.1217 s / img. ETA=0:04:18
[32m[04/15 15:40:41 d2.evaluation.evaluator]: [0mInference done 175/1257. 0.1213 s / img. ETA=0:03:56
[32m[04/15 15:40:46 d2.evaluation.evaluator]: [0mInference done 203/1257. 0.1212 s / img. ETA=0:03:46
[32m[04/15 15:40:51 d2.evaluation.evaluator]: [0mInference done 234/1257. 0.1212 s / img. ETA=0:03:32
[32m[04/15 15:40:56 d2.evaluation.evaluator]: [0mInference done 271/1257. 0.1211 s / img. ETA=0:03:14
[32m[04/15 15:41:02 d2.evaluation.evaluator]: [0mInference done 311/1257. 0.1211 s / img. ETA=0:02:58
[32m[04/15 15:41:07 d2.evaluation.evaluator]: [0mInference done 352/1257. 0.1212 s / img. ETA=0:02:43
[32m[04/15 15:41:12 d2.evaluation.evaluator]: [0mInference done 393/1257. 0.1211 s / img. ETA=0:02:31
[32m[04/15 15:41:17 d2.evaluation.evaluator]: [0mInference done 433/1257. 0.1210 s / img. ETA=0:02:20
[32m[04/15 15:41:22 d2.evaluation.evaluator]: [0mInference done 471/1257. 0.1208 s / img. ETA=0:02:11
[32m[04/15 15:41:27 d2.evaluation.evaluator]: [0mInference done 512/1257. 0.1208 s / img. ETA=0:02:01
[32m[04/15 15:41:32 d2.evaluation.evaluator]: [0mInference done 552/1257. 0.1208 s / img. ETA=0:01:53
[32m[04/15 15:41:37 d2.evaluation.evaluator]: [0mInference done 592/1257. 0.1209 s / img. ETA=0:01:45
[32m[04/15 15:41:42 d2.evaluation.evaluator]: [0mInference done 630/1257. 0.1211 s / img. ETA=0:01:38
[32m[04/15 15:41:47 d2.evaluation.evaluator]: [0mInference done 665/1257. 0.1211 s / img. ETA=0:01:32
[32m[04/15 15:41:52 d2.evaluation.evaluator]: [0mInference done 706/1257. 0.1211 s / img. ETA=0:01:25
[32m[04/15 15:41:57 d2.evaluation.evaluator]: [0mInference done 747/1257. 0.1211 s / img. ETA=0:01:17
[32m[04/15 15:42:02 d2.evaluation.evaluator]: [0mInference done 787/1257. 0.1212 s / img. ETA=0:01:11
[32m[04/15 15:42:07 d2.evaluation.evaluator]: [0mInference done 827/1257. 0.1212 s / img. ETA=0:01:04
[32m[04/15 15:42:12 d2.evaluation.evaluator]: [0mInference done 867/1257. 0.1213 s / img. ETA=0:00:58
[32m[04/15 15:42:17 d2.evaluation.evaluator]: [0mInference done 907/1257. 0.1213 s / img. ETA=0:00:51
[32m[04/15 15:42:22 d2.evaluation.evaluator]: [0mInference done 948/1257. 0.1213 s / img. ETA=0:00:45
[32m[04/15 15:42:27 d2.evaluation.evaluator]: [0mInference done 988/1257. 0.1213 s / img. ETA=0:00:39
[32m[04/15 15:42:32 d2.evaluation.evaluator]: [0mInference done 1028/1257. 0.1213 s / img. ETA=0:00:33
[32m[04/15 15:42:38 d2.evaluation.evaluator]: [0mInference done 1067/1257. 0.1213 s / img. ETA=0:00:27
[32m[04/15 15:42:43 d2.evaluation.evaluator]: [0mInference done 1101/1257. 0.1213 s / img. ETA=0:00:22
[32m[04/15 15:42:48 d2.evaluation.evaluator]: [0mInference done 1136/1257. 0.1213 s / img. ETA=0:00:17
[32m[04/15 15:42:53 d2.evaluation.evaluator]: [0mInference done 1176/1257. 0.1213 s / img. ETA=0:00:11
[32m[04/15 15:42:58 d2.evaluation.evaluator]: [0mInference done 1216/1257. 0.1213 s / img. ETA=0:00:05
[32m[04/15 15:43:03 d2.evaluation.evaluator]: [0mInference done 1256/1257. 0.1213 s / img. ETA=0:00:00
[32m[04/15 15:43:03 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:59.192405 (0.143125 s / img per device, on 1 devices)
[32m[04/15 15:43:03 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:31 (0.121318 s / img per device, on 1 devices)
[32m[04/15 15:43:03 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 15:43:03 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 15:43:03 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.38s).
Accumulating evaluation results...
DONE (t=0.84s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.193
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.385
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.169
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.125
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.233
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.093
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.239
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.242
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.182
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.279
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.434
[32m[04/15 15:43:11 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.292 | 38.470 | 16.934 | 12.537 | 23.332 | 39.617 |
[32m[04/15 15:43:11 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 27.765 | bicycle       | 7.134 | car            | 42.268 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  26  *  2000  iterations ============
4 channel input
[32m[04/15 15:43:12 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 15:43:14 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.67 seconds.
[5m[31mWARNING[0m [32m[04/15 15:43:14 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 15:43:14 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 15:43:15 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 15:43:15 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 15:43:15 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 15:43:15 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 15:43:27 d2.utils.events]: [0m eta: 0:17:40  iter: 19  total_loss: 0.577  loss_cls: 0.182  loss_box_reg: 0.313  loss_rpn_cls: 0.024  loss_rpn_loc: 0.050  time: 0.5319  data_time: 0.0440  lr: 0.000200  max_mem: 3070M
[32m[04/15 15:43:37 d2.utils.events]: [0m eta: 0:17:19  iter: 39  total_loss: 0.557  loss_cls: 0.176  loss_box_reg: 0.295  loss_rpn_cls: 0.015  loss_rpn_loc: 0.052  time: 0.5343  data_time: 0.0082  lr: 0.000400  max_mem: 3070M
[32m[04/15 15:43:48 d2.utils.events]: [0m eta: 0:17:20  iter: 59  total_loss: 0.672  loss_cls: 0.219  loss_box_reg: 0.366  loss_rpn_cls: 0.016  loss_rpn_loc: 0.062  time: 0.5358  data_time: 0.0087  lr: 0.000599  max_mem: 3070M
[32m[04/15 15:43:59 d2.utils.events]: [0m eta: 0:16:59  iter: 79  total_loss: 0.628  loss_cls: 0.201  loss_box_reg: 0.330  loss_rpn_cls: 0.024  loss_rpn_loc: 0.054  time: 0.5317  data_time: 0.0072  lr: 0.000799  max_mem: 3070M
[32m[04/15 15:44:10 d2.utils.events]: [0m eta: 0:16:50  iter: 99  total_loss: 0.701  loss_cls: 0.221  loss_box_reg: 0.405  loss_rpn_cls: 0.022  loss_rpn_loc: 0.061  time: 0.5323  data_time: 0.0081  lr: 0.000999  max_mem: 3070M
[32m[04/15 15:44:20 d2.utils.events]: [0m eta: 0:16:52  iter: 119  total_loss: 0.702  loss_cls: 0.219  loss_box_reg: 0.383  loss_rpn_cls: 0.012  loss_rpn_loc: 0.060  time: 0.5338  data_time: 0.0088  lr: 0.001199  max_mem: 3070M
[32m[04/15 15:44:31 d2.utils.events]: [0m eta: 0:16:37  iter: 139  total_loss: 0.592  loss_cls: 0.205  loss_box_reg: 0.350  loss_rpn_cls: 0.017  loss_rpn_loc: 0.049  time: 0.5334  data_time: 0.0076  lr: 0.001399  max_mem: 3070M
[32m[04/15 15:44:42 d2.utils.events]: [0m eta: 0:16:29  iter: 159  total_loss: 0.548  loss_cls: 0.178  loss_box_reg: 0.297  loss_rpn_cls: 0.018  loss_rpn_loc: 0.039  time: 0.5343  data_time: 0.0076  lr: 0.001598  max_mem: 3070M
[32m[04/15 15:44:53 d2.utils.events]: [0m eta: 0:16:13  iter: 179  total_loss: 0.601  loss_cls: 0.188  loss_box_reg: 0.330  loss_rpn_cls: 0.015  loss_rpn_loc: 0.067  time: 0.5330  data_time: 0.0084  lr: 0.001798  max_mem: 3070M
[32m[04/15 15:45:03 d2.utils.events]: [0m eta: 0:16:02  iter: 199  total_loss: 0.611  loss_cls: 0.184  loss_box_reg: 0.351  loss_rpn_cls: 0.016  loss_rpn_loc: 0.065  time: 0.5332  data_time: 0.0084  lr: 0.001998  max_mem: 3070M
[32m[04/15 15:45:14 d2.utils.events]: [0m eta: 0:15:56  iter: 219  total_loss: 0.614  loss_cls: 0.200  loss_box_reg: 0.342  loss_rpn_cls: 0.014  loss_rpn_loc: 0.052  time: 0.5337  data_time: 0.0081  lr: 0.002198  max_mem: 3070M
[32m[04/15 15:45:25 d2.utils.events]: [0m eta: 0:15:45  iter: 239  total_loss: 0.531  loss_cls: 0.184  loss_box_reg: 0.304  loss_rpn_cls: 0.014  loss_rpn_loc: 0.048  time: 0.5344  data_time: 0.0082  lr: 0.002398  max_mem: 3070M
[32m[04/15 15:45:36 d2.utils.events]: [0m eta: 0:15:35  iter: 259  total_loss: 0.469  loss_cls: 0.148  loss_box_reg: 0.292  loss_rpn_cls: 0.011  loss_rpn_loc: 0.029  time: 0.5357  data_time: 0.0091  lr: 0.002597  max_mem: 3070M
[32m[04/15 15:45:47 d2.utils.events]: [0m eta: 0:15:24  iter: 279  total_loss: 0.660  loss_cls: 0.211  loss_box_reg: 0.366  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5354  data_time: 0.0078  lr: 0.002797  max_mem: 3070M
[32m[04/15 15:45:58 d2.utils.events]: [0m eta: 0:15:15  iter: 299  total_loss: 0.512  loss_cls: 0.173  loss_box_reg: 0.297  loss_rpn_cls: 0.019  loss_rpn_loc: 0.045  time: 0.5366  data_time: 0.0076  lr: 0.002997  max_mem: 3070M
[32m[04/15 15:46:09 d2.utils.events]: [0m eta: 0:15:05  iter: 319  total_loss: 0.485  loss_cls: 0.136  loss_box_reg: 0.262  loss_rpn_cls: 0.006  loss_rpn_loc: 0.039  time: 0.5370  data_time: 0.0071  lr: 0.003197  max_mem: 3070M
[32m[04/15 15:46:20 d2.utils.events]: [0m eta: 0:14:57  iter: 339  total_loss: 0.668  loss_cls: 0.204  loss_box_reg: 0.382  loss_rpn_cls: 0.011  loss_rpn_loc: 0.071  time: 0.5377  data_time: 0.0090  lr: 0.003397  max_mem: 3070M
[32m[04/15 15:46:31 d2.utils.events]: [0m eta: 0:14:43  iter: 359  total_loss: 0.627  loss_cls: 0.196  loss_box_reg: 0.367  loss_rpn_cls: 0.016  loss_rpn_loc: 0.055  time: 0.5366  data_time: 0.0078  lr: 0.003596  max_mem: 3070M
[32m[04/15 15:46:42 d2.utils.events]: [0m eta: 0:14:35  iter: 379  total_loss: 0.636  loss_cls: 0.197  loss_box_reg: 0.391  loss_rpn_cls: 0.013  loss_rpn_loc: 0.056  time: 0.5373  data_time: 0.0074  lr: 0.003796  max_mem: 3070M
[32m[04/15 15:46:53 d2.utils.events]: [0m eta: 0:14:24  iter: 399  total_loss: 0.606  loss_cls: 0.200  loss_box_reg: 0.349  loss_rpn_cls: 0.017  loss_rpn_loc: 0.043  time: 0.5374  data_time: 0.0084  lr: 0.003996  max_mem: 3070M
[32m[04/15 15:47:03 d2.utils.events]: [0m eta: 0:14:11  iter: 419  total_loss: 0.609  loss_cls: 0.167  loss_box_reg: 0.311  loss_rpn_cls: 0.016  loss_rpn_loc: 0.061  time: 0.5370  data_time: 0.0085  lr: 0.004196  max_mem: 3070M
[32m[04/15 15:47:14 d2.utils.events]: [0m eta: 0:14:01  iter: 439  total_loss: 0.548  loss_cls: 0.176  loss_box_reg: 0.305  loss_rpn_cls: 0.017  loss_rpn_loc: 0.040  time: 0.5369  data_time: 0.0086  lr: 0.004396  max_mem: 3070M
[32m[04/15 15:47:25 d2.utils.events]: [0m eta: 0:13:51  iter: 459  total_loss: 0.668  loss_cls: 0.199  loss_box_reg: 0.371  loss_rpn_cls: 0.010  loss_rpn_loc: 0.055  time: 0.5373  data_time: 0.0076  lr: 0.004595  max_mem: 3070M
[32m[04/15 15:47:36 d2.utils.events]: [0m eta: 0:13:40  iter: 479  total_loss: 0.482  loss_cls: 0.154  loss_box_reg: 0.271  loss_rpn_cls: 0.010  loss_rpn_loc: 0.047  time: 0.5373  data_time: 0.0084  lr: 0.004795  max_mem: 3070M
[32m[04/15 15:47:47 d2.utils.events]: [0m eta: 0:13:30  iter: 499  total_loss: 0.468  loss_cls: 0.143  loss_box_reg: 0.287  loss_rpn_cls: 0.007  loss_rpn_loc: 0.037  time: 0.5379  data_time: 0.0073  lr: 0.004995  max_mem: 3070M
[32m[04/15 15:47:57 d2.utils.events]: [0m eta: 0:13:18  iter: 519  total_loss: 0.642  loss_cls: 0.195  loss_box_reg: 0.360  loss_rpn_cls: 0.018  loss_rpn_loc: 0.051  time: 0.5374  data_time: 0.0085  lr: 0.005195  max_mem: 3070M
[32m[04/15 15:48:08 d2.utils.events]: [0m eta: 0:13:07  iter: 539  total_loss: 0.612  loss_cls: 0.187  loss_box_reg: 0.335  loss_rpn_cls: 0.017  loss_rpn_loc: 0.053  time: 0.5373  data_time: 0.0073  lr: 0.005395  max_mem: 3070M
[32m[04/15 15:48:19 d2.utils.events]: [0m eta: 0:12:58  iter: 559  total_loss: 0.616  loss_cls: 0.199  loss_box_reg: 0.344  loss_rpn_cls: 0.019  loss_rpn_loc: 0.055  time: 0.5375  data_time: 0.0070  lr: 0.005594  max_mem: 3070M
[32m[04/15 15:48:30 d2.utils.events]: [0m eta: 0:12:47  iter: 579  total_loss: 0.606  loss_cls: 0.188  loss_box_reg: 0.307  loss_rpn_cls: 0.019  loss_rpn_loc: 0.049  time: 0.5374  data_time: 0.0074  lr: 0.005794  max_mem: 3070M
[32m[04/15 15:48:41 d2.utils.events]: [0m eta: 0:12:36  iter: 599  total_loss: 0.691  loss_cls: 0.213  loss_box_reg: 0.374  loss_rpn_cls: 0.011  loss_rpn_loc: 0.053  time: 0.5375  data_time: 0.0089  lr: 0.005994  max_mem: 3070M
[32m[04/15 15:48:51 d2.utils.events]: [0m eta: 0:12:24  iter: 619  total_loss: 0.613  loss_cls: 0.191  loss_box_reg: 0.359  loss_rpn_cls: 0.014  loss_rpn_loc: 0.055  time: 0.5370  data_time: 0.0083  lr: 0.006194  max_mem: 3070M
[32m[04/15 15:49:02 d2.utils.events]: [0m eta: 0:12:13  iter: 639  total_loss: 0.592  loss_cls: 0.179  loss_box_reg: 0.343  loss_rpn_cls: 0.017  loss_rpn_loc: 0.057  time: 0.5366  data_time: 0.0104  lr: 0.006394  max_mem: 3070M
[32m[04/15 15:49:13 d2.utils.events]: [0m eta: 0:12:03  iter: 659  total_loss: 0.726  loss_cls: 0.209  loss_box_reg: 0.382  loss_rpn_cls: 0.015  loss_rpn_loc: 0.060  time: 0.5369  data_time: 0.0082  lr: 0.006593  max_mem: 3070M
[32m[04/15 15:49:24 d2.utils.events]: [0m eta: 0:11:52  iter: 679  total_loss: 0.565  loss_cls: 0.182  loss_box_reg: 0.320  loss_rpn_cls: 0.018  loss_rpn_loc: 0.051  time: 0.5368  data_time: 0.0070  lr: 0.006793  max_mem: 3070M
[32m[04/15 15:49:35 d2.utils.events]: [0m eta: 0:11:41  iter: 699  total_loss: 0.540  loss_cls: 0.170  loss_box_reg: 0.301  loss_rpn_cls: 0.011  loss_rpn_loc: 0.041  time: 0.5370  data_time: 0.0080  lr: 0.006993  max_mem: 3070M
[32m[04/15 15:49:46 d2.utils.events]: [0m eta: 0:11:31  iter: 719  total_loss: 0.658  loss_cls: 0.202  loss_box_reg: 0.326  loss_rpn_cls: 0.019  loss_rpn_loc: 0.048  time: 0.5372  data_time: 0.0082  lr: 0.007193  max_mem: 3070M
[32m[04/15 15:49:56 d2.utils.events]: [0m eta: 0:11:20  iter: 739  total_loss: 0.567  loss_cls: 0.189  loss_box_reg: 0.304  loss_rpn_cls: 0.013  loss_rpn_loc: 0.064  time: 0.5372  data_time: 0.0068  lr: 0.007393  max_mem: 3070M
[32m[04/15 15:50:07 d2.utils.events]: [0m eta: 0:11:09  iter: 759  total_loss: 0.775  loss_cls: 0.248  loss_box_reg: 0.412  loss_rpn_cls: 0.019  loss_rpn_loc: 0.073  time: 0.5374  data_time: 0.0082  lr: 0.007592  max_mem: 3070M
[32m[04/15 15:50:18 d2.utils.events]: [0m eta: 0:10:58  iter: 779  total_loss: 0.565  loss_cls: 0.179  loss_box_reg: 0.332  loss_rpn_cls: 0.010  loss_rpn_loc: 0.055  time: 0.5373  data_time: 0.0074  lr: 0.007792  max_mem: 3070M
[32m[04/15 15:50:29 d2.utils.events]: [0m eta: 0:10:47  iter: 799  total_loss: 0.760  loss_cls: 0.222  loss_box_reg: 0.429  loss_rpn_cls: 0.020  loss_rpn_loc: 0.068  time: 0.5371  data_time: 0.0073  lr: 0.007992  max_mem: 3070M
[32m[04/15 15:50:39 d2.utils.events]: [0m eta: 0:10:36  iter: 819  total_loss: 0.665  loss_cls: 0.207  loss_box_reg: 0.369  loss_rpn_cls: 0.014  loss_rpn_loc: 0.062  time: 0.5368  data_time: 0.0081  lr: 0.008192  max_mem: 3070M
[32m[04/15 15:50:50 d2.utils.events]: [0m eta: 0:10:25  iter: 839  total_loss: 0.673  loss_cls: 0.201  loss_box_reg: 0.407  loss_rpn_cls: 0.016  loss_rpn_loc: 0.063  time: 0.5367  data_time: 0.0081  lr: 0.008392  max_mem: 3070M
[32m[04/15 15:51:01 d2.utils.events]: [0m eta: 0:10:14  iter: 859  total_loss: 0.687  loss_cls: 0.204  loss_box_reg: 0.372  loss_rpn_cls: 0.018  loss_rpn_loc: 0.058  time: 0.5366  data_time: 0.0098  lr: 0.008591  max_mem: 3070M
[32m[04/15 15:51:12 d2.utils.events]: [0m eta: 0:10:03  iter: 879  total_loss: 0.665  loss_cls: 0.183  loss_box_reg: 0.370  loss_rpn_cls: 0.016  loss_rpn_loc: 0.071  time: 0.5364  data_time: 0.0085  lr: 0.008791  max_mem: 3070M
[32m[04/15 15:51:23 d2.utils.events]: [0m eta: 0:09:53  iter: 899  total_loss: 0.637  loss_cls: 0.188  loss_box_reg: 0.354  loss_rpn_cls: 0.016  loss_rpn_loc: 0.050  time: 0.5368  data_time: 0.0080  lr: 0.008991  max_mem: 3070M
[32m[04/15 15:51:33 d2.utils.events]: [0m eta: 0:09:42  iter: 919  total_loss: 0.636  loss_cls: 0.197  loss_box_reg: 0.378  loss_rpn_cls: 0.011  loss_rpn_loc: 0.057  time: 0.5368  data_time: 0.0084  lr: 0.009191  max_mem: 3070M
[32m[04/15 15:51:44 d2.utils.events]: [0m eta: 0:09:32  iter: 939  total_loss: 0.655  loss_cls: 0.215  loss_box_reg: 0.351  loss_rpn_cls: 0.015  loss_rpn_loc: 0.078  time: 0.5369  data_time: 0.0082  lr: 0.009391  max_mem: 3070M
[32m[04/15 15:51:55 d2.utils.events]: [0m eta: 0:09:21  iter: 959  total_loss: 0.747  loss_cls: 0.225  loss_box_reg: 0.418  loss_rpn_cls: 0.015  loss_rpn_loc: 0.067  time: 0.5369  data_time: 0.0084  lr: 0.009590  max_mem: 3070M
[32m[04/15 15:52:06 d2.utils.events]: [0m eta: 0:09:10  iter: 979  total_loss: 0.586  loss_cls: 0.188  loss_box_reg: 0.277  loss_rpn_cls: 0.020  loss_rpn_loc: 0.065  time: 0.5367  data_time: 0.0083  lr: 0.009790  max_mem: 3070M
[32m[04/15 15:52:17 d2.utils.events]: [0m eta: 0:08:59  iter: 999  total_loss: 0.514  loss_cls: 0.163  loss_box_reg: 0.283  loss_rpn_cls: 0.014  loss_rpn_loc: 0.047  time: 0.5368  data_time: 0.0077  lr: 0.009990  max_mem: 3070M
[32m[04/15 15:52:28 d2.utils.events]: [0m eta: 0:08:49  iter: 1019  total_loss: 0.657  loss_cls: 0.210  loss_box_reg: 0.386  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5372  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:52:39 d2.utils.events]: [0m eta: 0:08:39  iter: 1039  total_loss: 0.702  loss_cls: 0.212  loss_box_reg: 0.352  loss_rpn_cls: 0.021  loss_rpn_loc: 0.059  time: 0.5373  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:52:50 d2.utils.events]: [0m eta: 0:08:28  iter: 1059  total_loss: 0.654  loss_cls: 0.211  loss_box_reg: 0.376  loss_rpn_cls: 0.022  loss_rpn_loc: 0.064  time: 0.5374  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:53:01 d2.utils.events]: [0m eta: 0:08:17  iter: 1079  total_loss: 0.659  loss_cls: 0.191  loss_box_reg: 0.371  loss_rpn_cls: 0.016  loss_rpn_loc: 0.069  time: 0.5376  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:53:12 d2.utils.events]: [0m eta: 0:08:06  iter: 1099  total_loss: 0.580  loss_cls: 0.183  loss_box_reg: 0.363  loss_rpn_cls: 0.015  loss_rpn_loc: 0.043  time: 0.5377  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:53:22 d2.utils.events]: [0m eta: 0:07:55  iter: 1119  total_loss: 0.547  loss_cls: 0.197  loss_box_reg: 0.295  loss_rpn_cls: 0.018  loss_rpn_loc: 0.053  time: 0.5374  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:53:33 d2.utils.events]: [0m eta: 0:07:45  iter: 1139  total_loss: 0.635  loss_cls: 0.207  loss_box_reg: 0.350  loss_rpn_cls: 0.012  loss_rpn_loc: 0.049  time: 0.5376  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:53:44 d2.utils.events]: [0m eta: 0:07:33  iter: 1159  total_loss: 0.569  loss_cls: 0.193  loss_box_reg: 0.330  loss_rpn_cls: 0.013  loss_rpn_loc: 0.060  time: 0.5374  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:53:55 d2.utils.events]: [0m eta: 0:07:23  iter: 1179  total_loss: 0.723  loss_cls: 0.233  loss_box_reg: 0.405  loss_rpn_cls: 0.016  loss_rpn_loc: 0.057  time: 0.5374  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:54:05 d2.utils.events]: [0m eta: 0:07:12  iter: 1199  total_loss: 0.654  loss_cls: 0.214  loss_box_reg: 0.359  loss_rpn_cls: 0.015  loss_rpn_loc: 0.062  time: 0.5373  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:54:16 d2.utils.events]: [0m eta: 0:07:01  iter: 1219  total_loss: 0.582  loss_cls: 0.185  loss_box_reg: 0.343  loss_rpn_cls: 0.017  loss_rpn_loc: 0.052  time: 0.5372  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:54:27 d2.utils.events]: [0m eta: 0:06:51  iter: 1239  total_loss: 0.713  loss_cls: 0.222  loss_box_reg: 0.392  loss_rpn_cls: 0.019  loss_rpn_loc: 0.077  time: 0.5375  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:54:39 d2.utils.events]: [0m eta: 0:06:40  iter: 1259  total_loss: 0.678  loss_cls: 0.208  loss_box_reg: 0.373  loss_rpn_cls: 0.023  loss_rpn_loc: 0.069  time: 0.5379  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:54:49 d2.utils.events]: [0m eta: 0:06:29  iter: 1279  total_loss: 0.684  loss_cls: 0.207  loss_box_reg: 0.361  loss_rpn_cls: 0.021  loss_rpn_loc: 0.064  time: 0.5378  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:55:00 d2.utils.events]: [0m eta: 0:06:18  iter: 1299  total_loss: 0.742  loss_cls: 0.222  loss_box_reg: 0.410  loss_rpn_cls: 0.024  loss_rpn_loc: 0.078  time: 0.5376  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:55:11 d2.utils.events]: [0m eta: 0:06:07  iter: 1319  total_loss: 0.551  loss_cls: 0.165  loss_box_reg: 0.321  loss_rpn_cls: 0.015  loss_rpn_loc: 0.046  time: 0.5378  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:55:22 d2.utils.events]: [0m eta: 0:05:56  iter: 1339  total_loss: 0.705  loss_cls: 0.224  loss_box_reg: 0.369  loss_rpn_cls: 0.018  loss_rpn_loc: 0.073  time: 0.5378  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:55:33 d2.utils.events]: [0m eta: 0:05:46  iter: 1359  total_loss: 0.552  loss_cls: 0.182  loss_box_reg: 0.323  loss_rpn_cls: 0.019  loss_rpn_loc: 0.048  time: 0.5379  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:55:44 d2.utils.events]: [0m eta: 0:05:35  iter: 1379  total_loss: 0.663  loss_cls: 0.198  loss_box_reg: 0.336  loss_rpn_cls: 0.025  loss_rpn_loc: 0.063  time: 0.5378  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:55:54 d2.utils.events]: [0m eta: 0:05:24  iter: 1399  total_loss: 0.633  loss_cls: 0.207  loss_box_reg: 0.340  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5379  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:56:05 d2.utils.events]: [0m eta: 0:05:13  iter: 1419  total_loss: 0.651  loss_cls: 0.211  loss_box_reg: 0.373  loss_rpn_cls: 0.022  loss_rpn_loc: 0.066  time: 0.5379  data_time: 0.0091  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:56:16 d2.utils.events]: [0m eta: 0:05:02  iter: 1439  total_loss: 0.645  loss_cls: 0.200  loss_box_reg: 0.348  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5375  data_time: 0.0096  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:56:26 d2.utils.events]: [0m eta: 0:04:51  iter: 1459  total_loss: 0.643  loss_cls: 0.194  loss_box_reg: 0.354  loss_rpn_cls: 0.011  loss_rpn_loc: 0.067  time: 0.5373  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:56:37 d2.utils.events]: [0m eta: 0:04:40  iter: 1479  total_loss: 0.516  loss_cls: 0.175  loss_box_reg: 0.276  loss_rpn_cls: 0.015  loss_rpn_loc: 0.049  time: 0.5372  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:56:48 d2.utils.events]: [0m eta: 0:04:29  iter: 1499  total_loss: 0.684  loss_cls: 0.247  loss_box_reg: 0.368  loss_rpn_cls: 0.023  loss_rpn_loc: 0.065  time: 0.5374  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:56:59 d2.utils.events]: [0m eta: 0:04:19  iter: 1519  total_loss: 0.655  loss_cls: 0.214  loss_box_reg: 0.377  loss_rpn_cls: 0.020  loss_rpn_loc: 0.050  time: 0.5375  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:57:10 d2.utils.events]: [0m eta: 0:04:08  iter: 1539  total_loss: 0.603  loss_cls: 0.177  loss_box_reg: 0.338  loss_rpn_cls: 0.012  loss_rpn_loc: 0.054  time: 0.5374  data_time: 0.0064  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:57:20 d2.utils.events]: [0m eta: 0:03:57  iter: 1559  total_loss: 0.687  loss_cls: 0.212  loss_box_reg: 0.394  loss_rpn_cls: 0.010  loss_rpn_loc: 0.051  time: 0.5373  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:57:31 d2.utils.events]: [0m eta: 0:03:46  iter: 1579  total_loss: 0.712  loss_cls: 0.219  loss_box_reg: 0.384  loss_rpn_cls: 0.020  loss_rpn_loc: 0.056  time: 0.5374  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:57:42 d2.utils.events]: [0m eta: 0:03:35  iter: 1599  total_loss: 0.772  loss_cls: 0.233  loss_box_reg: 0.417  loss_rpn_cls: 0.016  loss_rpn_loc: 0.070  time: 0.5374  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:57:53 d2.utils.events]: [0m eta: 0:03:25  iter: 1619  total_loss: 0.683  loss_cls: 0.222  loss_box_reg: 0.345  loss_rpn_cls: 0.021  loss_rpn_loc: 0.084  time: 0.5372  data_time: 0.0087  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:58:03 d2.utils.events]: [0m eta: 0:03:14  iter: 1639  total_loss: 0.776  loss_cls: 0.241  loss_box_reg: 0.416  loss_rpn_cls: 0.016  loss_rpn_loc: 0.069  time: 0.5372  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:58:14 d2.utils.events]: [0m eta: 0:03:03  iter: 1659  total_loss: 0.625  loss_cls: 0.193  loss_box_reg: 0.344  loss_rpn_cls: 0.015  loss_rpn_loc: 0.063  time: 0.5372  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:58:25 d2.utils.events]: [0m eta: 0:02:52  iter: 1679  total_loss: 0.552  loss_cls: 0.171  loss_box_reg: 0.297  loss_rpn_cls: 0.023  loss_rpn_loc: 0.059  time: 0.5371  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:58:36 d2.utils.events]: [0m eta: 0:02:41  iter: 1699  total_loss: 0.738  loss_cls: 0.215  loss_box_reg: 0.386  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5371  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:58:46 d2.utils.events]: [0m eta: 0:02:31  iter: 1719  total_loss: 0.669  loss_cls: 0.213  loss_box_reg: 0.367  loss_rpn_cls: 0.019  loss_rpn_loc: 0.064  time: 0.5370  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:58:57 d2.utils.events]: [0m eta: 0:02:20  iter: 1739  total_loss: 0.635  loss_cls: 0.181  loss_box_reg: 0.357  loss_rpn_cls: 0.019  loss_rpn_loc: 0.066  time: 0.5368  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:59:07 d2.utils.events]: [0m eta: 0:02:09  iter: 1759  total_loss: 0.682  loss_cls: 0.219  loss_box_reg: 0.329  loss_rpn_cls: 0.015  loss_rpn_loc: 0.057  time: 0.5367  data_time: 0.0059  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:59:18 d2.utils.events]: [0m eta: 0:01:58  iter: 1779  total_loss: 0.592  loss_cls: 0.168  loss_box_reg: 0.297  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5365  data_time: 0.0043  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:59:28 d2.utils.events]: [0m eta: 0:01:47  iter: 1799  total_loss: 0.669  loss_cls: 0.223  loss_box_reg: 0.374  loss_rpn_cls: 0.022  loss_rpn_loc: 0.074  time: 0.5364  data_time: 0.0053  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:59:39 d2.utils.events]: [0m eta: 0:01:37  iter: 1819  total_loss: 0.655  loss_cls: 0.237  loss_box_reg: 0.327  loss_rpn_cls: 0.021  loss_rpn_loc: 0.050  time: 0.5363  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 15:59:50 d2.utils.events]: [0m eta: 0:01:26  iter: 1839  total_loss: 0.661  loss_cls: 0.208  loss_box_reg: 0.379  loss_rpn_cls: 0.014  loss_rpn_loc: 0.052  time: 0.5364  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:00:01 d2.utils.events]: [0m eta: 0:01:15  iter: 1859  total_loss: 0.827  loss_cls: 0.260  loss_box_reg: 0.460  loss_rpn_cls: 0.019  loss_rpn_loc: 0.058  time: 0.5365  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:00:12 d2.utils.events]: [0m eta: 0:01:05  iter: 1879  total_loss: 0.604  loss_cls: 0.198  loss_box_reg: 0.336  loss_rpn_cls: 0.020  loss_rpn_loc: 0.052  time: 0.5366  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:00:23 d2.utils.events]: [0m eta: 0:00:54  iter: 1899  total_loss: 0.606  loss_cls: 0.208  loss_box_reg: 0.337  loss_rpn_cls: 0.017  loss_rpn_loc: 0.050  time: 0.5366  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:00:34 d2.utils.events]: [0m eta: 0:00:43  iter: 1919  total_loss: 0.570  loss_cls: 0.206  loss_box_reg: 0.316  loss_rpn_cls: 0.018  loss_rpn_loc: 0.058  time: 0.5366  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:00:44 d2.utils.events]: [0m eta: 0:00:32  iter: 1939  total_loss: 0.687  loss_cls: 0.245  loss_box_reg: 0.388  loss_rpn_cls: 0.015  loss_rpn_loc: 0.062  time: 0.5365  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:00:55 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.607  loss_cls: 0.184  loss_box_reg: 0.317  loss_rpn_cls: 0.016  loss_rpn_loc: 0.051  time: 0.5364  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:01:06 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.657  loss_cls: 0.205  loss_box_reg: 0.370  loss_rpn_cls: 0.019  loss_rpn_loc: 0.066  time: 0.5364  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 16:01:24 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 16:01:24 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 16:01:24 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 16:01:24 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.617  loss_cls: 0.193  loss_box_reg: 0.328  loss_rpn_cls: 0.024  loss_rpn_loc: 0.062  time: 0.5364  data_time: 0.0092  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:01:26 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:17:51 (0.5367 s / it)
[32m[04/15 16:01:26 d2.engine.hooks]: [0mTotal training time: 0:18:08 (0:00:17 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 16:01:34 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 16:01:34 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 16:01:34 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 16:01:37 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1246 s / img. ETA=0:02:38
[32m[04/15 16:01:42 d2.evaluation.evaluator]: [0mInference done 50/1257. 0.1218 s / img. ETA=0:02:36
[32m[04/15 16:01:47 d2.evaluation.evaluator]: [0mInference done 90/1257. 0.1220 s / img. ETA=0:02:29
[32m[04/15 16:01:52 d2.evaluation.evaluator]: [0mInference done 130/1257. 0.1220 s / img. ETA=0:02:23
[32m[04/15 16:01:57 d2.evaluation.evaluator]: [0mInference done 170/1257. 0.1217 s / img. ETA=0:02:17
[32m[04/15 16:02:02 d2.evaluation.evaluator]: [0mInference done 211/1257. 0.1215 s / img. ETA=0:02:11
[32m[04/15 16:02:07 d2.evaluation.evaluator]: [0mInference done 251/1257. 0.1216 s / img. ETA=0:02:06
[32m[04/15 16:02:12 d2.evaluation.evaluator]: [0mInference done 291/1257. 0.1219 s / img. ETA=0:02:01
[32m[04/15 16:02:17 d2.evaluation.evaluator]: [0mInference done 332/1257. 0.1217 s / img. ETA=0:01:56
[32m[04/15 16:02:22 d2.evaluation.evaluator]: [0mInference done 372/1257. 0.1217 s / img. ETA=0:01:51
[32m[04/15 16:02:27 d2.evaluation.evaluator]: [0mInference done 413/1257. 0.1216 s / img. ETA=0:01:46
[32m[04/15 16:02:32 d2.evaluation.evaluator]: [0mInference done 454/1257. 0.1214 s / img. ETA=0:01:40
[32m[04/15 16:02:37 d2.evaluation.evaluator]: [0mInference done 492/1257. 0.1213 s / img. ETA=0:01:36
[32m[04/15 16:02:43 d2.evaluation.evaluator]: [0mInference done 532/1257. 0.1213 s / img. ETA=0:01:31
[32m[04/15 16:02:48 d2.evaluation.evaluator]: [0mInference done 572/1257. 0.1214 s / img. ETA=0:01:26
[32m[04/15 16:02:53 d2.evaluation.evaluator]: [0mInference done 612/1257. 0.1214 s / img. ETA=0:01:21
[32m[04/15 16:02:58 d2.evaluation.evaluator]: [0mInference done 649/1257. 0.1215 s / img. ETA=0:01:17
[32m[04/15 16:03:03 d2.evaluation.evaluator]: [0mInference done 688/1257. 0.1215 s / img. ETA=0:01:12
[32m[04/15 16:03:08 d2.evaluation.evaluator]: [0mInference done 728/1257. 0.1215 s / img. ETA=0:01:07
[32m[04/15 16:03:13 d2.evaluation.evaluator]: [0mInference done 768/1257. 0.1215 s / img. ETA=0:01:02
[32m[04/15 16:03:18 d2.evaluation.evaluator]: [0mInference done 808/1257. 0.1216 s / img. ETA=0:00:57
[32m[04/15 16:03:23 d2.evaluation.evaluator]: [0mInference done 848/1257. 0.1217 s / img. ETA=0:00:51
[32m[04/15 16:03:28 d2.evaluation.evaluator]: [0mInference done 888/1257. 0.1216 s / img. ETA=0:00:46
[32m[04/15 16:03:33 d2.evaluation.evaluator]: [0mInference done 928/1257. 0.1217 s / img. ETA=0:00:41
[32m[04/15 16:03:38 d2.evaluation.evaluator]: [0mInference done 969/1257. 0.1217 s / img. ETA=0:00:36
[32m[04/15 16:03:43 d2.evaluation.evaluator]: [0mInference done 1010/1257. 0.1216 s / img. ETA=0:00:31
[32m[04/15 16:03:49 d2.evaluation.evaluator]: [0mInference done 1051/1257. 0.1216 s / img. ETA=0:00:26
[32m[04/15 16:03:54 d2.evaluation.evaluator]: [0mInference done 1091/1257. 0.1217 s / img. ETA=0:00:21
[32m[04/15 16:03:59 d2.evaluation.evaluator]: [0mInference done 1131/1257. 0.1217 s / img. ETA=0:00:15
[32m[04/15 16:04:04 d2.evaluation.evaluator]: [0mInference done 1171/1257. 0.1217 s / img. ETA=0:00:10
[32m[04/15 16:04:09 d2.evaluation.evaluator]: [0mInference done 1212/1257. 0.1216 s / img. ETA=0:00:05
[32m[04/15 16:04:14 d2.evaluation.evaluator]: [0mInference done 1253/1257. 0.1216 s / img. ETA=0:00:00
[32m[04/15 16:04:14 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:38.375507 (0.126498 s / img per device, on 1 devices)
[32m[04/15 16:04:14 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:32 (0.121598 s / img per device, on 1 devices)
[32m[04/15 16:04:15 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 16:04:15 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 16:04:15 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=6.22s).
Accumulating evaluation results...
DONE (t=0.78s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.196
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.394
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.167
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.125
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.239
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.411
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.102
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.250
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.254
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.190
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.294
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.463
[32m[04/15 16:04:22 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.552 | 39.417 | 16.739 | 12.481 | 23.885 | 41.112 |
[32m[04/15 16:04:22 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 27.811 | bicycle       | 8.844 | car            | 41.552 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  27  *  2000  iterations ============
4 channel input
[32m[04/15 16:04:23 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 16:04:25 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.46 seconds.
[5m[31mWARNING[0m [32m[04/15 16:04:25 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 16:04:25 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 16:04:26 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 16:04:26 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 16:04:26 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 16:04:26 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 16:04:37 d2.utils.events]: [0m eta: 0:17:21  iter: 19  total_loss: 0.692  loss_cls: 0.210  loss_box_reg: 0.409  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5342  data_time: 0.0370  lr: 0.000200  max_mem: 3070M
[32m[04/15 16:04:48 d2.utils.events]: [0m eta: 0:17:28  iter: 39  total_loss: 0.557  loss_cls: 0.170  loss_box_reg: 0.329  loss_rpn_cls: 0.013  loss_rpn_loc: 0.056  time: 0.5388  data_time: 0.0087  lr: 0.000400  max_mem: 3070M
[32m[04/15 16:05:00 d2.utils.events]: [0m eta: 0:17:35  iter: 59  total_loss: 0.612  loss_cls: 0.179  loss_box_reg: 0.331  loss_rpn_cls: 0.016  loss_rpn_loc: 0.046  time: 0.5445  data_time: 0.0081  lr: 0.000599  max_mem: 3070M
[32m[04/15 16:05:11 d2.utils.events]: [0m eta: 0:17:24  iter: 79  total_loss: 0.612  loss_cls: 0.193  loss_box_reg: 0.331  loss_rpn_cls: 0.017  loss_rpn_loc: 0.058  time: 0.5464  data_time: 0.0082  lr: 0.000799  max_mem: 3070M
[32m[04/15 16:05:22 d2.utils.events]: [0m eta: 0:17:18  iter: 99  total_loss: 0.613  loss_cls: 0.205  loss_box_reg: 0.346  loss_rpn_cls: 0.018  loss_rpn_loc: 0.051  time: 0.5466  data_time: 0.0079  lr: 0.000999  max_mem: 3070M
[32m[04/15 16:05:32 d2.utils.events]: [0m eta: 0:16:52  iter: 119  total_loss: 0.595  loss_cls: 0.181  loss_box_reg: 0.317  loss_rpn_cls: 0.017  loss_rpn_loc: 0.055  time: 0.5431  data_time: 0.0082  lr: 0.001199  max_mem: 3070M
[32m[04/15 16:05:43 d2.utils.events]: [0m eta: 0:16:51  iter: 139  total_loss: 0.523  loss_cls: 0.171  loss_box_reg: 0.304  loss_rpn_cls: 0.011  loss_rpn_loc: 0.041  time: 0.5435  data_time: 0.0079  lr: 0.001399  max_mem: 3070M
[32m[04/15 16:05:54 d2.utils.events]: [0m eta: 0:16:40  iter: 159  total_loss: 0.433  loss_cls: 0.148  loss_box_reg: 0.239  loss_rpn_cls: 0.015  loss_rpn_loc: 0.031  time: 0.5421  data_time: 0.0097  lr: 0.001598  max_mem: 3070M
[32m[04/15 16:06:05 d2.utils.events]: [0m eta: 0:16:31  iter: 179  total_loss: 0.577  loss_cls: 0.185  loss_box_reg: 0.347  loss_rpn_cls: 0.015  loss_rpn_loc: 0.043  time: 0.5431  data_time: 0.0086  lr: 0.001798  max_mem: 3070M
[32m[04/15 16:06:16 d2.utils.events]: [0m eta: 0:16:23  iter: 199  total_loss: 0.555  loss_cls: 0.177  loss_box_reg: 0.299  loss_rpn_cls: 0.015  loss_rpn_loc: 0.042  time: 0.5432  data_time: 0.0084  lr: 0.001998  max_mem: 3070M
[32m[04/15 16:06:27 d2.utils.events]: [0m eta: 0:16:07  iter: 219  total_loss: 0.689  loss_cls: 0.231  loss_box_reg: 0.377  loss_rpn_cls: 0.017  loss_rpn_loc: 0.070  time: 0.5415  data_time: 0.0092  lr: 0.002198  max_mem: 3070M
[32m[04/15 16:06:37 d2.utils.events]: [0m eta: 0:15:52  iter: 239  total_loss: 0.557  loss_cls: 0.170  loss_box_reg: 0.324  loss_rpn_cls: 0.013  loss_rpn_loc: 0.035  time: 0.5408  data_time: 0.0077  lr: 0.002398  max_mem: 3070M
[32m[04/15 16:06:48 d2.utils.events]: [0m eta: 0:15:39  iter: 259  total_loss: 0.618  loss_cls: 0.210  loss_box_reg: 0.344  loss_rpn_cls: 0.018  loss_rpn_loc: 0.050  time: 0.5397  data_time: 0.0081  lr: 0.002597  max_mem: 3070M
[32m[04/15 16:06:59 d2.utils.events]: [0m eta: 0:15:30  iter: 279  total_loss: 0.494  loss_cls: 0.158  loss_box_reg: 0.279  loss_rpn_cls: 0.010  loss_rpn_loc: 0.043  time: 0.5401  data_time: 0.0087  lr: 0.002797  max_mem: 3070M
[32m[04/15 16:07:10 d2.utils.events]: [0m eta: 0:15:19  iter: 299  total_loss: 0.658  loss_cls: 0.184  loss_box_reg: 0.368  loss_rpn_cls: 0.013  loss_rpn_loc: 0.048  time: 0.5398  data_time: 0.0075  lr: 0.002997  max_mem: 3070M
[32m[04/15 16:07:21 d2.utils.events]: [0m eta: 0:15:09  iter: 319  total_loss: 0.476  loss_cls: 0.162  loss_box_reg: 0.275  loss_rpn_cls: 0.005  loss_rpn_loc: 0.036  time: 0.5400  data_time: 0.0079  lr: 0.003197  max_mem: 3070M
[32m[04/15 16:07:31 d2.utils.events]: [0m eta: 0:14:56  iter: 339  total_loss: 0.710  loss_cls: 0.232  loss_box_reg: 0.339  loss_rpn_cls: 0.014  loss_rpn_loc: 0.069  time: 0.5389  data_time: 0.0086  lr: 0.003397  max_mem: 3070M
[32m[04/15 16:07:42 d2.utils.events]: [0m eta: 0:14:47  iter: 359  total_loss: 0.701  loss_cls: 0.219  loss_box_reg: 0.376  loss_rpn_cls: 0.013  loss_rpn_loc: 0.063  time: 0.5389  data_time: 0.0073  lr: 0.003596  max_mem: 3070M
[32m[04/15 16:07:53 d2.utils.events]: [0m eta: 0:14:35  iter: 379  total_loss: 0.603  loss_cls: 0.205  loss_box_reg: 0.335  loss_rpn_cls: 0.014  loss_rpn_loc: 0.043  time: 0.5381  data_time: 0.0079  lr: 0.003796  max_mem: 3070M
[32m[04/15 16:08:03 d2.utils.events]: [0m eta: 0:14:23  iter: 399  total_loss: 0.562  loss_cls: 0.182  loss_box_reg: 0.313  loss_rpn_cls: 0.016  loss_rpn_loc: 0.041  time: 0.5380  data_time: 0.0099  lr: 0.003996  max_mem: 3070M
[32m[04/15 16:08:14 d2.utils.events]: [0m eta: 0:14:14  iter: 419  total_loss: 0.606  loss_cls: 0.189  loss_box_reg: 0.321  loss_rpn_cls: 0.011  loss_rpn_loc: 0.050  time: 0.5383  data_time: 0.0099  lr: 0.004196  max_mem: 3070M
[32m[04/15 16:08:26 d2.utils.events]: [0m eta: 0:14:04  iter: 439  total_loss: 0.585  loss_cls: 0.196  loss_box_reg: 0.301  loss_rpn_cls: 0.018  loss_rpn_loc: 0.050  time: 0.5390  data_time: 0.0079  lr: 0.004396  max_mem: 3070M
[32m[04/15 16:08:37 d2.utils.events]: [0m eta: 0:13:53  iter: 459  total_loss: 0.569  loss_cls: 0.203  loss_box_reg: 0.307  loss_rpn_cls: 0.013  loss_rpn_loc: 0.053  time: 0.5393  data_time: 0.0098  lr: 0.004595  max_mem: 3070M
[32m[04/15 16:08:47 d2.utils.events]: [0m eta: 0:13:42  iter: 479  total_loss: 0.512  loss_cls: 0.164  loss_box_reg: 0.297  loss_rpn_cls: 0.010  loss_rpn_loc: 0.039  time: 0.5393  data_time: 0.0085  lr: 0.004795  max_mem: 3070M
[32m[04/15 16:08:58 d2.utils.events]: [0m eta: 0:13:31  iter: 499  total_loss: 0.569  loss_cls: 0.186  loss_box_reg: 0.322  loss_rpn_cls: 0.015  loss_rpn_loc: 0.054  time: 0.5391  data_time: 0.0086  lr: 0.004995  max_mem: 3070M
[32m[04/15 16:09:09 d2.utils.events]: [0m eta: 0:13:21  iter: 519  total_loss: 0.488  loss_cls: 0.134  loss_box_reg: 0.282  loss_rpn_cls: 0.015  loss_rpn_loc: 0.043  time: 0.5389  data_time: 0.0088  lr: 0.005195  max_mem: 3070M
[32m[04/15 16:09:20 d2.utils.events]: [0m eta: 0:13:10  iter: 539  total_loss: 0.668  loss_cls: 0.195  loss_box_reg: 0.374  loss_rpn_cls: 0.013  loss_rpn_loc: 0.062  time: 0.5394  data_time: 0.0076  lr: 0.005395  max_mem: 3070M
[32m[04/15 16:09:31 d2.utils.events]: [0m eta: 0:12:59  iter: 559  total_loss: 0.620  loss_cls: 0.197  loss_box_reg: 0.339  loss_rpn_cls: 0.010  loss_rpn_loc: 0.056  time: 0.5396  data_time: 0.0074  lr: 0.005594  max_mem: 3070M
[32m[04/15 16:09:42 d2.utils.events]: [0m eta: 0:12:49  iter: 579  total_loss: 0.635  loss_cls: 0.193  loss_box_reg: 0.369  loss_rpn_cls: 0.013  loss_rpn_loc: 0.062  time: 0.5396  data_time: 0.0082  lr: 0.005794  max_mem: 3070M
[32m[04/15 16:09:53 d2.utils.events]: [0m eta: 0:12:37  iter: 599  total_loss: 0.675  loss_cls: 0.216  loss_box_reg: 0.388  loss_rpn_cls: 0.019  loss_rpn_loc: 0.055  time: 0.5393  data_time: 0.0086  lr: 0.005994  max_mem: 3070M
[32m[04/15 16:10:04 d2.utils.events]: [0m eta: 0:12:27  iter: 619  total_loss: 0.588  loss_cls: 0.211  loss_box_reg: 0.328  loss_rpn_cls: 0.019  loss_rpn_loc: 0.044  time: 0.5398  data_time: 0.0074  lr: 0.006194  max_mem: 3070M
[32m[04/15 16:10:15 d2.utils.events]: [0m eta: 0:12:16  iter: 639  total_loss: 0.672  loss_cls: 0.213  loss_box_reg: 0.381  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5402  data_time: 0.0077  lr: 0.006394  max_mem: 3070M
[32m[04/15 16:10:25 d2.utils.events]: [0m eta: 0:12:05  iter: 659  total_loss: 0.671  loss_cls: 0.198  loss_box_reg: 0.368  loss_rpn_cls: 0.011  loss_rpn_loc: 0.059  time: 0.5396  data_time: 0.0074  lr: 0.006593  max_mem: 3070M
[32m[04/15 16:10:36 d2.utils.events]: [0m eta: 0:11:54  iter: 679  total_loss: 0.569  loss_cls: 0.196  loss_box_reg: 0.328  loss_rpn_cls: 0.013  loss_rpn_loc: 0.050  time: 0.5394  data_time: 0.0082  lr: 0.006793  max_mem: 3070M
[32m[04/15 16:10:47 d2.utils.events]: [0m eta: 0:11:43  iter: 699  total_loss: 0.709  loss_cls: 0.223  loss_box_reg: 0.395  loss_rpn_cls: 0.022  loss_rpn_loc: 0.056  time: 0.5396  data_time: 0.0082  lr: 0.006993  max_mem: 3070M
[32m[04/15 16:10:58 d2.utils.events]: [0m eta: 0:11:32  iter: 719  total_loss: 0.438  loss_cls: 0.135  loss_box_reg: 0.262  loss_rpn_cls: 0.013  loss_rpn_loc: 0.038  time: 0.5401  data_time: 0.0086  lr: 0.007193  max_mem: 3070M
[32m[04/15 16:11:09 d2.utils.events]: [0m eta: 0:11:22  iter: 739  total_loss: 0.549  loss_cls: 0.167  loss_box_reg: 0.320  loss_rpn_cls: 0.015  loss_rpn_loc: 0.046  time: 0.5402  data_time: 0.0083  lr: 0.007393  max_mem: 3070M
[32m[04/15 16:11:20 d2.utils.events]: [0m eta: 0:11:10  iter: 759  total_loss: 0.701  loss_cls: 0.231  loss_box_reg: 0.408  loss_rpn_cls: 0.015  loss_rpn_loc: 0.079  time: 0.5398  data_time: 0.0088  lr: 0.007592  max_mem: 3070M
[32m[04/15 16:11:31 d2.utils.events]: [0m eta: 0:11:00  iter: 779  total_loss: 0.667  loss_cls: 0.218  loss_box_reg: 0.384  loss_rpn_cls: 0.012  loss_rpn_loc: 0.066  time: 0.5400  data_time: 0.0088  lr: 0.007792  max_mem: 3070M
[32m[04/15 16:11:42 d2.utils.events]: [0m eta: 0:10:49  iter: 799  total_loss: 0.602  loss_cls: 0.182  loss_box_reg: 0.312  loss_rpn_cls: 0.010  loss_rpn_loc: 0.048  time: 0.5405  data_time: 0.0085  lr: 0.007992  max_mem: 3070M
[32m[04/15 16:11:53 d2.utils.events]: [0m eta: 0:10:39  iter: 819  total_loss: 0.681  loss_cls: 0.215  loss_box_reg: 0.352  loss_rpn_cls: 0.017  loss_rpn_loc: 0.074  time: 0.5405  data_time: 0.0068  lr: 0.008192  max_mem: 3070M
[32m[04/15 16:12:04 d2.utils.events]: [0m eta: 0:10:28  iter: 839  total_loss: 0.620  loss_cls: 0.190  loss_box_reg: 0.340  loss_rpn_cls: 0.014  loss_rpn_loc: 0.063  time: 0.5405  data_time: 0.0083  lr: 0.008392  max_mem: 3070M
[32m[04/15 16:12:15 d2.utils.events]: [0m eta: 0:10:17  iter: 859  total_loss: 0.672  loss_cls: 0.209  loss_box_reg: 0.389  loss_rpn_cls: 0.016  loss_rpn_loc: 0.064  time: 0.5404  data_time: 0.0084  lr: 0.008591  max_mem: 3070M
[32m[04/15 16:12:26 d2.utils.events]: [0m eta: 0:10:06  iter: 879  total_loss: 0.516  loss_cls: 0.173  loss_box_reg: 0.295  loss_rpn_cls: 0.012  loss_rpn_loc: 0.047  time: 0.5403  data_time: 0.0092  lr: 0.008791  max_mem: 3070M
[32m[04/15 16:12:37 d2.utils.events]: [0m eta: 0:09:56  iter: 899  total_loss: 0.671  loss_cls: 0.213  loss_box_reg: 0.364  loss_rpn_cls: 0.015  loss_rpn_loc: 0.072  time: 0.5404  data_time: 0.0080  lr: 0.008991  max_mem: 3070M
[32m[04/15 16:12:48 d2.utils.events]: [0m eta: 0:09:45  iter: 919  total_loss: 0.593  loss_cls: 0.196  loss_box_reg: 0.319  loss_rpn_cls: 0.019  loss_rpn_loc: 0.054  time: 0.5408  data_time: 0.0086  lr: 0.009191  max_mem: 3070M
[32m[04/15 16:12:59 d2.utils.events]: [0m eta: 0:09:34  iter: 939  total_loss: 0.686  loss_cls: 0.225  loss_box_reg: 0.355  loss_rpn_cls: 0.022  loss_rpn_loc: 0.050  time: 0.5409  data_time: 0.0082  lr: 0.009391  max_mem: 3070M
[32m[04/15 16:13:10 d2.utils.events]: [0m eta: 0:09:24  iter: 959  total_loss: 0.581  loss_cls: 0.179  loss_box_reg: 0.349  loss_rpn_cls: 0.015  loss_rpn_loc: 0.045  time: 0.5410  data_time: 0.0088  lr: 0.009590  max_mem: 3070M
[32m[04/15 16:13:21 d2.utils.events]: [0m eta: 0:09:13  iter: 979  total_loss: 0.610  loss_cls: 0.207  loss_box_reg: 0.325  loss_rpn_cls: 0.015  loss_rpn_loc: 0.061  time: 0.5410  data_time: 0.0079  lr: 0.009790  max_mem: 3070M
[32m[04/15 16:13:31 d2.utils.events]: [0m eta: 0:09:02  iter: 999  total_loss: 0.692  loss_cls: 0.208  loss_box_reg: 0.380  loss_rpn_cls: 0.014  loss_rpn_loc: 0.064  time: 0.5405  data_time: 0.0087  lr: 0.009990  max_mem: 3070M
[32m[04/15 16:13:42 d2.utils.events]: [0m eta: 0:08:51  iter: 1019  total_loss: 0.673  loss_cls: 0.232  loss_box_reg: 0.363  loss_rpn_cls: 0.022  loss_rpn_loc: 0.058  time: 0.5406  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:13:52 d2.utils.events]: [0m eta: 0:08:40  iter: 1039  total_loss: 0.689  loss_cls: 0.211  loss_box_reg: 0.393  loss_rpn_cls: 0.022  loss_rpn_loc: 0.064  time: 0.5402  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:14:03 d2.utils.events]: [0m eta: 0:08:29  iter: 1059  total_loss: 0.661  loss_cls: 0.247  loss_box_reg: 0.369  loss_rpn_cls: 0.014  loss_rpn_loc: 0.072  time: 0.5401  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:14:14 d2.utils.events]: [0m eta: 0:08:18  iter: 1079  total_loss: 0.641  loss_cls: 0.205  loss_box_reg: 0.342  loss_rpn_cls: 0.017  loss_rpn_loc: 0.065  time: 0.5398  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:14:24 d2.utils.events]: [0m eta: 0:08:07  iter: 1099  total_loss: 0.760  loss_cls: 0.223  loss_box_reg: 0.435  loss_rpn_cls: 0.020  loss_rpn_loc: 0.073  time: 0.5395  data_time: 0.0073  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:14:35 d2.utils.events]: [0m eta: 0:07:56  iter: 1119  total_loss: 0.587  loss_cls: 0.193  loss_box_reg: 0.304  loss_rpn_cls: 0.017  loss_rpn_loc: 0.051  time: 0.5392  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:14:46 d2.utils.events]: [0m eta: 0:07:45  iter: 1139  total_loss: 0.749  loss_cls: 0.232  loss_box_reg: 0.393  loss_rpn_cls: 0.021  loss_rpn_loc: 0.067  time: 0.5393  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:14:56 d2.utils.events]: [0m eta: 0:07:34  iter: 1159  total_loss: 0.682  loss_cls: 0.193  loss_box_reg: 0.370  loss_rpn_cls: 0.022  loss_rpn_loc: 0.075  time: 0.5391  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:15:07 d2.utils.events]: [0m eta: 0:07:24  iter: 1179  total_loss: 0.642  loss_cls: 0.199  loss_box_reg: 0.361  loss_rpn_cls: 0.025  loss_rpn_loc: 0.067  time: 0.5392  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:15:19 d2.utils.events]: [0m eta: 0:07:13  iter: 1199  total_loss: 0.659  loss_cls: 0.211  loss_box_reg: 0.368  loss_rpn_cls: 0.022  loss_rpn_loc: 0.060  time: 0.5394  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:15:29 d2.utils.events]: [0m eta: 0:07:02  iter: 1219  total_loss: 0.692  loss_cls: 0.236  loss_box_reg: 0.368  loss_rpn_cls: 0.024  loss_rpn_loc: 0.058  time: 0.5393  data_time: 0.0085  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:15:40 d2.utils.events]: [0m eta: 0:06:51  iter: 1239  total_loss: 0.688  loss_cls: 0.187  loss_box_reg: 0.386  loss_rpn_cls: 0.031  loss_rpn_loc: 0.066  time: 0.5393  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:15:51 d2.utils.events]: [0m eta: 0:06:40  iter: 1259  total_loss: 0.545  loss_cls: 0.167  loss_box_reg: 0.290  loss_rpn_cls: 0.019  loss_rpn_loc: 0.067  time: 0.5390  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:16:02 d2.utils.events]: [0m eta: 0:06:30  iter: 1279  total_loss: 0.612  loss_cls: 0.169  loss_box_reg: 0.320  loss_rpn_cls: 0.019  loss_rpn_loc: 0.061  time: 0.5391  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:16:13 d2.utils.events]: [0m eta: 0:06:19  iter: 1299  total_loss: 0.680  loss_cls: 0.197  loss_box_reg: 0.387  loss_rpn_cls: 0.021  loss_rpn_loc: 0.057  time: 0.5393  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:16:23 d2.utils.events]: [0m eta: 0:06:08  iter: 1319  total_loss: 0.711  loss_cls: 0.207  loss_box_reg: 0.396  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5392  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:16:34 d2.utils.events]: [0m eta: 0:05:57  iter: 1339  total_loss: 0.712  loss_cls: 0.217  loss_box_reg: 0.388  loss_rpn_cls: 0.019  loss_rpn_loc: 0.072  time: 0.5392  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:16:45 d2.utils.events]: [0m eta: 0:05:46  iter: 1359  total_loss: 0.605  loss_cls: 0.194  loss_box_reg: 0.324  loss_rpn_cls: 0.015  loss_rpn_loc: 0.037  time: 0.5389  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:16:56 d2.utils.events]: [0m eta: 0:05:35  iter: 1379  total_loss: 0.646  loss_cls: 0.204  loss_box_reg: 0.340  loss_rpn_cls: 0.014  loss_rpn_loc: 0.062  time: 0.5390  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:17:06 d2.utils.events]: [0m eta: 0:05:25  iter: 1399  total_loss: 0.777  loss_cls: 0.246  loss_box_reg: 0.429  loss_rpn_cls: 0.018  loss_rpn_loc: 0.069  time: 0.5388  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:17:17 d2.utils.events]: [0m eta: 0:05:14  iter: 1419  total_loss: 0.639  loss_cls: 0.178  loss_box_reg: 0.319  loss_rpn_cls: 0.016  loss_rpn_loc: 0.069  time: 0.5389  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:17:28 d2.utils.events]: [0m eta: 0:05:03  iter: 1439  total_loss: 0.621  loss_cls: 0.194  loss_box_reg: 0.359  loss_rpn_cls: 0.014  loss_rpn_loc: 0.054  time: 0.5386  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:17:39 d2.utils.events]: [0m eta: 0:04:52  iter: 1459  total_loss: 0.653  loss_cls: 0.225  loss_box_reg: 0.331  loss_rpn_cls: 0.024  loss_rpn_loc: 0.061  time: 0.5387  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:17:50 d2.utils.events]: [0m eta: 0:04:41  iter: 1479  total_loss: 0.609  loss_cls: 0.200  loss_box_reg: 0.317  loss_rpn_cls: 0.022  loss_rpn_loc: 0.069  time: 0.5387  data_time: 0.0072  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:18:01 d2.utils.events]: [0m eta: 0:04:30  iter: 1499  total_loss: 0.496  loss_cls: 0.158  loss_box_reg: 0.285  loss_rpn_cls: 0.017  loss_rpn_loc: 0.038  time: 0.5389  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:18:12 d2.utils.events]: [0m eta: 0:04:20  iter: 1519  total_loss: 0.685  loss_cls: 0.221  loss_box_reg: 0.362  loss_rpn_cls: 0.020  loss_rpn_loc: 0.065  time: 0.5391  data_time: 0.0092  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:18:23 d2.utils.events]: [0m eta: 0:04:09  iter: 1539  total_loss: 0.580  loss_cls: 0.182  loss_box_reg: 0.322  loss_rpn_cls: 0.015  loss_rpn_loc: 0.057  time: 0.5394  data_time: 0.0075  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:18:34 d2.utils.events]: [0m eta: 0:03:58  iter: 1559  total_loss: 0.654  loss_cls: 0.235  loss_box_reg: 0.348  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5396  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:18:45 d2.utils.events]: [0m eta: 0:03:47  iter: 1579  total_loss: 0.593  loss_cls: 0.174  loss_box_reg: 0.327  loss_rpn_cls: 0.028  loss_rpn_loc: 0.057  time: 0.5396  data_time: 0.0098  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:18:56 d2.utils.events]: [0m eta: 0:03:37  iter: 1599  total_loss: 0.529  loss_cls: 0.179  loss_box_reg: 0.309  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5395  data_time: 0.0083  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:19:07 d2.utils.events]: [0m eta: 0:03:26  iter: 1619  total_loss: 0.550  loss_cls: 0.166  loss_box_reg: 0.325  loss_rpn_cls: 0.012  loss_rpn_loc: 0.038  time: 0.5395  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:19:18 d2.utils.events]: [0m eta: 0:03:15  iter: 1639  total_loss: 0.702  loss_cls: 0.230  loss_box_reg: 0.386  loss_rpn_cls: 0.022  loss_rpn_loc: 0.083  time: 0.5394  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:19:28 d2.utils.events]: [0m eta: 0:03:04  iter: 1659  total_loss: 0.613  loss_cls: 0.187  loss_box_reg: 0.348  loss_rpn_cls: 0.014  loss_rpn_loc: 0.059  time: 0.5393  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:19:39 d2.utils.events]: [0m eta: 0:02:53  iter: 1679  total_loss: 0.559  loss_cls: 0.172  loss_box_reg: 0.318  loss_rpn_cls: 0.017  loss_rpn_loc: 0.037  time: 0.5389  data_time: 0.0074  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:19:50 d2.utils.events]: [0m eta: 0:02:43  iter: 1699  total_loss: 0.613  loss_cls: 0.201  loss_box_reg: 0.319  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5390  data_time: 0.0082  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:20:01 d2.utils.events]: [0m eta: 0:02:32  iter: 1719  total_loss: 0.617  loss_cls: 0.193  loss_box_reg: 0.305  loss_rpn_cls: 0.024  loss_rpn_loc: 0.077  time: 0.5390  data_time: 0.0109  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:20:11 d2.utils.events]: [0m eta: 0:02:21  iter: 1739  total_loss: 0.654  loss_cls: 0.219  loss_box_reg: 0.362  loss_rpn_cls: 0.017  loss_rpn_loc: 0.052  time: 0.5390  data_time: 0.0066  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:20:22 d2.utils.events]: [0m eta: 0:02:10  iter: 1759  total_loss: 0.628  loss_cls: 0.195  loss_box_reg: 0.348  loss_rpn_cls: 0.023  loss_rpn_loc: 0.060  time: 0.5389  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:20:33 d2.utils.events]: [0m eta: 0:01:59  iter: 1779  total_loss: 0.598  loss_cls: 0.183  loss_box_reg: 0.341  loss_rpn_cls: 0.017  loss_rpn_loc: 0.050  time: 0.5387  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:20:43 d2.utils.events]: [0m eta: 0:01:48  iter: 1799  total_loss: 0.636  loss_cls: 0.199  loss_box_reg: 0.331  loss_rpn_cls: 0.025  loss_rpn_loc: 0.057  time: 0.5386  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:20:54 d2.utils.events]: [0m eta: 0:01:37  iter: 1819  total_loss: 0.710  loss_cls: 0.218  loss_box_reg: 0.364  loss_rpn_cls: 0.018  loss_rpn_loc: 0.067  time: 0.5385  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:21:05 d2.utils.events]: [0m eta: 0:01:26  iter: 1839  total_loss: 0.678  loss_cls: 0.198  loss_box_reg: 0.357  loss_rpn_cls: 0.017  loss_rpn_loc: 0.066  time: 0.5386  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:21:16 d2.utils.events]: [0m eta: 0:01:16  iter: 1859  total_loss: 0.630  loss_cls: 0.200  loss_box_reg: 0.345  loss_rpn_cls: 0.016  loss_rpn_loc: 0.061  time: 0.5385  data_time: 0.0089  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:21:27 d2.utils.events]: [0m eta: 0:01:05  iter: 1879  total_loss: 0.616  loss_cls: 0.194  loss_box_reg: 0.313  loss_rpn_cls: 0.020  loss_rpn_loc: 0.048  time: 0.5385  data_time: 0.0087  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:21:37 d2.utils.events]: [0m eta: 0:00:54  iter: 1899  total_loss: 0.536  loss_cls: 0.170  loss_box_reg: 0.317  loss_rpn_cls: 0.020  loss_rpn_loc: 0.037  time: 0.5384  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:21:48 d2.utils.events]: [0m eta: 0:00:43  iter: 1919  total_loss: 0.642  loss_cls: 0.197  loss_box_reg: 0.363  loss_rpn_cls: 0.024  loss_rpn_loc: 0.059  time: 0.5386  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:21:59 d2.utils.events]: [0m eta: 0:00:32  iter: 1939  total_loss: 0.505  loss_cls: 0.150  loss_box_reg: 0.273  loss_rpn_cls: 0.019  loss_rpn_loc: 0.051  time: 0.5384  data_time: 0.0088  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:22:10 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.690  loss_cls: 0.228  loss_box_reg: 0.385  loss_rpn_cls: 0.019  loss_rpn_loc: 0.058  time: 0.5384  data_time: 0.0093  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:22:21 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.661  loss_cls: 0.208  loss_box_reg: 0.351  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5385  data_time: 0.0076  lr: 0.010000  max_mem: 3070M
[5m[31mWARNING[0m [32m[04/15 16:22:40 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 16:22:40 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 16:22:40 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 16:22:40 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.732  loss_cls: 0.239  loss_box_reg: 0.378  loss_rpn_cls: 0.022  loss_rpn_loc: 0.069  time: 0.5385  data_time: 0.0096  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:22:43 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:17:56 (0.5388 s / it)
[32m[04/15 16:22:43 d2.engine.hooks]: [0mTotal training time: 0:18:14 (0:00:18 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 16:22:52 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 16:22:52 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 16:22:52 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 16:22:54 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1220 s / img. ETA=0:02:35
[32m[04/15 16:22:59 d2.evaluation.evaluator]: [0mInference done 48/1257. 0.1214 s / img. ETA=0:02:43
[32m[04/15 16:23:05 d2.evaluation.evaluator]: [0mInference done 87/1257. 0.1221 s / img. ETA=0:02:35
[32m[04/15 16:23:10 d2.evaluation.evaluator]: [0mInference done 126/1257. 0.1234 s / img. ETA=0:02:29
[32m[04/15 16:23:15 d2.evaluation.evaluator]: [0mInference done 165/1257. 0.1228 s / img. ETA=0:02:23
[32m[04/15 16:23:20 d2.evaluation.evaluator]: [0mInference done 206/1257. 0.1226 s / img. ETA=0:02:17
[32m[04/15 16:23:25 d2.evaluation.evaluator]: [0mInference done 246/1257. 0.1226 s / img. ETA=0:02:11
[32m[04/15 16:23:30 d2.evaluation.evaluator]: [0mInference done 286/1257. 0.1225 s / img. ETA=0:02:05
[32m[04/15 16:23:35 d2.evaluation.evaluator]: [0mInference done 327/1257. 0.1224 s / img. ETA=0:01:59
[32m[04/15 16:23:40 d2.evaluation.evaluator]: [0mInference done 367/1257. 0.1225 s / img. ETA=0:01:54
[32m[04/15 16:23:45 d2.evaluation.evaluator]: [0mInference done 407/1257. 0.1224 s / img. ETA=0:01:48
[32m[04/15 16:23:50 d2.evaluation.evaluator]: [0mInference done 446/1257. 0.1223 s / img. ETA=0:01:43
[32m[04/15 16:23:55 d2.evaluation.evaluator]: [0mInference done 482/1257. 0.1222 s / img. ETA=0:01:40
[32m[04/15 16:24:00 d2.evaluation.evaluator]: [0mInference done 520/1257. 0.1221 s / img. ETA=0:01:35
[32m[04/15 16:24:05 d2.evaluation.evaluator]: [0mInference done 559/1257. 0.1221 s / img. ETA=0:01:30
[32m[04/15 16:24:11 d2.evaluation.evaluator]: [0mInference done 599/1257. 0.1221 s / img. ETA=0:01:25
[32m[04/15 16:24:16 d2.evaluation.evaluator]: [0mInference done 636/1257. 0.1221 s / img. ETA=0:01:20
[32m[04/15 16:24:21 d2.evaluation.evaluator]: [0mInference done 676/1257. 0.1221 s / img. ETA=0:01:15
[32m[04/15 16:24:26 d2.evaluation.evaluator]: [0mInference done 715/1257. 0.1221 s / img. ETA=0:01:10
[32m[04/15 16:24:31 d2.evaluation.evaluator]: [0mInference done 756/1257. 0.1221 s / img. ETA=0:01:04
[32m[04/15 16:24:36 d2.evaluation.evaluator]: [0mInference done 797/1257. 0.1220 s / img. ETA=0:00:59
[32m[04/15 16:24:41 d2.evaluation.evaluator]: [0mInference done 837/1257. 0.1220 s / img. ETA=0:00:54
[32m[04/15 16:24:46 d2.evaluation.evaluator]: [0mInference done 878/1257. 0.1220 s / img. ETA=0:00:48
[32m[04/15 16:24:51 d2.evaluation.evaluator]: [0mInference done 918/1257. 0.1220 s / img. ETA=0:00:43
[32m[04/15 16:24:56 d2.evaluation.evaluator]: [0mInference done 958/1257. 0.1220 s / img. ETA=0:00:38
[32m[04/15 16:25:01 d2.evaluation.evaluator]: [0mInference done 998/1257. 0.1220 s / img. ETA=0:00:33
[32m[04/15 16:25:06 d2.evaluation.evaluator]: [0mInference done 1038/1257. 0.1220 s / img. ETA=0:00:28
[32m[04/15 16:25:11 d2.evaluation.evaluator]: [0mInference done 1078/1257. 0.1220 s / img. ETA=0:00:22
[32m[04/15 16:25:16 d2.evaluation.evaluator]: [0mInference done 1118/1257. 0.1220 s / img. ETA=0:00:17
[32m[04/15 16:25:21 d2.evaluation.evaluator]: [0mInference done 1157/1257. 0.1220 s / img. ETA=0:00:12
[32m[04/15 16:25:26 d2.evaluation.evaluator]: [0mInference done 1197/1257. 0.1220 s / img. ETA=0:00:07
[32m[04/15 16:25:31 d2.evaluation.evaluator]: [0mInference done 1238/1257. 0.1220 s / img. ETA=0:00:02
[32m[04/15 16:25:34 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:40.280352 (0.128019 s / img per device, on 1 devices)
[32m[04/15 16:25:34 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:32 (0.122008 s / img per device, on 1 devices)
[32m[04/15 16:25:34 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 16:25:34 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 16:25:34 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.05s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=6.06s).
Accumulating evaluation results...
DONE (t=0.76s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.204
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.418
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.171
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.254
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.126
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.285
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.288
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.189
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.449
[32m[04/15 16:25:42 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 20.362 | 41.764 | 17.137 | 12.935 | 25.356 | 40.185 |
[32m[04/15 16:25:42 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 28.748 | bicycle       | 9.380 | car            | 41.804 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 1.516 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  28  *  2000  iterations ============
4 channel input
[32m[04/15 16:25:43 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 16:25:44 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.28 seconds.
[5m[31mWARNING[0m [32m[04/15 16:25:44 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 16:25:45 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 16:25:45 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 16:25:46 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 16:25:46 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 16:25:46 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 16:25:58 d2.utils.events]: [0m eta: 0:17:52  iter: 19  total_loss: 0.694  loss_cls: 0.232  loss_box_reg: 0.364  loss_rpn_cls: 0.017  loss_rpn_loc: 0.059  time: 0.5444  data_time: 0.0532  lr: 0.000200  max_mem: 3070M
[32m[04/15 16:26:09 d2.utils.events]: [0m eta: 0:17:41  iter: 39  total_loss: 0.526  loss_cls: 0.146  loss_box_reg: 0.280  loss_rpn_cls: 0.017  loss_rpn_loc: 0.043  time: 0.5426  data_time: 0.0080  lr: 0.000400  max_mem: 3070M
[32m[04/15 16:26:19 d2.utils.events]: [0m eta: 0:17:24  iter: 59  total_loss: 0.670  loss_cls: 0.206  loss_box_reg: 0.361  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5366  data_time: 0.0072  lr: 0.000599  max_mem: 3070M
[32m[04/15 16:26:30 d2.utils.events]: [0m eta: 0:17:18  iter: 79  total_loss: 0.631  loss_cls: 0.200  loss_box_reg: 0.335  loss_rpn_cls: 0.016  loss_rpn_loc: 0.063  time: 0.5396  data_time: 0.0082  lr: 0.000799  max_mem: 3070M
[32m[04/15 16:26:41 d2.utils.events]: [0m eta: 0:17:08  iter: 99  total_loss: 0.626  loss_cls: 0.197  loss_box_reg: 0.338  loss_rpn_cls: 0.013  loss_rpn_loc: 0.041  time: 0.5397  data_time: 0.0080  lr: 0.000999  max_mem: 3070M
[32m[04/15 16:26:52 d2.utils.events]: [0m eta: 0:16:56  iter: 119  total_loss: 0.588  loss_cls: 0.196  loss_box_reg: 0.314  loss_rpn_cls: 0.016  loss_rpn_loc: 0.037  time: 0.5381  data_time: 0.0080  lr: 0.001199  max_mem: 3070M
[32m[04/15 16:27:03 d2.utils.events]: [0m eta: 0:16:41  iter: 139  total_loss: 0.571  loss_cls: 0.169  loss_box_reg: 0.329  loss_rpn_cls: 0.013  loss_rpn_loc: 0.046  time: 0.5372  data_time: 0.0070  lr: 0.001399  max_mem: 3070M
[32m[04/15 16:27:14 d2.utils.events]: [0m eta: 0:16:33  iter: 159  total_loss: 0.586  loss_cls: 0.186  loss_box_reg: 0.341  loss_rpn_cls: 0.012  loss_rpn_loc: 0.049  time: 0.5385  data_time: 0.0081  lr: 0.001598  max_mem: 3070M
[32m[04/15 16:27:24 d2.utils.events]: [0m eta: 0:16:22  iter: 179  total_loss: 0.554  loss_cls: 0.179  loss_box_reg: 0.328  loss_rpn_cls: 0.014  loss_rpn_loc: 0.033  time: 0.5384  data_time: 0.0067  lr: 0.001798  max_mem: 3070M
[32m[04/15 16:27:35 d2.utils.events]: [0m eta: 0:16:11  iter: 199  total_loss: 0.574  loss_cls: 0.176  loss_box_reg: 0.347  loss_rpn_cls: 0.014  loss_rpn_loc: 0.049  time: 0.5382  data_time: 0.0083  lr: 0.001998  max_mem: 3070M
[32m[04/15 16:27:46 d2.utils.events]: [0m eta: 0:16:02  iter: 219  total_loss: 0.563  loss_cls: 0.185  loss_box_reg: 0.314  loss_rpn_cls: 0.017  loss_rpn_loc: 0.046  time: 0.5392  data_time: 0.0077  lr: 0.002198  max_mem: 3070M
[32m[04/15 16:27:57 d2.utils.events]: [0m eta: 0:15:52  iter: 239  total_loss: 0.649  loss_cls: 0.196  loss_box_reg: 0.345  loss_rpn_cls: 0.018  loss_rpn_loc: 0.046  time: 0.5396  data_time: 0.0076  lr: 0.002398  max_mem: 3070M
[32m[04/15 16:28:08 d2.utils.events]: [0m eta: 0:15:40  iter: 259  total_loss: 0.638  loss_cls: 0.203  loss_box_reg: 0.349  loss_rpn_cls: 0.018  loss_rpn_loc: 0.054  time: 0.5379  data_time: 0.0080  lr: 0.002597  max_mem: 3070M
[32m[04/15 16:28:19 d2.utils.events]: [0m eta: 0:15:30  iter: 279  total_loss: 0.679  loss_cls: 0.232  loss_box_reg: 0.360  loss_rpn_cls: 0.017  loss_rpn_loc: 0.055  time: 0.5387  data_time: 0.0087  lr: 0.002797  max_mem: 3070M
[32m[04/15 16:28:30 d2.utils.events]: [0m eta: 0:15:19  iter: 299  total_loss: 0.645  loss_cls: 0.185  loss_box_reg: 0.361  loss_rpn_cls: 0.019  loss_rpn_loc: 0.054  time: 0.5386  data_time: 0.0077  lr: 0.002997  max_mem: 3070M
[32m[04/15 16:28:40 d2.utils.events]: [0m eta: 0:15:06  iter: 319  total_loss: 0.485  loss_cls: 0.159  loss_box_reg: 0.263  loss_rpn_cls: 0.011  loss_rpn_loc: 0.050  time: 0.5372  data_time: 0.0079  lr: 0.003197  max_mem: 3070M
[32m[04/15 16:28:51 d2.utils.events]: [0m eta: 0:14:54  iter: 339  total_loss: 0.483  loss_cls: 0.146  loss_box_reg: 0.276  loss_rpn_cls: 0.012  loss_rpn_loc: 0.039  time: 0.5367  data_time: 0.0084  lr: 0.003397  max_mem: 3070M
[32m[04/15 16:29:01 d2.utils.events]: [0m eta: 0:14:42  iter: 359  total_loss: 0.601  loss_cls: 0.183  loss_box_reg: 0.333  loss_rpn_cls: 0.010  loss_rpn_loc: 0.062  time: 0.5367  data_time: 0.0080  lr: 0.003596  max_mem: 3070M
[32m[04/15 16:29:12 d2.utils.events]: [0m eta: 0:14:31  iter: 379  total_loss: 0.598  loss_cls: 0.194  loss_box_reg: 0.325  loss_rpn_cls: 0.008  loss_rpn_loc: 0.056  time: 0.5366  data_time: 0.0086  lr: 0.003796  max_mem: 3070M
[32m[04/15 16:29:23 d2.utils.events]: [0m eta: 0:14:21  iter: 399  total_loss: 0.603  loss_cls: 0.186  loss_box_reg: 0.344  loss_rpn_cls: 0.009  loss_rpn_loc: 0.047  time: 0.5367  data_time: 0.0090  lr: 0.003996  max_mem: 3070M
[32m[04/15 16:29:34 d2.utils.events]: [0m eta: 0:14:09  iter: 419  total_loss: 0.499  loss_cls: 0.157  loss_box_reg: 0.281  loss_rpn_cls: 0.010  loss_rpn_loc: 0.038  time: 0.5367  data_time: 0.0081  lr: 0.004196  max_mem: 3070M
[32m[04/15 16:29:45 d2.utils.events]: [0m eta: 0:13:59  iter: 439  total_loss: 0.627  loss_cls: 0.193  loss_box_reg: 0.381  loss_rpn_cls: 0.012  loss_rpn_loc: 0.058  time: 0.5366  data_time: 0.0088  lr: 0.004396  max_mem: 3070M
[32m[04/15 16:29:55 d2.utils.events]: [0m eta: 0:13:48  iter: 459  total_loss: 0.722  loss_cls: 0.226  loss_box_reg: 0.404  loss_rpn_cls: 0.021  loss_rpn_loc: 0.060  time: 0.5358  data_time: 0.0084  lr: 0.004595  max_mem: 3070M
[32m[04/15 16:30:06 d2.utils.events]: [0m eta: 0:13:37  iter: 479  total_loss: 0.582  loss_cls: 0.193  loss_box_reg: 0.315  loss_rpn_cls: 0.017  loss_rpn_loc: 0.042  time: 0.5359  data_time: 0.0075  lr: 0.004795  max_mem: 3070M
[32m[04/15 16:30:17 d2.utils.events]: [0m eta: 0:13:26  iter: 499  total_loss: 0.670  loss_cls: 0.214  loss_box_reg: 0.376  loss_rpn_cls: 0.011  loss_rpn_loc: 0.058  time: 0.5357  data_time: 0.0082  lr: 0.004995  max_mem: 3070M
[32m[04/15 16:30:28 d2.utils.events]: [0m eta: 0:13:17  iter: 519  total_loss: 0.620  loss_cls: 0.189  loss_box_reg: 0.309  loss_rpn_cls: 0.015  loss_rpn_loc: 0.055  time: 0.5368  data_time: 0.0090  lr: 0.005195  max_mem: 3070M
[32m[04/15 16:30:38 d2.utils.events]: [0m eta: 0:13:05  iter: 539  total_loss: 0.494  loss_cls: 0.159  loss_box_reg: 0.290  loss_rpn_cls: 0.013  loss_rpn_loc: 0.044  time: 0.5359  data_time: 0.0079  lr: 0.005395  max_mem: 3070M
[32m[04/15 16:30:49 d2.utils.events]: [0m eta: 0:12:55  iter: 559  total_loss: 0.554  loss_cls: 0.195  loss_box_reg: 0.288  loss_rpn_cls: 0.012  loss_rpn_loc: 0.041  time: 0.5358  data_time: 0.0083  lr: 0.005594  max_mem: 3070M
[32m[04/15 16:31:00 d2.utils.events]: [0m eta: 0:12:43  iter: 579  total_loss: 0.578  loss_cls: 0.178  loss_box_reg: 0.334  loss_rpn_cls: 0.010  loss_rpn_loc: 0.045  time: 0.5349  data_time: 0.0082  lr: 0.005794  max_mem: 3070M
[32m[04/15 16:31:10 d2.utils.events]: [0m eta: 0:12:32  iter: 599  total_loss: 0.612  loss_cls: 0.185  loss_box_reg: 0.349  loss_rpn_cls: 0.010  loss_rpn_loc: 0.058  time: 0.5347  data_time: 0.0079  lr: 0.005994  max_mem: 3070M
[32m[04/15 16:31:21 d2.utils.events]: [0m eta: 0:12:21  iter: 619  total_loss: 0.533  loss_cls: 0.165  loss_box_reg: 0.283  loss_rpn_cls: 0.017  loss_rpn_loc: 0.037  time: 0.5346  data_time: 0.0083  lr: 0.006194  max_mem: 3070M
[32m[04/15 16:31:32 d2.utils.events]: [0m eta: 0:12:10  iter: 639  total_loss: 0.618  loss_cls: 0.178  loss_box_reg: 0.336  loss_rpn_cls: 0.019  loss_rpn_loc: 0.071  time: 0.5346  data_time: 0.0072  lr: 0.006394  max_mem: 3070M
[32m[04/15 16:31:42 d2.utils.events]: [0m eta: 0:12:00  iter: 659  total_loss: 0.520  loss_cls: 0.180  loss_box_reg: 0.302  loss_rpn_cls: 0.015  loss_rpn_loc: 0.042  time: 0.5345  data_time: 0.0080  lr: 0.006593  max_mem: 3070M
[32m[04/15 16:31:53 d2.utils.events]: [0m eta: 0:11:50  iter: 679  total_loss: 0.585  loss_cls: 0.179  loss_box_reg: 0.344  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5351  data_time: 0.0085  lr: 0.006793  max_mem: 3070M
[32m[04/15 16:32:04 d2.utils.events]: [0m eta: 0:11:38  iter: 699  total_loss: 0.630  loss_cls: 0.193  loss_box_reg: 0.331  loss_rpn_cls: 0.016  loss_rpn_loc: 0.056  time: 0.5350  data_time: 0.0081  lr: 0.006993  max_mem: 3070M
[32m[04/15 16:32:15 d2.utils.events]: [0m eta: 0:11:28  iter: 719  total_loss: 0.508  loss_cls: 0.164  loss_box_reg: 0.286  loss_rpn_cls: 0.013  loss_rpn_loc: 0.039  time: 0.5353  data_time: 0.0076  lr: 0.007193  max_mem: 3070M
[32m[04/15 16:32:27 d2.utils.events]: [0m eta: 0:11:18  iter: 739  total_loss: 0.634  loss_cls: 0.204  loss_box_reg: 0.328  loss_rpn_cls: 0.016  loss_rpn_loc: 0.045  time: 0.5363  data_time: 0.0509  lr: 0.007393  max_mem: 3070M
[32m[04/15 16:32:39 d2.utils.events]: [0m eta: 0:11:08  iter: 759  total_loss: 0.678  loss_cls: 0.208  loss_box_reg: 0.391  loss_rpn_cls: 0.016  loss_rpn_loc: 0.057  time: 0.5375  data_time: 0.0594  lr: 0.007592  max_mem: 3070M
[32m[04/15 16:32:49 d2.utils.events]: [0m eta: 0:10:57  iter: 779  total_loss: 0.645  loss_cls: 0.202  loss_box_reg: 0.370  loss_rpn_cls: 0.013  loss_rpn_loc: 0.056  time: 0.5375  data_time: 0.0076  lr: 0.007792  max_mem: 3070M
[32m[04/15 16:33:00 d2.utils.events]: [0m eta: 0:10:46  iter: 799  total_loss: 0.604  loss_cls: 0.188  loss_box_reg: 0.342  loss_rpn_cls: 0.015  loss_rpn_loc: 0.052  time: 0.5372  data_time: 0.0091  lr: 0.007992  max_mem: 3070M
[32m[04/15 16:33:11 d2.utils.events]: [0m eta: 0:10:36  iter: 819  total_loss: 0.519  loss_cls: 0.176  loss_box_reg: 0.300  loss_rpn_cls: 0.014  loss_rpn_loc: 0.043  time: 0.5374  data_time: 0.0098  lr: 0.008192  max_mem: 3070M
[32m[04/15 16:33:22 d2.utils.events]: [0m eta: 0:10:24  iter: 839  total_loss: 0.711  loss_cls: 0.238  loss_box_reg: 0.391  loss_rpn_cls: 0.015  loss_rpn_loc: 0.065  time: 0.5371  data_time: 0.0083  lr: 0.008392  max_mem: 3070M
[32m[04/15 16:33:32 d2.utils.events]: [0m eta: 0:10:13  iter: 859  total_loss: 0.479  loss_cls: 0.159  loss_box_reg: 0.247  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5368  data_time: 0.0073  lr: 0.008591  max_mem: 3070M
[32m[04/15 16:33:43 d2.utils.events]: [0m eta: 0:10:02  iter: 879  total_loss: 0.544  loss_cls: 0.170  loss_box_reg: 0.279  loss_rpn_cls: 0.011  loss_rpn_loc: 0.048  time: 0.5371  data_time: 0.0096  lr: 0.008791  max_mem: 3070M
[32m[04/15 16:33:55 d2.utils.events]: [0m eta: 0:09:53  iter: 899  total_loss: 0.569  loss_cls: 0.185  loss_box_reg: 0.329  loss_rpn_cls: 0.016  loss_rpn_loc: 0.045  time: 0.5376  data_time: 0.0083  lr: 0.008991  max_mem: 3070M
[32m[04/15 16:34:06 d2.utils.events]: [0m eta: 0:09:42  iter: 919  total_loss: 0.566  loss_cls: 0.177  loss_box_reg: 0.328  loss_rpn_cls: 0.013  loss_rpn_loc: 0.047  time: 0.5378  data_time: 0.0080  lr: 0.009191  max_mem: 3070M
[32m[04/15 16:34:16 d2.utils.events]: [0m eta: 0:09:31  iter: 939  total_loss: 0.516  loss_cls: 0.179  loss_box_reg: 0.284  loss_rpn_cls: 0.014  loss_rpn_loc: 0.036  time: 0.5378  data_time: 0.0087  lr: 0.009391  max_mem: 3070M
[32m[04/15 16:34:27 d2.utils.events]: [0m eta: 0:09:20  iter: 959  total_loss: 0.671  loss_cls: 0.209  loss_box_reg: 0.354  loss_rpn_cls: 0.020  loss_rpn_loc: 0.065  time: 0.5378  data_time: 0.0074  lr: 0.009590  max_mem: 3070M
[32m[04/15 16:34:38 d2.utils.events]: [0m eta: 0:09:10  iter: 979  total_loss: 0.623  loss_cls: 0.188  loss_box_reg: 0.338  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5378  data_time: 0.0073  lr: 0.009790  max_mem: 3070M
[32m[04/15 16:34:49 d2.utils.events]: [0m eta: 0:08:59  iter: 999  total_loss: 0.634  loss_cls: 0.195  loss_box_reg: 0.374  loss_rpn_cls: 0.027  loss_rpn_loc: 0.056  time: 0.5376  data_time: 0.0074  lr: 0.009990  max_mem: 3070M
[32m[04/15 16:35:00 d2.utils.events]: [0m eta: 0:08:48  iter: 1019  total_loss: 0.656  loss_cls: 0.201  loss_box_reg: 0.347  loss_rpn_cls: 0.030  loss_rpn_loc: 0.068  time: 0.5377  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:35:10 d2.utils.events]: [0m eta: 0:08:37  iter: 1039  total_loss: 0.519  loss_cls: 0.158  loss_box_reg: 0.266  loss_rpn_cls: 0.024  loss_rpn_loc: 0.058  time: 0.5375  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:35:21 d2.utils.events]: [0m eta: 0:08:26  iter: 1059  total_loss: 0.624  loss_cls: 0.189  loss_box_reg: 0.322  loss_rpn_cls: 0.027  loss_rpn_loc: 0.066  time: 0.5374  data_time: 0.0096  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:35:32 d2.utils.events]: [0m eta: 0:08:15  iter: 1079  total_loss: 0.611  loss_cls: 0.176  loss_box_reg: 0.316  loss_rpn_cls: 0.021  loss_rpn_loc: 0.056  time: 0.5374  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:35:43 d2.utils.events]: [0m eta: 0:08:04  iter: 1099  total_loss: 0.526  loss_cls: 0.159  loss_box_reg: 0.280  loss_rpn_cls: 0.021  loss_rpn_loc: 0.050  time: 0.5373  data_time: 0.0077  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:35:53 d2.utils.events]: [0m eta: 0:07:53  iter: 1119  total_loss: 0.659  loss_cls: 0.218  loss_box_reg: 0.346  loss_rpn_cls: 0.027  loss_rpn_loc: 0.067  time: 0.5372  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:36:04 d2.utils.events]: [0m eta: 0:07:43  iter: 1139  total_loss: 0.628  loss_cls: 0.194  loss_box_reg: 0.355  loss_rpn_cls: 0.027  loss_rpn_loc: 0.052  time: 0.5371  data_time: 0.0070  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:36:15 d2.utils.events]: [0m eta: 0:07:32  iter: 1159  total_loss: 0.552  loss_cls: 0.183  loss_box_reg: 0.307  loss_rpn_cls: 0.014  loss_rpn_loc: 0.055  time: 0.5373  data_time: 0.0090  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:36:26 d2.utils.events]: [0m eta: 0:07:21  iter: 1179  total_loss: 0.649  loss_cls: 0.210  loss_box_reg: 0.360  loss_rpn_cls: 0.023  loss_rpn_loc: 0.060  time: 0.5373  data_time: 0.0086  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:36:37 d2.utils.events]: [0m eta: 0:07:10  iter: 1199  total_loss: 0.672  loss_cls: 0.220  loss_box_reg: 0.350  loss_rpn_cls: 0.033  loss_rpn_loc: 0.064  time: 0.5371  data_time: 0.0081  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:36:47 d2.utils.events]: [0m eta: 0:06:59  iter: 1219  total_loss: 0.607  loss_cls: 0.198  loss_box_reg: 0.296  loss_rpn_cls: 0.022  loss_rpn_loc: 0.058  time: 0.5369  data_time: 0.0084  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:36:58 d2.utils.events]: [0m eta: 0:06:48  iter: 1239  total_loss: 0.704  loss_cls: 0.243  loss_box_reg: 0.371  loss_rpn_cls: 0.026  loss_rpn_loc: 0.058  time: 0.5368  data_time: 0.0078  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:37:08 d2.utils.events]: [0m eta: 0:06:38  iter: 1259  total_loss: 0.613  loss_cls: 0.179  loss_box_reg: 0.332  loss_rpn_cls: 0.021  loss_rpn_loc: 0.063  time: 0.5366  data_time: 0.0079  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:37:19 d2.utils.events]: [0m eta: 0:06:26  iter: 1279  total_loss: 0.753  loss_cls: 0.238  loss_box_reg: 0.394  loss_rpn_cls: 0.030  loss_rpn_loc: 0.061  time: 0.5362  data_time: 0.0080  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:37:30 d2.utils.events]: [0m eta: 0:06:16  iter: 1299  total_loss: 0.616  loss_cls: 0.221  loss_box_reg: 0.324  loss_rpn_cls: 0.024  loss_rpn_loc: 0.047  time: 0.5362  data_time: 0.0090  lr: 0.010000  max_mem: 3070M
[32m[04/15 16:37:40 d2.utils.events]: [0m eta: 0:06:05  iter: 1319  total_loss: 0.693  loss_cls: 0.203  loss_box_reg: 0.374  loss_rpn_cls: 0.022  loss_rpn_loc: 0.058  time: 0.5361  data_time: 0.0069  lr: 0.010000  max_mem: 3070M
