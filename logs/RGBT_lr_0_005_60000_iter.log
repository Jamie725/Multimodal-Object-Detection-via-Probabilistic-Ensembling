../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
4 channel input
[32m[04/15 03:28:05 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 03:28:07 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.18 seconds.
[5m[31mWARNING[0m [32m[04/15 03:28:07 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 03:28:07 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 03:28:07 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 03:28:08 d2.data.build]: [0mDistribution of instances among all 79 categories:
[36m|   category    | #instances   |   category    | #instances   |   category    | #instances   |
|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|
|    person     | 21744        |    bicycle    | 3806         |      car      | 39372        |
|  motorcycle   | 0            |   airplane    | 0            |      bus      | 0            |
|     train     | 0            |     truck     | 0            |     boat      | 0            |
| traffic light | 0            | fire hydrant  | 0            |   stop sign   | 0            |
| parking meter | 0            |     bench     | 0            |     bird      | 0            |
|      cat      | 0            |      dog      | 219          |     horse     | 0            |
|     sheep     | 0            |      cow      | 0            |   elephant    | 0            |
|     bear      | 0            |     zebra     | 0            |    giraffe    | 0            |
|   backpack    | 0            |   umbrella    | 0            |    handbag    | 0            |
|      tie      | 0            |   suitcase    | 0            |    frisbee    | 0            |
|     skis      | 0            |   snowboard   | 0            |  sports ball  | 0            |
|     kite      | 0            | baseball bat  | 0            | baseball gl.. | 0            |
|  skateboard   | 0            |   surfboard   | 0            | tennis racket | 0            |
|    bottle     | 0            |  wine glass   | 0            |      cup      | 0            |
|     fork      | 0            |     knife     | 0            |     spoon     | 0            |
|     bowl      | 0            |    banana     | 0            |     apple     | 0            |
|   sandwich    | 0            |    orange     | 0            |   broccoli    | 0            |
|    carrot     | 0            |    hot dog    | 0            |     pizza     | 0            |
|     donut     | 0            |     cake      | 0            |     chair     | 0            |
|     couch     | 0            | potted plant  | 0            |      bed      | 0            |
| dining table  | 0            |    toilet     | 0            |      tv       | 0            |
|    laptop     | 0            |     mouse     | 0            |    remote     | 0            |
|   keyboard    | 0            |  cell phone   | 0            |   microwave   | 0            |
|     oven      | 0            |     bike      | 0            |    hydrant    | 0            |
|     motor     | 0            |     rider     | 0            |     light     | 0            |
|     sign      | 0            | motor vehicle | 0            |  human face   | 0            |
|  hair drier   | 0            |               |              |               |              |
|     total     | 65141        |               |              |               |              |[0m
[32m[04/15 03:28:08 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 03:28:08 d2.data.build]: [0mUsing training sampler TrainingSampler
============== The  0  *  2000  iterations ============
[32m[04/15 03:28:13 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 03:28:35 d2.utils.events]: [0m eta: 0:19:26  iter: 19  total_loss: 1.101  loss_cls: 0.364  loss_box_reg: 0.154  loss_rpn_cls: 0.414  loss_rpn_loc: 0.107  time: 0.5991  data_time: 0.0535  lr: 0.000100  max_mem: 3061M
[32m[04/15 03:28:47 d2.utils.events]: [0m eta: 0:18:42  iter: 39  total_loss: 0.877  loss_cls: 0.295  loss_box_reg: 0.197  loss_rpn_cls: 0.182  loss_rpn_loc: 0.096  time: 0.5909  data_time: 0.0092  lr: 0.000200  max_mem: 3061M
[32m[04/15 03:28:58 d2.utils.events]: [0m eta: 0:18:19  iter: 59  total_loss: 0.930  loss_cls: 0.292  loss_box_reg: 0.270  loss_rpn_cls: 0.146  loss_rpn_loc: 0.113  time: 0.5724  data_time: 0.0083  lr: 0.000300  max_mem: 3061M
[32m[04/15 03:29:09 d2.utils.events]: [0m eta: 0:17:59  iter: 79  total_loss: 0.670  loss_cls: 0.242  loss_box_reg: 0.197  loss_rpn_cls: 0.139  loss_rpn_loc: 0.074  time: 0.5720  data_time: 0.0077  lr: 0.000400  max_mem: 3061M
[32m[04/15 03:29:21 d2.utils.events]: [0m eta: 0:17:57  iter: 99  total_loss: 0.788  loss_cls: 0.277  loss_box_reg: 0.274  loss_rpn_cls: 0.135  loss_rpn_loc: 0.116  time: 0.5737  data_time: 0.0084  lr: 0.000500  max_mem: 3061M
[32m[04/15 03:29:32 d2.utils.events]: [0m eta: 0:17:38  iter: 119  total_loss: 0.774  loss_cls: 0.299  loss_box_reg: 0.264  loss_rpn_cls: 0.121  loss_rpn_loc: 0.099  time: 0.5668  data_time: 0.0083  lr: 0.000599  max_mem: 3061M
[32m[04/15 03:29:44 d2.utils.events]: [0m eta: 0:17:29  iter: 139  total_loss: 0.816  loss_cls: 0.288  loss_box_reg: 0.280  loss_rpn_cls: 0.129  loss_rpn_loc: 0.128  time: 0.5704  data_time: 0.0081  lr: 0.000699  max_mem: 3061M
[32m[04/15 03:29:55 d2.utils.events]: [0m eta: 0:17:17  iter: 159  total_loss: 0.873  loss_cls: 0.279  loss_box_reg: 0.316  loss_rpn_cls: 0.115  loss_rpn_loc: 0.112  time: 0.5693  data_time: 0.0086  lr: 0.000799  max_mem: 3061M
[32m[04/15 03:30:06 d2.utils.events]: [0m eta: 0:16:52  iter: 179  total_loss: 0.850  loss_cls: 0.304  loss_box_reg: 0.281  loss_rpn_cls: 0.096  loss_rpn_loc: 0.099  time: 0.5637  data_time: 0.0081  lr: 0.000899  max_mem: 3061M
[32m[04/15 03:30:18 d2.utils.events]: [0m eta: 0:16:50  iter: 199  total_loss: 0.751  loss_cls: 0.290  loss_box_reg: 0.258  loss_rpn_cls: 0.088  loss_rpn_loc: 0.077  time: 0.5698  data_time: 0.0077  lr: 0.000999  max_mem: 3061M
[32m[04/15 03:30:29 d2.utils.events]: [0m eta: 0:16:31  iter: 219  total_loss: 0.854  loss_cls: 0.318  loss_box_reg: 0.324  loss_rpn_cls: 0.107  loss_rpn_loc: 0.098  time: 0.5652  data_time: 0.0083  lr: 0.001099  max_mem: 3061M
[32m[04/15 03:30:40 d2.utils.events]: [0m eta: 0:16:17  iter: 239  total_loss: 0.823  loss_cls: 0.278  loss_box_reg: 0.337  loss_rpn_cls: 0.100  loss_rpn_loc: 0.100  time: 0.5638  data_time: 0.0078  lr: 0.001199  max_mem: 3061M
[32m[04/15 03:30:52 d2.utils.events]: [0m eta: 0:16:11  iter: 259  total_loss: 0.984  loss_cls: 0.320  loss_box_reg: 0.402  loss_rpn_cls: 0.089  loss_rpn_loc: 0.102  time: 0.5674  data_time: 0.0084  lr: 0.001299  max_mem: 3061M
[32m[04/15 03:31:02 d2.utils.events]: [0m eta: 0:15:53  iter: 279  total_loss: 0.817  loss_cls: 0.283  loss_box_reg: 0.335  loss_rpn_cls: 0.088  loss_rpn_loc: 0.073  time: 0.5642  data_time: 0.0074  lr: 0.001399  max_mem: 3061M
[32m[04/15 03:31:14 d2.utils.events]: [0m eta: 0:15:45  iter: 299  total_loss: 0.880  loss_cls: 0.319  loss_box_reg: 0.389  loss_rpn_cls: 0.074  loss_rpn_loc: 0.074  time: 0.5654  data_time: 0.0083  lr: 0.001499  max_mem: 3061M
[32m[04/15 03:31:26 d2.utils.events]: [0m eta: 0:15:34  iter: 319  total_loss: 0.794  loss_cls: 0.296  loss_box_reg: 0.338  loss_rpn_cls: 0.070  loss_rpn_loc: 0.087  time: 0.5669  data_time: 0.0084  lr: 0.001598  max_mem: 3061M
[32m[04/15 03:31:37 d2.utils.events]: [0m eta: 0:15:22  iter: 339  total_loss: 0.600  loss_cls: 0.220  loss_box_reg: 0.236  loss_rpn_cls: 0.072  loss_rpn_loc: 0.067  time: 0.5646  data_time: 0.0080  lr: 0.001698  max_mem: 3061M
[32m[04/15 03:31:48 d2.utils.events]: [0m eta: 0:15:12  iter: 359  total_loss: 0.822  loss_cls: 0.304  loss_box_reg: 0.290  loss_rpn_cls: 0.108  loss_rpn_loc: 0.105  time: 0.5650  data_time: 0.0077  lr: 0.001798  max_mem: 3061M
[32m[04/15 03:32:00 d2.utils.events]: [0m eta: 0:15:02  iter: 379  total_loss: 0.793  loss_cls: 0.265  loss_box_reg: 0.304  loss_rpn_cls: 0.082  loss_rpn_loc: 0.077  time: 0.5659  data_time: 0.0086  lr: 0.001898  max_mem: 3061M
[32m[04/15 03:32:10 d2.utils.events]: [0m eta: 0:14:44  iter: 399  total_loss: 0.908  loss_cls: 0.313  loss_box_reg: 0.387  loss_rpn_cls: 0.066  loss_rpn_loc: 0.088  time: 0.5635  data_time: 0.0092  lr: 0.001998  max_mem: 3061M
[32m[04/15 03:32:23 d2.utils.events]: [0m eta: 0:14:39  iter: 419  total_loss: 0.817  loss_cls: 0.303  loss_box_reg: 0.344  loss_rpn_cls: 0.084  loss_rpn_loc: 0.063  time: 0.5659  data_time: 0.0083  lr: 0.002098  max_mem: 3061M
[32m[04/15 03:32:34 d2.utils.events]: [0m eta: 0:14:27  iter: 439  total_loss: 0.758  loss_cls: 0.280  loss_box_reg: 0.337  loss_rpn_cls: 0.068  loss_rpn_loc: 0.083  time: 0.5658  data_time: 0.0082  lr: 0.002198  max_mem: 3061M
[32m[04/15 03:32:44 d2.utils.events]: [0m eta: 0:14:10  iter: 459  total_loss: 0.825  loss_cls: 0.296  loss_box_reg: 0.365  loss_rpn_cls: 0.088  loss_rpn_loc: 0.128  time: 0.5637  data_time: 0.0079  lr: 0.002298  max_mem: 3061M
[32m[04/15 03:32:57 d2.utils.events]: [0m eta: 0:14:05  iter: 479  total_loss: 0.725  loss_cls: 0.256  loss_box_reg: 0.280  loss_rpn_cls: 0.074  loss_rpn_loc: 0.064  time: 0.5665  data_time: 0.0081  lr: 0.002398  max_mem: 3061M
[32m[04/15 03:33:08 d2.utils.events]: [0m eta: 0:13:53  iter: 499  total_loss: 0.772  loss_cls: 0.277  loss_box_reg: 0.307  loss_rpn_cls: 0.074  loss_rpn_loc: 0.077  time: 0.5657  data_time: 0.0076  lr: 0.002498  max_mem: 3061M
[32m[04/15 03:33:20 d2.utils.events]: [0m eta: 0:13:42  iter: 519  total_loss: 0.814  loss_cls: 0.295  loss_box_reg: 0.299  loss_rpn_cls: 0.066  loss_rpn_loc: 0.065  time: 0.5664  data_time: 0.0084  lr: 0.002597  max_mem: 3061M
[32m[04/15 03:33:32 d2.utils.events]: [0m eta: 0:13:34  iter: 539  total_loss: 0.834  loss_cls: 0.308  loss_box_reg: 0.371  loss_rpn_cls: 0.061  loss_rpn_loc: 0.060  time: 0.5676  data_time: 0.0084  lr: 0.002697  max_mem: 3061M
[32m[04/15 03:33:43 d2.utils.events]: [0m eta: 0:13:22  iter: 559  total_loss: 0.807  loss_cls: 0.308  loss_box_reg: 0.343  loss_rpn_cls: 0.054  loss_rpn_loc: 0.077  time: 0.5662  data_time: 0.0084  lr: 0.002797  max_mem: 3061M
[32m[04/15 03:33:54 d2.utils.events]: [0m eta: 0:13:10  iter: 579  total_loss: 0.738  loss_cls: 0.269  loss_box_reg: 0.317  loss_rpn_cls: 0.065  loss_rpn_loc: 0.088  time: 0.5668  data_time: 0.0083  lr: 0.002897  max_mem: 3061M
[32m[04/15 03:34:06 d2.utils.events]: [0m eta: 0:13:00  iter: 599  total_loss: 0.790  loss_cls: 0.297  loss_box_reg: 0.351  loss_rpn_cls: 0.073  loss_rpn_loc: 0.079  time: 0.5671  data_time: 0.0079  lr: 0.002997  max_mem: 3061M
[32m[04/15 03:34:16 d2.utils.events]: [0m eta: 0:12:47  iter: 619  total_loss: 0.866  loss_cls: 0.299  loss_box_reg: 0.434  loss_rpn_cls: 0.062  loss_rpn_loc: 0.089  time: 0.5655  data_time: 0.0078  lr: 0.003097  max_mem: 3061M
[32m[04/15 03:34:28 d2.utils.events]: [0m eta: 0:12:37  iter: 639  total_loss: 0.715  loss_cls: 0.269  loss_box_reg: 0.319  loss_rpn_cls: 0.064  loss_rpn_loc: 0.079  time: 0.5665  data_time: 0.0097  lr: 0.003197  max_mem: 3061M
[32m[04/15 03:34:40 d2.utils.events]: [0m eta: 0:12:27  iter: 659  total_loss: 0.854  loss_cls: 0.307  loss_box_reg: 0.386  loss_rpn_cls: 0.064  loss_rpn_loc: 0.094  time: 0.5670  data_time: 0.0095  lr: 0.003297  max_mem: 3061M
[32m[04/15 03:34:51 d2.utils.events]: [0m eta: 0:12:14  iter: 679  total_loss: 1.031  loss_cls: 0.355  loss_box_reg: 0.470  loss_rpn_cls: 0.065  loss_rpn_loc: 0.092  time: 0.5660  data_time: 0.0081  lr: 0.003397  max_mem: 3061M
[32m[04/15 03:35:03 d2.utils.events]: [0m eta: 0:12:03  iter: 699  total_loss: 0.819  loss_cls: 0.286  loss_box_reg: 0.337  loss_rpn_cls: 0.053  loss_rpn_loc: 0.079  time: 0.5668  data_time: 0.0086  lr: 0.003497  max_mem: 3061M
[32m[04/15 03:35:14 d2.utils.events]: [0m eta: 0:11:52  iter: 719  total_loss: 0.919  loss_cls: 0.341  loss_box_reg: 0.405  loss_rpn_cls: 0.048  loss_rpn_loc: 0.095  time: 0.5670  data_time: 0.0086  lr: 0.003596  max_mem: 3061M
[32m[04/15 03:35:25 d2.utils.events]: [0m eta: 0:11:41  iter: 739  total_loss: 0.860  loss_cls: 0.306  loss_box_reg: 0.353  loss_rpn_cls: 0.054  loss_rpn_loc: 0.089  time: 0.5662  data_time: 0.0078  lr: 0.003696  max_mem: 3061M
[32m[04/15 03:35:37 d2.utils.events]: [0m eta: 0:11:29  iter: 759  total_loss: 0.689  loss_cls: 0.264  loss_box_reg: 0.300  loss_rpn_cls: 0.056  loss_rpn_loc: 0.059  time: 0.5669  data_time: 0.0085  lr: 0.003796  max_mem: 3061M
[32m[04/15 03:35:48 d2.utils.events]: [0m eta: 0:11:18  iter: 779  total_loss: 0.678  loss_cls: 0.255  loss_box_reg: 0.263  loss_rpn_cls: 0.068  loss_rpn_loc: 0.053  time: 0.5666  data_time: 0.0094  lr: 0.003896  max_mem: 3061M
[32m[04/15 03:35:59 d2.utils.events]: [0m eta: 0:11:06  iter: 799  total_loss: 0.768  loss_cls: 0.291  loss_box_reg: 0.366  loss_rpn_cls: 0.065  loss_rpn_loc: 0.047  time: 0.5664  data_time: 0.0083  lr: 0.003996  max_mem: 3061M
[32m[04/15 03:36:11 d2.utils.events]: [0m eta: 0:10:56  iter: 819  total_loss: 0.788  loss_cls: 0.273  loss_box_reg: 0.326  loss_rpn_cls: 0.072  loss_rpn_loc: 0.063  time: 0.5670  data_time: 0.0091  lr: 0.004096  max_mem: 3061M
[32m[04/15 03:36:22 d2.utils.events]: [0m eta: 0:10:43  iter: 839  total_loss: 0.831  loss_cls: 0.283  loss_box_reg: 0.372  loss_rpn_cls: 0.067  loss_rpn_loc: 0.089  time: 0.5660  data_time: 0.0081  lr: 0.004196  max_mem: 3061M
[32m[04/15 03:36:34 d2.utils.events]: [0m eta: 0:10:33  iter: 859  total_loss: 0.637  loss_cls: 0.211  loss_box_reg: 0.286  loss_rpn_cls: 0.053  loss_rpn_loc: 0.063  time: 0.5669  data_time: 0.0086  lr: 0.004296  max_mem: 3061M
[32m[04/15 03:36:46 d2.utils.events]: [0m eta: 0:10:23  iter: 879  total_loss: 0.730  loss_cls: 0.279  loss_box_reg: 0.338  loss_rpn_cls: 0.062  loss_rpn_loc: 0.058  time: 0.5672  data_time: 0.0085  lr: 0.004396  max_mem: 3061M
[32m[04/15 03:36:57 d2.utils.events]: [0m eta: 0:10:11  iter: 899  total_loss: 0.801  loss_cls: 0.279  loss_box_reg: 0.374  loss_rpn_cls: 0.068  loss_rpn_loc: 0.060  time: 0.5666  data_time: 0.0084  lr: 0.004496  max_mem: 3061M
[32m[04/15 03:37:08 d2.utils.events]: [0m eta: 0:10:00  iter: 919  total_loss: 0.714  loss_cls: 0.270  loss_box_reg: 0.342  loss_rpn_cls: 0.046  loss_rpn_loc: 0.055  time: 0.5669  data_time: 0.0084  lr: 0.004595  max_mem: 3061M
[32m[04/15 03:37:20 d2.utils.events]: [0m eta: 0:09:49  iter: 939  total_loss: 0.793  loss_cls: 0.297  loss_box_reg: 0.366  loss_rpn_cls: 0.057  loss_rpn_loc: 0.087  time: 0.5676  data_time: 0.0092  lr: 0.004695  max_mem: 3061M
[32m[04/15 03:37:31 d2.utils.events]: [0m eta: 0:09:38  iter: 959  total_loss: 0.954  loss_cls: 0.327  loss_box_reg: 0.444  loss_rpn_cls: 0.056  loss_rpn_loc: 0.080  time: 0.5669  data_time: 0.0076  lr: 0.004795  max_mem: 3061M
[32m[04/15 03:37:43 d2.utils.events]: [0m eta: 0:09:27  iter: 979  total_loss: 0.772  loss_cls: 0.258  loss_box_reg: 0.348  loss_rpn_cls: 0.058  loss_rpn_loc: 0.080  time: 0.5669  data_time: 0.0091  lr: 0.004895  max_mem: 3061M
[32m[04/15 03:37:55 d2.utils.events]: [0m eta: 0:09:16  iter: 999  total_loss: 0.697  loss_cls: 0.229  loss_box_reg: 0.308  loss_rpn_cls: 0.054  loss_rpn_loc: 0.085  time: 0.5674  data_time: 0.0078  lr: 0.004995  max_mem: 3061M
[32m[04/15 03:38:06 d2.utils.events]: [0m eta: 0:09:02  iter: 1019  total_loss: 0.660  loss_cls: 0.219  loss_box_reg: 0.243  loss_rpn_cls: 0.064  loss_rpn_loc: 0.059  time: 0.5670  data_time: 0.0076  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:38:17 d2.utils.events]: [0m eta: 0:08:51  iter: 1039  total_loss: 0.790  loss_cls: 0.259  loss_box_reg: 0.300  loss_rpn_cls: 0.072  loss_rpn_loc: 0.081  time: 0.5672  data_time: 0.0086  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:38:29 d2.utils.events]: [0m eta: 0:08:40  iter: 1059  total_loss: 0.783  loss_cls: 0.261  loss_box_reg: 0.369  loss_rpn_cls: 0.074  loss_rpn_loc: 0.061  time: 0.5672  data_time: 0.0088  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:38:40 d2.utils.events]: [0m eta: 0:08:29  iter: 1079  total_loss: 0.766  loss_cls: 0.291  loss_box_reg: 0.356  loss_rpn_cls: 0.058  loss_rpn_loc: 0.061  time: 0.5673  data_time: 0.0082  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:38:52 d2.utils.events]: [0m eta: 0:08:17  iter: 1099  total_loss: 0.846  loss_cls: 0.307  loss_box_reg: 0.361  loss_rpn_cls: 0.060  loss_rpn_loc: 0.107  time: 0.5673  data_time: 0.0083  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:39:03 d2.utils.events]: [0m eta: 0:08:06  iter: 1119  total_loss: 0.828  loss_cls: 0.287  loss_box_reg: 0.355  loss_rpn_cls: 0.074  loss_rpn_loc: 0.077  time: 0.5672  data_time: 0.0088  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:39:15 d2.utils.events]: [0m eta: 0:07:55  iter: 1139  total_loss: 0.688  loss_cls: 0.257  loss_box_reg: 0.286  loss_rpn_cls: 0.041  loss_rpn_loc: 0.056  time: 0.5675  data_time: 0.0085  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:39:26 d2.utils.events]: [0m eta: 0:07:43  iter: 1159  total_loss: 0.806  loss_cls: 0.264  loss_box_reg: 0.364  loss_rpn_cls: 0.044  loss_rpn_loc: 0.074  time: 0.5673  data_time: 0.0080  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:39:37 d2.utils.events]: [0m eta: 0:07:33  iter: 1179  total_loss: 0.774  loss_cls: 0.246  loss_box_reg: 0.333  loss_rpn_cls: 0.052  loss_rpn_loc: 0.074  time: 0.5672  data_time: 0.0081  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:39:49 d2.utils.events]: [0m eta: 0:07:22  iter: 1199  total_loss: 0.830  loss_cls: 0.274  loss_box_reg: 0.389  loss_rpn_cls: 0.055  loss_rpn_loc: 0.089  time: 0.5675  data_time: 0.0080  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:40:01 d2.utils.events]: [0m eta: 0:07:11  iter: 1219  total_loss: 0.683  loss_cls: 0.238  loss_box_reg: 0.348  loss_rpn_cls: 0.047  loss_rpn_loc: 0.054  time: 0.5680  data_time: 0.0087  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:40:12 d2.utils.events]: [0m eta: 0:07:00  iter: 1239  total_loss: 0.693  loss_cls: 0.256  loss_box_reg: 0.310  loss_rpn_cls: 0.042  loss_rpn_loc: 0.062  time: 0.5675  data_time: 0.0088  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:40:23 d2.utils.events]: [0m eta: 0:06:49  iter: 1259  total_loss: 0.844  loss_cls: 0.290  loss_box_reg: 0.401  loss_rpn_cls: 0.054  loss_rpn_loc: 0.090  time: 0.5676  data_time: 0.0085  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:40:35 d2.utils.events]: [0m eta: 0:06:39  iter: 1279  total_loss: 0.974  loss_cls: 0.322  loss_box_reg: 0.420  loss_rpn_cls: 0.072  loss_rpn_loc: 0.119  time: 0.5681  data_time: 0.0080  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:40:47 d2.utils.events]: [0m eta: 0:06:27  iter: 1299  total_loss: 0.793  loss_cls: 0.262  loss_box_reg: 0.350  loss_rpn_cls: 0.059  loss_rpn_loc: 0.089  time: 0.5679  data_time: 0.0078  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:40:58 d2.utils.events]: [0m eta: 0:06:16  iter: 1319  total_loss: 0.675  loss_cls: 0.257  loss_box_reg: 0.306  loss_rpn_cls: 0.065  loss_rpn_loc: 0.068  time: 0.5678  data_time: 0.0093  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:41:09 d2.utils.events]: [0m eta: 0:06:05  iter: 1339  total_loss: 0.766  loss_cls: 0.300  loss_box_reg: 0.359  loss_rpn_cls: 0.050  loss_rpn_loc: 0.086  time: 0.5678  data_time: 0.0084  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:41:21 d2.utils.events]: [0m eta: 0:05:54  iter: 1359  total_loss: 0.769  loss_cls: 0.273  loss_box_reg: 0.347  loss_rpn_cls: 0.064  loss_rpn_loc: 0.072  time: 0.5682  data_time: 0.0077  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:41:32 d2.utils.events]: [0m eta: 0:05:42  iter: 1379  total_loss: 0.768  loss_cls: 0.261  loss_box_reg: 0.336  loss_rpn_cls: 0.068  loss_rpn_loc: 0.077  time: 0.5677  data_time: 0.0074  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:41:43 d2.utils.events]: [0m eta: 0:05:32  iter: 1399  total_loss: 0.683  loss_cls: 0.208  loss_box_reg: 0.277  loss_rpn_cls: 0.039  loss_rpn_loc: 0.073  time: 0.5677  data_time: 0.0083  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:41:55 d2.utils.events]: [0m eta: 0:05:21  iter: 1419  total_loss: 0.755  loss_cls: 0.269  loss_box_reg: 0.354  loss_rpn_cls: 0.057  loss_rpn_loc: 0.067  time: 0.5681  data_time: 0.0089  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:42:07 d2.utils.events]: [0m eta: 0:05:10  iter: 1439  total_loss: 0.773  loss_cls: 0.264  loss_box_reg: 0.344  loss_rpn_cls: 0.059  loss_rpn_loc: 0.083  time: 0.5681  data_time: 0.0080  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:42:19 d2.utils.events]: [0m eta: 0:05:00  iter: 1459  total_loss: 0.818  loss_cls: 0.289  loss_box_reg: 0.400  loss_rpn_cls: 0.042  loss_rpn_loc: 0.075  time: 0.5684  data_time: 0.0087  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:42:30 d2.utils.events]: [0m eta: 0:04:48  iter: 1479  total_loss: 0.836  loss_cls: 0.325  loss_box_reg: 0.386  loss_rpn_cls: 0.041  loss_rpn_loc: 0.087  time: 0.5682  data_time: 0.0077  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:42:41 d2.utils.events]: [0m eta: 0:04:37  iter: 1499  total_loss: 0.806  loss_cls: 0.298  loss_box_reg: 0.387  loss_rpn_cls: 0.042  loss_rpn_loc: 0.084  time: 0.5683  data_time: 0.0086  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:42:53 d2.utils.events]: [0m eta: 0:04:26  iter: 1519  total_loss: 0.864  loss_cls: 0.288  loss_box_reg: 0.365  loss_rpn_cls: 0.042  loss_rpn_loc: 0.050  time: 0.5680  data_time: 0.0083  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:43:04 d2.utils.events]: [0m eta: 0:04:14  iter: 1539  total_loss: 0.795  loss_cls: 0.262  loss_box_reg: 0.408  loss_rpn_cls: 0.046  loss_rpn_loc: 0.069  time: 0.5679  data_time: 0.0085  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:43:15 d2.utils.events]: [0m eta: 0:04:04  iter: 1559  total_loss: 0.800  loss_cls: 0.305  loss_box_reg: 0.369  loss_rpn_cls: 0.048  loss_rpn_loc: 0.072  time: 0.5679  data_time: 0.0089  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:43:27 d2.utils.events]: [0m eta: 0:03:53  iter: 1579  total_loss: 0.850  loss_cls: 0.270  loss_box_reg: 0.402  loss_rpn_cls: 0.046  loss_rpn_loc: 0.095  time: 0.5683  data_time: 0.0078  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:43:39 d2.utils.events]: [0m eta: 0:03:42  iter: 1599  total_loss: 0.857  loss_cls: 0.304  loss_box_reg: 0.391  loss_rpn_cls: 0.046  loss_rpn_loc: 0.074  time: 0.5682  data_time: 0.0076  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:43:50 d2.utils.events]: [0m eta: 0:03:31  iter: 1619  total_loss: 0.865  loss_cls: 0.299  loss_box_reg: 0.400  loss_rpn_cls: 0.054  loss_rpn_loc: 0.077  time: 0.5681  data_time: 0.0085  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:44:02 d2.utils.events]: [0m eta: 0:03:20  iter: 1639  total_loss: 0.725  loss_cls: 0.256  loss_box_reg: 0.336  loss_rpn_cls: 0.044  loss_rpn_loc: 0.052  time: 0.5683  data_time: 0.0077  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:44:14 d2.utils.events]: [0m eta: 0:03:09  iter: 1659  total_loss: 0.925  loss_cls: 0.298  loss_box_reg: 0.439  loss_rpn_cls: 0.052  loss_rpn_loc: 0.065  time: 0.5687  data_time: 0.0085  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:44:24 d2.utils.events]: [0m eta: 0:02:57  iter: 1679  total_loss: 0.842  loss_cls: 0.282  loss_box_reg: 0.378  loss_rpn_cls: 0.052  loss_rpn_loc: 0.090  time: 0.5682  data_time: 0.0078  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:44:36 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.843  loss_cls: 0.304  loss_box_reg: 0.388  loss_rpn_cls: 0.055  loss_rpn_loc: 0.082  time: 0.5685  data_time: 0.0090  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:44:48 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.790  loss_cls: 0.249  loss_box_reg: 0.360  loss_rpn_cls: 0.036  loss_rpn_loc: 0.074  time: 0.5688  data_time: 0.0085  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:45:00 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.613  loss_cls: 0.220  loss_box_reg: 0.282  loss_rpn_cls: 0.046  loss_rpn_loc: 0.055  time: 0.5687  data_time: 0.0083  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:45:11 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.865  loss_cls: 0.304  loss_box_reg: 0.425  loss_rpn_cls: 0.061  loss_rpn_loc: 0.069  time: 0.5685  data_time: 0.0097  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:45:23 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.683  loss_cls: 0.243  loss_box_reg: 0.349  loss_rpn_cls: 0.038  loss_rpn_loc: 0.075  time: 0.5688  data_time: 0.0080  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:45:34 d2.utils.events]: [0m eta: 0:01:52  iter: 1799  total_loss: 0.650  loss_cls: 0.233  loss_box_reg: 0.289  loss_rpn_cls: 0.045  loss_rpn_loc: 0.051  time: 0.5690  data_time: 0.0080  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:45:46 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.663  loss_cls: 0.229  loss_box_reg: 0.305  loss_rpn_cls: 0.047  loss_rpn_loc: 0.067  time: 0.5689  data_time: 0.0069  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:45:57 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.885  loss_cls: 0.308  loss_box_reg: 0.396  loss_rpn_cls: 0.068  loss_rpn_loc: 0.087  time: 0.5687  data_time: 0.0081  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:46:08 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.730  loss_cls: 0.231  loss_box_reg: 0.375  loss_rpn_cls: 0.041  loss_rpn_loc: 0.067  time: 0.5686  data_time: 0.0090  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:46:20 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.782  loss_cls: 0.252  loss_box_reg: 0.348  loss_rpn_cls: 0.032  loss_rpn_loc: 0.067  time: 0.5687  data_time: 0.0087  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:46:31 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.714  loss_cls: 0.268  loss_box_reg: 0.367  loss_rpn_cls: 0.046  loss_rpn_loc: 0.059  time: 0.5684  data_time: 0.0078  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:46:42 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.741  loss_cls: 0.255  loss_box_reg: 0.375  loss_rpn_cls: 0.042  loss_rpn_loc: 0.071  time: 0.5684  data_time: 0.0076  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:46:54 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.900  loss_cls: 0.300  loss_box_reg: 0.399  loss_rpn_cls: 0.047  loss_rpn_loc: 0.086  time: 0.5686  data_time: 0.0075  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:47:05 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.791  loss_cls: 0.266  loss_box_reg: 0.376  loss_rpn_cls: 0.046  loss_rpn_loc: 0.080  time: 0.5682  data_time: 0.0095  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:47:16 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.749  loss_cls: 0.254  loss_box_reg: 0.365  loss_rpn_cls: 0.060  loss_rpn_loc: 0.066  time: 0.5683  data_time: 0.0079  lr: 0.005000  max_mem: 3061M
[5m[31mWARNING[0m [32m[04/15 03:47:36 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 03:47:36 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 03:47:37 d2.data.build]: [0mDistribution of instances among all 80 categories:
[36m|   category    | #instances   |   category    | #instances   |   category    | #instances   |
|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|
|    person     | 5406         |    bicycle    | 420          |      car      | 5008         |
|  motorcycle   | 0            |   airplane    | 0            |      bus      | 0            |
|     train     | 0            |     truck     | 0            |     boat      | 0            |
| traffic light | 0            | fire hydrant  | 0            |   stop sign   | 0            |
| parking meter | 0            |     bench     | 0            |     bird      | 0            |
|      cat      | 0            |      dog      | 13           |     horse     | 0            |
|     sheep     | 0            |      cow      | 0            |   elephant    | 0            |
|     bear      | 0            |     zebra     | 0            |    giraffe    | 0            |
|   backpack    | 0            |   umbrella    | 0            |    handbag    | 0            |
|      tie      | 0            |   suitcase    | 0            |    frisbee    | 0            |
|     skis      | 0            |   snowboard   | 0            |  sports ball  | 0            |
|     kite      | 0            | baseball bat  | 0            | baseball gl.. | 0            |
|  skateboard   | 0            |   surfboard   | 0            | tennis racket | 0            |
|    bottle     | 0            |  wine glass   | 0            |      cup      | 0            |
|     fork      | 0            |     knife     | 0            |     spoon     | 0            |
|     bowl      | 0            |    banana     | 0            |     apple     | 0            |
|   sandwich    | 0            |    orange     | 0            |   broccoli    | 0            |
|    carrot     | 0            |    hot dog    | 0            |     pizza     | 0            |
|     donut     | 0            |     cake      | 0            |     chair     | 0            |
|     couch     | 0            | potted plant  | 0            |      bed      | 0            |
| dining table  | 0            |    toilet     | 0            |      tv       | 0            |
|    laptop     | 0            |     mouse     | 0            |    remote     | 0            |
|   keyboard    | 0            |  cell phone   | 0            |   microwave   | 0            |
|     oven      | 0            |     bike      | 0            |    hydrant    | 0            |
|     motor     | 0            |     rider     | 0            |     light     | 0            |
|     sign      | 0            | motor vehicle | 0            |  human face   | 0            |
|  hair drier   | 0            | license plate | 0            |               |              |
|     total     | 10847        |               |              |               |              |[0m
[5m[31mWARNING[0m [32m[04/15 03:47:37 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 03:47:37 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.737  loss_cls: 0.244  loss_box_reg: 0.381  loss_rpn_cls: 0.040  loss_rpn_loc: 0.064  time: 0.5685  data_time: 0.0093  lr: 0.005000  max_mem: 3061M
[32m[04/15 03:47:38 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:56 (0.5689 s / it)
[32m[04/15 03:47:38 d2.engine.hooks]: [0mTotal training time: 0:19:13 (0:00:17 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 03:47:46 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 03:47:46 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 03:47:46 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 03:47:48 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1206 s / img. ETA=0:02:32
[32m[04/15 03:47:53 d2.evaluation.evaluator]: [0mInference done 50/1257. 0.1210 s / img. ETA=0:02:36
[32m[04/15 03:47:58 d2.evaluation.evaluator]: [0mInference done 83/1257. 0.1326 s / img. ETA=0:02:43
[32m[04/15 03:48:04 d2.evaluation.evaluator]: [0mInference done 123/1257. 0.1286 s / img. ETA=0:02:33
[32m[04/15 03:48:09 d2.evaluation.evaluator]: [0mInference done 164/1257. 0.1263 s / img. ETA=0:02:24
[32m[04/15 03:48:14 d2.evaluation.evaluator]: [0mInference done 204/1257. 0.1255 s / img. ETA=0:02:17
[32m[04/15 03:48:19 d2.evaluation.evaluator]: [0mInference done 240/1257. 0.1277 s / img. ETA=0:02:14
[32m[04/15 03:48:24 d2.evaluation.evaluator]: [0mInference done 278/1257. 0.1280 s / img. ETA=0:02:09
[32m[04/15 03:48:29 d2.evaluation.evaluator]: [0mInference done 315/1257. 0.1285 s / img. ETA=0:02:05
[32m[04/15 03:48:34 d2.evaluation.evaluator]: [0mInference done 351/1257. 0.1294 s / img. ETA=0:02:01
[32m[04/15 03:48:39 d2.evaluation.evaluator]: [0mInference done 391/1257. 0.1288 s / img. ETA=0:01:55
[32m[04/15 03:48:44 d2.evaluation.evaluator]: [0mInference done 432/1257. 0.1281 s / img. ETA=0:01:49
[32m[04/15 03:48:49 d2.evaluation.evaluator]: [0mInference done 469/1257. 0.1273 s / img. ETA=0:01:44
[32m[04/15 03:48:54 d2.evaluation.evaluator]: [0mInference done 504/1257. 0.1282 s / img. ETA=0:01:40
[32m[04/15 03:48:59 d2.evaluation.evaluator]: [0mInference done 542/1257. 0.1283 s / img. ETA=0:01:35
[32m[04/15 03:49:04 d2.evaluation.evaluator]: [0mInference done 580/1257. 0.1284 s / img. ETA=0:01:30
[32m[04/15 03:49:10 d2.evaluation.evaluator]: [0mInference done 616/1257. 0.1289 s / img. ETA=0:01:25
[32m[04/15 03:49:15 d2.evaluation.evaluator]: [0mInference done 654/1257. 0.1284 s / img. ETA=0:01:20
[32m[04/15 03:49:20 d2.evaluation.evaluator]: [0mInference done 692/1257. 0.1279 s / img. ETA=0:01:15
[32m[04/15 03:49:25 d2.evaluation.evaluator]: [0mInference done 732/1257. 0.1276 s / img. ETA=0:01:10
[32m[04/15 03:49:30 d2.evaluation.evaluator]: [0mInference done 767/1257. 0.1283 s / img. ETA=0:01:05
[32m[04/15 03:49:35 d2.evaluation.evaluator]: [0mInference done 806/1257. 0.1282 s / img. ETA=0:01:00
[32m[04/15 03:49:40 d2.evaluation.evaluator]: [0mInference done 840/1257. 0.1289 s / img. ETA=0:00:56
[32m[04/15 03:49:45 d2.evaluation.evaluator]: [0mInference done 879/1257. 0.1288 s / img. ETA=0:00:50
[32m[04/15 03:49:50 d2.evaluation.evaluator]: [0mInference done 919/1257. 0.1285 s / img. ETA=0:00:45
[32m[04/15 03:49:55 d2.evaluation.evaluator]: [0mInference done 960/1257. 0.1282 s / img. ETA=0:00:39
[32m[04/15 03:50:00 d2.evaluation.evaluator]: [0mInference done 1000/1257. 0.1279 s / img. ETA=0:00:34
[32m[04/15 03:50:05 d2.evaluation.evaluator]: [0mInference done 1035/1257. 0.1285 s / img. ETA=0:00:29
[32m[04/15 03:50:10 d2.evaluation.evaluator]: [0mInference done 1073/1257. 0.1286 s / img. ETA=0:00:24
[32m[04/15 03:50:16 d2.evaluation.evaluator]: [0mInference done 1108/1257. 0.1290 s / img. ETA=0:00:19
[32m[04/15 03:50:21 d2.evaluation.evaluator]: [0mInference done 1148/1257. 0.1288 s / img. ETA=0:00:14
[32m[04/15 03:50:26 d2.evaluation.evaluator]: [0mInference done 1188/1257. 0.1286 s / img. ETA=0:00:09
[32m[04/15 03:50:31 d2.evaluation.evaluator]: [0mInference done 1228/1257. 0.1284 s / img. ETA=0:00:03
[32m[04/15 03:50:34 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:46.836181 (0.133256 s / img per device, on 1 devices)
[32m[04/15 03:50:34 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:40 (0.128213 s / img per device, on 1 devices)
[32m[04/15 03:50:34 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 03:50:34 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 03:50:34 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.20s).
Accumulating evaluation results...
DONE (t=0.45s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.002
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.002
[32m[04/15 03:50:41 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.107 | 0.212  | 0.124  | 0.000 | 0.159 | 0.190 |
[32m[04/15 03:50:41 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP    | category      | AP    | category       | AP    |
|:--------------|:------|:--------------|:------|:---------------|:------|
| person        | 0.428 | bicycle       | 0.000 | car            | 0.000 |
| motorcycle    | nan   | airplane      | nan   | bus            | nan   |
| train         | nan   | truck         | nan   | boat           | nan   |
| traffic light | nan   | fire hydrant  | nan   | stop sign      | nan   |
| parking meter | nan   | bench         | nan   | bird           | nan   |
| cat           | nan   | dog           | 0.000 | horse          | nan   |
| sheep         | nan   | cow           | nan   | elephant       | nan   |
| bear          | nan   | zebra         | nan   | giraffe        | nan   |
| backpack      | nan   | umbrella      | nan   | handbag        | nan   |
| tie           | nan   | suitcase      | nan   | frisbee        | nan   |
| skis          | nan   | snowboard     | nan   | sports ball    | nan   |
| kite          | nan   | baseball bat  | nan   | baseball glove | nan   |
| skateboard    | nan   | surfboard     | nan   | tennis racket  | nan   |
| bottle        | nan   | wine glass    | nan   | cup            | nan   |
| fork          | nan   | knife         | nan   | spoon          | nan   |
| bowl          | nan   | banana        | nan   | apple          | nan   |
| sandwich      | nan   | orange        | nan   | broccoli       | nan   |
| carrot        | nan   | hot dog       | nan   | pizza          | nan   |
| donut         | nan   | cake          | nan   | chair          | nan   |
| couch         | nan   | potted plant  | nan   | bed            | nan   |
| dining table  | nan   | toilet        | nan   | tv             | nan   |
| laptop        | nan   | mouse         | nan   | remote         | nan   |
| keyboard      | nan   | cell phone    | nan   | microwave      | nan   |
| oven          | nan   | bike          | nan   | hydrant        | nan   |
| motor         | nan   | rider         | nan   | light          | nan   |
| sign          | nan   | motor vehicle | nan   | human face     | nan   |
| hair drier    | nan   | license plate | nan   |                |       |
============== The  1  *  2000  iterations ============
4 channel input
[32m[04/15 03:50:43 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 03:50:44 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.50 seconds.
[5m[31mWARNING[0m [32m[04/15 03:50:44 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 03:50:44 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 03:50:45 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 03:50:45 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 03:50:45 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 03:50:46 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 03:50:57 d2.utils.events]: [0m eta: 0:17:24  iter: 19  total_loss: 0.759  loss_cls: 0.250  loss_box_reg: 0.362  loss_rpn_cls: 0.037  loss_rpn_loc: 0.083  time: 0.5309  data_time: 0.0374  lr: 0.000100  max_mem: 3065M
[32m[04/15 03:51:08 d2.utils.events]: [0m eta: 0:17:06  iter: 39  total_loss: 0.783  loss_cls: 0.253  loss_box_reg: 0.401  loss_rpn_cls: 0.045  loss_rpn_loc: 0.061  time: 0.5262  data_time: 0.0077  lr: 0.000200  max_mem: 3065M
[32m[04/15 03:51:20 d2.utils.events]: [0m eta: 0:17:22  iter: 59  total_loss: 0.685  loss_cls: 0.254  loss_box_reg: 0.304  loss_rpn_cls: 0.039  loss_rpn_loc: 0.055  time: 0.5585  data_time: 0.0086  lr: 0.000300  max_mem: 3065M
[32m[04/15 03:51:31 d2.utils.events]: [0m eta: 0:17:12  iter: 79  total_loss: 0.632  loss_cls: 0.236  loss_box_reg: 0.324  loss_rpn_cls: 0.033  loss_rpn_loc: 0.059  time: 0.5564  data_time: 0.0080  lr: 0.000400  max_mem: 3065M
[32m[04/15 03:51:43 d2.utils.events]: [0m eta: 0:17:20  iter: 99  total_loss: 0.701  loss_cls: 0.247  loss_box_reg: 0.356  loss_rpn_cls: 0.038  loss_rpn_loc: 0.068  time: 0.5615  data_time: 0.0088  lr: 0.000500  max_mem: 3065M
[32m[04/15 03:51:55 d2.utils.events]: [0m eta: 0:17:21  iter: 119  total_loss: 0.656  loss_cls: 0.222  loss_box_reg: 0.329  loss_rpn_cls: 0.035  loss_rpn_loc: 0.071  time: 0.5710  data_time: 0.0097  lr: 0.000599  max_mem: 3065M
[32m[04/15 03:52:06 d2.utils.events]: [0m eta: 0:17:07  iter: 139  total_loss: 0.673  loss_cls: 0.232  loss_box_reg: 0.310  loss_rpn_cls: 0.041  loss_rpn_loc: 0.061  time: 0.5675  data_time: 0.0102  lr: 0.000699  max_mem: 3065M
[32m[04/15 03:52:18 d2.utils.events]: [0m eta: 0:16:57  iter: 159  total_loss: 0.805  loss_cls: 0.276  loss_box_reg: 0.402  loss_rpn_cls: 0.035  loss_rpn_loc: 0.071  time: 0.5677  data_time: 0.0079  lr: 0.000799  max_mem: 3065M
[32m[04/15 03:52:30 d2.utils.events]: [0m eta: 0:16:48  iter: 179  total_loss: 0.830  loss_cls: 0.277  loss_box_reg: 0.440  loss_rpn_cls: 0.031  loss_rpn_loc: 0.071  time: 0.5718  data_time: 0.0090  lr: 0.000899  max_mem: 3065M
[32m[04/15 03:52:41 d2.utils.events]: [0m eta: 0:16:35  iter: 199  total_loss: 0.711  loss_cls: 0.254  loss_box_reg: 0.340  loss_rpn_cls: 0.033  loss_rpn_loc: 0.059  time: 0.5685  data_time: 0.0081  lr: 0.000999  max_mem: 3065M
[32m[04/15 03:52:53 d2.utils.events]: [0m eta: 0:16:27  iter: 219  total_loss: 0.722  loss_cls: 0.252  loss_box_reg: 0.350  loss_rpn_cls: 0.031  loss_rpn_loc: 0.073  time: 0.5717  data_time: 0.0088  lr: 0.001099  max_mem: 3065M
[32m[04/15 03:53:05 d2.utils.events]: [0m eta: 0:16:16  iter: 239  total_loss: 0.758  loss_cls: 0.272  loss_box_reg: 0.376  loss_rpn_cls: 0.043  loss_rpn_loc: 0.060  time: 0.5714  data_time: 0.0086  lr: 0.001199  max_mem: 3065M
[32m[04/15 03:53:15 d2.utils.events]: [0m eta: 0:16:03  iter: 259  total_loss: 0.735  loss_cls: 0.257  loss_box_reg: 0.359  loss_rpn_cls: 0.040  loss_rpn_loc: 0.081  time: 0.5685  data_time: 0.0093  lr: 0.001299  max_mem: 3065M
[32m[04/15 03:53:28 d2.utils.events]: [0m eta: 0:16:01  iter: 279  total_loss: 0.622  loss_cls: 0.216  loss_box_reg: 0.317  loss_rpn_cls: 0.033  loss_rpn_loc: 0.047  time: 0.5738  data_time: 0.0085  lr: 0.001399  max_mem: 3065M
[32m[04/15 03:53:39 d2.utils.events]: [0m eta: 0:15:42  iter: 299  total_loss: 0.723  loss_cls: 0.257  loss_box_reg: 0.364  loss_rpn_cls: 0.036  loss_rpn_loc: 0.063  time: 0.5713  data_time: 0.0081  lr: 0.001499  max_mem: 3065M
[32m[04/15 03:53:50 d2.utils.events]: [0m eta: 0:15:30  iter: 319  total_loss: 0.837  loss_cls: 0.282  loss_box_reg: 0.453  loss_rpn_cls: 0.030  loss_rpn_loc: 0.060  time: 0.5704  data_time: 0.0083  lr: 0.001598  max_mem: 3065M
[32m[04/15 03:54:02 d2.utils.events]: [0m eta: 0:15:19  iter: 339  total_loss: 0.805  loss_cls: 0.265  loss_box_reg: 0.390  loss_rpn_cls: 0.050  loss_rpn_loc: 0.086  time: 0.5719  data_time: 0.0085  lr: 0.001698  max_mem: 3065M
[32m[04/15 03:54:13 d2.utils.events]: [0m eta: 0:15:07  iter: 359  total_loss: 0.683  loss_cls: 0.241  loss_box_reg: 0.328  loss_rpn_cls: 0.025  loss_rpn_loc: 0.068  time: 0.5691  data_time: 0.0086  lr: 0.001798  max_mem: 3065M
[32m[04/15 03:54:24 d2.utils.events]: [0m eta: 0:14:55  iter: 379  total_loss: 0.718  loss_cls: 0.238  loss_box_reg: 0.386  loss_rpn_cls: 0.035  loss_rpn_loc: 0.070  time: 0.5685  data_time: 0.0089  lr: 0.001898  max_mem: 3065M
[32m[04/15 03:54:36 d2.utils.events]: [0m eta: 0:14:44  iter: 399  total_loss: 0.726  loss_cls: 0.265  loss_box_reg: 0.366  loss_rpn_cls: 0.035  loss_rpn_loc: 0.066  time: 0.5688  data_time: 0.0090  lr: 0.001998  max_mem: 3065M
[32m[04/15 03:54:46 d2.utils.events]: [0m eta: 0:14:32  iter: 419  total_loss: 0.758  loss_cls: 0.265  loss_box_reg: 0.362  loss_rpn_cls: 0.038  loss_rpn_loc: 0.081  time: 0.5672  data_time: 0.0087  lr: 0.002098  max_mem: 3065M
[32m[04/15 03:54:58 d2.utils.events]: [0m eta: 0:14:21  iter: 439  total_loss: 0.784  loss_cls: 0.287  loss_box_reg: 0.423  loss_rpn_cls: 0.029  loss_rpn_loc: 0.075  time: 0.5680  data_time: 0.0093  lr: 0.002198  max_mem: 3065M
[32m[04/15 03:55:10 d2.utils.events]: [0m eta: 0:14:11  iter: 459  total_loss: 0.721  loss_cls: 0.250  loss_box_reg: 0.383  loss_rpn_cls: 0.038  loss_rpn_loc: 0.066  time: 0.5686  data_time: 0.0091  lr: 0.002298  max_mem: 3065M
[32m[04/15 03:55:21 d2.utils.events]: [0m eta: 0:13:59  iter: 479  total_loss: 0.684  loss_cls: 0.223  loss_box_reg: 0.370  loss_rpn_cls: 0.034  loss_rpn_loc: 0.051  time: 0.5679  data_time: 0.0083  lr: 0.002398  max_mem: 3065M
[32m[04/15 03:55:32 d2.utils.events]: [0m eta: 0:13:48  iter: 499  total_loss: 0.734  loss_cls: 0.260  loss_box_reg: 0.348  loss_rpn_cls: 0.027  loss_rpn_loc: 0.040  time: 0.5674  data_time: 0.0078  lr: 0.002498  max_mem: 3065M
[32m[04/15 03:55:44 d2.utils.events]: [0m eta: 0:13:37  iter: 519  total_loss: 0.580  loss_cls: 0.194  loss_box_reg: 0.283  loss_rpn_cls: 0.036  loss_rpn_loc: 0.034  time: 0.5678  data_time: 0.0088  lr: 0.002597  max_mem: 3065M
[32m[04/15 03:55:55 d2.utils.events]: [0m eta: 0:13:27  iter: 539  total_loss: 0.685  loss_cls: 0.239  loss_box_reg: 0.351  loss_rpn_cls: 0.031  loss_rpn_loc: 0.056  time: 0.5681  data_time: 0.0085  lr: 0.002697  max_mem: 3065M
[32m[04/15 03:56:06 d2.utils.events]: [0m eta: 0:13:16  iter: 559  total_loss: 0.886  loss_cls: 0.293  loss_box_reg: 0.483  loss_rpn_cls: 0.032  loss_rpn_loc: 0.062  time: 0.5675  data_time: 0.0079  lr: 0.002797  max_mem: 3065M
[32m[04/15 03:56:19 d2.utils.events]: [0m eta: 0:13:07  iter: 579  total_loss: 0.669  loss_cls: 0.229  loss_box_reg: 0.333  loss_rpn_cls: 0.026  loss_rpn_loc: 0.055  time: 0.5697  data_time: 0.0087  lr: 0.002897  max_mem: 3065M
[32m[04/15 03:56:30 d2.utils.events]: [0m eta: 0:12:55  iter: 599  total_loss: 0.783  loss_cls: 0.280  loss_box_reg: 0.402  loss_rpn_cls: 0.038  loss_rpn_loc: 0.068  time: 0.5693  data_time: 0.0076  lr: 0.002997  max_mem: 3065M
[32m[04/15 03:56:41 d2.utils.events]: [0m eta: 0:12:41  iter: 619  total_loss: 0.746  loss_cls: 0.244  loss_box_reg: 0.404  loss_rpn_cls: 0.030  loss_rpn_loc: 0.084  time: 0.5674  data_time: 0.0080  lr: 0.003097  max_mem: 3065M
[32m[04/15 03:56:53 d2.utils.events]: [0m eta: 0:12:32  iter: 639  total_loss: 0.696  loss_cls: 0.240  loss_box_reg: 0.334  loss_rpn_cls: 0.049  loss_rpn_loc: 0.073  time: 0.5688  data_time: 0.0086  lr: 0.003197  max_mem: 3065M
[32m[04/15 03:57:04 d2.utils.events]: [0m eta: 0:12:21  iter: 659  total_loss: 0.634  loss_cls: 0.220  loss_box_reg: 0.297  loss_rpn_cls: 0.037  loss_rpn_loc: 0.046  time: 0.5683  data_time: 0.0093  lr: 0.003297  max_mem: 3065M
[32m[04/15 03:57:15 d2.utils.events]: [0m eta: 0:12:08  iter: 679  total_loss: 0.753  loss_cls: 0.244  loss_box_reg: 0.333  loss_rpn_cls: 0.044  loss_rpn_loc: 0.074  time: 0.5672  data_time: 0.0089  lr: 0.003397  max_mem: 3065M
[32m[04/15 03:57:28 d2.utils.events]: [0m eta: 0:11:59  iter: 699  total_loss: 0.781  loss_cls: 0.260  loss_box_reg: 0.379  loss_rpn_cls: 0.047  loss_rpn_loc: 0.083  time: 0.5693  data_time: 0.0103  lr: 0.003497  max_mem: 3065M
[32m[04/15 03:57:38 d2.utils.events]: [0m eta: 0:11:47  iter: 719  total_loss: 0.701  loss_cls: 0.248  loss_box_reg: 0.329  loss_rpn_cls: 0.033  loss_rpn_loc: 0.070  time: 0.5678  data_time: 0.0093  lr: 0.003596  max_mem: 3065M
[32m[04/15 03:57:49 d2.utils.events]: [0m eta: 0:11:34  iter: 739  total_loss: 0.747  loss_cls: 0.234  loss_box_reg: 0.350  loss_rpn_cls: 0.037  loss_rpn_loc: 0.075  time: 0.5671  data_time: 0.0098  lr: 0.003696  max_mem: 3065M
[32m[04/15 03:58:02 d2.utils.events]: [0m eta: 0:11:25  iter: 759  total_loss: 0.719  loss_cls: 0.249  loss_box_reg: 0.357  loss_rpn_cls: 0.035  loss_rpn_loc: 0.059  time: 0.5687  data_time: 0.0083  lr: 0.003796  max_mem: 3065M
[32m[04/15 03:58:12 d2.utils.events]: [0m eta: 0:11:13  iter: 779  total_loss: 0.851  loss_cls: 0.279  loss_box_reg: 0.381  loss_rpn_cls: 0.046  loss_rpn_loc: 0.099  time: 0.5676  data_time: 0.0078  lr: 0.003896  max_mem: 3065M
[32m[04/15 03:58:24 d2.utils.events]: [0m eta: 0:11:02  iter: 799  total_loss: 0.836  loss_cls: 0.281  loss_box_reg: 0.421  loss_rpn_cls: 0.038  loss_rpn_loc: 0.079  time: 0.5679  data_time: 0.0073  lr: 0.003996  max_mem: 3065M
[32m[04/15 03:58:36 d2.utils.events]: [0m eta: 0:10:51  iter: 819  total_loss: 0.781  loss_cls: 0.259  loss_box_reg: 0.354  loss_rpn_cls: 0.046  loss_rpn_loc: 0.085  time: 0.5687  data_time: 0.0085  lr: 0.004096  max_mem: 3065M
[32m[04/15 03:58:47 d2.utils.events]: [0m eta: 0:10:40  iter: 839  total_loss: 0.780  loss_cls: 0.271  loss_box_reg: 0.428  loss_rpn_cls: 0.036  loss_rpn_loc: 0.060  time: 0.5678  data_time: 0.0091  lr: 0.004196  max_mem: 3065M
[32m[04/15 03:58:59 d2.utils.events]: [0m eta: 0:10:29  iter: 859  total_loss: 0.849  loss_cls: 0.272  loss_box_reg: 0.393  loss_rpn_cls: 0.030  loss_rpn_loc: 0.092  time: 0.5683  data_time: 0.0081  lr: 0.004296  max_mem: 3065M
[32m[04/15 03:59:10 d2.utils.events]: [0m eta: 0:10:18  iter: 879  total_loss: 0.846  loss_cls: 0.264  loss_box_reg: 0.459  loss_rpn_cls: 0.040  loss_rpn_loc: 0.064  time: 0.5685  data_time: 0.0082  lr: 0.004396  max_mem: 3065M
[32m[04/15 03:59:21 d2.utils.events]: [0m eta: 0:10:07  iter: 899  total_loss: 0.714  loss_cls: 0.233  loss_box_reg: 0.343  loss_rpn_cls: 0.046  loss_rpn_loc: 0.070  time: 0.5676  data_time: 0.0085  lr: 0.004496  max_mem: 3065M
[32m[04/15 03:59:33 d2.utils.events]: [0m eta: 0:09:56  iter: 919  total_loss: 0.869  loss_cls: 0.271  loss_box_reg: 0.441  loss_rpn_cls: 0.046  loss_rpn_loc: 0.100  time: 0.5684  data_time: 0.0075  lr: 0.004595  max_mem: 3065M
[32m[04/15 03:59:44 d2.utils.events]: [0m eta: 0:09:45  iter: 939  total_loss: 0.756  loss_cls: 0.264  loss_box_reg: 0.344  loss_rpn_cls: 0.049  loss_rpn_loc: 0.063  time: 0.5683  data_time: 0.0085  lr: 0.004695  max_mem: 3065M
[32m[04/15 03:59:55 d2.utils.events]: [0m eta: 0:09:33  iter: 959  total_loss: 0.718  loss_cls: 0.280  loss_box_reg: 0.362  loss_rpn_cls: 0.034  loss_rpn_loc: 0.074  time: 0.5674  data_time: 0.0077  lr: 0.004795  max_mem: 3065M
[32m[04/15 04:00:07 d2.utils.events]: [0m eta: 0:09:23  iter: 979  total_loss: 0.686  loss_cls: 0.269  loss_box_reg: 0.351  loss_rpn_cls: 0.038  loss_rpn_loc: 0.049  time: 0.5683  data_time: 0.0086  lr: 0.004895  max_mem: 3065M
[32m[04/15 04:00:18 d2.utils.events]: [0m eta: 0:09:11  iter: 999  total_loss: 0.644  loss_cls: 0.224  loss_box_reg: 0.297  loss_rpn_cls: 0.060  loss_rpn_loc: 0.067  time: 0.5676  data_time: 0.0087  lr: 0.004995  max_mem: 3065M
[32m[04/15 04:00:29 d2.utils.events]: [0m eta: 0:09:00  iter: 1019  total_loss: 0.764  loss_cls: 0.268  loss_box_reg: 0.392  loss_rpn_cls: 0.046  loss_rpn_loc: 0.068  time: 0.5673  data_time: 0.0098  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:00:42 d2.utils.events]: [0m eta: 0:08:51  iter: 1039  total_loss: 0.660  loss_cls: 0.221  loss_box_reg: 0.314  loss_rpn_cls: 0.032  loss_rpn_loc: 0.051  time: 0.5684  data_time: 0.0086  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:00:53 d2.utils.events]: [0m eta: 0:08:40  iter: 1059  total_loss: 0.719  loss_cls: 0.246  loss_box_reg: 0.384  loss_rpn_cls: 0.036  loss_rpn_loc: 0.053  time: 0.5680  data_time: 0.0080  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:01:04 d2.utils.events]: [0m eta: 0:08:29  iter: 1079  total_loss: 0.764  loss_cls: 0.249  loss_box_reg: 0.363  loss_rpn_cls: 0.047  loss_rpn_loc: 0.077  time: 0.5683  data_time: 0.0079  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:01:16 d2.utils.events]: [0m eta: 0:08:18  iter: 1099  total_loss: 0.807  loss_cls: 0.264  loss_box_reg: 0.390  loss_rpn_cls: 0.049  loss_rpn_loc: 0.068  time: 0.5687  data_time: 0.0080  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:01:27 d2.utils.events]: [0m eta: 0:08:06  iter: 1119  total_loss: 0.662  loss_cls: 0.250  loss_box_reg: 0.337  loss_rpn_cls: 0.034  loss_rpn_loc: 0.057  time: 0.5679  data_time: 0.0082  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:01:39 d2.utils.events]: [0m eta: 0:07:56  iter: 1139  total_loss: 0.709  loss_cls: 0.249  loss_box_reg: 0.340  loss_rpn_cls: 0.037  loss_rpn_loc: 0.060  time: 0.5688  data_time: 0.0090  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:01:51 d2.utils.events]: [0m eta: 0:07:44  iter: 1159  total_loss: 0.913  loss_cls: 0.305  loss_box_reg: 0.477  loss_rpn_cls: 0.047  loss_rpn_loc: 0.079  time: 0.5687  data_time: 0.0081  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:02:02 d2.utils.events]: [0m eta: 0:07:32  iter: 1179  total_loss: 0.766  loss_cls: 0.267  loss_box_reg: 0.408  loss_rpn_cls: 0.035  loss_rpn_loc: 0.056  time: 0.5683  data_time: 0.0081  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:02:14 d2.utils.events]: [0m eta: 0:07:22  iter: 1199  total_loss: 0.762  loss_cls: 0.259  loss_box_reg: 0.360  loss_rpn_cls: 0.039  loss_rpn_loc: 0.074  time: 0.5691  data_time: 0.0075  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:02:25 d2.utils.events]: [0m eta: 0:07:10  iter: 1219  total_loss: 0.780  loss_cls: 0.252  loss_box_reg: 0.372  loss_rpn_cls: 0.047  loss_rpn_loc: 0.079  time: 0.5683  data_time: 0.0075  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:02:35 d2.utils.events]: [0m eta: 0:06:58  iter: 1239  total_loss: 0.635  loss_cls: 0.226  loss_box_reg: 0.307  loss_rpn_cls: 0.037  loss_rpn_loc: 0.074  time: 0.5677  data_time: 0.0076  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:02:48 d2.utils.events]: [0m eta: 0:06:49  iter: 1259  total_loss: 0.629  loss_cls: 0.206  loss_box_reg: 0.358  loss_rpn_cls: 0.023  loss_rpn_loc: 0.063  time: 0.5689  data_time: 0.0091  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:02:59 d2.utils.events]: [0m eta: 0:06:36  iter: 1279  total_loss: 0.728  loss_cls: 0.267  loss_box_reg: 0.370  loss_rpn_cls: 0.029  loss_rpn_loc: 0.065  time: 0.5682  data_time: 0.0090  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:03:10 d2.utils.events]: [0m eta: 0:06:25  iter: 1299  total_loss: 0.993  loss_cls: 0.295  loss_box_reg: 0.505  loss_rpn_cls: 0.032  loss_rpn_loc: 0.078  time: 0.5678  data_time: 0.0081  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:03:22 d2.utils.events]: [0m eta: 0:06:14  iter: 1319  total_loss: 0.773  loss_cls: 0.263  loss_box_reg: 0.383  loss_rpn_cls: 0.036  loss_rpn_loc: 0.077  time: 0.5688  data_time: 0.0090  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:03:33 d2.utils.events]: [0m eta: 0:06:03  iter: 1339  total_loss: 0.813  loss_cls: 0.285  loss_box_reg: 0.374  loss_rpn_cls: 0.041  loss_rpn_loc: 0.072  time: 0.5683  data_time: 0.0079  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:03:45 d2.utils.events]: [0m eta: 0:05:53  iter: 1359  total_loss: 0.664  loss_cls: 0.228  loss_box_reg: 0.329  loss_rpn_cls: 0.033  loss_rpn_loc: 0.078  time: 0.5687  data_time: 0.0082  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:03:57 d2.utils.events]: [0m eta: 0:05:42  iter: 1379  total_loss: 0.642  loss_cls: 0.213  loss_box_reg: 0.326  loss_rpn_cls: 0.038  loss_rpn_loc: 0.065  time: 0.5692  data_time: 0.0091  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:04:08 d2.utils.events]: [0m eta: 0:05:31  iter: 1399  total_loss: 0.725  loss_cls: 0.240  loss_box_reg: 0.375  loss_rpn_cls: 0.037  loss_rpn_loc: 0.061  time: 0.5688  data_time: 0.0084  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:04:21 d2.utils.events]: [0m eta: 0:05:21  iter: 1419  total_loss: 0.798  loss_cls: 0.262  loss_box_reg: 0.396  loss_rpn_cls: 0.046  loss_rpn_loc: 0.081  time: 0.5695  data_time: 0.0093  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:04:33 d2.utils.events]: [0m eta: 0:05:10  iter: 1439  total_loss: 0.797  loss_cls: 0.258  loss_box_reg: 0.426  loss_rpn_cls: 0.032  loss_rpn_loc: 0.066  time: 0.5697  data_time: 0.0084  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:04:43 d2.utils.events]: [0m eta: 0:04:58  iter: 1459  total_loss: 0.815  loss_cls: 0.261  loss_box_reg: 0.405  loss_rpn_cls: 0.039  loss_rpn_loc: 0.076  time: 0.5691  data_time: 0.0078  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:04:55 d2.utils.events]: [0m eta: 0:04:47  iter: 1479  total_loss: 0.840  loss_cls: 0.273  loss_box_reg: 0.432  loss_rpn_cls: 0.037  loss_rpn_loc: 0.083  time: 0.5696  data_time: 0.0087  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:05:06 d2.utils.events]: [0m eta: 0:04:36  iter: 1499  total_loss: 0.672  loss_cls: 0.240  loss_box_reg: 0.306  loss_rpn_cls: 0.044  loss_rpn_loc: 0.055  time: 0.5693  data_time: 0.0097  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:05:17 d2.utils.events]: [0m eta: 0:04:24  iter: 1519  total_loss: 0.678  loss_cls: 0.234  loss_box_reg: 0.317  loss_rpn_cls: 0.040  loss_rpn_loc: 0.055  time: 0.5686  data_time: 0.0083  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:05:29 d2.utils.events]: [0m eta: 0:04:14  iter: 1539  total_loss: 0.708  loss_cls: 0.233  loss_box_reg: 0.359  loss_rpn_cls: 0.031  loss_rpn_loc: 0.059  time: 0.5692  data_time: 0.0080  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:05:40 d2.utils.events]: [0m eta: 0:04:02  iter: 1559  total_loss: 0.677  loss_cls: 0.250  loss_box_reg: 0.340  loss_rpn_cls: 0.031  loss_rpn_loc: 0.050  time: 0.5688  data_time: 0.0078  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:05:51 d2.utils.events]: [0m eta: 0:03:51  iter: 1579  total_loss: 0.771  loss_cls: 0.259  loss_box_reg: 0.408  loss_rpn_cls: 0.033  loss_rpn_loc: 0.078  time: 0.5686  data_time: 0.0076  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:06:04 d2.utils.events]: [0m eta: 0:03:40  iter: 1599  total_loss: 0.694  loss_cls: 0.215  loss_box_reg: 0.310  loss_rpn_cls: 0.040  loss_rpn_loc: 0.079  time: 0.5693  data_time: 0.0089  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:06:14 d2.utils.events]: [0m eta: 0:03:30  iter: 1619  total_loss: 0.630  loss_cls: 0.236  loss_box_reg: 0.312  loss_rpn_cls: 0.043  loss_rpn_loc: 0.067  time: 0.5688  data_time: 0.0077  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:06:25 d2.utils.events]: [0m eta: 0:03:18  iter: 1639  total_loss: 0.683  loss_cls: 0.213  loss_box_reg: 0.356  loss_rpn_cls: 0.037  loss_rpn_loc: 0.068  time: 0.5686  data_time: 0.0084  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:06:38 d2.utils.events]: [0m eta: 0:03:08  iter: 1659  total_loss: 0.734  loss_cls: 0.246  loss_box_reg: 0.387  loss_rpn_cls: 0.042  loss_rpn_loc: 0.075  time: 0.5690  data_time: 0.0089  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:06:48 d2.utils.events]: [0m eta: 0:02:57  iter: 1679  total_loss: 0.772  loss_cls: 0.284  loss_box_reg: 0.385  loss_rpn_cls: 0.049  loss_rpn_loc: 0.068  time: 0.5684  data_time: 0.0084  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:07:00 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.810  loss_cls: 0.259  loss_box_reg: 0.355  loss_rpn_cls: 0.040  loss_rpn_loc: 0.073  time: 0.5688  data_time: 0.0088  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:07:12 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.715  loss_cls: 0.236  loss_box_reg: 0.325  loss_rpn_cls: 0.034  loss_rpn_loc: 0.076  time: 0.5687  data_time: 0.0066  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:07:24 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.829  loss_cls: 0.270  loss_box_reg: 0.400  loss_rpn_cls: 0.044  loss_rpn_loc: 0.081  time: 0.5692  data_time: 0.0046  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:07:36 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.774  loss_cls: 0.262  loss_box_reg: 0.386  loss_rpn_cls: 0.029  loss_rpn_loc: 0.077  time: 0.5695  data_time: 0.0056  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:07:46 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.779  loss_cls: 0.269  loss_box_reg: 0.395  loss_rpn_cls: 0.042  loss_rpn_loc: 0.070  time: 0.5690  data_time: 0.0073  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:07:57 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.682  loss_cls: 0.213  loss_box_reg: 0.351  loss_rpn_cls: 0.031  loss_rpn_loc: 0.046  time: 0.5687  data_time: 0.0077  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:08:08 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.811  loss_cls: 0.273  loss_box_reg: 0.405  loss_rpn_cls: 0.048  loss_rpn_loc: 0.075  time: 0.5685  data_time: 0.0078  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:08:21 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.791  loss_cls: 0.290  loss_box_reg: 0.398  loss_rpn_cls: 0.043  loss_rpn_loc: 0.047  time: 0.5692  data_time: 0.0094  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:08:32 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.653  loss_cls: 0.213  loss_box_reg: 0.368  loss_rpn_cls: 0.027  loss_rpn_loc: 0.045  time: 0.5688  data_time: 0.0082  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:08:43 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.708  loss_cls: 0.257  loss_box_reg: 0.351  loss_rpn_cls: 0.046  loss_rpn_loc: 0.067  time: 0.5689  data_time: 0.0085  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:08:55 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.599  loss_cls: 0.237  loss_box_reg: 0.308  loss_rpn_cls: 0.031  loss_rpn_loc: 0.047  time: 0.5688  data_time: 0.0080  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:09:06 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.733  loss_cls: 0.227  loss_box_reg: 0.347  loss_rpn_cls: 0.030  loss_rpn_loc: 0.073  time: 0.5688  data_time: 0.0085  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:09:17 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.849  loss_cls: 0.266  loss_box_reg: 0.377  loss_rpn_cls: 0.044  loss_rpn_loc: 0.100  time: 0.5687  data_time: 0.0075  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:09:29 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.708  loss_cls: 0.228  loss_box_reg: 0.327  loss_rpn_cls: 0.035  loss_rpn_loc: 0.054  time: 0.5689  data_time: 0.0086  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:09:41 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.681  loss_cls: 0.244  loss_box_reg: 0.333  loss_rpn_cls: 0.039  loss_rpn_loc: 0.070  time: 0.5691  data_time: 0.0088  lr: 0.005000  max_mem: 3065M
[5m[31mWARNING[0m [32m[04/15 04:09:58 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 04:09:58 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 04:09:59 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 04:09:59 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.710  loss_cls: 0.251  loss_box_reg: 0.336  loss_rpn_cls: 0.044  loss_rpn_loc: 0.068  time: 0.5687  data_time: 0.0073  lr: 0.005000  max_mem: 3065M
[32m[04/15 04:10:00 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:56 (0.5690 s / it)
[32m[04/15 04:10:00 d2.engine.hooks]: [0mTotal training time: 0:19:12 (0:00:16 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 04:10:05 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 04:10:05 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 04:10:06 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 04:10:08 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1205 s / img. ETA=0:02:33
[32m[04/15 04:10:13 d2.evaluation.evaluator]: [0mInference done 52/1257. 0.1209 s / img. ETA=0:02:29
[32m[04/15 04:10:18 d2.evaluation.evaluator]: [0mInference done 93/1257. 0.1210 s / img. ETA=0:02:24
[32m[04/15 04:10:23 d2.evaluation.evaluator]: [0mInference done 133/1257. 0.1211 s / img. ETA=0:02:20
[32m[04/15 04:10:28 d2.evaluation.evaluator]: [0mInference done 172/1257. 0.1225 s / img. ETA=0:02:16
[32m[04/15 04:10:34 d2.evaluation.evaluator]: [0mInference done 204/1257. 0.1276 s / img. ETA=0:02:18
[32m[04/15 04:10:39 d2.evaluation.evaluator]: [0mInference done 239/1257. 0.1282 s / img. ETA=0:02:15
[32m[04/15 04:10:44 d2.evaluation.evaluator]: [0mInference done 279/1257. 0.1273 s / img. ETA=0:02:09
[32m[04/15 04:10:49 d2.evaluation.evaluator]: [0mInference done 319/1257. 0.1266 s / img. ETA=0:02:03
[32m[04/15 04:10:54 d2.evaluation.evaluator]: [0mInference done 359/1257. 0.1261 s / img. ETA=0:01:57
[32m[04/15 04:10:59 d2.evaluation.evaluator]: [0mInference done 399/1257. 0.1256 s / img. ETA=0:01:51
[32m[04/15 04:11:04 d2.evaluation.evaluator]: [0mInference done 436/1257. 0.1262 s / img. ETA=0:01:47
[32m[04/15 04:11:09 d2.evaluation.evaluator]: [0mInference done 470/1257. 0.1280 s / img. ETA=0:01:44
[32m[04/15 04:11:14 d2.evaluation.evaluator]: [0mInference done 507/1257. 0.1282 s / img. ETA=0:01:39
[32m[04/15 04:11:19 d2.evaluation.evaluator]: [0mInference done 547/1257. 0.1277 s / img. ETA=0:01:33
[32m[04/15 04:11:24 d2.evaluation.evaluator]: [0mInference done 587/1257. 0.1273 s / img. ETA=0:01:28
[32m[04/15 04:11:29 d2.evaluation.evaluator]: [0mInference done 625/1257. 0.1270 s / img. ETA=0:01:23
[32m[04/15 04:11:34 d2.evaluation.evaluator]: [0mInference done 662/1257. 0.1266 s / img. ETA=0:01:18
[32m[04/15 04:11:39 d2.evaluation.evaluator]: [0mInference done 698/1257. 0.1272 s / img. ETA=0:01:14
[32m[04/15 04:11:44 d2.evaluation.evaluator]: [0mInference done 733/1257. 0.1280 s / img. ETA=0:01:09
[32m[04/15 04:11:49 d2.evaluation.evaluator]: [0mInference done 770/1257. 0.1282 s / img. ETA=0:01:04
[32m[04/15 04:11:54 d2.evaluation.evaluator]: [0mInference done 810/1257. 0.1279 s / img. ETA=0:00:59
[32m[04/15 04:12:00 d2.evaluation.evaluator]: [0mInference done 850/1257. 0.1276 s / img. ETA=0:00:54
[32m[04/15 04:12:05 d2.evaluation.evaluator]: [0mInference done 891/1257. 0.1273 s / img. ETA=0:00:48
[32m[04/15 04:12:10 d2.evaluation.evaluator]: [0mInference done 932/1257. 0.1270 s / img. ETA=0:00:42
[32m[04/15 04:12:15 d2.evaluation.evaluator]: [0mInference done 967/1257. 0.1275 s / img. ETA=0:00:38
[32m[04/15 04:12:20 d2.evaluation.evaluator]: [0mInference done 1002/1257. 0.1282 s / img. ETA=0:00:33
[32m[04/15 04:12:25 d2.evaluation.evaluator]: [0mInference done 1041/1257. 0.1281 s / img. ETA=0:00:28
[32m[04/15 04:12:30 d2.evaluation.evaluator]: [0mInference done 1082/1257. 0.1278 s / img. ETA=0:00:23
[32m[04/15 04:12:35 d2.evaluation.evaluator]: [0mInference done 1123/1257. 0.1275 s / img. ETA=0:00:17
[32m[04/15 04:12:40 d2.evaluation.evaluator]: [0mInference done 1163/1257. 0.1273 s / img. ETA=0:00:12
[32m[04/15 04:12:46 d2.evaluation.evaluator]: [0mInference done 1202/1257. 0.1274 s / img. ETA=0:00:07
[32m[04/15 04:12:51 d2.evaluation.evaluator]: [0mInference done 1237/1257. 0.1278 s / img. ETA=0:00:02
[32m[04/15 04:12:54 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:46.356302 (0.132872 s / img per device, on 1 devices)
[32m[04/15 04:12:54 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:40 (0.128157 s / img per device, on 1 devices)
[32m[04/15 04:12:54 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 04:12:54 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 04:12:54 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.56s).
Accumulating evaluation results...
DONE (t=0.73s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.143
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.287
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.125
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.075
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.174
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.070
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.174
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.176
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.108
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.432
[32m[04/15 04:13:01 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 14.289 | 28.655 | 12.459 | 7.488 | 17.374 | 39.253 |
[32m[04/15 04:13:01 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 22.049 | bicycle       | 2.848 | car            | 32.257 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  2  *  2000  iterations ============
4 channel input
[32m[04/15 04:13:02 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 04:13:03 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.31 seconds.
[5m[31mWARNING[0m [32m[04/15 04:13:03 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 04:13:03 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 04:13:04 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 04:13:04 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 04:13:04 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 04:13:04 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 04:13:15 d2.utils.events]: [0m eta: 0:17:06  iter: 19  total_loss: 0.724  loss_cls: 0.234  loss_box_reg: 0.354  loss_rpn_cls: 0.033  loss_rpn_loc: 0.062  time: 0.5187  data_time: 0.0358  lr: 0.000100  max_mem: 3067M
[32m[04/15 04:13:28 d2.utils.events]: [0m eta: 0:18:00  iter: 39  total_loss: 0.659  loss_cls: 0.218  loss_box_reg: 0.319  loss_rpn_cls: 0.031  loss_rpn_loc: 0.062  time: 0.5797  data_time: 0.0080  lr: 0.000200  max_mem: 3067M
[32m[04/15 04:13:39 d2.utils.events]: [0m eta: 0:17:49  iter: 59  total_loss: 0.813  loss_cls: 0.260  loss_box_reg: 0.411  loss_rpn_cls: 0.034  loss_rpn_loc: 0.070  time: 0.5712  data_time: 0.0087  lr: 0.000300  max_mem: 3067M
[32m[04/15 04:13:50 d2.utils.events]: [0m eta: 0:17:22  iter: 79  total_loss: 0.728  loss_cls: 0.239  loss_box_reg: 0.414  loss_rpn_cls: 0.034  loss_rpn_loc: 0.074  time: 0.5663  data_time: 0.0088  lr: 0.000400  max_mem: 3067M
[32m[04/15 04:14:03 d2.utils.events]: [0m eta: 0:17:36  iter: 99  total_loss: 0.640  loss_cls: 0.222  loss_box_reg: 0.316  loss_rpn_cls: 0.029  loss_rpn_loc: 0.058  time: 0.5769  data_time: 0.0085  lr: 0.000500  max_mem: 3067M
[32m[04/15 04:14:13 d2.utils.events]: [0m eta: 0:17:20  iter: 119  total_loss: 0.710  loss_cls: 0.222  loss_box_reg: 0.375  loss_rpn_cls: 0.036  loss_rpn_loc: 0.056  time: 0.5699  data_time: 0.0092  lr: 0.000599  max_mem: 3067M
[32m[04/15 04:14:25 d2.utils.events]: [0m eta: 0:17:10  iter: 139  total_loss: 0.743  loss_cls: 0.261  loss_box_reg: 0.374  loss_rpn_cls: 0.028  loss_rpn_loc: 0.062  time: 0.5713  data_time: 0.0076  lr: 0.000699  max_mem: 3067M
[32m[04/15 04:14:37 d2.utils.events]: [0m eta: 0:16:59  iter: 159  total_loss: 0.660  loss_cls: 0.250  loss_box_reg: 0.332  loss_rpn_cls: 0.036  loss_rpn_loc: 0.052  time: 0.5722  data_time: 0.0082  lr: 0.000799  max_mem: 3067M
[32m[04/15 04:14:47 d2.utils.events]: [0m eta: 0:16:38  iter: 179  total_loss: 0.717  loss_cls: 0.248  loss_box_reg: 0.363  loss_rpn_cls: 0.031  loss_rpn_loc: 0.054  time: 0.5671  data_time: 0.0076  lr: 0.000899  max_mem: 3067M
[32m[04/15 04:14:59 d2.utils.events]: [0m eta: 0:16:33  iter: 199  total_loss: 0.699  loss_cls: 0.246  loss_box_reg: 0.346  loss_rpn_cls: 0.030  loss_rpn_loc: 0.049  time: 0.5701  data_time: 0.0093  lr: 0.000999  max_mem: 3067M
[32m[04/15 04:15:11 d2.utils.events]: [0m eta: 0:16:25  iter: 219  total_loss: 0.635  loss_cls: 0.233  loss_box_reg: 0.341  loss_rpn_cls: 0.029  loss_rpn_loc: 0.052  time: 0.5722  data_time: 0.0081  lr: 0.001099  max_mem: 3067M
[32m[04/15 04:15:22 d2.utils.events]: [0m eta: 0:16:07  iter: 239  total_loss: 0.573  loss_cls: 0.196  loss_box_reg: 0.298  loss_rpn_cls: 0.033  loss_rpn_loc: 0.053  time: 0.5691  data_time: 0.0078  lr: 0.001199  max_mem: 3067M
[32m[04/15 04:15:34 d2.utils.events]: [0m eta: 0:16:02  iter: 259  total_loss: 0.675  loss_cls: 0.225  loss_box_reg: 0.352  loss_rpn_cls: 0.026  loss_rpn_loc: 0.059  time: 0.5711  data_time: 0.0085  lr: 0.001299  max_mem: 3067M
[32m[04/15 04:15:46 d2.utils.events]: [0m eta: 0:15:52  iter: 279  total_loss: 0.703  loss_cls: 0.252  loss_box_reg: 0.346  loss_rpn_cls: 0.030  loss_rpn_loc: 0.045  time: 0.5713  data_time: 0.0082  lr: 0.001399  max_mem: 3067M
[32m[04/15 04:15:56 d2.utils.events]: [0m eta: 0:15:35  iter: 299  total_loss: 0.637  loss_cls: 0.213  loss_box_reg: 0.334  loss_rpn_cls: 0.026  loss_rpn_loc: 0.051  time: 0.5689  data_time: 0.0081  lr: 0.001499  max_mem: 3067M
[32m[04/15 04:16:09 d2.utils.events]: [0m eta: 0:15:31  iter: 319  total_loss: 0.711  loss_cls: 0.232  loss_box_reg: 0.372  loss_rpn_cls: 0.035  loss_rpn_loc: 0.054  time: 0.5727  data_time: 0.0098  lr: 0.001598  max_mem: 3067M
[32m[04/15 04:16:20 d2.utils.events]: [0m eta: 0:15:20  iter: 339  total_loss: 0.566  loss_cls: 0.201  loss_box_reg: 0.300  loss_rpn_cls: 0.025  loss_rpn_loc: 0.048  time: 0.5723  data_time: 0.0097  lr: 0.001698  max_mem: 3067M
[32m[04/15 04:16:31 d2.utils.events]: [0m eta: 0:15:08  iter: 359  total_loss: 0.682  loss_cls: 0.234  loss_box_reg: 0.334  loss_rpn_cls: 0.030  loss_rpn_loc: 0.049  time: 0.5694  data_time: 0.0083  lr: 0.001798  max_mem: 3067M
[32m[04/15 04:16:43 d2.utils.events]: [0m eta: 0:14:59  iter: 379  total_loss: 0.583  loss_cls: 0.189  loss_box_reg: 0.280  loss_rpn_cls: 0.027  loss_rpn_loc: 0.058  time: 0.5718  data_time: 0.0091  lr: 0.001898  max_mem: 3067M
[32m[04/15 04:16:54 d2.utils.events]: [0m eta: 0:14:46  iter: 399  total_loss: 0.613  loss_cls: 0.200  loss_box_reg: 0.331  loss_rpn_cls: 0.034  loss_rpn_loc: 0.065  time: 0.5696  data_time: 0.0086  lr: 0.001998  max_mem: 3067M
[32m[04/15 04:17:05 d2.utils.events]: [0m eta: 0:14:35  iter: 419  total_loss: 0.781  loss_cls: 0.277  loss_box_reg: 0.429  loss_rpn_cls: 0.029  loss_rpn_loc: 0.055  time: 0.5687  data_time: 0.0082  lr: 0.002098  max_mem: 3067M
[32m[04/15 04:17:17 d2.utils.events]: [0m eta: 0:14:24  iter: 439  total_loss: 0.820  loss_cls: 0.269  loss_box_reg: 0.439  loss_rpn_cls: 0.038  loss_rpn_loc: 0.079  time: 0.5705  data_time: 0.0077  lr: 0.002198  max_mem: 3067M
[32m[04/15 04:17:28 d2.utils.events]: [0m eta: 0:14:11  iter: 459  total_loss: 0.550  loss_cls: 0.207  loss_box_reg: 0.253  loss_rpn_cls: 0.030  loss_rpn_loc: 0.051  time: 0.5683  data_time: 0.0082  lr: 0.002298  max_mem: 3067M
[32m[04/15 04:17:39 d2.utils.events]: [0m eta: 0:14:00  iter: 479  total_loss: 0.757  loss_cls: 0.260  loss_box_reg: 0.405  loss_rpn_cls: 0.033  loss_rpn_loc: 0.065  time: 0.5684  data_time: 0.0080  lr: 0.002398  max_mem: 3067M
[32m[04/15 04:17:51 d2.utils.events]: [0m eta: 0:13:49  iter: 499  total_loss: 0.695  loss_cls: 0.237  loss_box_reg: 0.347  loss_rpn_cls: 0.043  loss_rpn_loc: 0.058  time: 0.5691  data_time: 0.0090  lr: 0.002498  max_mem: 3067M
[32m[04/15 04:18:02 d2.utils.events]: [0m eta: 0:13:38  iter: 519  total_loss: 0.681  loss_cls: 0.218  loss_box_reg: 0.369  loss_rpn_cls: 0.028  loss_rpn_loc: 0.053  time: 0.5683  data_time: 0.0093  lr: 0.002597  max_mem: 3067M
[32m[04/15 04:18:14 d2.utils.events]: [0m eta: 0:13:28  iter: 539  total_loss: 0.681  loss_cls: 0.219  loss_box_reg: 0.362  loss_rpn_cls: 0.034  loss_rpn_loc: 0.066  time: 0.5692  data_time: 0.0083  lr: 0.002697  max_mem: 3067M
[32m[04/15 04:18:26 d2.utils.events]: [0m eta: 0:13:18  iter: 559  total_loss: 0.769  loss_cls: 0.253  loss_box_reg: 0.405  loss_rpn_cls: 0.024  loss_rpn_loc: 0.068  time: 0.5699  data_time: 0.0087  lr: 0.002797  max_mem: 3067M
[32m[04/15 04:18:36 d2.utils.events]: [0m eta: 0:13:04  iter: 579  total_loss: 0.772  loss_cls: 0.267  loss_box_reg: 0.387  loss_rpn_cls: 0.027  loss_rpn_loc: 0.084  time: 0.5680  data_time: 0.0085  lr: 0.002897  max_mem: 3067M
[32m[04/15 04:18:49 d2.utils.events]: [0m eta: 0:12:55  iter: 599  total_loss: 0.787  loss_cls: 0.244  loss_box_reg: 0.395  loss_rpn_cls: 0.038  loss_rpn_loc: 0.073  time: 0.5699  data_time: 0.0095  lr: 0.002997  max_mem: 3067M
[32m[04/15 04:19:00 d2.utils.events]: [0m eta: 0:12:42  iter: 619  total_loss: 0.746  loss_cls: 0.244  loss_box_reg: 0.373  loss_rpn_cls: 0.027  loss_rpn_loc: 0.067  time: 0.5694  data_time: 0.0087  lr: 0.003097  max_mem: 3067M
[32m[04/15 04:19:11 d2.utils.events]: [0m eta: 0:12:28  iter: 639  total_loss: 0.796  loss_cls: 0.264  loss_box_reg: 0.392  loss_rpn_cls: 0.033  loss_rpn_loc: 0.073  time: 0.5680  data_time: 0.0072  lr: 0.003197  max_mem: 3067M
[32m[04/15 04:19:23 d2.utils.events]: [0m eta: 0:12:19  iter: 659  total_loss: 0.776  loss_cls: 0.251  loss_box_reg: 0.417  loss_rpn_cls: 0.039  loss_rpn_loc: 0.070  time: 0.5702  data_time: 0.0096  lr: 0.003297  max_mem: 3067M
[32m[04/15 04:19:34 d2.utils.events]: [0m eta: 0:12:05  iter: 679  total_loss: 0.644  loss_cls: 0.191  loss_box_reg: 0.339  loss_rpn_cls: 0.026  loss_rpn_loc: 0.050  time: 0.5683  data_time: 0.0083  lr: 0.003397  max_mem: 3067M
[32m[04/15 04:19:45 d2.utils.events]: [0m eta: 0:11:53  iter: 699  total_loss: 0.780  loss_cls: 0.261  loss_box_reg: 0.406  loss_rpn_cls: 0.045  loss_rpn_loc: 0.069  time: 0.5675  data_time: 0.0086  lr: 0.003497  max_mem: 3067M
[32m[04/15 04:19:57 d2.utils.events]: [0m eta: 0:11:43  iter: 719  total_loss: 0.675  loss_cls: 0.230  loss_box_reg: 0.331  loss_rpn_cls: 0.028  loss_rpn_loc: 0.057  time: 0.5685  data_time: 0.0097  lr: 0.003596  max_mem: 3067M
[32m[04/15 04:20:08 d2.utils.events]: [0m eta: 0:11:31  iter: 739  total_loss: 0.625  loss_cls: 0.231  loss_box_reg: 0.299  loss_rpn_cls: 0.023  loss_rpn_loc: 0.043  time: 0.5675  data_time: 0.0073  lr: 0.003696  max_mem: 3067M
[32m[04/15 04:20:19 d2.utils.events]: [0m eta: 0:11:20  iter: 759  total_loss: 0.692  loss_cls: 0.240  loss_box_reg: 0.333  loss_rpn_cls: 0.031  loss_rpn_loc: 0.055  time: 0.5674  data_time: 0.0077  lr: 0.003796  max_mem: 3067M
[32m[04/15 04:20:31 d2.utils.events]: [0m eta: 0:11:10  iter: 779  total_loss: 0.629  loss_cls: 0.216  loss_box_reg: 0.302  loss_rpn_cls: 0.033  loss_rpn_loc: 0.065  time: 0.5678  data_time: 0.0093  lr: 0.003896  max_mem: 3067M
[32m[04/15 04:20:41 d2.utils.events]: [0m eta: 0:10:58  iter: 799  total_loss: 0.761  loss_cls: 0.270  loss_box_reg: 0.426  loss_rpn_cls: 0.033  loss_rpn_loc: 0.061  time: 0.5668  data_time: 0.0086  lr: 0.003996  max_mem: 3067M
[32m[04/15 04:20:53 d2.utils.events]: [0m eta: 0:10:47  iter: 819  total_loss: 0.677  loss_cls: 0.232  loss_box_reg: 0.360  loss_rpn_cls: 0.035  loss_rpn_loc: 0.061  time: 0.5668  data_time: 0.0086  lr: 0.004096  max_mem: 3067M
[32m[04/15 04:21:04 d2.utils.events]: [0m eta: 0:10:36  iter: 839  total_loss: 0.730  loss_cls: 0.245  loss_box_reg: 0.385  loss_rpn_cls: 0.029  loss_rpn_loc: 0.054  time: 0.5668  data_time: 0.0072  lr: 0.004196  max_mem: 3067M
[32m[04/15 04:21:15 d2.utils.events]: [0m eta: 0:10:25  iter: 859  total_loss: 0.697  loss_cls: 0.234  loss_box_reg: 0.331  loss_rpn_cls: 0.052  loss_rpn_loc: 0.072  time: 0.5657  data_time: 0.0086  lr: 0.004296  max_mem: 3067M
[32m[04/15 04:21:27 d2.utils.events]: [0m eta: 0:10:15  iter: 879  total_loss: 0.769  loss_cls: 0.259  loss_box_reg: 0.385  loss_rpn_cls: 0.031  loss_rpn_loc: 0.071  time: 0.5668  data_time: 0.0074  lr: 0.004396  max_mem: 3067M
[32m[04/15 04:21:38 d2.utils.events]: [0m eta: 0:10:03  iter: 899  total_loss: 0.666  loss_cls: 0.206  loss_box_reg: 0.336  loss_rpn_cls: 0.028  loss_rpn_loc: 0.076  time: 0.5663  data_time: 0.0080  lr: 0.004496  max_mem: 3067M
[32m[04/15 04:21:49 d2.utils.events]: [0m eta: 0:09:51  iter: 919  total_loss: 0.840  loss_cls: 0.270  loss_box_reg: 0.438  loss_rpn_cls: 0.037  loss_rpn_loc: 0.070  time: 0.5654  data_time: 0.0086  lr: 0.004595  max_mem: 3067M
[32m[04/15 04:22:01 d2.utils.events]: [0m eta: 0:09:41  iter: 939  total_loss: 0.858  loss_cls: 0.277  loss_box_reg: 0.390  loss_rpn_cls: 0.036  loss_rpn_loc: 0.074  time: 0.5667  data_time: 0.0089  lr: 0.004695  max_mem: 3067M
[32m[04/15 04:22:12 d2.utils.events]: [0m eta: 0:09:30  iter: 959  total_loss: 0.737  loss_cls: 0.251  loss_box_reg: 0.394  loss_rpn_cls: 0.030  loss_rpn_loc: 0.052  time: 0.5663  data_time: 0.0087  lr: 0.004795  max_mem: 3067M
[32m[04/15 04:22:24 d2.utils.events]: [0m eta: 0:09:19  iter: 979  total_loss: 0.718  loss_cls: 0.246  loss_box_reg: 0.386  loss_rpn_cls: 0.029  loss_rpn_loc: 0.054  time: 0.5660  data_time: 0.0082  lr: 0.004895  max_mem: 3067M
[32m[04/15 04:22:35 d2.utils.events]: [0m eta: 0:09:09  iter: 999  total_loss: 0.707  loss_cls: 0.235  loss_box_reg: 0.372  loss_rpn_cls: 0.038  loss_rpn_loc: 0.053  time: 0.5662  data_time: 0.0088  lr: 0.004995  max_mem: 3067M
[32m[04/15 04:22:47 d2.utils.events]: [0m eta: 0:08:59  iter: 1019  total_loss: 0.766  loss_cls: 0.273  loss_box_reg: 0.425  loss_rpn_cls: 0.028  loss_rpn_loc: 0.053  time: 0.5662  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:22:58 d2.utils.events]: [0m eta: 0:08:47  iter: 1039  total_loss: 0.724  loss_cls: 0.257  loss_box_reg: 0.352  loss_rpn_cls: 0.029  loss_rpn_loc: 0.063  time: 0.5663  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:23:10 d2.utils.events]: [0m eta: 0:08:37  iter: 1059  total_loss: 0.870  loss_cls: 0.277  loss_box_reg: 0.442  loss_rpn_cls: 0.037  loss_rpn_loc: 0.063  time: 0.5668  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:23:22 d2.utils.events]: [0m eta: 0:08:27  iter: 1079  total_loss: 0.712  loss_cls: 0.236  loss_box_reg: 0.334  loss_rpn_cls: 0.038  loss_rpn_loc: 0.066  time: 0.5675  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:23:33 d2.utils.events]: [0m eta: 0:08:15  iter: 1099  total_loss: 0.664  loss_cls: 0.241  loss_box_reg: 0.356  loss_rpn_cls: 0.025  loss_rpn_loc: 0.053  time: 0.5670  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:23:46 d2.utils.events]: [0m eta: 0:08:05  iter: 1119  total_loss: 0.861  loss_cls: 0.312  loss_box_reg: 0.403  loss_rpn_cls: 0.046  loss_rpn_loc: 0.071  time: 0.5683  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:23:57 d2.utils.events]: [0m eta: 0:07:54  iter: 1139  total_loss: 0.874  loss_cls: 0.297  loss_box_reg: 0.431  loss_rpn_cls: 0.041  loss_rpn_loc: 0.069  time: 0.5683  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:24:08 d2.utils.events]: [0m eta: 0:07:42  iter: 1159  total_loss: 0.766  loss_cls: 0.260  loss_box_reg: 0.432  loss_rpn_cls: 0.035  loss_rpn_loc: 0.073  time: 0.5675  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:24:20 d2.utils.events]: [0m eta: 0:07:33  iter: 1179  total_loss: 0.659  loss_cls: 0.233  loss_box_reg: 0.295  loss_rpn_cls: 0.033  loss_rpn_loc: 0.066  time: 0.5685  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:24:31 d2.utils.events]: [0m eta: 0:07:21  iter: 1199  total_loss: 0.758  loss_cls: 0.253  loss_box_reg: 0.393  loss_rpn_cls: 0.033  loss_rpn_loc: 0.081  time: 0.5682  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:24:42 d2.utils.events]: [0m eta: 0:07:10  iter: 1219  total_loss: 0.798  loss_cls: 0.283  loss_box_reg: 0.377  loss_rpn_cls: 0.042  loss_rpn_loc: 0.091  time: 0.5676  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:24:55 d2.utils.events]: [0m eta: 0:07:00  iter: 1239  total_loss: 0.767  loss_cls: 0.260  loss_box_reg: 0.421  loss_rpn_cls: 0.035  loss_rpn_loc: 0.064  time: 0.5687  data_time: 0.0097  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:25:06 d2.utils.events]: [0m eta: 0:06:48  iter: 1259  total_loss: 0.643  loss_cls: 0.222  loss_box_reg: 0.317  loss_rpn_cls: 0.035  loss_rpn_loc: 0.043  time: 0.5682  data_time: 0.0110  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:25:18 d2.utils.events]: [0m eta: 0:06:37  iter: 1279  total_loss: 0.623  loss_cls: 0.224  loss_box_reg: 0.294  loss_rpn_cls: 0.042  loss_rpn_loc: 0.078  time: 0.5684  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:25:29 d2.utils.events]: [0m eta: 0:06:27  iter: 1299  total_loss: 0.828  loss_cls: 0.284  loss_box_reg: 0.422  loss_rpn_cls: 0.040  loss_rpn_loc: 0.089  time: 0.5688  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:25:40 d2.utils.events]: [0m eta: 0:06:14  iter: 1319  total_loss: 0.817  loss_cls: 0.260  loss_box_reg: 0.408  loss_rpn_cls: 0.041  loss_rpn_loc: 0.078  time: 0.5681  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:25:52 d2.utils.events]: [0m eta: 0:06:03  iter: 1339  total_loss: 0.854  loss_cls: 0.276  loss_box_reg: 0.454  loss_rpn_cls: 0.041  loss_rpn_loc: 0.066  time: 0.5685  data_time: 0.0095  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:26:04 d2.utils.events]: [0m eta: 0:05:53  iter: 1359  total_loss: 0.741  loss_cls: 0.248  loss_box_reg: 0.392  loss_rpn_cls: 0.038  loss_rpn_loc: 0.045  time: 0.5686  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:26:14 d2.utils.events]: [0m eta: 0:05:41  iter: 1379  total_loss: 0.780  loss_cls: 0.249  loss_box_reg: 0.406  loss_rpn_cls: 0.034  loss_rpn_loc: 0.065  time: 0.5682  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:26:27 d2.utils.events]: [0m eta: 0:05:31  iter: 1399  total_loss: 0.653  loss_cls: 0.218  loss_box_reg: 0.312  loss_rpn_cls: 0.030  loss_rpn_loc: 0.058  time: 0.5690  data_time: 0.0096  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:26:39 d2.utils.events]: [0m eta: 0:05:20  iter: 1419  total_loss: 0.859  loss_cls: 0.290  loss_box_reg: 0.401  loss_rpn_cls: 0.041  loss_rpn_loc: 0.080  time: 0.5690  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:26:49 d2.utils.events]: [0m eta: 0:05:09  iter: 1439  total_loss: 0.794  loss_cls: 0.274  loss_box_reg: 0.377  loss_rpn_cls: 0.044  loss_rpn_loc: 0.089  time: 0.5684  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:27:02 d2.utils.events]: [0m eta: 0:04:59  iter: 1459  total_loss: 0.757  loss_cls: 0.269  loss_box_reg: 0.368  loss_rpn_cls: 0.035  loss_rpn_loc: 0.051  time: 0.5693  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:27:12 d2.utils.events]: [0m eta: 0:04:47  iter: 1479  total_loss: 0.846  loss_cls: 0.262  loss_box_reg: 0.396  loss_rpn_cls: 0.031  loss_rpn_loc: 0.061  time: 0.5688  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:27:23 d2.utils.events]: [0m eta: 0:04:36  iter: 1499  total_loss: 0.834  loss_cls: 0.302  loss_box_reg: 0.433  loss_rpn_cls: 0.034  loss_rpn_loc: 0.064  time: 0.5685  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:27:36 d2.utils.events]: [0m eta: 0:04:25  iter: 1519  total_loss: 0.698  loss_cls: 0.219  loss_box_reg: 0.337  loss_rpn_cls: 0.036  loss_rpn_loc: 0.062  time: 0.5690  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:27:47 d2.utils.events]: [0m eta: 0:04:14  iter: 1539  total_loss: 0.570  loss_cls: 0.211  loss_box_reg: 0.300  loss_rpn_cls: 0.025  loss_rpn_loc: 0.042  time: 0.5686  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:27:58 d2.utils.events]: [0m eta: 0:04:03  iter: 1559  total_loss: 0.711  loss_cls: 0.240  loss_box_reg: 0.340  loss_rpn_cls: 0.031  loss_rpn_loc: 0.070  time: 0.5688  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:28:10 d2.utils.events]: [0m eta: 0:03:52  iter: 1579  total_loss: 0.718  loss_cls: 0.258  loss_box_reg: 0.372  loss_rpn_cls: 0.035  loss_rpn_loc: 0.038  time: 0.5691  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:28:21 d2.utils.events]: [0m eta: 0:03:41  iter: 1599  total_loss: 0.637  loss_cls: 0.217  loss_box_reg: 0.319  loss_rpn_cls: 0.030  loss_rpn_loc: 0.059  time: 0.5687  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:28:34 d2.utils.events]: [0m eta: 0:03:30  iter: 1619  total_loss: 0.757  loss_cls: 0.257  loss_box_reg: 0.391  loss_rpn_cls: 0.027  loss_rpn_loc: 0.061  time: 0.5693  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:28:45 d2.utils.events]: [0m eta: 0:03:19  iter: 1639  total_loss: 0.874  loss_cls: 0.275  loss_box_reg: 0.419  loss_rpn_cls: 0.039  loss_rpn_loc: 0.096  time: 0.5691  data_time: 0.0107  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:28:55 d2.utils.events]: [0m eta: 0:03:08  iter: 1659  total_loss: 0.730  loss_cls: 0.230  loss_box_reg: 0.387  loss_rpn_cls: 0.042  loss_rpn_loc: 0.059  time: 0.5687  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:29:09 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.610  loss_cls: 0.204  loss_box_reg: 0.330  loss_rpn_cls: 0.029  loss_rpn_loc: 0.058  time: 0.5697  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:29:19 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.790  loss_cls: 0.247  loss_box_reg: 0.409  loss_rpn_cls: 0.037  loss_rpn_loc: 0.085  time: 0.5692  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:29:31 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.666  loss_cls: 0.227  loss_box_reg: 0.317  loss_rpn_cls: 0.034  loss_rpn_loc: 0.061  time: 0.5692  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:29:43 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.705  loss_cls: 0.244  loss_box_reg: 0.337  loss_rpn_cls: 0.027  loss_rpn_loc: 0.063  time: 0.5695  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:29:53 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.747  loss_cls: 0.231  loss_box_reg: 0.371  loss_rpn_cls: 0.035  loss_rpn_loc: 0.058  time: 0.5689  data_time: 0.0095  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:30:05 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.809  loss_cls: 0.269  loss_box_reg: 0.396  loss_rpn_cls: 0.039  loss_rpn_loc: 0.069  time: 0.5691  data_time: 0.0098  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:30:17 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.774  loss_cls: 0.256  loss_box_reg: 0.420  loss_rpn_cls: 0.033  loss_rpn_loc: 0.082  time: 0.5693  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:30:28 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.821  loss_cls: 0.284  loss_box_reg: 0.460  loss_rpn_cls: 0.043  loss_rpn_loc: 0.073  time: 0.5691  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:30:40 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.745  loss_cls: 0.260  loss_box_reg: 0.402  loss_rpn_cls: 0.028  loss_rpn_loc: 0.063  time: 0.5695  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:30:52 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.671  loss_cls: 0.224  loss_box_reg: 0.354  loss_rpn_cls: 0.028  loss_rpn_loc: 0.072  time: 0.5697  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:31:03 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.735  loss_cls: 0.259  loss_box_reg: 0.362  loss_rpn_cls: 0.035  loss_rpn_loc: 0.072  time: 0.5697  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:31:15 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.843  loss_cls: 0.267  loss_box_reg: 0.359  loss_rpn_cls: 0.036  loss_rpn_loc: 0.084  time: 0.5699  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:31:26 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.755  loss_cls: 0.259  loss_box_reg: 0.348  loss_rpn_cls: 0.035  loss_rpn_loc: 0.071  time: 0.5694  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:31:37 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.774  loss_cls: 0.256  loss_box_reg: 0.423  loss_rpn_cls: 0.033  loss_rpn_loc: 0.056  time: 0.5693  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:31:49 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.767  loss_cls: 0.270  loss_box_reg: 0.419  loss_rpn_cls: 0.024  loss_rpn_loc: 0.068  time: 0.5699  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:32:00 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.710  loss_cls: 0.236  loss_box_reg: 0.377  loss_rpn_cls: 0.029  loss_rpn_loc: 0.057  time: 0.5696  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 04:32:22 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 04:32:22 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 04:32:22 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 04:32:22 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.715  loss_cls: 0.220  loss_box_reg: 0.375  loss_rpn_cls: 0.027  loss_rpn_loc: 0.063  time: 0.5697  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:32:24 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:58 (0.5700 s / it)
[32m[04/15 04:32:24 d2.engine.hooks]: [0mTotal training time: 0:19:18 (0:00:19 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 04:32:30 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 04:32:30 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 04:32:30 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 04:32:33 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1201 s / img. ETA=0:02:32
[32m[04/15 04:32:38 d2.evaluation.evaluator]: [0mInference done 48/1257. 0.1208 s / img. ETA=0:02:41
[32m[04/15 04:32:43 d2.evaluation.evaluator]: [0mInference done 86/1257. 0.1254 s / img. ETA=0:02:37
[32m[04/15 04:32:48 d2.evaluation.evaluator]: [0mInference done 119/1257. 0.1320 s / img. ETA=0:02:38
[32m[04/15 04:32:53 d2.evaluation.evaluator]: [0mInference done 150/1257. 0.1360 s / img. ETA=0:02:39
[32m[04/15 04:32:58 d2.evaluation.evaluator]: [0mInference done 190/1257. 0.1324 s / img. ETA=0:02:30
[32m[04/15 04:33:03 d2.evaluation.evaluator]: [0mInference done 225/1257. 0.1303 s / img. ETA=0:02:26
[32m[04/15 04:33:08 d2.evaluation.evaluator]: [0mInference done 260/1257. 0.1290 s / img. ETA=0:02:21
[32m[04/15 04:33:13 d2.evaluation.evaluator]: [0mInference done 300/1257. 0.1280 s / img. ETA=0:02:13
[32m[04/15 04:33:18 d2.evaluation.evaluator]: [0mInference done 336/1257. 0.1288 s / img. ETA=0:02:08
[32m[04/15 04:33:23 d2.evaluation.evaluator]: [0mInference done 370/1257. 0.1304 s / img. ETA=0:02:04
[32m[04/15 04:33:28 d2.evaluation.evaluator]: [0mInference done 408/1257. 0.1302 s / img. ETA=0:01:58
[32m[04/15 04:33:33 d2.evaluation.evaluator]: [0mInference done 446/1257. 0.1292 s / img. ETA=0:01:53
[32m[04/15 04:33:38 d2.evaluation.evaluator]: [0mInference done 481/1257. 0.1284 s / img. ETA=0:01:48
[32m[04/15 04:33:44 d2.evaluation.evaluator]: [0mInference done 520/1257. 0.1278 s / img. ETA=0:01:42
[32m[04/15 04:33:49 d2.evaluation.evaluator]: [0mInference done 560/1257. 0.1273 s / img. ETA=0:01:36
[32m[04/15 04:33:54 d2.evaluation.evaluator]: [0mInference done 595/1257. 0.1281 s / img. ETA=0:01:31
[32m[04/15 04:33:59 d2.evaluation.evaluator]: [0mInference done 629/1257. 0.1290 s / img. ETA=0:01:27
[32m[04/15 04:34:04 d2.evaluation.evaluator]: [0mInference done 665/1257. 0.1289 s / img. ETA=0:01:22
[32m[04/15 04:34:09 d2.evaluation.evaluator]: [0mInference done 706/1257. 0.1284 s / img. ETA=0:01:16
[32m[04/15 04:34:14 d2.evaluation.evaluator]: [0mInference done 747/1257. 0.1279 s / img. ETA=0:01:10
[32m[04/15 04:34:19 d2.evaluation.evaluator]: [0mInference done 788/1257. 0.1275 s / img. ETA=0:01:04
[32m[04/15 04:34:24 d2.evaluation.evaluator]: [0mInference done 828/1257. 0.1274 s / img. ETA=0:00:58
[32m[04/15 04:34:30 d2.evaluation.evaluator]: [0mInference done 865/1257. 0.1277 s / img. ETA=0:00:53
[32m[04/15 04:34:35 d2.evaluation.evaluator]: [0mInference done 900/1257. 0.1283 s / img. ETA=0:00:48
[32m[04/15 04:34:40 d2.evaluation.evaluator]: [0mInference done 938/1257. 0.1283 s / img. ETA=0:00:43
[32m[04/15 04:34:45 d2.evaluation.evaluator]: [0mInference done 979/1257. 0.1279 s / img. ETA=0:00:37
[32m[04/15 04:34:50 d2.evaluation.evaluator]: [0mInference done 1020/1257. 0.1277 s / img. ETA=0:00:32
[32m[04/15 04:34:55 d2.evaluation.evaluator]: [0mInference done 1061/1257. 0.1274 s / img. ETA=0:00:26
[32m[04/15 04:35:00 d2.evaluation.evaluator]: [0mInference done 1099/1257. 0.1274 s / img. ETA=0:00:21
[32m[04/15 04:35:05 d2.evaluation.evaluator]: [0mInference done 1134/1257. 0.1279 s / img. ETA=0:00:16
[32m[04/15 04:35:10 d2.evaluation.evaluator]: [0mInference done 1169/1257. 0.1283 s / img. ETA=0:00:11
[32m[04/15 04:35:15 d2.evaluation.evaluator]: [0mInference done 1210/1257. 0.1280 s / img. ETA=0:00:06
[32m[04/15 04:35:20 d2.evaluation.evaluator]: [0mInference done 1251/1257. 0.1277 s / img. ETA=0:00:00
[32m[04/15 04:35:21 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:49.338698 (0.135255 s / img per device, on 1 devices)
[32m[04/15 04:35:21 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:39 (0.127706 s / img per device, on 1 devices)
[32m[04/15 04:35:21 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 04:35:21 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 04:35:21 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.03s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=4.26s).
Accumulating evaluation results...
DONE (t=0.67s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.284
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.127
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.073
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.181
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.401
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.084
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.184
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.185
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.103
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.234
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.459
[32m[04/15 04:35:27 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 14.425 | 28.389 | 12.736 | 7.292 | 18.119 | 40.069 |
[32m[04/15 04:35:27 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 19.359 | bicycle       | 4.822 | car            | 33.519 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  3  *  2000  iterations ============
4 channel input
[32m[04/15 04:35:28 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 04:35:31 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 2.15 seconds.
[5m[31mWARNING[0m [32m[04/15 04:35:31 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 04:35:31 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 04:35:32 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 04:35:32 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 04:35:32 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 04:35:32 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 04:35:45 d2.utils.events]: [0m eta: 0:20:12  iter: 19  total_loss: 0.645  loss_cls: 0.228  loss_box_reg: 0.318  loss_rpn_cls: 0.035  loss_rpn_loc: 0.059  time: 0.6250  data_time: 0.0475  lr: 0.000100  max_mem: 3067M
[32m[04/15 04:35:56 d2.utils.events]: [0m eta: 0:18:13  iter: 39  total_loss: 0.765  loss_cls: 0.250  loss_box_reg: 0.377  loss_rpn_cls: 0.038  loss_rpn_loc: 0.083  time: 0.5792  data_time: 0.0085  lr: 0.000200  max_mem: 3067M
[32m[04/15 04:36:07 d2.utils.events]: [0m eta: 0:17:40  iter: 59  total_loss: 0.712  loss_cls: 0.223  loss_box_reg: 0.402  loss_rpn_cls: 0.030  loss_rpn_loc: 0.069  time: 0.5668  data_time: 0.0081  lr: 0.000300  max_mem: 3067M
[32m[04/15 04:36:19 d2.utils.events]: [0m eta: 0:17:38  iter: 79  total_loss: 0.620  loss_cls: 0.218  loss_box_reg: 0.299  loss_rpn_cls: 0.030  loss_rpn_loc: 0.054  time: 0.5713  data_time: 0.0074  lr: 0.000400  max_mem: 3067M
[32m[04/15 04:36:30 d2.utils.events]: [0m eta: 0:17:29  iter: 99  total_loss: 0.631  loss_cls: 0.213  loss_box_reg: 0.320  loss_rpn_cls: 0.026  loss_rpn_loc: 0.070  time: 0.5656  data_time: 0.0072  lr: 0.000500  max_mem: 3067M
[32m[04/15 04:36:42 d2.utils.events]: [0m eta: 0:17:39  iter: 119  total_loss: 0.697  loss_cls: 0.244  loss_box_reg: 0.351  loss_rpn_cls: 0.028  loss_rpn_loc: 0.072  time: 0.5710  data_time: 0.0076  lr: 0.000599  max_mem: 3067M
[32m[04/15 04:36:53 d2.utils.events]: [0m eta: 0:17:34  iter: 139  total_loss: 0.704  loss_cls: 0.214  loss_box_reg: 0.388  loss_rpn_cls: 0.031  loss_rpn_loc: 0.064  time: 0.5739  data_time: 0.0082  lr: 0.000699  max_mem: 3067M
[32m[04/15 04:37:05 d2.utils.events]: [0m eta: 0:17:17  iter: 159  total_loss: 0.689  loss_cls: 0.221  loss_box_reg: 0.350  loss_rpn_cls: 0.030  loss_rpn_loc: 0.069  time: 0.5741  data_time: 0.0089  lr: 0.000799  max_mem: 3067M
[32m[04/15 04:37:16 d2.utils.events]: [0m eta: 0:16:58  iter: 179  total_loss: 0.661  loss_cls: 0.212  loss_box_reg: 0.312  loss_rpn_cls: 0.025  loss_rpn_loc: 0.047  time: 0.5696  data_time: 0.0089  lr: 0.000899  max_mem: 3067M
[32m[04/15 04:37:28 d2.utils.events]: [0m eta: 0:16:56  iter: 199  total_loss: 0.613  loss_cls: 0.206  loss_box_reg: 0.302  loss_rpn_cls: 0.021  loss_rpn_loc: 0.058  time: 0.5737  data_time: 0.0073  lr: 0.000999  max_mem: 3067M
[32m[04/15 04:37:39 d2.utils.events]: [0m eta: 0:16:36  iter: 219  total_loss: 0.624  loss_cls: 0.223  loss_box_reg: 0.317  loss_rpn_cls: 0.027  loss_rpn_loc: 0.044  time: 0.5709  data_time: 0.0088  lr: 0.001099  max_mem: 3067M
[32m[04/15 04:37:50 d2.utils.events]: [0m eta: 0:16:15  iter: 239  total_loss: 0.714  loss_cls: 0.222  loss_box_reg: 0.387  loss_rpn_cls: 0.029  loss_rpn_loc: 0.062  time: 0.5676  data_time: 0.0075  lr: 0.001199  max_mem: 3067M
[32m[04/15 04:38:02 d2.utils.events]: [0m eta: 0:16:08  iter: 259  total_loss: 0.621  loss_cls: 0.223  loss_box_reg: 0.350  loss_rpn_cls: 0.027  loss_rpn_loc: 0.053  time: 0.5710  data_time: 0.0086  lr: 0.001299  max_mem: 3067M
[32m[04/15 04:38:13 d2.utils.events]: [0m eta: 0:15:52  iter: 279  total_loss: 0.680  loss_cls: 0.232  loss_box_reg: 0.373  loss_rpn_cls: 0.022  loss_rpn_loc: 0.052  time: 0.5682  data_time: 0.0078  lr: 0.001399  max_mem: 3067M
[32m[04/15 04:38:24 d2.utils.events]: [0m eta: 0:15:40  iter: 299  total_loss: 0.677  loss_cls: 0.222  loss_box_reg: 0.346  loss_rpn_cls: 0.028  loss_rpn_loc: 0.054  time: 0.5667  data_time: 0.0077  lr: 0.001499  max_mem: 3067M
[32m[04/15 04:38:36 d2.utils.events]: [0m eta: 0:15:33  iter: 319  total_loss: 0.689  loss_cls: 0.230  loss_box_reg: 0.360  loss_rpn_cls: 0.023  loss_rpn_loc: 0.047  time: 0.5701  data_time: 0.0085  lr: 0.001598  max_mem: 3067M
[32m[04/15 04:38:47 d2.utils.events]: [0m eta: 0:15:21  iter: 339  total_loss: 0.669  loss_cls: 0.246  loss_box_reg: 0.364  loss_rpn_cls: 0.022  loss_rpn_loc: 0.042  time: 0.5680  data_time: 0.0080  lr: 0.001698  max_mem: 3067M
[32m[04/15 04:38:58 d2.utils.events]: [0m eta: 0:15:09  iter: 359  total_loss: 0.626  loss_cls: 0.223  loss_box_reg: 0.344  loss_rpn_cls: 0.023  loss_rpn_loc: 0.053  time: 0.5672  data_time: 0.0076  lr: 0.001798  max_mem: 3067M
[32m[04/15 04:39:10 d2.utils.events]: [0m eta: 0:15:01  iter: 379  total_loss: 0.656  loss_cls: 0.218  loss_box_reg: 0.357  loss_rpn_cls: 0.026  loss_rpn_loc: 0.047  time: 0.5690  data_time: 0.0083  lr: 0.001898  max_mem: 3067M
[32m[04/15 04:39:21 d2.utils.events]: [0m eta: 0:14:48  iter: 399  total_loss: 0.773  loss_cls: 0.243  loss_box_reg: 0.402  loss_rpn_cls: 0.033  loss_rpn_loc: 0.087  time: 0.5667  data_time: 0.0094  lr: 0.001998  max_mem: 3067M
[32m[04/15 04:39:32 d2.utils.events]: [0m eta: 0:14:34  iter: 419  total_loss: 0.732  loss_cls: 0.254  loss_box_reg: 0.357  loss_rpn_cls: 0.034  loss_rpn_loc: 0.076  time: 0.5657  data_time: 0.0092  lr: 0.002098  max_mem: 3067M
[32m[04/15 04:39:44 d2.utils.events]: [0m eta: 0:14:26  iter: 439  total_loss: 0.788  loss_cls: 0.282  loss_box_reg: 0.449  loss_rpn_cls: 0.032  loss_rpn_loc: 0.052  time: 0.5668  data_time: 0.0087  lr: 0.002198  max_mem: 3067M
[32m[04/15 04:39:54 d2.utils.events]: [0m eta: 0:14:12  iter: 459  total_loss: 0.711  loss_cls: 0.238  loss_box_reg: 0.389  loss_rpn_cls: 0.031  loss_rpn_loc: 0.053  time: 0.5647  data_time: 0.0082  lr: 0.002298  max_mem: 3067M
[32m[04/15 04:40:06 d2.utils.events]: [0m eta: 0:14:02  iter: 479  total_loss: 0.650  loss_cls: 0.238  loss_box_reg: 0.321  loss_rpn_cls: 0.032  loss_rpn_loc: 0.057  time: 0.5654  data_time: 0.0083  lr: 0.002398  max_mem: 3067M
[32m[04/15 04:40:18 d2.utils.events]: [0m eta: 0:13:53  iter: 499  total_loss: 0.591  loss_cls: 0.203  loss_box_reg: 0.361  loss_rpn_cls: 0.024  loss_rpn_loc: 0.046  time: 0.5666  data_time: 0.0100  lr: 0.002498  max_mem: 3067M
[32m[04/15 04:40:28 d2.utils.events]: [0m eta: 0:13:39  iter: 519  total_loss: 0.667  loss_cls: 0.215  loss_box_reg: 0.339  loss_rpn_cls: 0.021  loss_rpn_loc: 0.050  time: 0.5652  data_time: 0.0086  lr: 0.002597  max_mem: 3067M
[32m[04/15 04:40:41 d2.utils.events]: [0m eta: 0:13:30  iter: 539  total_loss: 0.804  loss_cls: 0.262  loss_box_reg: 0.396  loss_rpn_cls: 0.023  loss_rpn_loc: 0.070  time: 0.5667  data_time: 0.0078  lr: 0.002697  max_mem: 3067M
[32m[04/15 04:40:53 d2.utils.events]: [0m eta: 0:13:20  iter: 559  total_loss: 0.633  loss_cls: 0.216  loss_box_reg: 0.322  loss_rpn_cls: 0.034  loss_rpn_loc: 0.043  time: 0.5677  data_time: 0.0084  lr: 0.002797  max_mem: 3067M
[32m[04/15 04:41:04 d2.utils.events]: [0m eta: 0:13:09  iter: 579  total_loss: 0.604  loss_cls: 0.216  loss_box_reg: 0.327  loss_rpn_cls: 0.025  loss_rpn_loc: 0.050  time: 0.5677  data_time: 0.0092  lr: 0.002897  max_mem: 3067M
[32m[04/15 04:41:16 d2.utils.events]: [0m eta: 0:13:00  iter: 599  total_loss: 0.681  loss_cls: 0.210  loss_box_reg: 0.324  loss_rpn_cls: 0.026  loss_rpn_loc: 0.067  time: 0.5687  data_time: 0.0085  lr: 0.002997  max_mem: 3067M
[32m[04/15 04:41:27 d2.utils.events]: [0m eta: 0:12:47  iter: 619  total_loss: 0.694  loss_cls: 0.217  loss_box_reg: 0.345  loss_rpn_cls: 0.024  loss_rpn_loc: 0.070  time: 0.5683  data_time: 0.0086  lr: 0.003097  max_mem: 3067M
[32m[04/15 04:41:38 d2.utils.events]: [0m eta: 0:12:36  iter: 639  total_loss: 0.693  loss_cls: 0.258  loss_box_reg: 0.360  loss_rpn_cls: 0.034  loss_rpn_loc: 0.056  time: 0.5676  data_time: 0.0081  lr: 0.003197  max_mem: 3067M
[32m[04/15 04:41:50 d2.utils.events]: [0m eta: 0:12:26  iter: 659  total_loss: 0.675  loss_cls: 0.233  loss_box_reg: 0.349  loss_rpn_cls: 0.025  loss_rpn_loc: 0.055  time: 0.5689  data_time: 0.0084  lr: 0.003297  max_mem: 3067M
[32m[04/15 04:42:01 d2.utils.events]: [0m eta: 0:12:15  iter: 679  total_loss: 0.677  loss_cls: 0.221  loss_box_reg: 0.329  loss_rpn_cls: 0.028  loss_rpn_loc: 0.065  time: 0.5680  data_time: 0.0076  lr: 0.003397  max_mem: 3067M
[32m[04/15 04:42:13 d2.utils.events]: [0m eta: 0:12:05  iter: 699  total_loss: 0.634  loss_cls: 0.205  loss_box_reg: 0.335  loss_rpn_cls: 0.032  loss_rpn_loc: 0.056  time: 0.5687  data_time: 0.0078  lr: 0.003497  max_mem: 3067M
[32m[04/15 04:42:25 d2.utils.events]: [0m eta: 0:11:56  iter: 719  total_loss: 0.767  loss_cls: 0.273  loss_box_reg: 0.414  loss_rpn_cls: 0.028  loss_rpn_loc: 0.062  time: 0.5696  data_time: 0.0088  lr: 0.003596  max_mem: 3067M
[32m[04/15 04:42:36 d2.utils.events]: [0m eta: 0:11:43  iter: 739  total_loss: 0.747  loss_cls: 0.265  loss_box_reg: 0.359  loss_rpn_cls: 0.028  loss_rpn_loc: 0.074  time: 0.5688  data_time: 0.0083  lr: 0.003696  max_mem: 3067M
[32m[04/15 04:42:48 d2.utils.events]: [0m eta: 0:11:30  iter: 759  total_loss: 0.680  loss_cls: 0.215  loss_box_reg: 0.357  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5688  data_time: 0.0078  lr: 0.003796  max_mem: 3067M
[32m[04/15 04:42:59 d2.utils.events]: [0m eta: 0:11:18  iter: 779  total_loss: 0.718  loss_cls: 0.219  loss_box_reg: 0.392  loss_rpn_cls: 0.027  loss_rpn_loc: 0.055  time: 0.5687  data_time: 0.0088  lr: 0.003896  max_mem: 3067M
[32m[04/15 04:43:10 d2.utils.events]: [0m eta: 0:11:07  iter: 799  total_loss: 0.832  loss_cls: 0.259  loss_box_reg: 0.452  loss_rpn_cls: 0.040  loss_rpn_loc: 0.073  time: 0.5679  data_time: 0.0091  lr: 0.003996  max_mem: 3067M
[32m[04/15 04:43:21 d2.utils.events]: [0m eta: 0:10:55  iter: 819  total_loss: 0.669  loss_cls: 0.236  loss_box_reg: 0.341  loss_rpn_cls: 0.030  loss_rpn_loc: 0.059  time: 0.5681  data_time: 0.0096  lr: 0.004096  max_mem: 3067M
[32m[04/15 04:43:33 d2.utils.events]: [0m eta: 0:10:44  iter: 839  total_loss: 0.671  loss_cls: 0.225  loss_box_reg: 0.349  loss_rpn_cls: 0.025  loss_rpn_loc: 0.062  time: 0.5679  data_time: 0.0080  lr: 0.004196  max_mem: 3067M
[32m[04/15 04:43:44 d2.utils.events]: [0m eta: 0:10:32  iter: 859  total_loss: 0.634  loss_cls: 0.217  loss_box_reg: 0.317  loss_rpn_cls: 0.025  loss_rpn_loc: 0.052  time: 0.5674  data_time: 0.0087  lr: 0.004296  max_mem: 3067M
[32m[04/15 04:43:55 d2.utils.events]: [0m eta: 0:10:21  iter: 879  total_loss: 0.690  loss_cls: 0.224  loss_box_reg: 0.361  loss_rpn_cls: 0.028  loss_rpn_loc: 0.064  time: 0.5673  data_time: 0.0083  lr: 0.004396  max_mem: 3067M
[32m[04/15 04:44:07 d2.utils.events]: [0m eta: 0:10:10  iter: 899  total_loss: 0.854  loss_cls: 0.286  loss_box_reg: 0.416  loss_rpn_cls: 0.034  loss_rpn_loc: 0.063  time: 0.5673  data_time: 0.0088  lr: 0.004496  max_mem: 3067M
[32m[04/15 04:44:18 d2.utils.events]: [0m eta: 0:09:58  iter: 919  total_loss: 0.706  loss_cls: 0.232  loss_box_reg: 0.377  loss_rpn_cls: 0.032  loss_rpn_loc: 0.053  time: 0.5671  data_time: 0.0085  lr: 0.004595  max_mem: 3067M
[32m[04/15 04:44:30 d2.utils.events]: [0m eta: 0:09:48  iter: 939  total_loss: 0.895  loss_cls: 0.285  loss_box_reg: 0.444  loss_rpn_cls: 0.030  loss_rpn_loc: 0.091  time: 0.5680  data_time: 0.0085  lr: 0.004695  max_mem: 3067M
[32m[04/15 04:44:41 d2.utils.events]: [0m eta: 0:09:37  iter: 959  total_loss: 0.625  loss_cls: 0.244  loss_box_reg: 0.333  loss_rpn_cls: 0.030  loss_rpn_loc: 0.050  time: 0.5675  data_time: 0.0088  lr: 0.004795  max_mem: 3067M
[32m[04/15 04:44:52 d2.utils.events]: [0m eta: 0:09:26  iter: 979  total_loss: 0.668  loss_cls: 0.234  loss_box_reg: 0.350  loss_rpn_cls: 0.034  loss_rpn_loc: 0.060  time: 0.5672  data_time: 0.0079  lr: 0.004895  max_mem: 3067M
[32m[04/15 04:45:04 d2.utils.events]: [0m eta: 0:09:15  iter: 999  total_loss: 0.968  loss_cls: 0.311  loss_box_reg: 0.500  loss_rpn_cls: 0.030  loss_rpn_loc: 0.084  time: 0.5682  data_time: 0.0085  lr: 0.004995  max_mem: 3067M
[32m[04/15 04:45:15 d2.utils.events]: [0m eta: 0:09:02  iter: 1019  total_loss: 0.812  loss_cls: 0.257  loss_box_reg: 0.414  loss_rpn_cls: 0.031  loss_rpn_loc: 0.087  time: 0.5673  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:45:26 d2.utils.events]: [0m eta: 0:08:51  iter: 1039  total_loss: 0.729  loss_cls: 0.246  loss_box_reg: 0.342  loss_rpn_cls: 0.030  loss_rpn_loc: 0.062  time: 0.5672  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:45:39 d2.utils.events]: [0m eta: 0:08:42  iter: 1059  total_loss: 0.716  loss_cls: 0.256  loss_box_reg: 0.348  loss_rpn_cls: 0.033  loss_rpn_loc: 0.058  time: 0.5679  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:45:50 d2.utils.events]: [0m eta: 0:08:30  iter: 1079  total_loss: 0.667  loss_cls: 0.224  loss_box_reg: 0.327  loss_rpn_cls: 0.028  loss_rpn_loc: 0.064  time: 0.5679  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:46:01 d2.utils.events]: [0m eta: 0:08:19  iter: 1099  total_loss: 0.604  loss_cls: 0.217  loss_box_reg: 0.322  loss_rpn_cls: 0.031  loss_rpn_loc: 0.053  time: 0.5679  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:46:13 d2.utils.events]: [0m eta: 0:08:07  iter: 1119  total_loss: 0.709  loss_cls: 0.233  loss_box_reg: 0.396  loss_rpn_cls: 0.017  loss_rpn_loc: 0.059  time: 0.5681  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:46:24 d2.utils.events]: [0m eta: 0:07:56  iter: 1139  total_loss: 0.695  loss_cls: 0.243  loss_box_reg: 0.369  loss_rpn_cls: 0.033  loss_rpn_loc: 0.048  time: 0.5680  data_time: 0.0094  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:46:36 d2.utils.events]: [0m eta: 0:07:45  iter: 1159  total_loss: 0.864  loss_cls: 0.305  loss_box_reg: 0.472  loss_rpn_cls: 0.031  loss_rpn_loc: 0.072  time: 0.5682  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:46:48 d2.utils.events]: [0m eta: 0:07:35  iter: 1179  total_loss: 0.584  loss_cls: 0.229  loss_box_reg: 0.317  loss_rpn_cls: 0.023  loss_rpn_loc: 0.048  time: 0.5683  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:46:59 d2.utils.events]: [0m eta: 0:07:22  iter: 1199  total_loss: 0.791  loss_cls: 0.253  loss_box_reg: 0.388  loss_rpn_cls: 0.038  loss_rpn_loc: 0.079  time: 0.5679  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:47:11 d2.utils.events]: [0m eta: 0:07:12  iter: 1219  total_loss: 0.788  loss_cls: 0.286  loss_box_reg: 0.404  loss_rpn_cls: 0.038  loss_rpn_loc: 0.062  time: 0.5683  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:47:22 d2.utils.events]: [0m eta: 0:07:01  iter: 1239  total_loss: 0.716  loss_cls: 0.243  loss_box_reg: 0.375  loss_rpn_cls: 0.025  loss_rpn_loc: 0.058  time: 0.5681  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:47:33 d2.utils.events]: [0m eta: 0:06:49  iter: 1259  total_loss: 0.812  loss_cls: 0.277  loss_box_reg: 0.387  loss_rpn_cls: 0.034  loss_rpn_loc: 0.082  time: 0.5677  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:47:45 d2.utils.events]: [0m eta: 0:06:40  iter: 1279  total_loss: 0.573  loss_cls: 0.212  loss_box_reg: 0.307  loss_rpn_cls: 0.025  loss_rpn_loc: 0.046  time: 0.5687  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:47:56 d2.utils.events]: [0m eta: 0:06:28  iter: 1299  total_loss: 0.748  loss_cls: 0.227  loss_box_reg: 0.368  loss_rpn_cls: 0.032  loss_rpn_loc: 0.071  time: 0.5683  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:48:07 d2.utils.events]: [0m eta: 0:06:16  iter: 1319  total_loss: 0.724  loss_cls: 0.237  loss_box_reg: 0.369  loss_rpn_cls: 0.036  loss_rpn_loc: 0.058  time: 0.5678  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:48:19 d2.utils.events]: [0m eta: 0:06:06  iter: 1339  total_loss: 0.607  loss_cls: 0.216  loss_box_reg: 0.322  loss_rpn_cls: 0.025  loss_rpn_loc: 0.066  time: 0.5685  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:48:30 d2.utils.events]: [0m eta: 0:05:55  iter: 1359  total_loss: 0.699  loss_cls: 0.243  loss_box_reg: 0.381  loss_rpn_cls: 0.038  loss_rpn_loc: 0.055  time: 0.5682  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:48:42 d2.utils.events]: [0m eta: 0:05:43  iter: 1379  total_loss: 0.749  loss_cls: 0.250  loss_box_reg: 0.393  loss_rpn_cls: 0.040  loss_rpn_loc: 0.073  time: 0.5680  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:48:53 d2.utils.events]: [0m eta: 0:05:33  iter: 1399  total_loss: 0.748  loss_cls: 0.248  loss_box_reg: 0.375  loss_rpn_cls: 0.038  loss_rpn_loc: 0.080  time: 0.5682  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:49:04 d2.utils.events]: [0m eta: 0:05:22  iter: 1419  total_loss: 0.835  loss_cls: 0.258  loss_box_reg: 0.369  loss_rpn_cls: 0.040  loss_rpn_loc: 0.079  time: 0.5678  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:49:16 d2.utils.events]: [0m eta: 0:05:10  iter: 1439  total_loss: 0.704  loss_cls: 0.226  loss_box_reg: 0.352  loss_rpn_cls: 0.029  loss_rpn_loc: 0.086  time: 0.5679  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:49:27 d2.utils.events]: [0m eta: 0:04:59  iter: 1459  total_loss: 0.732  loss_cls: 0.245  loss_box_reg: 0.378  loss_rpn_cls: 0.037  loss_rpn_loc: 0.068  time: 0.5678  data_time: 0.0095  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:49:38 d2.utils.events]: [0m eta: 0:04:48  iter: 1479  total_loss: 0.709  loss_cls: 0.238  loss_box_reg: 0.357  loss_rpn_cls: 0.025  loss_rpn_loc: 0.080  time: 0.5677  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:49:50 d2.utils.events]: [0m eta: 0:04:37  iter: 1499  total_loss: 0.778  loss_cls: 0.269  loss_box_reg: 0.379  loss_rpn_cls: 0.034  loss_rpn_loc: 0.072  time: 0.5681  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:50:01 d2.utils.events]: [0m eta: 0:04:26  iter: 1519  total_loss: 0.506  loss_cls: 0.165  loss_box_reg: 0.270  loss_rpn_cls: 0.029  loss_rpn_loc: 0.040  time: 0.5677  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:50:12 d2.utils.events]: [0m eta: 0:04:15  iter: 1539  total_loss: 0.609  loss_cls: 0.211  loss_box_reg: 0.329  loss_rpn_cls: 0.027  loss_rpn_loc: 0.048  time: 0.5673  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:50:24 d2.utils.events]: [0m eta: 0:04:03  iter: 1559  total_loss: 0.644  loss_cls: 0.204  loss_box_reg: 0.328  loss_rpn_cls: 0.019  loss_rpn_loc: 0.078  time: 0.5674  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:50:36 d2.utils.events]: [0m eta: 0:03:53  iter: 1579  total_loss: 0.596  loss_cls: 0.211  loss_box_reg: 0.311  loss_rpn_cls: 0.028  loss_rpn_loc: 0.067  time: 0.5682  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:50:47 d2.utils.events]: [0m eta: 0:03:41  iter: 1599  total_loss: 0.774  loss_cls: 0.234  loss_box_reg: 0.409  loss_rpn_cls: 0.036  loss_rpn_loc: 0.066  time: 0.5679  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:50:58 d2.utils.events]: [0m eta: 0:03:30  iter: 1619  total_loss: 0.852  loss_cls: 0.254  loss_box_reg: 0.444  loss_rpn_cls: 0.029  loss_rpn_loc: 0.073  time: 0.5675  data_time: 0.0095  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:51:10 d2.utils.events]: [0m eta: 0:03:19  iter: 1639  total_loss: 0.789  loss_cls: 0.266  loss_box_reg: 0.438  loss_rpn_cls: 0.043  loss_rpn_loc: 0.068  time: 0.5679  data_time: 0.0094  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:51:21 d2.utils.events]: [0m eta: 0:03:08  iter: 1659  total_loss: 0.668  loss_cls: 0.217  loss_box_reg: 0.321  loss_rpn_cls: 0.033  loss_rpn_loc: 0.055  time: 0.5677  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:51:32 d2.utils.events]: [0m eta: 0:02:57  iter: 1679  total_loss: 0.815  loss_cls: 0.267  loss_box_reg: 0.425  loss_rpn_cls: 0.036  loss_rpn_loc: 0.068  time: 0.5672  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:51:43 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.667  loss_cls: 0.234  loss_box_reg: 0.318  loss_rpn_cls: 0.044  loss_rpn_loc: 0.071  time: 0.5673  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:51:54 d2.utils.events]: [0m eta: 0:02:34  iter: 1719  total_loss: 0.682  loss_cls: 0.240  loss_box_reg: 0.308  loss_rpn_cls: 0.044  loss_rpn_loc: 0.067  time: 0.5670  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:52:06 d2.utils.events]: [0m eta: 0:02:23  iter: 1739  total_loss: 0.726  loss_cls: 0.241  loss_box_reg: 0.368  loss_rpn_cls: 0.039  loss_rpn_loc: 0.054  time: 0.5670  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:52:17 d2.utils.events]: [0m eta: 0:02:12  iter: 1759  total_loss: 0.771  loss_cls: 0.259  loss_box_reg: 0.404  loss_rpn_cls: 0.036  loss_rpn_loc: 0.071  time: 0.5671  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:52:29 d2.utils.events]: [0m eta: 0:02:01  iter: 1779  total_loss: 0.677  loss_cls: 0.223  loss_box_reg: 0.343  loss_rpn_cls: 0.027  loss_rpn_loc: 0.058  time: 0.5671  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:52:42 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.937  loss_cls: 0.313  loss_box_reg: 0.491  loss_rpn_cls: 0.037  loss_rpn_loc: 0.092  time: 0.5678  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:52:52 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.586  loss_cls: 0.215  loss_box_reg: 0.300  loss_rpn_cls: 0.044  loss_rpn_loc: 0.059  time: 0.5675  data_time: 0.0056  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:53:04 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.706  loss_cls: 0.263  loss_box_reg: 0.377  loss_rpn_cls: 0.033  loss_rpn_loc: 0.059  time: 0.5674  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:53:15 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.811  loss_cls: 0.272  loss_box_reg: 0.442  loss_rpn_cls: 0.032  loss_rpn_loc: 0.072  time: 0.5674  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:53:26 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.695  loss_cls: 0.233  loss_box_reg: 0.364  loss_rpn_cls: 0.026  loss_rpn_loc: 0.068  time: 0.5669  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:53:37 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.657  loss_cls: 0.220  loss_box_reg: 0.334  loss_rpn_cls: 0.027  loss_rpn_loc: 0.055  time: 0.5670  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:53:50 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.711  loss_cls: 0.253  loss_box_reg: 0.381  loss_rpn_cls: 0.028  loss_rpn_loc: 0.059  time: 0.5675  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:54:00 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.647  loss_cls: 0.243  loss_box_reg: 0.326  loss_rpn_cls: 0.031  loss_rpn_loc: 0.051  time: 0.5673  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:54:13 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.848  loss_cls: 0.274  loss_box_reg: 0.424  loss_rpn_cls: 0.032  loss_rpn_loc: 0.056  time: 0.5677  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:54:24 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.738  loss_cls: 0.254  loss_box_reg: 0.348  loss_rpn_cls: 0.041  loss_rpn_loc: 0.078  time: 0.5677  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 04:54:41 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 04:54:41 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 04:54:41 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 04:54:41 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.747  loss_cls: 0.248  loss_box_reg: 0.376  loss_rpn_cls: 0.035  loss_rpn_loc: 0.064  time: 0.5673  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 04:54:43 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:53 (0.5677 s / it)
[32m[04/15 04:54:43 d2.engine.hooks]: [0mTotal training time: 0:19:09 (0:00:15 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 04:54:48 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 04:54:48 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 04:54:49 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 04:54:51 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1207 s / img. ETA=0:02:33
[32m[04/15 04:54:56 d2.evaluation.evaluator]: [0mInference done 52/1257. 0.1205 s / img. ETA=0:02:29
[32m[04/15 04:55:01 d2.evaluation.evaluator]: [0mInference done 93/1257. 0.1205 s / img. ETA=0:02:24
[32m[04/15 04:55:06 d2.evaluation.evaluator]: [0mInference done 134/1257. 0.1208 s / img. ETA=0:02:19
[32m[04/15 04:55:11 d2.evaluation.evaluator]: [0mInference done 173/1257. 0.1219 s / img. ETA=0:02:16
[32m[04/15 04:55:16 d2.evaluation.evaluator]: [0mInference done 208/1257. 0.1255 s / img. ETA=0:02:15
[32m[04/15 04:55:21 d2.evaluation.evaluator]: [0mInference done 242/1257. 0.1285 s / img. ETA=0:02:14
[32m[04/15 04:55:27 d2.evaluation.evaluator]: [0mInference done 283/1257. 0.1274 s / img. ETA=0:02:07
[32m[04/15 04:55:32 d2.evaluation.evaluator]: [0mInference done 324/1257. 0.1264 s / img. ETA=0:02:01
[32m[04/15 04:55:37 d2.evaluation.evaluator]: [0mInference done 364/1257. 0.1259 s / img. ETA=0:01:55
[32m[04/15 04:55:42 d2.evaluation.evaluator]: [0mInference done 405/1257. 0.1252 s / img. ETA=0:01:49
[32m[04/15 04:55:47 d2.evaluation.evaluator]: [0mInference done 443/1257. 0.1257 s / img. ETA=0:01:45
[32m[04/15 04:55:52 d2.evaluation.evaluator]: [0mInference done 477/1257. 0.1272 s / img. ETA=0:01:42
[32m[04/15 04:55:57 d2.evaluation.evaluator]: [0mInference done 511/1257. 0.1278 s / img. ETA=0:01:38
[32m[04/15 04:56:02 d2.evaluation.evaluator]: [0mInference done 550/1257. 0.1273 s / img. ETA=0:01:33
[32m[04/15 04:56:07 d2.evaluation.evaluator]: [0mInference done 589/1257. 0.1269 s / img. ETA=0:01:28
[32m[04/15 04:56:12 d2.evaluation.evaluator]: [0mInference done 629/1257. 0.1265 s / img. ETA=0:01:22
[32m[04/15 04:56:17 d2.evaluation.evaluator]: [0mInference done 669/1257. 0.1261 s / img. ETA=0:01:17
[32m[04/15 04:56:22 d2.evaluation.evaluator]: [0mInference done 707/1257. 0.1264 s / img. ETA=0:01:12
[32m[04/15 04:56:27 d2.evaluation.evaluator]: [0mInference done 742/1257. 0.1271 s / img. ETA=0:01:07
[32m[04/15 04:56:32 d2.evaluation.evaluator]: [0mInference done 777/1257. 0.1278 s / img. ETA=0:01:03
[32m[04/15 04:56:38 d2.evaluation.evaluator]: [0mInference done 818/1257. 0.1275 s / img. ETA=0:00:58
[32m[04/15 04:56:43 d2.evaluation.evaluator]: [0mInference done 859/1257. 0.1271 s / img. ETA=0:00:52
[32m[04/15 04:56:48 d2.evaluation.evaluator]: [0mInference done 900/1257. 0.1268 s / img. ETA=0:00:46
[32m[04/15 04:56:53 d2.evaluation.evaluator]: [0mInference done 941/1257. 0.1266 s / img. ETA=0:00:41
[32m[04/15 04:56:58 d2.evaluation.evaluator]: [0mInference done 979/1257. 0.1267 s / img. ETA=0:00:36
[32m[04/15 04:57:03 d2.evaluation.evaluator]: [0mInference done 1012/1257. 0.1274 s / img. ETA=0:00:32
[32m[04/15 04:57:08 d2.evaluation.evaluator]: [0mInference done 1048/1257. 0.1277 s / img. ETA=0:00:27
[32m[04/15 04:57:13 d2.evaluation.evaluator]: [0mInference done 1089/1257. 0.1275 s / img. ETA=0:00:22
[32m[04/15 04:57:18 d2.evaluation.evaluator]: [0mInference done 1130/1257. 0.1272 s / img. ETA=0:00:16
[32m[04/15 04:57:23 d2.evaluation.evaluator]: [0mInference done 1171/1257. 0.1269 s / img. ETA=0:00:11
[32m[04/15 04:57:28 d2.evaluation.evaluator]: [0mInference done 1211/1257. 0.1269 s / img. ETA=0:00:06
[32m[04/15 04:57:33 d2.evaluation.evaluator]: [0mInference done 1249/1257. 0.1270 s / img. ETA=0:00:01
[32m[04/15 04:57:35 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:44.690570 (0.131542 s / img per device, on 1 devices)
[32m[04/15 04:57:35 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:39 (0.127114 s / img per device, on 1 devices)
[32m[04/15 04:57:35 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 04:57:35 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 04:57:35 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.00s).
Accumulating evaluation results...
DONE (t=0.76s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.167
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.358
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.096
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.085
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.138
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.421
[32m[04/15 04:57:41 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 16.716 | 35.841 | 14.008 | 9.635 | 20.897 | 37.904 |
[32m[04/15 04:57:41 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 22.489 | bicycle       | 4.849 | car            | 38.535 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.990 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  4  *  2000  iterations ============
4 channel input
[32m[04/15 04:57:43 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 04:57:44 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.47 seconds.
[5m[31mWARNING[0m [32m[04/15 04:57:44 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 04:57:44 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 04:57:45 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 04:57:45 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 04:57:45 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 04:57:46 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 04:57:57 d2.utils.events]: [0m eta: 0:17:54  iter: 19  total_loss: 0.709  loss_cls: 0.236  loss_box_reg: 0.350  loss_rpn_cls: 0.040  loss_rpn_loc: 0.057  time: 0.5397  data_time: 0.0504  lr: 0.000100  max_mem: 3067M
[32m[04/15 04:58:09 d2.utils.events]: [0m eta: 0:18:09  iter: 39  total_loss: 0.811  loss_cls: 0.255  loss_box_reg: 0.427  loss_rpn_cls: 0.037  loss_rpn_loc: 0.074  time: 0.5653  data_time: 0.0083  lr: 0.000200  max_mem: 3067M
[32m[04/15 04:58:20 d2.utils.events]: [0m eta: 0:17:57  iter: 59  total_loss: 0.634  loss_cls: 0.225  loss_box_reg: 0.319  loss_rpn_cls: 0.034  loss_rpn_loc: 0.068  time: 0.5654  data_time: 0.0085  lr: 0.000300  max_mem: 3067M
[32m[04/15 04:58:31 d2.utils.events]: [0m eta: 0:17:30  iter: 79  total_loss: 0.661  loss_cls: 0.232  loss_box_reg: 0.344  loss_rpn_cls: 0.042  loss_rpn_loc: 0.065  time: 0.5584  data_time: 0.0079  lr: 0.000400  max_mem: 3067M
[32m[04/15 04:58:44 d2.utils.events]: [0m eta: 0:17:43  iter: 99  total_loss: 0.800  loss_cls: 0.257  loss_box_reg: 0.433  loss_rpn_cls: 0.036  loss_rpn_loc: 0.072  time: 0.5709  data_time: 0.0089  lr: 0.000500  max_mem: 3067M
[32m[04/15 04:58:55 d2.utils.events]: [0m eta: 0:17:30  iter: 119  total_loss: 0.650  loss_cls: 0.207  loss_box_reg: 0.356  loss_rpn_cls: 0.025  loss_rpn_loc: 0.053  time: 0.5696  data_time: 0.0086  lr: 0.000599  max_mem: 3067M
[32m[04/15 04:59:06 d2.utils.events]: [0m eta: 0:17:13  iter: 139  total_loss: 0.783  loss_cls: 0.240  loss_box_reg: 0.399  loss_rpn_cls: 0.036  loss_rpn_loc: 0.074  time: 0.5643  data_time: 0.0075  lr: 0.000699  max_mem: 3067M
[32m[04/15 04:59:18 d2.utils.events]: [0m eta: 0:17:08  iter: 159  total_loss: 0.683  loss_cls: 0.215  loss_box_reg: 0.366  loss_rpn_cls: 0.025  loss_rpn_loc: 0.062  time: 0.5688  data_time: 0.0088  lr: 0.000799  max_mem: 3067M
[32m[04/15 04:59:29 d2.utils.events]: [0m eta: 0:16:55  iter: 179  total_loss: 0.653  loss_cls: 0.228  loss_box_reg: 0.351  loss_rpn_cls: 0.030  loss_rpn_loc: 0.044  time: 0.5673  data_time: 0.0082  lr: 0.000899  max_mem: 3067M
[32m[04/15 04:59:40 d2.utils.events]: [0m eta: 0:16:41  iter: 199  total_loss: 0.662  loss_cls: 0.227  loss_box_reg: 0.356  loss_rpn_cls: 0.026  loss_rpn_loc: 0.062  time: 0.5650  data_time: 0.0082  lr: 0.000999  max_mem: 3067M
[32m[04/15 04:59:52 d2.utils.events]: [0m eta: 0:16:34  iter: 219  total_loss: 0.566  loss_cls: 0.189  loss_box_reg: 0.313  loss_rpn_cls: 0.030  loss_rpn_loc: 0.031  time: 0.5704  data_time: 0.0092  lr: 0.001099  max_mem: 3067M
[32m[04/15 05:00:03 d2.utils.events]: [0m eta: 0:16:18  iter: 239  total_loss: 0.603  loss_cls: 0.212  loss_box_reg: 0.341  loss_rpn_cls: 0.033  loss_rpn_loc: 0.042  time: 0.5665  data_time: 0.0079  lr: 0.001199  max_mem: 3067M
[32m[04/15 05:00:14 d2.utils.events]: [0m eta: 0:16:02  iter: 259  total_loss: 0.658  loss_cls: 0.224  loss_box_reg: 0.343  loss_rpn_cls: 0.032  loss_rpn_loc: 0.051  time: 0.5648  data_time: 0.0079  lr: 0.001299  max_mem: 3067M
[32m[04/15 05:00:27 d2.utils.events]: [0m eta: 0:15:58  iter: 279  total_loss: 0.590  loss_cls: 0.202  loss_box_reg: 0.309  loss_rpn_cls: 0.024  loss_rpn_loc: 0.060  time: 0.5705  data_time: 0.0087  lr: 0.001399  max_mem: 3067M
[32m[04/15 05:00:37 d2.utils.events]: [0m eta: 0:15:44  iter: 299  total_loss: 0.697  loss_cls: 0.215  loss_box_reg: 0.386  loss_rpn_cls: 0.030  loss_rpn_loc: 0.057  time: 0.5672  data_time: 0.0083  lr: 0.001499  max_mem: 3067M
[32m[04/15 05:00:49 d2.utils.events]: [0m eta: 0:15:33  iter: 319  total_loss: 0.644  loss_cls: 0.226  loss_box_reg: 0.326  loss_rpn_cls: 0.029  loss_rpn_loc: 0.057  time: 0.5672  data_time: 0.0079  lr: 0.001598  max_mem: 3067M
[32m[04/15 05:01:01 d2.utils.events]: [0m eta: 0:15:23  iter: 339  total_loss: 0.736  loss_cls: 0.249  loss_box_reg: 0.403  loss_rpn_cls: 0.026  loss_rpn_loc: 0.054  time: 0.5692  data_time: 0.0081  lr: 0.001698  max_mem: 3067M
[32m[04/15 05:01:12 d2.utils.events]: [0m eta: 0:15:11  iter: 359  total_loss: 0.634  loss_cls: 0.202  loss_box_reg: 0.313  loss_rpn_cls: 0.026  loss_rpn_loc: 0.064  time: 0.5675  data_time: 0.0086  lr: 0.001798  max_mem: 3067M
[32m[04/15 05:01:24 d2.utils.events]: [0m eta: 0:15:00  iter: 379  total_loss: 0.736  loss_cls: 0.233  loss_box_reg: 0.411  loss_rpn_cls: 0.021  loss_rpn_loc: 0.055  time: 0.5691  data_time: 0.0078  lr: 0.001898  max_mem: 3067M
[32m[04/15 05:01:35 d2.utils.events]: [0m eta: 0:14:48  iter: 399  total_loss: 0.834  loss_cls: 0.273  loss_box_reg: 0.453  loss_rpn_cls: 0.017  loss_rpn_loc: 0.062  time: 0.5692  data_time: 0.0080  lr: 0.001998  max_mem: 3067M
[32m[04/15 05:01:46 d2.utils.events]: [0m eta: 0:14:36  iter: 419  total_loss: 0.620  loss_cls: 0.204  loss_box_reg: 0.346  loss_rpn_cls: 0.028  loss_rpn_loc: 0.061  time: 0.5680  data_time: 0.0094  lr: 0.002098  max_mem: 3067M
[32m[04/15 05:01:58 d2.utils.events]: [0m eta: 0:14:26  iter: 439  total_loss: 0.698  loss_cls: 0.221  loss_box_reg: 0.365  loss_rpn_cls: 0.029  loss_rpn_loc: 0.068  time: 0.5691  data_time: 0.0081  lr: 0.002198  max_mem: 3067M
[32m[04/15 05:02:10 d2.utils.events]: [0m eta: 0:14:15  iter: 459  total_loss: 0.626  loss_cls: 0.203  loss_box_reg: 0.324  loss_rpn_cls: 0.023  loss_rpn_loc: 0.061  time: 0.5698  data_time: 0.0090  lr: 0.002298  max_mem: 3067M
[32m[04/15 05:02:21 d2.utils.events]: [0m eta: 0:14:03  iter: 479  total_loss: 0.680  loss_cls: 0.221  loss_box_reg: 0.373  loss_rpn_cls: 0.026  loss_rpn_loc: 0.069  time: 0.5683  data_time: 0.0069  lr: 0.002398  max_mem: 3067M
[32m[04/15 05:02:33 d2.utils.events]: [0m eta: 0:13:53  iter: 499  total_loss: 0.739  loss_cls: 0.235  loss_box_reg: 0.393  loss_rpn_cls: 0.025  loss_rpn_loc: 0.067  time: 0.5696  data_time: 0.0084  lr: 0.002498  max_mem: 3067M
[32m[04/15 05:02:44 d2.utils.events]: [0m eta: 0:13:42  iter: 519  total_loss: 0.682  loss_cls: 0.212  loss_box_reg: 0.385  loss_rpn_cls: 0.019  loss_rpn_loc: 0.054  time: 0.5693  data_time: 0.0089  lr: 0.002597  max_mem: 3067M
[32m[04/15 05:02:55 d2.utils.events]: [0m eta: 0:13:30  iter: 539  total_loss: 0.825  loss_cls: 0.286  loss_box_reg: 0.427  loss_rpn_cls: 0.028  loss_rpn_loc: 0.072  time: 0.5679  data_time: 0.0089  lr: 0.002697  max_mem: 3067M
[32m[04/15 05:03:07 d2.utils.events]: [0m eta: 0:13:20  iter: 559  total_loss: 0.684  loss_cls: 0.229  loss_box_reg: 0.379  loss_rpn_cls: 0.026  loss_rpn_loc: 0.059  time: 0.5699  data_time: 0.0085  lr: 0.002797  max_mem: 3067M
[32m[04/15 05:03:18 d2.utils.events]: [0m eta: 0:13:08  iter: 579  total_loss: 0.695  loss_cls: 0.232  loss_box_reg: 0.342  loss_rpn_cls: 0.023  loss_rpn_loc: 0.043  time: 0.5686  data_time: 0.0081  lr: 0.002897  max_mem: 3067M
[32m[04/15 05:03:30 d2.utils.events]: [0m eta: 0:12:58  iter: 599  total_loss: 0.817  loss_cls: 0.266  loss_box_reg: 0.403  loss_rpn_cls: 0.028  loss_rpn_loc: 0.071  time: 0.5688  data_time: 0.0081  lr: 0.002997  max_mem: 3067M
[32m[04/15 05:03:42 d2.utils.events]: [0m eta: 0:12:48  iter: 619  total_loss: 0.845  loss_cls: 0.275  loss_box_reg: 0.446  loss_rpn_cls: 0.030  loss_rpn_loc: 0.075  time: 0.5701  data_time: 0.0090  lr: 0.003097  max_mem: 3067M
[32m[04/15 05:03:53 d2.utils.events]: [0m eta: 0:12:36  iter: 639  total_loss: 0.572  loss_cls: 0.194  loss_box_reg: 0.297  loss_rpn_cls: 0.029  loss_rpn_loc: 0.047  time: 0.5696  data_time: 0.0089  lr: 0.003197  max_mem: 3067M
[32m[04/15 05:04:04 d2.utils.events]: [0m eta: 0:12:26  iter: 659  total_loss: 0.635  loss_cls: 0.204  loss_box_reg: 0.337  loss_rpn_cls: 0.025  loss_rpn_loc: 0.048  time: 0.5694  data_time: 0.0086  lr: 0.003297  max_mem: 3067M
[32m[04/15 05:04:16 d2.utils.events]: [0m eta: 0:12:15  iter: 679  total_loss: 0.659  loss_cls: 0.212  loss_box_reg: 0.355  loss_rpn_cls: 0.028  loss_rpn_loc: 0.060  time: 0.5695  data_time: 0.0088  lr: 0.003397  max_mem: 3067M
[32m[04/15 05:04:28 d2.utils.events]: [0m eta: 0:12:04  iter: 699  total_loss: 0.700  loss_cls: 0.245  loss_box_reg: 0.371  loss_rpn_cls: 0.028  loss_rpn_loc: 0.057  time: 0.5702  data_time: 0.0089  lr: 0.003497  max_mem: 3067M
[32m[04/15 05:04:39 d2.utils.events]: [0m eta: 0:11:52  iter: 719  total_loss: 0.709  loss_cls: 0.244  loss_box_reg: 0.385  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5694  data_time: 0.0084  lr: 0.003596  max_mem: 3067M
[32m[04/15 05:04:51 d2.utils.events]: [0m eta: 0:11:41  iter: 739  total_loss: 0.715  loss_cls: 0.245  loss_box_reg: 0.399  loss_rpn_cls: 0.034  loss_rpn_loc: 0.075  time: 0.5698  data_time: 0.0073  lr: 0.003696  max_mem: 3067M
[32m[04/15 05:05:02 d2.utils.events]: [0m eta: 0:11:31  iter: 759  total_loss: 0.714  loss_cls: 0.247  loss_box_reg: 0.378  loss_rpn_cls: 0.023  loss_rpn_loc: 0.055  time: 0.5703  data_time: 0.0093  lr: 0.003796  max_mem: 3067M
[32m[04/15 05:05:13 d2.utils.events]: [0m eta: 0:11:19  iter: 779  total_loss: 0.744  loss_cls: 0.257  loss_box_reg: 0.395  loss_rpn_cls: 0.021  loss_rpn_loc: 0.075  time: 0.5698  data_time: 0.0075  lr: 0.003896  max_mem: 3067M
[32m[04/15 05:05:25 d2.utils.events]: [0m eta: 0:11:09  iter: 799  total_loss: 0.581  loss_cls: 0.190  loss_box_reg: 0.326  loss_rpn_cls: 0.021  loss_rpn_loc: 0.048  time: 0.5704  data_time: 0.0092  lr: 0.003996  max_mem: 3067M
[32m[04/15 05:05:37 d2.utils.events]: [0m eta: 0:10:58  iter: 819  total_loss: 0.792  loss_cls: 0.250  loss_box_reg: 0.440  loss_rpn_cls: 0.027  loss_rpn_loc: 0.058  time: 0.5705  data_time: 0.0078  lr: 0.004096  max_mem: 3067M
[32m[04/15 05:05:48 d2.utils.events]: [0m eta: 0:10:46  iter: 839  total_loss: 0.686  loss_cls: 0.223  loss_box_reg: 0.380  loss_rpn_cls: 0.023  loss_rpn_loc: 0.059  time: 0.5696  data_time: 0.0073  lr: 0.004196  max_mem: 3067M
[32m[04/15 05:06:00 d2.utils.events]: [0m eta: 0:10:35  iter: 859  total_loss: 0.809  loss_cls: 0.283  loss_box_reg: 0.441  loss_rpn_cls: 0.029  loss_rpn_loc: 0.064  time: 0.5702  data_time: 0.0086  lr: 0.004296  max_mem: 3067M
[32m[04/15 05:06:11 d2.utils.events]: [0m eta: 0:10:23  iter: 879  total_loss: 0.725  loss_cls: 0.236  loss_box_reg: 0.377  loss_rpn_cls: 0.025  loss_rpn_loc: 0.062  time: 0.5696  data_time: 0.0085  lr: 0.004396  max_mem: 3067M
[32m[04/15 05:06:21 d2.utils.events]: [0m eta: 0:10:11  iter: 899  total_loss: 0.685  loss_cls: 0.215  loss_box_reg: 0.382  loss_rpn_cls: 0.023  loss_rpn_loc: 0.064  time: 0.5689  data_time: 0.0087  lr: 0.004496  max_mem: 3067M
[32m[04/15 05:06:34 d2.utils.events]: [0m eta: 0:10:01  iter: 919  total_loss: 0.570  loss_cls: 0.189  loss_box_reg: 0.290  loss_rpn_cls: 0.025  loss_rpn_loc: 0.052  time: 0.5697  data_time: 0.0083  lr: 0.004595  max_mem: 3067M
[32m[04/15 05:06:44 d2.utils.events]: [0m eta: 0:09:49  iter: 939  total_loss: 0.600  loss_cls: 0.207  loss_box_reg: 0.320  loss_rpn_cls: 0.028  loss_rpn_loc: 0.048  time: 0.5690  data_time: 0.0085  lr: 0.004695  max_mem: 3067M
[32m[04/15 05:06:56 d2.utils.events]: [0m eta: 0:09:37  iter: 959  total_loss: 0.765  loss_cls: 0.255  loss_box_reg: 0.383  loss_rpn_cls: 0.029  loss_rpn_loc: 0.080  time: 0.5687  data_time: 0.0080  lr: 0.004795  max_mem: 3067M
[32m[04/15 05:07:08 d2.utils.events]: [0m eta: 0:09:26  iter: 979  total_loss: 0.718  loss_cls: 0.238  loss_box_reg: 0.399  loss_rpn_cls: 0.022  loss_rpn_loc: 0.056  time: 0.5697  data_time: 0.0079  lr: 0.004895  max_mem: 3067M
[32m[04/15 05:07:19 d2.utils.events]: [0m eta: 0:09:15  iter: 999  total_loss: 0.690  loss_cls: 0.227  loss_box_reg: 0.349  loss_rpn_cls: 0.026  loss_rpn_loc: 0.074  time: 0.5691  data_time: 0.0081  lr: 0.004995  max_mem: 3067M
[32m[04/15 05:07:30 d2.utils.events]: [0m eta: 0:09:04  iter: 1019  total_loss: 0.609  loss_cls: 0.206  loss_box_reg: 0.345  loss_rpn_cls: 0.024  loss_rpn_loc: 0.057  time: 0.5689  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:07:42 d2.utils.events]: [0m eta: 0:08:53  iter: 1039  total_loss: 0.688  loss_cls: 0.219  loss_box_reg: 0.347  loss_rpn_cls: 0.028  loss_rpn_loc: 0.080  time: 0.5690  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:07:52 d2.utils.events]: [0m eta: 0:08:40  iter: 1059  total_loss: 0.816  loss_cls: 0.276  loss_box_reg: 0.445  loss_rpn_cls: 0.027  loss_rpn_loc: 0.059  time: 0.5684  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:08:04 d2.utils.events]: [0m eta: 0:08:30  iter: 1079  total_loss: 0.717  loss_cls: 0.223  loss_box_reg: 0.373  loss_rpn_cls: 0.019  loss_rpn_loc: 0.051  time: 0.5687  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:08:16 d2.utils.events]: [0m eta: 0:08:19  iter: 1099  total_loss: 0.769  loss_cls: 0.249  loss_box_reg: 0.392  loss_rpn_cls: 0.030  loss_rpn_loc: 0.063  time: 0.5692  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:08:27 d2.utils.events]: [0m eta: 0:08:07  iter: 1119  total_loss: 0.665  loss_cls: 0.232  loss_box_reg: 0.334  loss_rpn_cls: 0.036  loss_rpn_loc: 0.060  time: 0.5683  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:08:38 d2.utils.events]: [0m eta: 0:07:56  iter: 1139  total_loss: 0.665  loss_cls: 0.233  loss_box_reg: 0.362  loss_rpn_cls: 0.026  loss_rpn_loc: 0.050  time: 0.5684  data_time: 0.0095  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:08:50 d2.utils.events]: [0m eta: 0:07:44  iter: 1159  total_loss: 0.760  loss_cls: 0.245  loss_box_reg: 0.384  loss_rpn_cls: 0.029  loss_rpn_loc: 0.068  time: 0.5684  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:09:00 d2.utils.events]: [0m eta: 0:07:32  iter: 1179  total_loss: 0.794  loss_cls: 0.264  loss_box_reg: 0.401  loss_rpn_cls: 0.025  loss_rpn_loc: 0.073  time: 0.5677  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:09:13 d2.utils.events]: [0m eta: 0:07:22  iter: 1199  total_loss: 0.774  loss_cls: 0.238  loss_box_reg: 0.376  loss_rpn_cls: 0.035  loss_rpn_loc: 0.069  time: 0.5684  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:09:23 d2.utils.events]: [0m eta: 0:07:10  iter: 1219  total_loss: 0.770  loss_cls: 0.258  loss_box_reg: 0.363  loss_rpn_cls: 0.035  loss_rpn_loc: 0.062  time: 0.5679  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:09:34 d2.utils.events]: [0m eta: 0:06:59  iter: 1239  total_loss: 0.655  loss_cls: 0.233  loss_box_reg: 0.333  loss_rpn_cls: 0.033  loss_rpn_loc: 0.052  time: 0.5676  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:09:47 d2.utils.events]: [0m eta: 0:06:50  iter: 1259  total_loss: 0.660  loss_cls: 0.206  loss_box_reg: 0.346  loss_rpn_cls: 0.023  loss_rpn_loc: 0.059  time: 0.5682  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:09:57 d2.utils.events]: [0m eta: 0:06:37  iter: 1279  total_loss: 0.720  loss_cls: 0.228  loss_box_reg: 0.355  loss_rpn_cls: 0.036  loss_rpn_loc: 0.071  time: 0.5675  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:10:08 d2.utils.events]: [0m eta: 0:06:26  iter: 1299  total_loss: 0.569  loss_cls: 0.191  loss_box_reg: 0.277  loss_rpn_cls: 0.029  loss_rpn_loc: 0.046  time: 0.5671  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:10:20 d2.utils.events]: [0m eta: 0:06:15  iter: 1319  total_loss: 0.679  loss_cls: 0.256  loss_box_reg: 0.354  loss_rpn_cls: 0.030  loss_rpn_loc: 0.053  time: 0.5678  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:10:31 d2.utils.events]: [0m eta: 0:06:04  iter: 1339  total_loss: 0.722  loss_cls: 0.250  loss_box_reg: 0.392  loss_rpn_cls: 0.025  loss_rpn_loc: 0.052  time: 0.5673  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:10:43 d2.utils.events]: [0m eta: 0:05:53  iter: 1359  total_loss: 0.715  loss_cls: 0.243  loss_box_reg: 0.391  loss_rpn_cls: 0.028  loss_rpn_loc: 0.043  time: 0.5676  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:10:55 d2.utils.events]: [0m eta: 0:05:42  iter: 1379  total_loss: 0.693  loss_cls: 0.231  loss_box_reg: 0.363  loss_rpn_cls: 0.031  loss_rpn_loc: 0.059  time: 0.5681  data_time: 0.0092  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:11:06 d2.utils.events]: [0m eta: 0:05:31  iter: 1399  total_loss: 0.526  loss_cls: 0.191  loss_box_reg: 0.285  loss_rpn_cls: 0.025  loss_rpn_loc: 0.033  time: 0.5677  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:11:18 d2.utils.events]: [0m eta: 0:05:20  iter: 1419  total_loss: 0.765  loss_cls: 0.277  loss_box_reg: 0.429  loss_rpn_cls: 0.026  loss_rpn_loc: 0.076  time: 0.5682  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:11:29 d2.utils.events]: [0m eta: 0:05:09  iter: 1439  total_loss: 0.670  loss_cls: 0.242  loss_box_reg: 0.356  loss_rpn_cls: 0.024  loss_rpn_loc: 0.063  time: 0.5682  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:11:40 d2.utils.events]: [0m eta: 0:04:57  iter: 1459  total_loss: 0.670  loss_cls: 0.232  loss_box_reg: 0.327  loss_rpn_cls: 0.028  loss_rpn_loc: 0.056  time: 0.5678  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:11:53 d2.utils.events]: [0m eta: 0:04:47  iter: 1479  total_loss: 0.839  loss_cls: 0.280  loss_box_reg: 0.453  loss_rpn_cls: 0.032  loss_rpn_loc: 0.065  time: 0.5684  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:12:04 d2.utils.events]: [0m eta: 0:04:35  iter: 1499  total_loss: 0.752  loss_cls: 0.237  loss_box_reg: 0.355  loss_rpn_cls: 0.034  loss_rpn_loc: 0.083  time: 0.5682  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:12:15 d2.utils.events]: [0m eta: 0:04:24  iter: 1519  total_loss: 0.767  loss_cls: 0.243  loss_box_reg: 0.422  loss_rpn_cls: 0.030  loss_rpn_loc: 0.064  time: 0.5682  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:12:27 d2.utils.events]: [0m eta: 0:04:14  iter: 1539  total_loss: 0.716  loss_cls: 0.237  loss_box_reg: 0.369  loss_rpn_cls: 0.023  loss_rpn_loc: 0.062  time: 0.5685  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:12:38 d2.utils.events]: [0m eta: 0:04:02  iter: 1559  total_loss: 0.731  loss_cls: 0.231  loss_box_reg: 0.369  loss_rpn_cls: 0.023  loss_rpn_loc: 0.059  time: 0.5678  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:12:49 d2.utils.events]: [0m eta: 0:03:51  iter: 1579  total_loss: 0.582  loss_cls: 0.211  loss_box_reg: 0.301  loss_rpn_cls: 0.020  loss_rpn_loc: 0.052  time: 0.5678  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:13:01 d2.utils.events]: [0m eta: 0:03:40  iter: 1599  total_loss: 0.770  loss_cls: 0.227  loss_box_reg: 0.377  loss_rpn_cls: 0.036  loss_rpn_loc: 0.061  time: 0.5680  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:13:11 d2.utils.events]: [0m eta: 0:03:28  iter: 1619  total_loss: 0.667  loss_cls: 0.244  loss_box_reg: 0.357  loss_rpn_cls: 0.033  loss_rpn_loc: 0.053  time: 0.5674  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:13:23 d2.utils.events]: [0m eta: 0:03:18  iter: 1639  total_loss: 0.720  loss_cls: 0.224  loss_box_reg: 0.358  loss_rpn_cls: 0.035  loss_rpn_loc: 0.067  time: 0.5674  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:13:34 d2.utils.events]: [0m eta: 0:03:06  iter: 1659  total_loss: 0.711  loss_cls: 0.250  loss_box_reg: 0.377  loss_rpn_cls: 0.042  loss_rpn_loc: 0.070  time: 0.5676  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:13:45 d2.utils.events]: [0m eta: 0:02:55  iter: 1679  total_loss: 0.753  loss_cls: 0.269  loss_box_reg: 0.363  loss_rpn_cls: 0.032  loss_rpn_loc: 0.073  time: 0.5671  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:13:56 d2.utils.events]: [0m eta: 0:02:44  iter: 1699  total_loss: 0.559  loss_cls: 0.192  loss_box_reg: 0.284  loss_rpn_cls: 0.026  loss_rpn_loc: 0.053  time: 0.5671  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:14:08 d2.utils.events]: [0m eta: 0:02:33  iter: 1719  total_loss: 0.682  loss_cls: 0.235  loss_box_reg: 0.367  loss_rpn_cls: 0.039  loss_rpn_loc: 0.062  time: 0.5671  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:14:19 d2.utils.events]: [0m eta: 0:02:22  iter: 1739  total_loss: 0.618  loss_cls: 0.201  loss_box_reg: 0.309  loss_rpn_cls: 0.025  loss_rpn_loc: 0.060  time: 0.5668  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:14:30 d2.utils.events]: [0m eta: 0:02:11  iter: 1759  total_loss: 0.516  loss_cls: 0.169  loss_box_reg: 0.312  loss_rpn_cls: 0.034  loss_rpn_loc: 0.045  time: 0.5670  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:14:41 d2.utils.events]: [0m eta: 0:02:00  iter: 1779  total_loss: 0.454  loss_cls: 0.151  loss_box_reg: 0.247  loss_rpn_cls: 0.028  loss_rpn_loc: 0.047  time: 0.5667  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:14:52 d2.utils.events]: [0m eta: 0:01:49  iter: 1799  total_loss: 0.788  loss_cls: 0.261  loss_box_reg: 0.426  loss_rpn_cls: 0.033  loss_rpn_loc: 0.075  time: 0.5664  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:15:04 d2.utils.events]: [0m eta: 0:01:38  iter: 1819  total_loss: 0.692  loss_cls: 0.228  loss_box_reg: 0.372  loss_rpn_cls: 0.022  loss_rpn_loc: 0.059  time: 0.5669  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:15:16 d2.utils.events]: [0m eta: 0:01:27  iter: 1839  total_loss: 0.749  loss_cls: 0.272  loss_box_reg: 0.401  loss_rpn_cls: 0.030  loss_rpn_loc: 0.070  time: 0.5668  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:15:27 d2.utils.events]: [0m eta: 0:01:16  iter: 1859  total_loss: 0.790  loss_cls: 0.275  loss_box_reg: 0.415  loss_rpn_cls: 0.033  loss_rpn_loc: 0.070  time: 0.5667  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:15:39 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.587  loss_cls: 0.177  loss_box_reg: 0.317  loss_rpn_cls: 0.025  loss_rpn_loc: 0.075  time: 0.5672  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:15:50 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.750  loss_cls: 0.238  loss_box_reg: 0.409  loss_rpn_cls: 0.025  loss_rpn_loc: 0.082  time: 0.5669  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:16:01 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.637  loss_cls: 0.204  loss_box_reg: 0.326  loss_rpn_cls: 0.022  loss_rpn_loc: 0.045  time: 0.5669  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:16:14 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.767  loss_cls: 0.241  loss_box_reg: 0.363  loss_rpn_cls: 0.032  loss_rpn_loc: 0.066  time: 0.5673  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:16:24 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.734  loss_cls: 0.242  loss_box_reg: 0.358  loss_rpn_cls: 0.027  loss_rpn_loc: 0.055  time: 0.5669  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:16:36 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.724  loss_cls: 0.231  loss_box_reg: 0.364  loss_rpn_cls: 0.032  loss_rpn_loc: 0.068  time: 0.5669  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 05:16:55 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 05:16:55 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 05:16:55 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 05:16:55 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.740  loss_cls: 0.243  loss_box_reg: 0.378  loss_rpn_cls: 0.029  loss_rpn_loc: 0.076  time: 0.5672  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:16:57 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:53 (0.5675 s / it)
[32m[04/15 05:16:57 d2.engine.hooks]: [0mTotal training time: 0:19:09 (0:00:16 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 05:17:02 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 05:17:02 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 05:17:02 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 05:17:04 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1221 s / img. ETA=0:02:35
[32m[04/15 05:17:09 d2.evaluation.evaluator]: [0mInference done 45/1257. 0.1351 s / img. ETA=0:02:54
[32m[04/15 05:17:15 d2.evaluation.evaluator]: [0mInference done 80/1257. 0.1382 s / img. ETA=0:02:50
[32m[04/15 05:17:20 d2.evaluation.evaluator]: [0mInference done 119/1257. 0.1331 s / img. ETA=0:02:38
[32m[04/15 05:17:25 d2.evaluation.evaluator]: [0mInference done 142/1257. 0.1317 s / img. ETA=0:02:50
[32m[04/15 05:17:30 d2.evaluation.evaluator]: [0mInference done 176/1257. 0.1301 s / img. ETA=0:02:44
[32m[04/15 05:17:35 d2.evaluation.evaluator]: [0mInference done 213/1257. 0.1290 s / img. ETA=0:02:35
[32m[04/15 05:17:40 d2.evaluation.evaluator]: [0mInference done 252/1257. 0.1293 s / img. ETA=0:02:27
[32m[04/15 05:17:45 d2.evaluation.evaluator]: [0mInference done 291/1257. 0.1280 s / img. ETA=0:02:19
[32m[04/15 05:17:50 d2.evaluation.evaluator]: [0mInference done 327/1257. 0.1282 s / img. ETA=0:02:14
[32m[04/15 05:17:55 d2.evaluation.evaluator]: [0mInference done 363/1257. 0.1290 s / img. ETA=0:02:08
[32m[04/15 05:18:00 d2.evaluation.evaluator]: [0mInference done 396/1257. 0.1305 s / img. ETA=0:02:04
[32m[04/15 05:18:05 d2.evaluation.evaluator]: [0mInference done 434/1257. 0.1295 s / img. ETA=0:01:58
[32m[04/15 05:18:10 d2.evaluation.evaluator]: [0mInference done 466/1257. 0.1287 s / img. ETA=0:01:54
[32m[04/15 05:18:15 d2.evaluation.evaluator]: [0mInference done 503/1257. 0.1281 s / img. ETA=0:01:48
[32m[04/15 05:18:21 d2.evaluation.evaluator]: [0mInference done 537/1257. 0.1276 s / img. ETA=0:01:43
[32m[04/15 05:18:26 d2.evaluation.evaluator]: [0mInference done 574/1257. 0.1277 s / img. ETA=0:01:38
[32m[04/15 05:18:31 d2.evaluation.evaluator]: [0mInference done 612/1257. 0.1279 s / img. ETA=0:01:32
[32m[04/15 05:18:36 d2.evaluation.evaluator]: [0mInference done 645/1257. 0.1289 s / img. ETA=0:01:28
[32m[04/15 05:18:41 d2.evaluation.evaluator]: [0mInference done 680/1257. 0.1287 s / img. ETA=0:01:23
[32m[04/15 05:18:46 d2.evaluation.evaluator]: [0mInference done 718/1257. 0.1284 s / img. ETA=0:01:17
[32m[04/15 05:18:51 d2.evaluation.evaluator]: [0mInference done 759/1257. 0.1279 s / img. ETA=0:01:10
[32m[04/15 05:18:56 d2.evaluation.evaluator]: [0mInference done 800/1257. 0.1275 s / img. ETA=0:01:04
[32m[04/15 05:19:01 d2.evaluation.evaluator]: [0mInference done 838/1257. 0.1276 s / img. ETA=0:00:59
[32m[04/15 05:19:06 d2.evaluation.evaluator]: [0mInference done 876/1257. 0.1278 s / img. ETA=0:00:53
[32m[04/15 05:19:11 d2.evaluation.evaluator]: [0mInference done 909/1257. 0.1285 s / img. ETA=0:00:49
[32m[04/15 05:19:16 d2.evaluation.evaluator]: [0mInference done 948/1257. 0.1284 s / img. ETA=0:00:43
[32m[04/15 05:19:22 d2.evaluation.evaluator]: [0mInference done 989/1257. 0.1280 s / img. ETA=0:00:37
[32m[04/15 05:19:27 d2.evaluation.evaluator]: [0mInference done 1030/1257. 0.1277 s / img. ETA=0:00:31
[32m[04/15 05:19:32 d2.evaluation.evaluator]: [0mInference done 1071/1257. 0.1274 s / img. ETA=0:00:25
[32m[04/15 05:19:37 d2.evaluation.evaluator]: [0mInference done 1109/1257. 0.1275 s / img. ETA=0:00:20
[32m[04/15 05:19:42 d2.evaluation.evaluator]: [0mInference done 1145/1257. 0.1278 s / img. ETA=0:00:15
[32m[04/15 05:19:47 d2.evaluation.evaluator]: [0mInference done 1178/1257. 0.1284 s / img. ETA=0:00:10
[32m[04/15 05:19:52 d2.evaluation.evaluator]: [0mInference done 1219/1257. 0.1281 s / img. ETA=0:00:05
[32m[04/15 05:19:57 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:52.999529 (0.138179 s / img per device, on 1 devices)
[32m[04/15 05:19:57 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:40 (0.127889 s / img per device, on 1 devices)
[32m[04/15 05:19:57 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 05:19:57 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 05:19:57 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.03s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.24s).
Accumulating evaluation results...
DONE (t=0.73s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.147
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.284
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.131
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.172
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.071
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.174
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.176
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.122
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.199
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.426
[32m[04/15 05:20:04 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |
|:------:|:------:|:------:|:-----:|:------:|:------:|
| 14.674 | 28.434 | 13.109 | 8.983 | 17.243 | 38.142 |
[32m[04/15 05:20:04 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 22.089 | bicycle       | 3.181 | car            | 33.425 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  5  *  2000  iterations ============
4 channel input
[32m[04/15 05:20:05 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 05:20:06 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.27 seconds.
[5m[31mWARNING[0m [32m[04/15 05:20:06 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 05:20:06 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 05:20:07 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 05:20:07 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 05:20:07 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 05:20:08 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 05:20:21 d2.utils.events]: [0m eta: 0:19:37  iter: 19  total_loss: 0.716  loss_cls: 0.251  loss_box_reg: 0.356  loss_rpn_cls: 0.033  loss_rpn_loc: 0.076  time: 0.6166  data_time: 0.0383  lr: 0.000100  max_mem: 3067M
[32m[04/15 05:20:31 d2.utils.events]: [0m eta: 0:18:13  iter: 39  total_loss: 0.512  loss_cls: 0.139  loss_box_reg: 0.276  loss_rpn_cls: 0.023  loss_rpn_loc: 0.058  time: 0.5759  data_time: 0.0092  lr: 0.000200  max_mem: 3067M
[32m[04/15 05:20:43 d2.utils.events]: [0m eta: 0:18:02  iter: 59  total_loss: 0.751  loss_cls: 0.252  loss_box_reg: 0.369  loss_rpn_cls: 0.024  loss_rpn_loc: 0.074  time: 0.5729  data_time: 0.0072  lr: 0.000300  max_mem: 3067M
[32m[04/15 05:20:55 d2.utils.events]: [0m eta: 0:18:10  iter: 79  total_loss: 0.690  loss_cls: 0.215  loss_box_reg: 0.389  loss_rpn_cls: 0.027  loss_rpn_loc: 0.063  time: 0.5826  data_time: 0.0089  lr: 0.000400  max_mem: 3067M
[32m[04/15 05:21:06 d2.utils.events]: [0m eta: 0:17:37  iter: 99  total_loss: 0.445  loss_cls: 0.146  loss_box_reg: 0.245  loss_rpn_cls: 0.021  loss_rpn_loc: 0.035  time: 0.5717  data_time: 0.0078  lr: 0.000500  max_mem: 3067M
[32m[04/15 05:21:17 d2.utils.events]: [0m eta: 0:17:23  iter: 119  total_loss: 0.710  loss_cls: 0.240  loss_box_reg: 0.393  loss_rpn_cls: 0.019  loss_rpn_loc: 0.063  time: 0.5708  data_time: 0.0095  lr: 0.000599  max_mem: 3067M
[32m[04/15 05:21:29 d2.utils.events]: [0m eta: 0:17:21  iter: 139  total_loss: 0.623  loss_cls: 0.229  loss_box_reg: 0.292  loss_rpn_cls: 0.027  loss_rpn_loc: 0.066  time: 0.5759  data_time: 0.0083  lr: 0.000699  max_mem: 3067M
[32m[04/15 05:21:40 d2.utils.events]: [0m eta: 0:17:02  iter: 159  total_loss: 0.722  loss_cls: 0.224  loss_box_reg: 0.384  loss_rpn_cls: 0.033  loss_rpn_loc: 0.056  time: 0.5704  data_time: 0.0077  lr: 0.000799  max_mem: 3067M
[32m[04/15 05:21:52 d2.utils.events]: [0m eta: 0:16:55  iter: 179  total_loss: 0.625  loss_cls: 0.199  loss_box_reg: 0.364  loss_rpn_cls: 0.020  loss_rpn_loc: 0.063  time: 0.5730  data_time: 0.0090  lr: 0.000899  max_mem: 3067M
[32m[04/15 05:22:03 d2.utils.events]: [0m eta: 0:16:40  iter: 199  total_loss: 0.712  loss_cls: 0.214  loss_box_reg: 0.389  loss_rpn_cls: 0.025  loss_rpn_loc: 0.071  time: 0.5724  data_time: 0.0087  lr: 0.000999  max_mem: 3067M
[32m[04/15 05:22:14 d2.utils.events]: [0m eta: 0:16:27  iter: 219  total_loss: 0.555  loss_cls: 0.182  loss_box_reg: 0.302  loss_rpn_cls: 0.022  loss_rpn_loc: 0.050  time: 0.5692  data_time: 0.0076  lr: 0.001099  max_mem: 3067M
[32m[04/15 05:22:26 d2.utils.events]: [0m eta: 0:16:18  iter: 239  total_loss: 0.596  loss_cls: 0.204  loss_box_reg: 0.341  loss_rpn_cls: 0.021  loss_rpn_loc: 0.052  time: 0.5721  data_time: 0.0083  lr: 0.001199  max_mem: 3067M
[32m[04/15 05:22:38 d2.utils.events]: [0m eta: 0:16:04  iter: 259  total_loss: 0.717  loss_cls: 0.227  loss_box_reg: 0.378  loss_rpn_cls: 0.029  loss_rpn_loc: 0.067  time: 0.5714  data_time: 0.0079  lr: 0.001299  max_mem: 3067M
[32m[04/15 05:22:49 d2.utils.events]: [0m eta: 0:15:53  iter: 279  total_loss: 0.660  loss_cls: 0.230  loss_box_reg: 0.367  loss_rpn_cls: 0.022  loss_rpn_loc: 0.064  time: 0.5697  data_time: 0.0069  lr: 0.001399  max_mem: 3067M
[32m[04/15 05:23:00 d2.utils.events]: [0m eta: 0:15:42  iter: 299  total_loss: 0.794  loss_cls: 0.248  loss_box_reg: 0.423  loss_rpn_cls: 0.021  loss_rpn_loc: 0.069  time: 0.5702  data_time: 0.0088  lr: 0.001499  max_mem: 3067M
[32m[04/15 05:23:11 d2.utils.events]: [0m eta: 0:15:29  iter: 319  total_loss: 0.678  loss_cls: 0.230  loss_box_reg: 0.348  loss_rpn_cls: 0.023  loss_rpn_loc: 0.058  time: 0.5683  data_time: 0.0069  lr: 0.001598  max_mem: 3067M
[32m[04/15 05:23:22 d2.utils.events]: [0m eta: 0:15:16  iter: 339  total_loss: 0.591  loss_cls: 0.197  loss_box_reg: 0.328  loss_rpn_cls: 0.021  loss_rpn_loc: 0.056  time: 0.5662  data_time: 0.0080  lr: 0.001698  max_mem: 3067M
[32m[04/15 05:23:35 d2.utils.events]: [0m eta: 0:15:07  iter: 359  total_loss: 0.628  loss_cls: 0.218  loss_box_reg: 0.334  loss_rpn_cls: 0.013  loss_rpn_loc: 0.057  time: 0.5691  data_time: 0.0077  lr: 0.001798  max_mem: 3067M
[32m[04/15 05:23:46 d2.utils.events]: [0m eta: 0:14:57  iter: 379  total_loss: 0.551  loss_cls: 0.180  loss_box_reg: 0.287  loss_rpn_cls: 0.022  loss_rpn_loc: 0.047  time: 0.5687  data_time: 0.0081  lr: 0.001898  max_mem: 3067M
[32m[04/15 05:23:57 d2.utils.events]: [0m eta: 0:14:45  iter: 399  total_loss: 0.649  loss_cls: 0.215  loss_box_reg: 0.382  loss_rpn_cls: 0.021  loss_rpn_loc: 0.057  time: 0.5682  data_time: 0.0085  lr: 0.001998  max_mem: 3067M
[32m[04/15 05:24:09 d2.utils.events]: [0m eta: 0:14:34  iter: 419  total_loss: 0.719  loss_cls: 0.226  loss_box_reg: 0.396  loss_rpn_cls: 0.023  loss_rpn_loc: 0.063  time: 0.5698  data_time: 0.0089  lr: 0.002098  max_mem: 3067M
[32m[04/15 05:24:20 d2.utils.events]: [0m eta: 0:14:22  iter: 439  total_loss: 0.613  loss_cls: 0.213  loss_box_reg: 0.329  loss_rpn_cls: 0.021  loss_rpn_loc: 0.052  time: 0.5675  data_time: 0.0076  lr: 0.002198  max_mem: 3067M
[32m[04/15 05:24:31 d2.utils.events]: [0m eta: 0:14:11  iter: 459  total_loss: 0.746  loss_cls: 0.256  loss_box_reg: 0.372  loss_rpn_cls: 0.024  loss_rpn_loc: 0.056  time: 0.5664  data_time: 0.0068  lr: 0.002298  max_mem: 3067M
[32m[04/15 05:24:43 d2.utils.events]: [0m eta: 0:14:01  iter: 479  total_loss: 0.726  loss_cls: 0.237  loss_box_reg: 0.390  loss_rpn_cls: 0.019  loss_rpn_loc: 0.066  time: 0.5683  data_time: 0.0084  lr: 0.002398  max_mem: 3067M
[32m[04/15 05:24:54 d2.utils.events]: [0m eta: 0:13:50  iter: 499  total_loss: 0.677  loss_cls: 0.209  loss_box_reg: 0.373  loss_rpn_cls: 0.019  loss_rpn_loc: 0.047  time: 0.5670  data_time: 0.0057  lr: 0.002498  max_mem: 3067M
[32m[04/15 05:25:05 d2.utils.events]: [0m eta: 0:13:38  iter: 519  total_loss: 0.571  loss_cls: 0.179  loss_box_reg: 0.316  loss_rpn_cls: 0.013  loss_rpn_loc: 0.040  time: 0.5662  data_time: 0.0066  lr: 0.002597  max_mem: 3067M
[32m[04/15 05:25:18 d2.utils.events]: [0m eta: 0:13:28  iter: 539  total_loss: 0.676  loss_cls: 0.224  loss_box_reg: 0.365  loss_rpn_cls: 0.020  loss_rpn_loc: 0.051  time: 0.5685  data_time: 0.0086  lr: 0.002697  max_mem: 3067M
[32m[04/15 05:25:29 d2.utils.events]: [0m eta: 0:13:17  iter: 559  total_loss: 0.704  loss_cls: 0.211  loss_box_reg: 0.363  loss_rpn_cls: 0.026  loss_rpn_loc: 0.058  time: 0.5672  data_time: 0.0068  lr: 0.002797  max_mem: 3067M
[32m[04/15 05:25:40 d2.utils.events]: [0m eta: 0:13:05  iter: 579  total_loss: 0.752  loss_cls: 0.228  loss_box_reg: 0.389  loss_rpn_cls: 0.025  loss_rpn_loc: 0.072  time: 0.5667  data_time: 0.0082  lr: 0.002897  max_mem: 3067M
[32m[04/15 05:25:53 d2.utils.events]: [0m eta: 0:12:55  iter: 599  total_loss: 0.683  loss_cls: 0.212  loss_box_reg: 0.396  loss_rpn_cls: 0.025  loss_rpn_loc: 0.057  time: 0.5680  data_time: 0.0094  lr: 0.002997  max_mem: 3067M
[32m[04/15 05:26:04 d2.utils.events]: [0m eta: 0:12:44  iter: 619  total_loss: 0.662  loss_cls: 0.214  loss_box_reg: 0.341  loss_rpn_cls: 0.028  loss_rpn_loc: 0.052  time: 0.5677  data_time: 0.0083  lr: 0.003097  max_mem: 3067M
[32m[04/15 05:26:15 d2.utils.events]: [0m eta: 0:12:32  iter: 639  total_loss: 0.514  loss_cls: 0.174  loss_box_reg: 0.278  loss_rpn_cls: 0.016  loss_rpn_loc: 0.046  time: 0.5668  data_time: 0.0071  lr: 0.003197  max_mem: 3067M
[32m[04/15 05:26:26 d2.utils.events]: [0m eta: 0:12:21  iter: 659  total_loss: 0.654  loss_cls: 0.231  loss_box_reg: 0.351  loss_rpn_cls: 0.026  loss_rpn_loc: 0.061  time: 0.5674  data_time: 0.0091  lr: 0.003297  max_mem: 3067M
[32m[04/15 05:26:38 d2.utils.events]: [0m eta: 0:12:10  iter: 679  total_loss: 0.771  loss_cls: 0.251  loss_box_reg: 0.395  loss_rpn_cls: 0.041  loss_rpn_loc: 0.082  time: 0.5675  data_time: 0.0068  lr: 0.003397  max_mem: 3067M
[32m[04/15 05:26:49 d2.utils.events]: [0m eta: 0:11:59  iter: 699  total_loss: 0.831  loss_cls: 0.255  loss_box_reg: 0.424  loss_rpn_cls: 0.023  loss_rpn_loc: 0.090  time: 0.5668  data_time: 0.0078  lr: 0.003497  max_mem: 3067M
[32m[04/15 05:27:01 d2.utils.events]: [0m eta: 0:11:48  iter: 719  total_loss: 0.689  loss_cls: 0.220  loss_box_reg: 0.376  loss_rpn_cls: 0.027  loss_rpn_loc: 0.058  time: 0.5677  data_time: 0.0080  lr: 0.003596  max_mem: 3067M
[32m[04/15 05:27:13 d2.utils.events]: [0m eta: 0:11:37  iter: 739  total_loss: 0.634  loss_cls: 0.208  loss_box_reg: 0.327  loss_rpn_cls: 0.021  loss_rpn_loc: 0.043  time: 0.5682  data_time: 0.0069  lr: 0.003696  max_mem: 3067M
[32m[04/15 05:27:23 d2.utils.events]: [0m eta: 0:11:25  iter: 759  total_loss: 0.754  loss_cls: 0.251  loss_box_reg: 0.384  loss_rpn_cls: 0.032  loss_rpn_loc: 0.064  time: 0.5672  data_time: 0.0069  lr: 0.003796  max_mem: 3067M
[32m[04/15 05:27:35 d2.utils.events]: [0m eta: 0:11:14  iter: 779  total_loss: 0.709  loss_cls: 0.234  loss_box_reg: 0.385  loss_rpn_cls: 0.027  loss_rpn_loc: 0.068  time: 0.5670  data_time: 0.0073  lr: 0.003896  max_mem: 3067M
[32m[04/15 05:27:47 d2.utils.events]: [0m eta: 0:11:03  iter: 799  total_loss: 0.784  loss_cls: 0.239  loss_box_reg: 0.408  loss_rpn_cls: 0.025  loss_rpn_loc: 0.065  time: 0.5676  data_time: 0.0073  lr: 0.003996  max_mem: 3067M
[32m[04/15 05:27:58 d2.utils.events]: [0m eta: 0:10:52  iter: 819  total_loss: 0.628  loss_cls: 0.203  loss_box_reg: 0.327  loss_rpn_cls: 0.019  loss_rpn_loc: 0.040  time: 0.5670  data_time: 0.0082  lr: 0.004096  max_mem: 3067M
[32m[04/15 05:28:08 d2.utils.events]: [0m eta: 0:10:40  iter: 839  total_loss: 0.751  loss_cls: 0.238  loss_box_reg: 0.354  loss_rpn_cls: 0.034  loss_rpn_loc: 0.088  time: 0.5660  data_time: 0.0072  lr: 0.004196  max_mem: 3067M
[32m[04/15 05:28:20 d2.utils.events]: [0m eta: 0:10:30  iter: 859  total_loss: 0.590  loss_cls: 0.201  loss_box_reg: 0.296  loss_rpn_cls: 0.027  loss_rpn_loc: 0.053  time: 0.5670  data_time: 0.0078  lr: 0.004296  max_mem: 3067M
[32m[04/15 05:28:31 d2.utils.events]: [0m eta: 0:10:18  iter: 879  total_loss: 0.630  loss_cls: 0.189  loss_box_reg: 0.323  loss_rpn_cls: 0.024  loss_rpn_loc: 0.050  time: 0.5663  data_time: 0.0067  lr: 0.004396  max_mem: 3067M
[32m[04/15 05:28:42 d2.utils.events]: [0m eta: 0:10:06  iter: 899  total_loss: 0.696  loss_cls: 0.209  loss_box_reg: 0.307  loss_rpn_cls: 0.027  loss_rpn_loc: 0.063  time: 0.5657  data_time: 0.0082  lr: 0.004496  max_mem: 3067M
[32m[04/15 05:28:54 d2.utils.events]: [0m eta: 0:09:55  iter: 919  total_loss: 0.688  loss_cls: 0.226  loss_box_reg: 0.376  loss_rpn_cls: 0.025  loss_rpn_loc: 0.062  time: 0.5660  data_time: 0.0071  lr: 0.004595  max_mem: 3067M
[32m[04/15 05:29:05 d2.utils.events]: [0m eta: 0:09:44  iter: 939  total_loss: 0.705  loss_cls: 0.230  loss_box_reg: 0.375  loss_rpn_cls: 0.027  loss_rpn_loc: 0.071  time: 0.5660  data_time: 0.0094  lr: 0.004695  max_mem: 3067M
[32m[04/15 05:29:16 d2.utils.events]: [0m eta: 0:09:33  iter: 959  total_loss: 0.767  loss_cls: 0.254  loss_box_reg: 0.382  loss_rpn_cls: 0.031  loss_rpn_loc: 0.067  time: 0.5652  data_time: 0.0065  lr: 0.004795  max_mem: 3067M
[32m[04/15 05:29:27 d2.utils.events]: [0m eta: 0:09:22  iter: 979  total_loss: 0.692  loss_cls: 0.226  loss_box_reg: 0.368  loss_rpn_cls: 0.022  loss_rpn_loc: 0.047  time: 0.5651  data_time: 0.0066  lr: 0.004895  max_mem: 3067M
[32m[04/15 05:29:40 d2.utils.events]: [0m eta: 0:09:11  iter: 999  total_loss: 0.713  loss_cls: 0.234  loss_box_reg: 0.364  loss_rpn_cls: 0.030  loss_rpn_loc: 0.056  time: 0.5663  data_time: 0.0078  lr: 0.004995  max_mem: 3067M
[32m[04/15 05:29:51 d2.utils.events]: [0m eta: 0:09:00  iter: 1019  total_loss: 0.843  loss_cls: 0.266  loss_box_reg: 0.439  loss_rpn_cls: 0.024  loss_rpn_loc: 0.080  time: 0.5658  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:30:01 d2.utils.events]: [0m eta: 0:08:49  iter: 1039  total_loss: 0.714  loss_cls: 0.239  loss_box_reg: 0.362  loss_rpn_cls: 0.033  loss_rpn_loc: 0.055  time: 0.5651  data_time: 0.0104  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:30:14 d2.utils.events]: [0m eta: 0:08:38  iter: 1059  total_loss: 0.642  loss_cls: 0.220  loss_box_reg: 0.354  loss_rpn_cls: 0.031  loss_rpn_loc: 0.063  time: 0.5661  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:30:25 d2.utils.events]: [0m eta: 0:08:27  iter: 1079  total_loss: 0.653  loss_cls: 0.214  loss_box_reg: 0.352  loss_rpn_cls: 0.024  loss_rpn_loc: 0.059  time: 0.5661  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:30:36 d2.utils.events]: [0m eta: 0:08:16  iter: 1099  total_loss: 0.652  loss_cls: 0.203  loss_box_reg: 0.344  loss_rpn_cls: 0.032  loss_rpn_loc: 0.062  time: 0.5658  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:30:48 d2.utils.events]: [0m eta: 0:08:05  iter: 1119  total_loss: 0.716  loss_cls: 0.229  loss_box_reg: 0.356  loss_rpn_cls: 0.030  loss_rpn_loc: 0.079  time: 0.5658  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:31:00 d2.utils.events]: [0m eta: 0:07:54  iter: 1139  total_loss: 0.748  loss_cls: 0.255  loss_box_reg: 0.405  loss_rpn_cls: 0.038  loss_rpn_loc: 0.075  time: 0.5663  data_time: 0.0098  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:31:11 d2.utils.events]: [0m eta: 0:07:43  iter: 1159  total_loss: 0.760  loss_cls: 0.283  loss_box_reg: 0.405  loss_rpn_cls: 0.029  loss_rpn_loc: 0.061  time: 0.5658  data_time: 0.0062  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:31:22 d2.utils.events]: [0m eta: 0:07:31  iter: 1179  total_loss: 0.739  loss_cls: 0.259  loss_box_reg: 0.366  loss_rpn_cls: 0.033  loss_rpn_loc: 0.059  time: 0.5657  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:31:34 d2.utils.events]: [0m eta: 0:07:21  iter: 1199  total_loss: 0.693  loss_cls: 0.224  loss_box_reg: 0.347  loss_rpn_cls: 0.029  loss_rpn_loc: 0.089  time: 0.5664  data_time: 0.0097  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:31:45 d2.utils.events]: [0m eta: 0:07:09  iter: 1219  total_loss: 0.583  loss_cls: 0.194  loss_box_reg: 0.295  loss_rpn_cls: 0.027  loss_rpn_loc: 0.051  time: 0.5658  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:31:56 d2.utils.events]: [0m eta: 0:06:59  iter: 1239  total_loss: 0.607  loss_cls: 0.215  loss_box_reg: 0.311  loss_rpn_cls: 0.025  loss_rpn_loc: 0.050  time: 0.5658  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:32:09 d2.utils.events]: [0m eta: 0:06:48  iter: 1259  total_loss: 0.655  loss_cls: 0.228  loss_box_reg: 0.330  loss_rpn_cls: 0.036  loss_rpn_loc: 0.068  time: 0.5665  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:32:23 d2.utils.events]: [0m eta: 0:06:37  iter: 1279  total_loss: 0.479  loss_cls: 0.159  loss_box_reg: 0.243  loss_rpn_cls: 0.029  loss_rpn_loc: 0.048  time: 0.5688  data_time: 0.1934  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:32:34 d2.utils.events]: [0m eta: 0:06:26  iter: 1299  total_loss: 0.733  loss_cls: 0.262  loss_box_reg: 0.391  loss_rpn_cls: 0.034  loss_rpn_loc: 0.068  time: 0.5684  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:32:46 d2.utils.events]: [0m eta: 0:06:16  iter: 1319  total_loss: 0.673  loss_cls: 0.233  loss_box_reg: 0.379  loss_rpn_cls: 0.023  loss_rpn_loc: 0.050  time: 0.5690  data_time: 0.0117  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:32:58 d2.utils.events]: [0m eta: 0:06:05  iter: 1339  total_loss: 0.724  loss_cls: 0.227  loss_box_reg: 0.364  loss_rpn_cls: 0.025  loss_rpn_loc: 0.059  time: 0.5697  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:33:09 d2.utils.events]: [0m eta: 0:05:54  iter: 1359  total_loss: 0.731  loss_cls: 0.237  loss_box_reg: 0.363  loss_rpn_cls: 0.036  loss_rpn_loc: 0.073  time: 0.5689  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:33:20 d2.utils.events]: [0m eta: 0:05:43  iter: 1379  total_loss: 0.601  loss_cls: 0.195  loss_box_reg: 0.319  loss_rpn_cls: 0.024  loss_rpn_loc: 0.054  time: 0.5688  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:33:32 d2.utils.events]: [0m eta: 0:05:32  iter: 1399  total_loss: 0.719  loss_cls: 0.237  loss_box_reg: 0.376  loss_rpn_cls: 0.026  loss_rpn_loc: 0.071  time: 0.5692  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:33:44 d2.utils.events]: [0m eta: 0:05:21  iter: 1419  total_loss: 0.569  loss_cls: 0.206  loss_box_reg: 0.299  loss_rpn_cls: 0.030  loss_rpn_loc: 0.043  time: 0.5691  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:33:55 d2.utils.events]: [0m eta: 0:05:10  iter: 1439  total_loss: 0.813  loss_cls: 0.255  loss_box_reg: 0.434  loss_rpn_cls: 0.028  loss_rpn_loc: 0.068  time: 0.5690  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:34:07 d2.utils.events]: [0m eta: 0:04:59  iter: 1459  total_loss: 0.811  loss_cls: 0.259  loss_box_reg: 0.417  loss_rpn_cls: 0.036  loss_rpn_loc: 0.078  time: 0.5691  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:34:18 d2.utils.events]: [0m eta: 0:04:47  iter: 1479  total_loss: 0.702  loss_cls: 0.247  loss_box_reg: 0.343  loss_rpn_cls: 0.021  loss_rpn_loc: 0.065  time: 0.5691  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:34:29 d2.utils.events]: [0m eta: 0:04:36  iter: 1499  total_loss: 0.675  loss_cls: 0.202  loss_box_reg: 0.329  loss_rpn_cls: 0.038  loss_rpn_loc: 0.056  time: 0.5686  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:34:40 d2.utils.events]: [0m eta: 0:04:26  iter: 1519  total_loss: 0.721  loss_cls: 0.239  loss_box_reg: 0.384  loss_rpn_cls: 0.028  loss_rpn_loc: 0.062  time: 0.5686  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:34:53 d2.utils.events]: [0m eta: 0:04:14  iter: 1539  total_loss: 0.855  loss_cls: 0.301  loss_box_reg: 0.422  loss_rpn_cls: 0.030  loss_rpn_loc: 0.075  time: 0.5692  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:35:03 d2.utils.events]: [0m eta: 0:04:03  iter: 1559  total_loss: 0.756  loss_cls: 0.231  loss_box_reg: 0.403  loss_rpn_cls: 0.033  loss_rpn_loc: 0.069  time: 0.5687  data_time: 0.0137  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:35:15 d2.utils.events]: [0m eta: 0:03:52  iter: 1579  total_loss: 0.810  loss_cls: 0.248  loss_box_reg: 0.387  loss_rpn_cls: 0.019  loss_rpn_loc: 0.063  time: 0.5686  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:35:27 d2.utils.events]: [0m eta: 0:03:41  iter: 1599  total_loss: 0.743  loss_cls: 0.247  loss_box_reg: 0.407  loss_rpn_cls: 0.030  loss_rpn_loc: 0.068  time: 0.5689  data_time: 0.0102  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:35:38 d2.utils.events]: [0m eta: 0:03:30  iter: 1619  total_loss: 0.687  loss_cls: 0.245  loss_box_reg: 0.367  loss_rpn_cls: 0.028  loss_rpn_loc: 0.052  time: 0.5687  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:35:49 d2.utils.events]: [0m eta: 0:03:20  iter: 1639  total_loss: 0.636  loss_cls: 0.227  loss_box_reg: 0.343  loss_rpn_cls: 0.018  loss_rpn_loc: 0.046  time: 0.5686  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:36:00 d2.utils.events]: [0m eta: 0:03:08  iter: 1659  total_loss: 0.844  loss_cls: 0.271  loss_box_reg: 0.461  loss_rpn_cls: 0.032  loss_rpn_loc: 0.076  time: 0.5684  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:36:12 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.639  loss_cls: 0.208  loss_box_reg: 0.338  loss_rpn_cls: 0.028  loss_rpn_loc: 0.053  time: 0.5687  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:36:23 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.803  loss_cls: 0.256  loss_box_reg: 0.396  loss_rpn_cls: 0.031  loss_rpn_loc: 0.064  time: 0.5685  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:36:34 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.663  loss_cls: 0.243  loss_box_reg: 0.331  loss_rpn_cls: 0.024  loss_rpn_loc: 0.050  time: 0.5681  data_time: 0.0061  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:36:46 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.730  loss_cls: 0.235  loss_box_reg: 0.379  loss_rpn_cls: 0.037  loss_rpn_loc: 0.083  time: 0.5685  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:36:57 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.726  loss_cls: 0.254  loss_box_reg: 0.377  loss_rpn_cls: 0.027  loss_rpn_loc: 0.060  time: 0.5682  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:37:08 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.653  loss_cls: 0.234  loss_box_reg: 0.328  loss_rpn_cls: 0.034  loss_rpn_loc: 0.063  time: 0.5678  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:37:20 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.744  loss_cls: 0.239  loss_box_reg: 0.406  loss_rpn_cls: 0.031  loss_rpn_loc: 0.060  time: 0.5682  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:37:31 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.740  loss_cls: 0.233  loss_box_reg: 0.383  loss_rpn_cls: 0.030  loss_rpn_loc: 0.061  time: 0.5677  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:37:42 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.671  loss_cls: 0.227  loss_box_reg: 0.355  loss_rpn_cls: 0.025  loss_rpn_loc: 0.053  time: 0.5677  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:37:53 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.589  loss_cls: 0.200  loss_box_reg: 0.292  loss_rpn_cls: 0.019  loss_rpn_loc: 0.046  time: 0.5676  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:38:05 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.531  loss_cls: 0.165  loss_box_reg: 0.302  loss_rpn_cls: 0.020  loss_rpn_loc: 0.051  time: 0.5679  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:38:16 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.674  loss_cls: 0.231  loss_box_reg: 0.332  loss_rpn_cls: 0.028  loss_rpn_loc: 0.072  time: 0.5677  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:38:28 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.654  loss_cls: 0.212  loss_box_reg: 0.398  loss_rpn_cls: 0.024  loss_rpn_loc: 0.054  time: 0.5676  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:38:40 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.755  loss_cls: 0.229  loss_box_reg: 0.411  loss_rpn_cls: 0.026  loss_rpn_loc: 0.049  time: 0.5679  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:38:51 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.613  loss_cls: 0.220  loss_box_reg: 0.301  loss_rpn_cls: 0.021  loss_rpn_loc: 0.054  time: 0.5678  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:39:02 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.762  loss_cls: 0.254  loss_box_reg: 0.385  loss_rpn_cls: 0.028  loss_rpn_loc: 0.053  time: 0.5675  data_time: 0.0095  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 05:39:26 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 05:39:26 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 05:39:27 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 05:39:27 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.766  loss_cls: 0.267  loss_box_reg: 0.390  loss_rpn_cls: 0.031  loss_rpn_loc: 0.068  time: 0.5680  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:39:31 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:54 (0.5683 s / it)
[32m[04/15 05:39:31 d2.engine.hooks]: [0mTotal training time: 0:19:21 (0:00:26 on hooks)
4 channel input
[32m[04/15 05:39:39 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json takes 1.01 seconds.
[5m[31mWARNING[0m [32m[04/15 05:39:39 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 05:39:39 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 05:39:39 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 05:39:45 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1478 s / img. ETA=0:03:07
[32m[04/15 05:39:50 d2.evaluation.evaluator]: [0mInference done 46/1257. 0.1287 s / img. ETA=0:02:57
[32m[04/15 05:39:55 d2.evaluation.evaluator]: [0mInference done 83/1257. 0.1247 s / img. ETA=0:02:47
[32m[04/15 05:40:00 d2.evaluation.evaluator]: [0mInference done 116/1257. 0.1248 s / img. ETA=0:02:46
[32m[04/15 05:40:05 d2.evaluation.evaluator]: [0mInference done 150/1257. 0.1246 s / img. ETA=0:02:42
[32m[04/15 05:40:10 d2.evaluation.evaluator]: [0mInference done 190/1257. 0.1234 s / img. ETA=0:02:32
[32m[04/15 05:40:15 d2.evaluation.evaluator]: [0mInference done 228/1257. 0.1237 s / img. ETA=0:02:25
[32m[04/15 05:40:20 d2.evaluation.evaluator]: [0mInference done 261/1257. 0.1274 s / img. ETA=0:02:22
[32m[04/15 05:40:25 d2.evaluation.evaluator]: [0mInference done 301/1257. 0.1267 s / img. ETA=0:02:14
[32m[04/15 05:40:31 d2.evaluation.evaluator]: [0mInference done 342/1257. 0.1261 s / img. ETA=0:02:06
[32m[04/15 05:40:36 d2.evaluation.evaluator]: [0mInference done 383/1257. 0.1258 s / img. ETA=0:01:59
[32m[04/15 05:40:41 d2.evaluation.evaluator]: [0mInference done 418/1257. 0.1272 s / img. ETA=0:01:55
[32m[04/15 05:40:46 d2.evaluation.evaluator]: [0mInference done 451/1257. 0.1266 s / img. ETA=0:01:52
[32m[04/15 05:40:51 d2.evaluation.evaluator]: [0mInference done 485/1257. 0.1262 s / img. ETA=0:01:48
[32m[04/15 05:40:56 d2.evaluation.evaluator]: [0mInference done 520/1257. 0.1258 s / img. ETA=0:01:43
[32m[04/15 05:41:01 d2.evaluation.evaluator]: [0mInference done 559/1257. 0.1256 s / img. ETA=0:01:37
[32m[04/15 05:41:06 d2.evaluation.evaluator]: [0mInference done 597/1257. 0.1253 s / img. ETA=0:01:31
[32m[04/15 05:41:11 d2.evaluation.evaluator]: [0mInference done 632/1257. 0.1251 s / img. ETA=0:01:27
[32m[04/15 05:41:17 d2.evaluation.evaluator]: [0mInference done 656/1257. 0.1249 s / img. ETA=0:01:25
[32m[04/15 05:41:22 d2.evaluation.evaluator]: [0mInference done 690/1257. 0.1248 s / img. ETA=0:01:20
[32m[04/15 05:41:27 d2.evaluation.evaluator]: [0mInference done 724/1257. 0.1258 s / img. ETA=0:01:16
[32m[04/15 05:41:32 d2.evaluation.evaluator]: [0mInference done 758/1257. 0.1266 s / img. ETA=0:01:11
[32m[04/15 05:41:37 d2.evaluation.evaluator]: [0mInference done 795/1257. 0.1268 s / img. ETA=0:01:06
[32m[04/15 05:41:42 d2.evaluation.evaluator]: [0mInference done 835/1257. 0.1266 s / img. ETA=0:00:59
[32m[04/15 05:41:47 d2.evaluation.evaluator]: [0mInference done 875/1257. 0.1263 s / img. ETA=0:00:54
[32m[04/15 05:41:52 d2.evaluation.evaluator]: [0mInference done 916/1257. 0.1260 s / img. ETA=0:00:47
[32m[04/15 05:41:57 d2.evaluation.evaluator]: [0mInference done 956/1257. 0.1257 s / img. ETA=0:00:42
[32m[04/15 05:42:02 d2.evaluation.evaluator]: [0mInference done 993/1257. 0.1260 s / img. ETA=0:00:36
[32m[04/15 05:42:07 d2.evaluation.evaluator]: [0mInference done 1028/1257. 0.1265 s / img. ETA=0:00:32
[32m[04/15 05:42:12 d2.evaluation.evaluator]: [0mInference done 1063/1257. 0.1270 s / img. ETA=0:00:27
[32m[04/15 05:42:17 d2.evaluation.evaluator]: [0mInference done 1104/1257. 0.1268 s / img. ETA=0:00:21
[32m[04/15 05:42:22 d2.evaluation.evaluator]: [0mInference done 1144/1257. 0.1266 s / img. ETA=0:00:15
[32m[04/15 05:42:28 d2.evaluation.evaluator]: [0mInference done 1184/1257. 0.1264 s / img. ETA=0:00:10
[32m[04/15 05:42:33 d2.evaluation.evaluator]: [0mInference done 1224/1257. 0.1263 s / img. ETA=0:00:04
[32m[04/15 05:42:37 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:52.929866 (0.138123 s / img per device, on 1 devices)
[32m[04/15 05:42:37 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:37 (0.126108 s / img per device, on 1 devices)
[32m[04/15 05:42:37 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 05:42:37 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 05:42:37 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.05s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=6.26s).
Accumulating evaluation results...
DONE (t=1.03s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.172
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.346
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.147
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.105
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.082
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.151
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.246
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.437
[32m[04/15 05:42:45 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 17.194 | 34.595 | 14.678 | 10.534 | 21.142 | 39.610 |
[32m[04/15 05:42:45 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 23.563 | bicycle       | 5.262 | car            | 39.951 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  6  *  2000  iterations ============
4 channel input
[32m[04/15 05:42:47 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[5m[31mWARNING[0m [32m[04/15 05:42:48 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 05:42:48 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 05:42:48 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 05:42:49 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 05:42:49 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 05:42:49 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 05:43:00 d2.utils.events]: [0m eta: 0:18:16  iter: 19  total_loss: 0.606  loss_cls: 0.225  loss_box_reg: 0.305  loss_rpn_cls: 0.020  loss_rpn_loc: 0.057  time: 0.5422  data_time: 0.0442  lr: 0.000100  max_mem: 3067M
[32m[04/15 05:43:11 d2.utils.events]: [0m eta: 0:18:00  iter: 39  total_loss: 0.590  loss_cls: 0.214  loss_box_reg: 0.333  loss_rpn_cls: 0.019  loss_rpn_loc: 0.047  time: 0.5438  data_time: 0.0063  lr: 0.000200  max_mem: 3067M
[32m[04/15 05:43:23 d2.utils.events]: [0m eta: 0:17:49  iter: 59  total_loss: 0.728  loss_cls: 0.245  loss_box_reg: 0.379  loss_rpn_cls: 0.023  loss_rpn_loc: 0.055  time: 0.5530  data_time: 0.0061  lr: 0.000300  max_mem: 3067M
[32m[04/15 05:43:35 d2.utils.events]: [0m eta: 0:17:42  iter: 79  total_loss: 0.710  loss_cls: 0.244  loss_box_reg: 0.354  loss_rpn_cls: 0.029  loss_rpn_loc: 0.075  time: 0.5645  data_time: 0.0071  lr: 0.000400  max_mem: 3067M
[32m[04/15 05:43:46 d2.utils.events]: [0m eta: 0:17:23  iter: 99  total_loss: 0.678  loss_cls: 0.223  loss_box_reg: 0.393  loss_rpn_cls: 0.021  loss_rpn_loc: 0.050  time: 0.5586  data_time: 0.0085  lr: 0.000500  max_mem: 3067M
[32m[04/15 05:43:57 d2.utils.events]: [0m eta: 0:17:10  iter: 119  total_loss: 0.613  loss_cls: 0.194  loss_box_reg: 0.307  loss_rpn_cls: 0.029  loss_rpn_loc: 0.061  time: 0.5588  data_time: 0.0068  lr: 0.000599  max_mem: 3067M
[32m[04/15 05:44:12 d2.utils.events]: [0m eta: 0:16:52  iter: 139  total_loss: 0.722  loss_cls: 0.206  loss_box_reg: 0.345  loss_rpn_cls: 0.026  loss_rpn_loc: 0.071  time: 0.5579  data_time: 0.0431  lr: 0.000699  max_mem: 3067M
[32m[04/15 05:44:23 d2.utils.events]: [0m eta: 0:16:36  iter: 159  total_loss: 0.679  loss_cls: 0.239  loss_box_reg: 0.355  loss_rpn_cls: 0.028  loss_rpn_loc: 0.071  time: 0.5545  data_time: 0.0272  lr: 0.000799  max_mem: 3067M
[32m[04/15 05:44:34 d2.utils.events]: [0m eta: 0:16:30  iter: 179  total_loss: 0.581  loss_cls: 0.189  loss_box_reg: 0.311  loss_rpn_cls: 0.025  loss_rpn_loc: 0.055  time: 0.5561  data_time: 0.0072  lr: 0.000899  max_mem: 3067M
[32m[04/15 05:44:47 d2.utils.events]: [0m eta: 0:16:27  iter: 199  total_loss: 0.695  loss_cls: 0.238  loss_box_reg: 0.379  loss_rpn_cls: 0.023  loss_rpn_loc: 0.060  time: 0.5631  data_time: 0.0090  lr: 0.000999  max_mem: 3067M
[32m[04/15 05:44:58 d2.utils.events]: [0m eta: 0:16:13  iter: 219  total_loss: 0.747  loss_cls: 0.238  loss_box_reg: 0.437  loss_rpn_cls: 0.021  loss_rpn_loc: 0.060  time: 0.5608  data_time: 0.0119  lr: 0.001099  max_mem: 3067M
[32m[04/15 05:45:09 d2.utils.events]: [0m eta: 0:16:02  iter: 239  total_loss: 0.689  loss_cls: 0.222  loss_box_reg: 0.367  loss_rpn_cls: 0.022  loss_rpn_loc: 0.056  time: 0.5592  data_time: 0.0070  lr: 0.001199  max_mem: 3067M
[32m[04/15 05:45:21 d2.utils.events]: [0m eta: 0:15:54  iter: 259  total_loss: 0.649  loss_cls: 0.204  loss_box_reg: 0.383  loss_rpn_cls: 0.020  loss_rpn_loc: 0.055  time: 0.5648  data_time: 0.0076  lr: 0.001299  max_mem: 3067M
[32m[04/15 05:45:33 d2.utils.events]: [0m eta: 0:15:44  iter: 279  total_loss: 0.766  loss_cls: 0.216  loss_box_reg: 0.400  loss_rpn_cls: 0.029  loss_rpn_loc: 0.067  time: 0.5649  data_time: 0.0087  lr: 0.001399  max_mem: 3067M
[32m[04/15 05:45:44 d2.utils.events]: [0m eta: 0:15:33  iter: 299  total_loss: 0.675  loss_cls: 0.210  loss_box_reg: 0.311  loss_rpn_cls: 0.016  loss_rpn_loc: 0.056  time: 0.5632  data_time: 0.0069  lr: 0.001499  max_mem: 3067M
[32m[04/15 05:45:55 d2.utils.events]: [0m eta: 0:15:23  iter: 319  total_loss: 0.609  loss_cls: 0.205  loss_box_reg: 0.308  loss_rpn_cls: 0.018  loss_rpn_loc: 0.076  time: 0.5635  data_time: 0.0086  lr: 0.001598  max_mem: 3067M
[32m[04/15 05:46:07 d2.utils.events]: [0m eta: 0:15:14  iter: 339  total_loss: 0.669  loss_cls: 0.210  loss_box_reg: 0.370  loss_rpn_cls: 0.017  loss_rpn_loc: 0.059  time: 0.5663  data_time: 0.0070  lr: 0.001698  max_mem: 3067M
[32m[04/15 05:46:18 d2.utils.events]: [0m eta: 0:15:00  iter: 359  total_loss: 0.680  loss_cls: 0.229  loss_box_reg: 0.370  loss_rpn_cls: 0.026  loss_rpn_loc: 0.061  time: 0.5644  data_time: 0.0074  lr: 0.001798  max_mem: 3067M
[32m[04/15 05:46:29 d2.utils.events]: [0m eta: 0:14:50  iter: 379  total_loss: 0.688  loss_cls: 0.235  loss_box_reg: 0.383  loss_rpn_cls: 0.019  loss_rpn_loc: 0.051  time: 0.5637  data_time: 0.0080  lr: 0.001898  max_mem: 3067M
[32m[04/15 05:46:41 d2.utils.events]: [0m eta: 0:14:41  iter: 399  total_loss: 0.713  loss_cls: 0.221  loss_box_reg: 0.361  loss_rpn_cls: 0.027  loss_rpn_loc: 0.078  time: 0.5648  data_time: 0.0085  lr: 0.001998  max_mem: 3067M
[32m[04/15 05:46:53 d2.utils.events]: [0m eta: 0:14:31  iter: 419  total_loss: 0.593  loss_cls: 0.194  loss_box_reg: 0.310  loss_rpn_cls: 0.019  loss_rpn_loc: 0.056  time: 0.5650  data_time: 0.0118  lr: 0.002098  max_mem: 3067M
[32m[04/15 05:47:03 d2.utils.events]: [0m eta: 0:14:19  iter: 439  total_loss: 0.596  loss_cls: 0.209  loss_box_reg: 0.335  loss_rpn_cls: 0.019  loss_rpn_loc: 0.050  time: 0.5635  data_time: 0.0085  lr: 0.002198  max_mem: 3067M
[32m[04/15 05:47:15 d2.utils.events]: [0m eta: 0:14:08  iter: 459  total_loss: 0.604  loss_cls: 0.194  loss_box_reg: 0.290  loss_rpn_cls: 0.018  loss_rpn_loc: 0.044  time: 0.5636  data_time: 0.0073  lr: 0.002298  max_mem: 3067M
[32m[04/15 05:47:27 d2.utils.events]: [0m eta: 0:13:59  iter: 479  total_loss: 0.625  loss_cls: 0.207  loss_box_reg: 0.312  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5651  data_time: 0.0067  lr: 0.002398  max_mem: 3067M
[32m[04/15 05:47:38 d2.utils.events]: [0m eta: 0:13:48  iter: 499  total_loss: 0.680  loss_cls: 0.229  loss_box_reg: 0.374  loss_rpn_cls: 0.023  loss_rpn_loc: 0.061  time: 0.5643  data_time: 0.0076  lr: 0.002498  max_mem: 3067M
[32m[04/15 05:47:49 d2.utils.events]: [0m eta: 0:13:35  iter: 519  total_loss: 0.658  loss_cls: 0.207  loss_box_reg: 0.341  loss_rpn_cls: 0.019  loss_rpn_loc: 0.048  time: 0.5640  data_time: 0.0082  lr: 0.002597  max_mem: 3067M
[32m[04/15 05:48:02 d2.utils.events]: [0m eta: 0:13:26  iter: 539  total_loss: 0.710  loss_cls: 0.251  loss_box_reg: 0.374  loss_rpn_cls: 0.027  loss_rpn_loc: 0.056  time: 0.5660  data_time: 0.0066  lr: 0.002697  max_mem: 3067M
[32m[04/15 05:48:12 d2.utils.events]: [0m eta: 0:13:13  iter: 559  total_loss: 0.601  loss_cls: 0.200  loss_box_reg: 0.364  loss_rpn_cls: 0.017  loss_rpn_loc: 0.051  time: 0.5648  data_time: 0.0073  lr: 0.002797  max_mem: 3067M
[32m[04/15 05:48:23 d2.utils.events]: [0m eta: 0:13:02  iter: 579  total_loss: 0.680  loss_cls: 0.242  loss_box_reg: 0.362  loss_rpn_cls: 0.016  loss_rpn_loc: 0.055  time: 0.5637  data_time: 0.0069  lr: 0.002897  max_mem: 3067M
[32m[04/15 05:48:39 d2.utils.events]: [0m eta: 0:12:53  iter: 599  total_loss: 0.671  loss_cls: 0.228  loss_box_reg: 0.357  loss_rpn_cls: 0.024  loss_rpn_loc: 0.049  time: 0.5707  data_time: 0.2661  lr: 0.002997  max_mem: 3067M
[32m[04/15 05:48:50 d2.utils.events]: [0m eta: 0:12:42  iter: 619  total_loss: 0.647  loss_cls: 0.213  loss_box_reg: 0.313  loss_rpn_cls: 0.028  loss_rpn_loc: 0.059  time: 0.5701  data_time: 0.0088  lr: 0.003097  max_mem: 3067M
[32m[04/15 05:49:03 d2.utils.events]: [0m eta: 0:12:35  iter: 639  total_loss: 0.614  loss_cls: 0.192  loss_box_reg: 0.322  loss_rpn_cls: 0.020  loss_rpn_loc: 0.058  time: 0.5722  data_time: 0.0075  lr: 0.003197  max_mem: 3067M
[32m[04/15 05:49:14 d2.utils.events]: [0m eta: 0:12:21  iter: 659  total_loss: 0.644  loss_cls: 0.196  loss_box_reg: 0.330  loss_rpn_cls: 0.025  loss_rpn_loc: 0.060  time: 0.5715  data_time: 0.0069  lr: 0.003297  max_mem: 3067M
[32m[04/15 05:49:26 d2.utils.events]: [0m eta: 0:12:12  iter: 679  total_loss: 0.707  loss_cls: 0.228  loss_box_reg: 0.383  loss_rpn_cls: 0.027  loss_rpn_loc: 0.071  time: 0.5724  data_time: 0.0089  lr: 0.003397  max_mem: 3067M
[32m[04/15 05:49:37 d2.utils.events]: [0m eta: 0:11:59  iter: 699  total_loss: 0.656  loss_cls: 0.207  loss_box_reg: 0.350  loss_rpn_cls: 0.027  loss_rpn_loc: 0.073  time: 0.5716  data_time: 0.0089  lr: 0.003497  max_mem: 3067M
[32m[04/15 05:49:48 d2.utils.events]: [0m eta: 0:11:47  iter: 719  total_loss: 0.637  loss_cls: 0.209  loss_box_reg: 0.351  loss_rpn_cls: 0.020  loss_rpn_loc: 0.048  time: 0.5702  data_time: 0.0081  lr: 0.003596  max_mem: 3067M
[32m[04/15 05:49:59 d2.utils.events]: [0m eta: 0:11:36  iter: 739  total_loss: 0.519  loss_cls: 0.189  loss_box_reg: 0.265  loss_rpn_cls: 0.026  loss_rpn_loc: 0.042  time: 0.5703  data_time: 0.0078  lr: 0.003696  max_mem: 3067M
[32m[04/15 05:50:11 d2.utils.events]: [0m eta: 0:11:26  iter: 759  total_loss: 0.784  loss_cls: 0.261  loss_box_reg: 0.434  loss_rpn_cls: 0.030  loss_rpn_loc: 0.064  time: 0.5709  data_time: 0.0096  lr: 0.003796  max_mem: 3067M
[32m[04/15 05:50:23 d2.utils.events]: [0m eta: 0:11:17  iter: 779  total_loss: 0.686  loss_cls: 0.184  loss_box_reg: 0.350  loss_rpn_cls: 0.029  loss_rpn_loc: 0.071  time: 0.5708  data_time: 0.0069  lr: 0.003896  max_mem: 3067M
[32m[04/15 05:50:34 d2.utils.events]: [0m eta: 0:11:04  iter: 799  total_loss: 0.614  loss_cls: 0.199  loss_box_reg: 0.342  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5700  data_time: 0.0069  lr: 0.003996  max_mem: 3067M
[32m[04/15 05:50:46 d2.utils.events]: [0m eta: 0:10:54  iter: 819  total_loss: 0.745  loss_cls: 0.250  loss_box_reg: 0.399  loss_rpn_cls: 0.029  loss_rpn_loc: 0.062  time: 0.5713  data_time: 0.0078  lr: 0.004096  max_mem: 3067M
[32m[04/15 05:50:56 d2.utils.events]: [0m eta: 0:10:41  iter: 839  total_loss: 0.676  loss_cls: 0.208  loss_box_reg: 0.376  loss_rpn_cls: 0.024  loss_rpn_loc: 0.069  time: 0.5697  data_time: 0.0069  lr: 0.004196  max_mem: 3067M
[32m[04/15 05:51:07 d2.utils.events]: [0m eta: 0:10:30  iter: 859  total_loss: 0.558  loss_cls: 0.182  loss_box_reg: 0.326  loss_rpn_cls: 0.018  loss_rpn_loc: 0.053  time: 0.5692  data_time: 0.0083  lr: 0.004296  max_mem: 3067M
[32m[04/15 05:51:20 d2.utils.events]: [0m eta: 0:10:20  iter: 879  total_loss: 0.719  loss_cls: 0.231  loss_box_reg: 0.357  loss_rpn_cls: 0.028  loss_rpn_loc: 0.066  time: 0.5700  data_time: 0.0077  lr: 0.004396  max_mem: 3067M
[32m[04/15 05:51:31 d2.utils.events]: [0m eta: 0:10:08  iter: 899  total_loss: 0.811  loss_cls: 0.231  loss_box_reg: 0.366  loss_rpn_cls: 0.032  loss_rpn_loc: 0.070  time: 0.5697  data_time: 0.0096  lr: 0.004496  max_mem: 3067M
[32m[04/15 05:51:41 d2.utils.events]: [0m eta: 0:09:57  iter: 919  total_loss: 0.635  loss_cls: 0.200  loss_box_reg: 0.354  loss_rpn_cls: 0.020  loss_rpn_loc: 0.044  time: 0.5684  data_time: 0.0068  lr: 0.004595  max_mem: 3067M
[32m[04/15 05:51:53 d2.utils.events]: [0m eta: 0:09:45  iter: 939  total_loss: 0.706  loss_cls: 0.233  loss_box_reg: 0.379  loss_rpn_cls: 0.027  loss_rpn_loc: 0.060  time: 0.5685  data_time: 0.0071  lr: 0.004695  max_mem: 3067M
[32m[04/15 05:52:05 d2.utils.events]: [0m eta: 0:09:34  iter: 959  total_loss: 0.650  loss_cls: 0.223  loss_box_reg: 0.363  loss_rpn_cls: 0.017  loss_rpn_loc: 0.058  time: 0.5689  data_time: 0.0074  lr: 0.004795  max_mem: 3067M
[32m[04/15 05:52:15 d2.utils.events]: [0m eta: 0:09:23  iter: 979  total_loss: 0.722  loss_cls: 0.238  loss_box_reg: 0.403  loss_rpn_cls: 0.026  loss_rpn_loc: 0.054  time: 0.5682  data_time: 0.0113  lr: 0.004895  max_mem: 3067M
[32m[04/15 05:52:26 d2.utils.events]: [0m eta: 0:09:11  iter: 999  total_loss: 0.765  loss_cls: 0.255  loss_box_reg: 0.378  loss_rpn_cls: 0.024  loss_rpn_loc: 0.074  time: 0.5679  data_time: 0.0068  lr: 0.004995  max_mem: 3067M
[32m[04/15 05:52:39 d2.utils.events]: [0m eta: 0:09:02  iter: 1019  total_loss: 0.526  loss_cls: 0.180  loss_box_reg: 0.279  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5690  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:52:50 d2.utils.events]: [0m eta: 0:08:50  iter: 1039  total_loss: 0.626  loss_cls: 0.231  loss_box_reg: 0.338  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5685  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:53:01 d2.utils.events]: [0m eta: 0:08:39  iter: 1059  total_loss: 0.634  loss_cls: 0.211  loss_box_reg: 0.348  loss_rpn_cls: 0.026  loss_rpn_loc: 0.054  time: 0.5682  data_time: 0.0062  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:53:14 d2.utils.events]: [0m eta: 0:08:29  iter: 1079  total_loss: 0.637  loss_cls: 0.219  loss_box_reg: 0.320  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5693  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:53:24 d2.utils.events]: [0m eta: 0:08:18  iter: 1099  total_loss: 0.822  loss_cls: 0.260  loss_box_reg: 0.413  loss_rpn_cls: 0.021  loss_rpn_loc: 0.071  time: 0.5683  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:53:36 d2.utils.events]: [0m eta: 0:08:07  iter: 1119  total_loss: 0.559  loss_cls: 0.179  loss_box_reg: 0.282  loss_rpn_cls: 0.023  loss_rpn_loc: 0.056  time: 0.5684  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:53:47 d2.utils.events]: [0m eta: 0:07:57  iter: 1139  total_loss: 0.687  loss_cls: 0.213  loss_box_reg: 0.371  loss_rpn_cls: 0.025  loss_rpn_loc: 0.062  time: 0.5684  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:53:59 d2.utils.events]: [0m eta: 0:07:46  iter: 1159  total_loss: 0.671  loss_cls: 0.237  loss_box_reg: 0.364  loss_rpn_cls: 0.027  loss_rpn_loc: 0.053  time: 0.5686  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:54:10 d2.utils.events]: [0m eta: 0:07:34  iter: 1179  total_loss: 0.810  loss_cls: 0.234  loss_box_reg: 0.429  loss_rpn_cls: 0.027  loss_rpn_loc: 0.082  time: 0.5680  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:54:21 d2.utils.events]: [0m eta: 0:07:23  iter: 1199  total_loss: 0.786  loss_cls: 0.253  loss_box_reg: 0.416  loss_rpn_cls: 0.033  loss_rpn_loc: 0.062  time: 0.5679  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:54:33 d2.utils.events]: [0m eta: 0:07:12  iter: 1219  total_loss: 0.649  loss_cls: 0.209  loss_box_reg: 0.327  loss_rpn_cls: 0.026  loss_rpn_loc: 0.071  time: 0.5682  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:54:44 d2.utils.events]: [0m eta: 0:07:02  iter: 1239  total_loss: 0.757  loss_cls: 0.246  loss_box_reg: 0.389  loss_rpn_cls: 0.023  loss_rpn_loc: 0.064  time: 0.5679  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:54:55 d2.utils.events]: [0m eta: 0:06:50  iter: 1259  total_loss: 0.731  loss_cls: 0.249  loss_box_reg: 0.390  loss_rpn_cls: 0.025  loss_rpn_loc: 0.068  time: 0.5676  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:55:08 d2.utils.events]: [0m eta: 0:06:39  iter: 1279  total_loss: 0.578  loss_cls: 0.192  loss_box_reg: 0.333  loss_rpn_cls: 0.018  loss_rpn_loc: 0.059  time: 0.5683  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:55:18 d2.utils.events]: [0m eta: 0:06:28  iter: 1299  total_loss: 0.659  loss_cls: 0.221  loss_box_reg: 0.339  loss_rpn_cls: 0.019  loss_rpn_loc: 0.045  time: 0.5676  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:55:30 d2.utils.events]: [0m eta: 0:06:16  iter: 1319  total_loss: 0.671  loss_cls: 0.232  loss_box_reg: 0.352  loss_rpn_cls: 0.028  loss_rpn_loc: 0.053  time: 0.5676  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:55:41 d2.utils.events]: [0m eta: 0:06:05  iter: 1339  total_loss: 0.739  loss_cls: 0.246  loss_box_reg: 0.403  loss_rpn_cls: 0.026  loss_rpn_loc: 0.077  time: 0.5677  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:55:53 d2.utils.events]: [0m eta: 0:05:55  iter: 1359  total_loss: 0.639  loss_cls: 0.208  loss_box_reg: 0.345  loss_rpn_cls: 0.021  loss_rpn_loc: 0.053  time: 0.5677  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:56:04 d2.utils.events]: [0m eta: 0:05:43  iter: 1379  total_loss: 0.693  loss_cls: 0.234  loss_box_reg: 0.370  loss_rpn_cls: 0.023  loss_rpn_loc: 0.049  time: 0.5673  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:56:15 d2.utils.events]: [0m eta: 0:05:32  iter: 1399  total_loss: 0.705  loss_cls: 0.216  loss_box_reg: 0.375  loss_rpn_cls: 0.032  loss_rpn_loc: 0.074  time: 0.5671  data_time: 0.0062  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:56:27 d2.utils.events]: [0m eta: 0:05:21  iter: 1419  total_loss: 0.706  loss_cls: 0.234  loss_box_reg: 0.369  loss_rpn_cls: 0.032  loss_rpn_loc: 0.066  time: 0.5674  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:56:38 d2.utils.events]: [0m eta: 0:05:10  iter: 1439  total_loss: 0.620  loss_cls: 0.219  loss_box_reg: 0.318  loss_rpn_cls: 0.026  loss_rpn_loc: 0.058  time: 0.5668  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:56:49 d2.utils.events]: [0m eta: 0:04:59  iter: 1459  total_loss: 0.786  loss_cls: 0.248  loss_box_reg: 0.390  loss_rpn_cls: 0.024  loss_rpn_loc: 0.066  time: 0.5669  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:57:01 d2.utils.events]: [0m eta: 0:04:48  iter: 1479  total_loss: 0.613  loss_cls: 0.189  loss_box_reg: 0.355  loss_rpn_cls: 0.020  loss_rpn_loc: 0.061  time: 0.5674  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:57:12 d2.utils.events]: [0m eta: 0:04:37  iter: 1499  total_loss: 0.687  loss_cls: 0.237  loss_box_reg: 0.372  loss_rpn_cls: 0.026  loss_rpn_loc: 0.060  time: 0.5669  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:57:23 d2.utils.events]: [0m eta: 0:04:26  iter: 1519  total_loss: 0.636  loss_cls: 0.209  loss_box_reg: 0.358  loss_rpn_cls: 0.027  loss_rpn_loc: 0.045  time: 0.5666  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:57:35 d2.utils.events]: [0m eta: 0:04:14  iter: 1539  total_loss: 0.677  loss_cls: 0.218  loss_box_reg: 0.397  loss_rpn_cls: 0.023  loss_rpn_loc: 0.065  time: 0.5669  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:57:46 d2.utils.events]: [0m eta: 0:04:04  iter: 1559  total_loss: 0.807  loss_cls: 0.263  loss_box_reg: 0.421  loss_rpn_cls: 0.023  loss_rpn_loc: 0.077  time: 0.5667  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:57:57 d2.utils.events]: [0m eta: 0:03:53  iter: 1579  total_loss: 0.585  loss_cls: 0.197  loss_box_reg: 0.297  loss_rpn_cls: 0.021  loss_rpn_loc: 0.049  time: 0.5666  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:58:10 d2.utils.events]: [0m eta: 0:03:41  iter: 1599  total_loss: 0.747  loss_cls: 0.248  loss_box_reg: 0.357  loss_rpn_cls: 0.035  loss_rpn_loc: 0.060  time: 0.5670  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:58:21 d2.utils.events]: [0m eta: 0:03:30  iter: 1619  total_loss: 0.744  loss_cls: 0.234  loss_box_reg: 0.417  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5669  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:58:32 d2.utils.events]: [0m eta: 0:03:18  iter: 1639  total_loss: 0.793  loss_cls: 0.282  loss_box_reg: 0.416  loss_rpn_cls: 0.029  loss_rpn_loc: 0.081  time: 0.5665  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:58:43 d2.utils.events]: [0m eta: 0:03:07  iter: 1659  total_loss: 0.655  loss_cls: 0.222  loss_box_reg: 0.360  loss_rpn_cls: 0.025  loss_rpn_loc: 0.068  time: 0.5666  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:58:56 d2.utils.events]: [0m eta: 0:02:56  iter: 1679  total_loss: 0.666  loss_cls: 0.236  loss_box_reg: 0.348  loss_rpn_cls: 0.032  loss_rpn_loc: 0.057  time: 0.5670  data_time: 0.0097  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:59:06 d2.utils.events]: [0m eta: 0:02:45  iter: 1699  total_loss: 0.653  loss_cls: 0.228  loss_box_reg: 0.308  loss_rpn_cls: 0.030  loss_rpn_loc: 0.059  time: 0.5665  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:59:17 d2.utils.events]: [0m eta: 0:02:34  iter: 1719  total_loss: 0.699  loss_cls: 0.216  loss_box_reg: 0.388  loss_rpn_cls: 0.019  loss_rpn_loc: 0.050  time: 0.5663  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:59:29 d2.utils.events]: [0m eta: 0:02:23  iter: 1739  total_loss: 0.604  loss_cls: 0.221  loss_box_reg: 0.297  loss_rpn_cls: 0.023  loss_rpn_loc: 0.050  time: 0.5667  data_time: 0.0095  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:59:40 d2.utils.events]: [0m eta: 0:02:12  iter: 1759  total_loss: 0.674  loss_cls: 0.223  loss_box_reg: 0.338  loss_rpn_cls: 0.029  loss_rpn_loc: 0.059  time: 0.5663  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 05:59:51 d2.utils.events]: [0m eta: 0:02:01  iter: 1779  total_loss: 0.791  loss_cls: 0.229  loss_box_reg: 0.390  loss_rpn_cls: 0.032  loss_rpn_loc: 0.067  time: 0.5662  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:00:03 d2.utils.events]: [0m eta: 0:01:50  iter: 1799  total_loss: 0.646  loss_cls: 0.225  loss_box_reg: 0.334  loss_rpn_cls: 0.022  loss_rpn_loc: 0.046  time: 0.5665  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:00:14 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.599  loss_cls: 0.183  loss_box_reg: 0.296  loss_rpn_cls: 0.038  loss_rpn_loc: 0.073  time: 0.5661  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:00:25 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.684  loss_cls: 0.218  loss_box_reg: 0.378  loss_rpn_cls: 0.033  loss_rpn_loc: 0.063  time: 0.5658  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:00:36 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.580  loss_cls: 0.199  loss_box_reg: 0.344  loss_rpn_cls: 0.023  loss_rpn_loc: 0.047  time: 0.5657  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:00:48 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.801  loss_cls: 0.280  loss_box_reg: 0.432  loss_rpn_cls: 0.022  loss_rpn_loc: 0.073  time: 0.5660  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:00:59 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.635  loss_cls: 0.223  loss_box_reg: 0.361  loss_rpn_cls: 0.024  loss_rpn_loc: 0.060  time: 0.5658  data_time: 0.0103  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:01:10 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.628  loss_cls: 0.201  loss_box_reg: 0.315  loss_rpn_cls: 0.026  loss_rpn_loc: 0.073  time: 0.5656  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:01:22 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.670  loss_cls: 0.219  loss_box_reg: 0.321  loss_rpn_cls: 0.026  loss_rpn_loc: 0.070  time: 0.5658  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:01:33 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.777  loss_cls: 0.237  loss_box_reg: 0.381  loss_rpn_cls: 0.024  loss_rpn_loc: 0.066  time: 0.5656  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:01:44 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.605  loss_cls: 0.174  loss_box_reg: 0.277  loss_rpn_cls: 0.024  loss_rpn_loc: 0.079  time: 0.5654  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 06:02:07 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:02:07 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 06:02:08 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 06:02:08 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.711  loss_cls: 0.244  loss_box_reg: 0.343  loss_rpn_cls: 0.021  loss_rpn_loc: 0.063  time: 0.5659  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:02:11 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:50 (0.5662 s / it)
[32m[04/15 06:02:11 d2.engine.hooks]: [0mTotal training time: 0:19:19 (0:00:29 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 06:02:18 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:02:18 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 06:02:18 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 06:02:22 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1247 s / img. ETA=0:03:47
[32m[04/15 06:02:28 d2.evaluation.evaluator]: [0mInference done 43/1257. 0.1383 s / img. ETA=0:03:18
[32m[04/15 06:02:33 d2.evaluation.evaluator]: [0mInference done 75/1257. 0.1395 s / img. ETA=0:03:10
[32m[04/15 06:02:38 d2.evaluation.evaluator]: [0mInference done 111/1257. 0.1340 s / img. ETA=0:02:57
[32m[04/15 06:02:43 d2.evaluation.evaluator]: [0mInference done 140/1257. 0.1314 s / img. ETA=0:02:58
[32m[04/15 06:02:48 d2.evaluation.evaluator]: [0mInference done 167/1257. 0.1305 s / img. ETA=0:02:59
[32m[04/15 06:02:53 d2.evaluation.evaluator]: [0mInference done 205/1257. 0.1285 s / img. ETA=0:02:46
[32m[04/15 06:02:58 d2.evaluation.evaluator]: [0mInference done 236/1257. 0.1276 s / img. ETA=0:02:42
[32m[04/15 06:03:03 d2.evaluation.evaluator]: [0mInference done 269/1257. 0.1269 s / img. ETA=0:02:36
[32m[04/15 06:03:08 d2.evaluation.evaluator]: [0mInference done 303/1257. 0.1292 s / img. ETA=0:02:30
[32m[04/15 06:03:13 d2.evaluation.evaluator]: [0mInference done 339/1257. 0.1298 s / img. ETA=0:02:22
[32m[04/15 06:03:18 d2.evaluation.evaluator]: [0mInference done 375/1257. 0.1305 s / img. ETA=0:02:15
[32m[04/15 06:03:23 d2.evaluation.evaluator]: [0mInference done 414/1257. 0.1295 s / img. ETA=0:02:07
[32m[04/15 06:03:28 d2.evaluation.evaluator]: [0mInference done 443/1257. 0.1289 s / img. ETA=0:02:04
[32m[04/15 06:03:33 d2.evaluation.evaluator]: [0mInference done 474/1257. 0.1283 s / img. ETA=0:02:00
[32m[04/15 06:03:39 d2.evaluation.evaluator]: [0mInference done 508/1257. 0.1280 s / img. ETA=0:01:54
[32m[04/15 06:03:44 d2.evaluation.evaluator]: [0mInference done 537/1257. 0.1276 s / img. ETA=0:01:51
[32m[04/15 06:03:49 d2.evaluation.evaluator]: [0mInference done 569/1257. 0.1283 s / img. ETA=0:01:46
[32m[04/15 06:03:54 d2.evaluation.evaluator]: [0mInference done 604/1257. 0.1288 s / img. ETA=0:01:40
[32m[04/15 06:03:59 d2.evaluation.evaluator]: [0mInference done 632/1257. 0.1285 s / img. ETA=0:01:37
[32m[04/15 06:04:04 d2.evaluation.evaluator]: [0mInference done 656/1257. 0.1281 s / img. ETA=0:01:34
[32m[04/15 06:04:09 d2.evaluation.evaluator]: [0mInference done 686/1257. 0.1280 s / img. ETA=0:01:30
[32m[04/15 06:04:14 d2.evaluation.evaluator]: [0mInference done 710/1257. 0.1278 s / img. ETA=0:01:27
[32m[04/15 06:04:19 d2.evaluation.evaluator]: [0mInference done 721/1257. 0.1276 s / img. ETA=0:01:28
[32m[04/15 06:04:24 d2.evaluation.evaluator]: [0mInference done 737/1257. 0.1277 s / img. ETA=0:01:27
[32m[04/15 06:04:29 d2.evaluation.evaluator]: [0mInference done 759/1257. 0.1278 s / img. ETA=0:01:24
[32m[04/15 06:04:34 d2.evaluation.evaluator]: [0mInference done 797/1257. 0.1277 s / img. ETA=0:01:17
[32m[04/15 06:04:39 d2.evaluation.evaluator]: [0mInference done 834/1257. 0.1278 s / img. ETA=0:01:10
[32m[04/15 06:04:45 d2.evaluation.evaluator]: [0mInference done 873/1257. 0.1279 s / img. ETA=0:01:03
[32m[04/15 06:04:50 d2.evaluation.evaluator]: [0mInference done 908/1257. 0.1285 s / img. ETA=0:00:57
[32m[04/15 06:04:55 d2.evaluation.evaluator]: [0mInference done 943/1257. 0.1289 s / img. ETA=0:00:51
[32m[04/15 06:05:00 d2.evaluation.evaluator]: [0mInference done 980/1257. 0.1286 s / img. ETA=0:00:45
[32m[04/15 06:05:05 d2.evaluation.evaluator]: [0mInference done 1016/1257. 0.1285 s / img. ETA=0:00:39
[32m[04/15 06:05:10 d2.evaluation.evaluator]: [0mInference done 1051/1257. 0.1284 s / img. ETA=0:00:33
[32m[04/15 06:05:15 d2.evaluation.evaluator]: [0mInference done 1084/1257. 0.1282 s / img. ETA=0:00:27
[32m[04/15 06:05:21 d2.evaluation.evaluator]: [0mInference done 1117/1257. 0.1285 s / img. ETA=0:00:22
[32m[04/15 06:05:26 d2.evaluation.evaluator]: [0mInference done 1150/1257. 0.1290 s / img. ETA=0:00:17
[32m[04/15 06:05:31 d2.evaluation.evaluator]: [0mInference done 1180/1257. 0.1288 s / img. ETA=0:00:12
[32m[04/15 06:05:36 d2.evaluation.evaluator]: [0mInference done 1218/1257. 0.1286 s / img. ETA=0:00:06
[32m[04/15 06:05:41 d2.evaluation.evaluator]: [0mInference done 1255/1257. 0.1283 s / img. ETA=0:00:00
[32m[04/15 06:05:41 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:19.912609 (0.159675 s / img per device, on 1 devices)
[32m[04/15 06:05:41 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:40 (0.128314 s / img per device, on 1 devices)
[32m[04/15 06:05:41 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 06:05:41 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 06:05:42 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.42s).
Accumulating evaluation results...
DONE (t=1.43s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.181
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.350
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.166
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.113
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.084
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.217
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.153
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.260
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.429
[32m[04/15 06:05:50 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 18.117 | 34.967 | 16.553 | 11.277 | 21.887 | 39.560 |
[32m[04/15 06:05:50 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 27.616 | bicycle       | 5.674 | car            | 39.178 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  7  *  2000  iterations ============
4 channel input
[32m[04/15 06:05:51 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 06:05:52 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.40 seconds.
[5m[31mWARNING[0m [32m[04/15 06:05:52 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:05:52 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 06:05:53 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 06:05:54 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 06:05:54 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 06:05:57 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 06:06:09 d2.utils.events]: [0m eta: 0:17:24  iter: 19  total_loss: 0.764  loss_cls: 0.238  loss_box_reg: 0.376  loss_rpn_cls: 0.026  loss_rpn_loc: 0.082  time: 0.5712  data_time: 0.0432  lr: 0.000100  max_mem: 3067M
[32m[04/15 06:06:22 d2.utils.events]: [0m eta: 0:17:03  iter: 39  total_loss: 0.722  loss_cls: 0.235  loss_box_reg: 0.392  loss_rpn_cls: 0.027  loss_rpn_loc: 0.067  time: 0.5460  data_time: 0.0081  lr: 0.000200  max_mem: 3067M
[32m[04/15 06:06:36 d2.utils.events]: [0m eta: 0:16:53  iter: 59  total_loss: 0.709  loss_cls: 0.228  loss_box_reg: 0.369  loss_rpn_cls: 0.027  loss_rpn_loc: 0.053  time: 0.5484  data_time: 0.0193  lr: 0.000300  max_mem: 3067M
[32m[04/15 06:06:51 d2.utils.events]: [0m eta: 0:16:40  iter: 79  total_loss: 0.700  loss_cls: 0.230  loss_box_reg: 0.368  loss_rpn_cls: 0.028  loss_rpn_loc: 0.055  time: 0.5403  data_time: 0.0074  lr: 0.000400  max_mem: 3067M
[32m[04/15 06:07:02 d2.utils.events]: [0m eta: 0:16:49  iter: 99  total_loss: 0.660  loss_cls: 0.213  loss_box_reg: 0.354  loss_rpn_cls: 0.026  loss_rpn_loc: 0.068  time: 0.5429  data_time: 0.0069  lr: 0.000500  max_mem: 3067M
[32m[04/15 06:07:13 d2.utils.events]: [0m eta: 0:16:38  iter: 119  total_loss: 0.607  loss_cls: 0.187  loss_box_reg: 0.336  loss_rpn_cls: 0.022  loss_rpn_loc: 0.060  time: 0.5437  data_time: 0.0072  lr: 0.000599  max_mem: 3067M
[32m[04/15 06:07:26 d2.utils.events]: [0m eta: 0:16:33  iter: 139  total_loss: 0.727  loss_cls: 0.219  loss_box_reg: 0.382  loss_rpn_cls: 0.028  loss_rpn_loc: 0.059  time: 0.5543  data_time: 0.0093  lr: 0.000699  max_mem: 3067M
[32m[04/15 06:07:37 d2.utils.events]: [0m eta: 0:16:25  iter: 159  total_loss: 0.710  loss_cls: 0.232  loss_box_reg: 0.381  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5533  data_time: 0.0091  lr: 0.000799  max_mem: 3067M
[32m[04/15 06:07:48 d2.utils.events]: [0m eta: 0:16:14  iter: 179  total_loss: 0.638  loss_cls: 0.222  loss_box_reg: 0.324  loss_rpn_cls: 0.019  loss_rpn_loc: 0.056  time: 0.5519  data_time: 0.0066  lr: 0.000899  max_mem: 3067M
[32m[04/15 06:07:59 d2.utils.events]: [0m eta: 0:16:09  iter: 199  total_loss: 0.664  loss_cls: 0.198  loss_box_reg: 0.338  loss_rpn_cls: 0.023  loss_rpn_loc: 0.069  time: 0.5532  data_time: 0.0078  lr: 0.000999  max_mem: 3067M
[32m[04/15 06:08:11 d2.utils.events]: [0m eta: 0:16:04  iter: 219  total_loss: 0.715  loss_cls: 0.225  loss_box_reg: 0.356  loss_rpn_cls: 0.019  loss_rpn_loc: 0.071  time: 0.5550  data_time: 0.0068  lr: 0.001099  max_mem: 3067M
[32m[04/15 06:08:22 d2.utils.events]: [0m eta: 0:15:54  iter: 239  total_loss: 0.720  loss_cls: 0.216  loss_box_reg: 0.390  loss_rpn_cls: 0.022  loss_rpn_loc: 0.055  time: 0.5544  data_time: 0.0065  lr: 0.001199  max_mem: 3067M
[32m[04/15 06:08:33 d2.utils.events]: [0m eta: 0:15:45  iter: 259  total_loss: 0.639  loss_cls: 0.208  loss_box_reg: 0.321  loss_rpn_cls: 0.022  loss_rpn_loc: 0.050  time: 0.5552  data_time: 0.0075  lr: 0.001299  max_mem: 3067M
[32m[04/15 06:08:45 d2.utils.events]: [0m eta: 0:15:38  iter: 279  total_loss: 0.703  loss_cls: 0.220  loss_box_reg: 0.413  loss_rpn_cls: 0.025  loss_rpn_loc: 0.074  time: 0.5578  data_time: 0.0072  lr: 0.001399  max_mem: 3067M
[32m[04/15 06:08:56 d2.utils.events]: [0m eta: 0:15:27  iter: 299  total_loss: 0.604  loss_cls: 0.203  loss_box_reg: 0.328  loss_rpn_cls: 0.021  loss_rpn_loc: 0.047  time: 0.5569  data_time: 0.0071  lr: 0.001499  max_mem: 3067M
[32m[04/15 06:09:08 d2.utils.events]: [0m eta: 0:15:17  iter: 319  total_loss: 0.596  loss_cls: 0.200  loss_box_reg: 0.344  loss_rpn_cls: 0.017  loss_rpn_loc: 0.044  time: 0.5577  data_time: 0.0079  lr: 0.001598  max_mem: 3067M
[32m[04/15 06:09:20 d2.utils.events]: [0m eta: 0:15:09  iter: 339  total_loss: 0.651  loss_cls: 0.205  loss_box_reg: 0.360  loss_rpn_cls: 0.021  loss_rpn_loc: 0.050  time: 0.5602  data_time: 0.0134  lr: 0.001698  max_mem: 3067M
[32m[04/15 06:09:31 d2.utils.events]: [0m eta: 0:14:59  iter: 359  total_loss: 0.722  loss_cls: 0.245  loss_box_reg: 0.398  loss_rpn_cls: 0.023  loss_rpn_loc: 0.064  time: 0.5600  data_time: 0.0116  lr: 0.001798  max_mem: 3067M
[32m[04/15 06:09:42 d2.utils.events]: [0m eta: 0:14:48  iter: 379  total_loss: 0.581  loss_cls: 0.193  loss_box_reg: 0.306  loss_rpn_cls: 0.023  loss_rpn_loc: 0.056  time: 0.5595  data_time: 0.0067  lr: 0.001898  max_mem: 3067M
[32m[04/15 06:09:54 d2.utils.events]: [0m eta: 0:14:40  iter: 399  total_loss: 0.695  loss_cls: 0.233  loss_box_reg: 0.379  loss_rpn_cls: 0.020  loss_rpn_loc: 0.054  time: 0.5606  data_time: 0.0065  lr: 0.001998  max_mem: 3067M
[32m[04/15 06:10:05 d2.utils.events]: [0m eta: 0:14:31  iter: 419  total_loss: 0.632  loss_cls: 0.193  loss_box_reg: 0.324  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5617  data_time: 0.0082  lr: 0.002098  max_mem: 3067M
[32m[04/15 06:10:16 d2.utils.events]: [0m eta: 0:14:17  iter: 439  total_loss: 0.624  loss_cls: 0.197  loss_box_reg: 0.330  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5607  data_time: 0.0095  lr: 0.002198  max_mem: 3067M
[32m[04/15 06:10:28 d2.utils.events]: [0m eta: 0:14:08  iter: 459  total_loss: 0.666  loss_cls: 0.215  loss_box_reg: 0.391  loss_rpn_cls: 0.018  loss_rpn_loc: 0.050  time: 0.5616  data_time: 0.0094  lr: 0.002298  max_mem: 3067M
[32m[04/15 06:10:41 d2.utils.events]: [0m eta: 0:14:02  iter: 479  total_loss: 0.631  loss_cls: 0.198  loss_box_reg: 0.358  loss_rpn_cls: 0.020  loss_rpn_loc: 0.076  time: 0.5643  data_time: 0.0099  lr: 0.002398  max_mem: 3067M
[32m[04/15 06:10:52 d2.utils.events]: [0m eta: 0:13:50  iter: 499  total_loss: 0.728  loss_cls: 0.249  loss_box_reg: 0.380  loss_rpn_cls: 0.019  loss_rpn_loc: 0.058  time: 0.5635  data_time: 0.0074  lr: 0.002498  max_mem: 3067M
[32m[04/15 06:11:03 d2.utils.events]: [0m eta: 0:13:39  iter: 519  total_loss: 0.715  loss_cls: 0.235  loss_box_reg: 0.406  loss_rpn_cls: 0.022  loss_rpn_loc: 0.061  time: 0.5636  data_time: 0.0073  lr: 0.002597  max_mem: 3067M
[32m[04/15 06:11:15 d2.utils.events]: [0m eta: 0:13:29  iter: 539  total_loss: 0.564  loss_cls: 0.190  loss_box_reg: 0.303  loss_rpn_cls: 0.016  loss_rpn_loc: 0.045  time: 0.5653  data_time: 0.0078  lr: 0.002697  max_mem: 3067M
[32m[04/15 06:11:26 d2.utils.events]: [0m eta: 0:13:16  iter: 559  total_loss: 0.632  loss_cls: 0.220  loss_box_reg: 0.326  loss_rpn_cls: 0.018  loss_rpn_loc: 0.046  time: 0.5644  data_time: 0.0065  lr: 0.002797  max_mem: 3067M
[32m[04/15 06:11:37 d2.utils.events]: [0m eta: 0:13:05  iter: 579  total_loss: 0.449  loss_cls: 0.168  loss_box_reg: 0.245  loss_rpn_cls: 0.017  loss_rpn_loc: 0.055  time: 0.5642  data_time: 0.0094  lr: 0.002897  max_mem: 3067M
[32m[04/15 06:11:50 d2.utils.events]: [0m eta: 0:12:55  iter: 599  total_loss: 0.678  loss_cls: 0.214  loss_box_reg: 0.382  loss_rpn_cls: 0.022  loss_rpn_loc: 0.062  time: 0.5652  data_time: 0.0098  lr: 0.002997  max_mem: 3067M
[32m[04/15 06:12:01 d2.utils.events]: [0m eta: 0:12:44  iter: 619  total_loss: 0.699  loss_cls: 0.224  loss_box_reg: 0.389  loss_rpn_cls: 0.017  loss_rpn_loc: 0.053  time: 0.5660  data_time: 0.0068  lr: 0.003097  max_mem: 3067M
[32m[04/15 06:12:13 d2.utils.events]: [0m eta: 0:12:33  iter: 639  total_loss: 0.719  loss_cls: 0.207  loss_box_reg: 0.416  loss_rpn_cls: 0.024  loss_rpn_loc: 0.055  time: 0.5662  data_time: 0.0067  lr: 0.003197  max_mem: 3067M
[32m[04/15 06:12:25 d2.utils.events]: [0m eta: 0:12:22  iter: 659  total_loss: 0.660  loss_cls: 0.230  loss_box_reg: 0.347  loss_rpn_cls: 0.023  loss_rpn_loc: 0.054  time: 0.5668  data_time: 0.0101  lr: 0.003297  max_mem: 3067M
[32m[04/15 06:12:37 d2.utils.events]: [0m eta: 0:12:11  iter: 679  total_loss: 0.579  loss_cls: 0.193  loss_box_reg: 0.299  loss_rpn_cls: 0.013  loss_rpn_loc: 0.056  time: 0.5674  data_time: 0.0095  lr: 0.003397  max_mem: 3067M
[32m[04/15 06:12:48 d2.utils.events]: [0m eta: 0:12:00  iter: 699  total_loss: 0.686  loss_cls: 0.232  loss_box_reg: 0.347  loss_rpn_cls: 0.026  loss_rpn_loc: 0.049  time: 0.5670  data_time: 0.0079  lr: 0.003497  max_mem: 3067M
[32m[04/15 06:13:00 d2.utils.events]: [0m eta: 0:11:49  iter: 719  total_loss: 0.538  loss_cls: 0.192  loss_box_reg: 0.246  loss_rpn_cls: 0.020  loss_rpn_loc: 0.043  time: 0.5673  data_time: 0.0066  lr: 0.003596  max_mem: 3067M
[32m[04/15 06:13:12 d2.utils.events]: [0m eta: 0:11:39  iter: 739  total_loss: 0.584  loss_cls: 0.200  loss_box_reg: 0.341  loss_rpn_cls: 0.021  loss_rpn_loc: 0.058  time: 0.5681  data_time: 0.0079  lr: 0.003696  max_mem: 3067M
[32m[04/15 06:13:22 d2.utils.events]: [0m eta: 0:11:27  iter: 759  total_loss: 0.577  loss_cls: 0.190  loss_box_reg: 0.326  loss_rpn_cls: 0.022  loss_rpn_loc: 0.040  time: 0.5675  data_time: 0.0064  lr: 0.003796  max_mem: 3067M
[32m[04/15 06:13:34 d2.utils.events]: [0m eta: 0:11:16  iter: 779  total_loss: 0.662  loss_cls: 0.208  loss_box_reg: 0.344  loss_rpn_cls: 0.021  loss_rpn_loc: 0.066  time: 0.5670  data_time: 0.0072  lr: 0.003896  max_mem: 3067M
[32m[04/15 06:13:46 d2.utils.events]: [0m eta: 0:11:05  iter: 799  total_loss: 0.600  loss_cls: 0.188  loss_box_reg: 0.321  loss_rpn_cls: 0.027  loss_rpn_loc: 0.047  time: 0.5680  data_time: 0.0110  lr: 0.003996  max_mem: 3067M
[32m[04/15 06:13:57 d2.utils.events]: [0m eta: 0:10:54  iter: 819  total_loss: 0.567  loss_cls: 0.173  loss_box_reg: 0.277  loss_rpn_cls: 0.028  loss_rpn_loc: 0.054  time: 0.5673  data_time: 0.0064  lr: 0.004096  max_mem: 3067M
[32m[04/15 06:14:08 d2.utils.events]: [0m eta: 0:10:43  iter: 839  total_loss: 0.552  loss_cls: 0.199  loss_box_reg: 0.264  loss_rpn_cls: 0.030  loss_rpn_loc: 0.069  time: 0.5673  data_time: 0.0112  lr: 0.004196  max_mem: 3067M
[32m[04/15 06:14:20 d2.utils.events]: [0m eta: 0:10:32  iter: 859  total_loss: 0.767  loss_cls: 0.232  loss_box_reg: 0.363  loss_rpn_cls: 0.027  loss_rpn_loc: 0.072  time: 0.5675  data_time: 0.0075  lr: 0.004296  max_mem: 3067M
[32m[04/15 06:14:31 d2.utils.events]: [0m eta: 0:10:20  iter: 879  total_loss: 0.589  loss_cls: 0.211  loss_box_reg: 0.283  loss_rpn_cls: 0.025  loss_rpn_loc: 0.056  time: 0.5673  data_time: 0.0103  lr: 0.004396  max_mem: 3067M
[32m[04/15 06:14:42 d2.utils.events]: [0m eta: 0:10:08  iter: 899  total_loss: 0.534  loss_cls: 0.181  loss_box_reg: 0.295  loss_rpn_cls: 0.017  loss_rpn_loc: 0.048  time: 0.5667  data_time: 0.0073  lr: 0.004496  max_mem: 3067M
[32m[04/15 06:14:54 d2.utils.events]: [0m eta: 0:09:58  iter: 919  total_loss: 0.723  loss_cls: 0.251  loss_box_reg: 0.388  loss_rpn_cls: 0.026  loss_rpn_loc: 0.055  time: 0.5675  data_time: 0.0088  lr: 0.004595  max_mem: 3067M
[32m[04/15 06:15:06 d2.utils.events]: [0m eta: 0:09:47  iter: 939  total_loss: 0.693  loss_cls: 0.222  loss_box_reg: 0.382  loss_rpn_cls: 0.014  loss_rpn_loc: 0.081  time: 0.5679  data_time: 0.0073  lr: 0.004695  max_mem: 3067M
[32m[04/15 06:15:17 d2.utils.events]: [0m eta: 0:09:36  iter: 959  total_loss: 0.681  loss_cls: 0.236  loss_box_reg: 0.331  loss_rpn_cls: 0.020  loss_rpn_loc: 0.067  time: 0.5673  data_time: 0.0065  lr: 0.004795  max_mem: 3067M
[32m[04/15 06:15:28 d2.utils.events]: [0m eta: 0:09:25  iter: 979  total_loss: 0.720  loss_cls: 0.254  loss_box_reg: 0.395  loss_rpn_cls: 0.020  loss_rpn_loc: 0.045  time: 0.5675  data_time: 0.0077  lr: 0.004895  max_mem: 3067M
[32m[04/15 06:15:39 d2.utils.events]: [0m eta: 0:09:14  iter: 999  total_loss: 0.756  loss_cls: 0.239  loss_box_reg: 0.412  loss_rpn_cls: 0.027  loss_rpn_loc: 0.065  time: 0.5674  data_time: 0.0078  lr: 0.004995  max_mem: 3067M
[32m[04/15 06:15:50 d2.utils.events]: [0m eta: 0:09:02  iter: 1019  total_loss: 0.723  loss_cls: 0.226  loss_box_reg: 0.389  loss_rpn_cls: 0.018  loss_rpn_loc: 0.077  time: 0.5665  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:16:02 d2.utils.events]: [0m eta: 0:08:52  iter: 1039  total_loss: 0.777  loss_cls: 0.241  loss_box_reg: 0.413  loss_rpn_cls: 0.019  loss_rpn_loc: 0.064  time: 0.5669  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:16:13 d2.utils.events]: [0m eta: 0:08:41  iter: 1059  total_loss: 0.641  loss_cls: 0.224  loss_box_reg: 0.342  loss_rpn_cls: 0.020  loss_rpn_loc: 0.054  time: 0.5669  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:16:24 d2.utils.events]: [0m eta: 0:08:30  iter: 1079  total_loss: 0.656  loss_cls: 0.219  loss_box_reg: 0.356  loss_rpn_cls: 0.023  loss_rpn_loc: 0.067  time: 0.5661  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:16:36 d2.utils.events]: [0m eta: 0:08:20  iter: 1099  total_loss: 0.752  loss_cls: 0.241  loss_box_reg: 0.371  loss_rpn_cls: 0.031  loss_rpn_loc: 0.074  time: 0.5665  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:16:47 d2.utils.events]: [0m eta: 0:08:09  iter: 1119  total_loss: 0.659  loss_cls: 0.240  loss_box_reg: 0.368  loss_rpn_cls: 0.028  loss_rpn_loc: 0.052  time: 0.5668  data_time: 0.0116  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:16:59 d2.utils.events]: [0m eta: 0:07:57  iter: 1139  total_loss: 0.752  loss_cls: 0.234  loss_box_reg: 0.370  loss_rpn_cls: 0.032  loss_rpn_loc: 0.077  time: 0.5667  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:17:10 d2.utils.events]: [0m eta: 0:07:47  iter: 1159  total_loss: 0.658  loss_cls: 0.211  loss_box_reg: 0.375  loss_rpn_cls: 0.024  loss_rpn_loc: 0.063  time: 0.5665  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:17:21 d2.utils.events]: [0m eta: 0:07:35  iter: 1179  total_loss: 0.672  loss_cls: 0.220  loss_box_reg: 0.370  loss_rpn_cls: 0.021  loss_rpn_loc: 0.062  time: 0.5665  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:17:33 d2.utils.events]: [0m eta: 0:07:24  iter: 1199  total_loss: 0.559  loss_cls: 0.197  loss_box_reg: 0.282  loss_rpn_cls: 0.020  loss_rpn_loc: 0.042  time: 0.5666  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:17:44 d2.utils.events]: [0m eta: 0:07:13  iter: 1219  total_loss: 0.656  loss_cls: 0.231  loss_box_reg: 0.354  loss_rpn_cls: 0.022  loss_rpn_loc: 0.049  time: 0.5664  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:17:56 d2.utils.events]: [0m eta: 0:07:02  iter: 1239  total_loss: 0.543  loss_cls: 0.173  loss_box_reg: 0.266  loss_rpn_cls: 0.030  loss_rpn_loc: 0.051  time: 0.5665  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:18:07 d2.utils.events]: [0m eta: 0:06:50  iter: 1259  total_loss: 0.632  loss_cls: 0.229  loss_box_reg: 0.318  loss_rpn_cls: 0.031  loss_rpn_loc: 0.070  time: 0.5664  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:18:18 d2.utils.events]: [0m eta: 0:06:39  iter: 1279  total_loss: 0.627  loss_cls: 0.201  loss_box_reg: 0.353  loss_rpn_cls: 0.019  loss_rpn_loc: 0.051  time: 0.5665  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:18:30 d2.utils.events]: [0m eta: 0:06:28  iter: 1299  total_loss: 0.558  loss_cls: 0.199  loss_box_reg: 0.318  loss_rpn_cls: 0.023  loss_rpn_loc: 0.048  time: 0.5666  data_time: 0.0096  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:18:41 d2.utils.events]: [0m eta: 0:06:16  iter: 1319  total_loss: 0.786  loss_cls: 0.239  loss_box_reg: 0.412  loss_rpn_cls: 0.025  loss_rpn_loc: 0.053  time: 0.5661  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:18:52 d2.utils.events]: [0m eta: 0:06:05  iter: 1339  total_loss: 0.654  loss_cls: 0.213  loss_box_reg: 0.348  loss_rpn_cls: 0.026  loss_rpn_loc: 0.066  time: 0.5660  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:19:04 d2.utils.events]: [0m eta: 0:05:54  iter: 1359  total_loss: 0.745  loss_cls: 0.242  loss_box_reg: 0.402  loss_rpn_cls: 0.023  loss_rpn_loc: 0.062  time: 0.5665  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:19:15 d2.utils.events]: [0m eta: 0:05:43  iter: 1379  total_loss: 0.753  loss_cls: 0.240  loss_box_reg: 0.384  loss_rpn_cls: 0.028  loss_rpn_loc: 0.061  time: 0.5659  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:19:26 d2.utils.events]: [0m eta: 0:05:31  iter: 1399  total_loss: 0.690  loss_cls: 0.211  loss_box_reg: 0.401  loss_rpn_cls: 0.019  loss_rpn_loc: 0.051  time: 0.5659  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:19:38 d2.utils.events]: [0m eta: 0:05:21  iter: 1419  total_loss: 0.676  loss_cls: 0.229  loss_box_reg: 0.364  loss_rpn_cls: 0.022  loss_rpn_loc: 0.057  time: 0.5659  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:19:49 d2.utils.events]: [0m eta: 0:05:10  iter: 1439  total_loss: 0.693  loss_cls: 0.233  loss_box_reg: 0.346  loss_rpn_cls: 0.022  loss_rpn_loc: 0.057  time: 0.5660  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:20:01 d2.utils.events]: [0m eta: 0:04:59  iter: 1459  total_loss: 0.608  loss_cls: 0.223  loss_box_reg: 0.331  loss_rpn_cls: 0.030  loss_rpn_loc: 0.053  time: 0.5664  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:20:12 d2.utils.events]: [0m eta: 0:04:47  iter: 1479  total_loss: 0.597  loss_cls: 0.191  loss_box_reg: 0.308  loss_rpn_cls: 0.016  loss_rpn_loc: 0.057  time: 0.5662  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:20:28 d2.utils.events]: [0m eta: 0:04:37  iter: 1499  total_loss: 0.661  loss_cls: 0.218  loss_box_reg: 0.375  loss_rpn_cls: 0.024  loss_rpn_loc: 0.056  time: 0.5691  data_time: 0.2745  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:20:39 d2.utils.events]: [0m eta: 0:04:26  iter: 1519  total_loss: 0.767  loss_cls: 0.253  loss_box_reg: 0.417  loss_rpn_cls: 0.019  loss_rpn_loc: 0.074  time: 0.5688  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:20:51 d2.utils.events]: [0m eta: 0:04:15  iter: 1539  total_loss: 0.627  loss_cls: 0.188  loss_box_reg: 0.321  loss_rpn_cls: 0.025  loss_rpn_loc: 0.048  time: 0.5692  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:21:03 d2.utils.events]: [0m eta: 0:04:04  iter: 1559  total_loss: 0.721  loss_cls: 0.249  loss_box_reg: 0.411  loss_rpn_cls: 0.025  loss_rpn_loc: 0.061  time: 0.5694  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:21:14 d2.utils.events]: [0m eta: 0:03:53  iter: 1579  total_loss: 0.633  loss_cls: 0.204  loss_box_reg: 0.346  loss_rpn_cls: 0.016  loss_rpn_loc: 0.068  time: 0.5693  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:21:26 d2.utils.events]: [0m eta: 0:03:42  iter: 1599  total_loss: 0.869  loss_cls: 0.282  loss_box_reg: 0.469  loss_rpn_cls: 0.025  loss_rpn_loc: 0.072  time: 0.5694  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:21:37 d2.utils.events]: [0m eta: 0:03:30  iter: 1619  total_loss: 0.737  loss_cls: 0.241  loss_box_reg: 0.407  loss_rpn_cls: 0.026  loss_rpn_loc: 0.067  time: 0.5692  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:21:49 d2.utils.events]: [0m eta: 0:03:19  iter: 1639  total_loss: 0.706  loss_cls: 0.246  loss_box_reg: 0.368  loss_rpn_cls: 0.020  loss_rpn_loc: 0.073  time: 0.5693  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:22:00 d2.utils.events]: [0m eta: 0:03:08  iter: 1659  total_loss: 0.522  loss_cls: 0.168  loss_box_reg: 0.301  loss_rpn_cls: 0.019  loss_rpn_loc: 0.045  time: 0.5689  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:22:11 d2.utils.events]: [0m eta: 0:02:57  iter: 1679  total_loss: 0.654  loss_cls: 0.211  loss_box_reg: 0.366  loss_rpn_cls: 0.024  loss_rpn_loc: 0.060  time: 0.5689  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:22:23 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.821  loss_cls: 0.281  loss_box_reg: 0.425  loss_rpn_cls: 0.026  loss_rpn_loc: 0.077  time: 0.5691  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:22:34 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.831  loss_cls: 0.285  loss_box_reg: 0.441  loss_rpn_cls: 0.036  loss_rpn_loc: 0.064  time: 0.5686  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:22:45 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.538  loss_cls: 0.186  loss_box_reg: 0.281  loss_rpn_cls: 0.021  loss_rpn_loc: 0.056  time: 0.5688  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:22:57 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.646  loss_cls: 0.193  loss_box_reg: 0.335  loss_rpn_cls: 0.021  loss_rpn_loc: 0.066  time: 0.5687  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:23:08 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.550  loss_cls: 0.199  loss_box_reg: 0.302  loss_rpn_cls: 0.023  loss_rpn_loc: 0.058  time: 0.5688  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:23:20 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.678  loss_cls: 0.253  loss_box_reg: 0.364  loss_rpn_cls: 0.028  loss_rpn_loc: 0.053  time: 0.5690  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:23:31 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.634  loss_cls: 0.200  loss_box_reg: 0.322  loss_rpn_cls: 0.018  loss_rpn_loc: 0.050  time: 0.5688  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:23:43 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.693  loss_cls: 0.211  loss_box_reg: 0.343  loss_rpn_cls: 0.026  loss_rpn_loc: 0.056  time: 0.5689  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:23:55 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.787  loss_cls: 0.256  loss_box_reg: 0.423  loss_rpn_cls: 0.027  loss_rpn_loc: 0.074  time: 0.5690  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:24:06 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.711  loss_cls: 0.238  loss_box_reg: 0.354  loss_rpn_cls: 0.028  loss_rpn_loc: 0.053  time: 0.5689  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:24:18 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.714  loss_cls: 0.209  loss_box_reg: 0.406  loss_rpn_cls: 0.025  loss_rpn_loc: 0.057  time: 0.5690  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:24:29 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.655  loss_cls: 0.220  loss_box_reg: 0.350  loss_rpn_cls: 0.017  loss_rpn_loc: 0.055  time: 0.5688  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:24:40 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.715  loss_cls: 0.257  loss_box_reg: 0.372  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5688  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:24:52 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.848  loss_cls: 0.263  loss_box_reg: 0.429  loss_rpn_cls: 0.030  loss_rpn_loc: 0.066  time: 0.5689  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:25:03 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.703  loss_cls: 0.249  loss_box_reg: 0.320  loss_rpn_cls: 0.031  loss_rpn_loc: 0.050  time: 0.5686  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 06:25:34 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:25:34 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 06:25:35 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 06:25:35 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.635  loss_cls: 0.194  loss_box_reg: 0.340  loss_rpn_cls: 0.021  loss_rpn_loc: 0.053  time: 0.5686  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:25:35 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:56 (0.5689 s / it)
[32m[04/15 06:25:35 d2.engine.hooks]: [0mTotal training time: 0:19:36 (0:00:40 on hooks)
4 channel input
[32m[04/15 06:25:44 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json takes 1.10 seconds.
[5m[31mWARNING[0m [32m[04/15 06:25:44 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:25:44 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 06:25:44 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 06:25:48 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1248 s / img. ETA=0:02:44
[32m[04/15 06:25:53 d2.evaluation.evaluator]: [0mInference done 40/1257. 0.1213 s / img. ETA=0:03:22
[32m[04/15 06:25:58 d2.evaluation.evaluator]: [0mInference done 73/1257. 0.1231 s / img. ETA=0:03:09
[32m[04/15 06:26:03 d2.evaluation.evaluator]: [0mInference done 104/1257. 0.1327 s / img. ETA=0:03:05
[32m[04/15 06:26:08 d2.evaluation.evaluator]: [0mInference done 137/1257. 0.1299 s / img. ETA=0:02:59
[32m[04/15 06:26:14 d2.evaluation.evaluator]: [0mInference done 171/1257. 0.1279 s / img. ETA=0:02:51
[32m[04/15 06:26:19 d2.evaluation.evaluator]: [0mInference done 203/1257. 0.1267 s / img. ETA=0:02:45
[32m[04/15 06:26:24 d2.evaluation.evaluator]: [0mInference done 240/1257. 0.1274 s / img. ETA=0:02:36
[32m[04/15 06:26:29 d2.evaluation.evaluator]: [0mInference done 277/1257. 0.1279 s / img. ETA=0:02:28
[32m[04/15 06:26:34 d2.evaluation.evaluator]: [0mInference done 316/1257. 0.1272 s / img. ETA=0:02:20
[32m[04/15 06:26:39 d2.evaluation.evaluator]: [0mInference done 351/1257. 0.1284 s / img. ETA=0:02:15
[32m[04/15 06:26:44 d2.evaluation.evaluator]: [0mInference done 388/1257. 0.1286 s / img. ETA=0:02:08
[32m[04/15 06:26:49 d2.evaluation.evaluator]: [0mInference done 420/1257. 0.1281 s / img. ETA=0:02:04
[32m[04/15 06:26:54 d2.evaluation.evaluator]: [0mInference done 448/1257. 0.1274 s / img. ETA=0:02:02
[32m[04/15 06:26:59 d2.evaluation.evaluator]: [0mInference done 478/1257. 0.1269 s / img. ETA=0:01:58
[32m[04/15 06:27:04 d2.evaluation.evaluator]: [0mInference done 508/1257. 0.1271 s / img. ETA=0:01:54
[32m[04/15 06:27:09 d2.evaluation.evaluator]: [0mInference done 538/1257. 0.1268 s / img. ETA=0:01:50
[32m[04/15 06:27:14 d2.evaluation.evaluator]: [0mInference done 572/1257. 0.1268 s / img. ETA=0:01:45
[32m[04/15 06:27:20 d2.evaluation.evaluator]: [0mInference done 608/1257. 0.1273 s / img. ETA=0:01:39
[32m[04/15 06:27:25 d2.evaluation.evaluator]: [0mInference done 637/1257. 0.1275 s / img. ETA=0:01:35
[32m[04/15 06:27:30 d2.evaluation.evaluator]: [0mInference done 665/1257. 0.1272 s / img. ETA=0:01:32
[32m[04/15 06:27:35 d2.evaluation.evaluator]: [0mInference done 695/1257. 0.1269 s / img. ETA=0:01:27
[32m[04/15 06:27:40 d2.evaluation.evaluator]: [0mInference done 729/1257. 0.1267 s / img. ETA=0:01:22
[32m[04/15 06:27:45 d2.evaluation.evaluator]: [0mInference done 765/1257. 0.1272 s / img. ETA=0:01:16
[32m[04/15 06:27:50 d2.evaluation.evaluator]: [0mInference done 805/1257. 0.1269 s / img. ETA=0:01:09
[32m[04/15 06:27:55 d2.evaluation.evaluator]: [0mInference done 844/1257. 0.1269 s / img. ETA=0:01:03
[32m[04/15 06:28:00 d2.evaluation.evaluator]: [0mInference done 879/1257. 0.1274 s / img. ETA=0:00:57
[32m[04/15 06:28:06 d2.evaluation.evaluator]: [0mInference done 916/1257. 0.1275 s / img. ETA=0:00:51
[32m[04/15 06:28:11 d2.evaluation.evaluator]: [0mInference done 954/1257. 0.1272 s / img. ETA=0:00:45
[32m[04/15 06:28:16 d2.evaluation.evaluator]: [0mInference done 995/1257. 0.1269 s / img. ETA=0:00:39
[32m[04/15 06:28:21 d2.evaluation.evaluator]: [0mInference done 1031/1257. 0.1272 s / img. ETA=0:00:33
[32m[04/15 06:28:26 d2.evaluation.evaluator]: [0mInference done 1070/1257. 0.1271 s / img. ETA=0:00:27
[32m[04/15 06:28:31 d2.evaluation.evaluator]: [0mInference done 1108/1257. 0.1269 s / img. ETA=0:00:22
[32m[04/15 06:28:36 d2.evaluation.evaluator]: [0mInference done 1145/1257. 0.1269 s / img. ETA=0:00:16
[32m[04/15 06:28:41 d2.evaluation.evaluator]: [0mInference done 1180/1257. 0.1273 s / img. ETA=0:00:11
[32m[04/15 06:28:46 d2.evaluation.evaluator]: [0mInference done 1215/1257. 0.1273 s / img. ETA=0:00:06
[32m[04/15 06:28:51 d2.evaluation.evaluator]: [0mInference done 1255/1257. 0.1271 s / img. ETA=0:00:00
[32m[04/15 06:28:52 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:04.213332 (0.147135 s / img per device, on 1 devices)
[32m[04/15 06:28:52 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:39 (0.127066 s / img per device, on 1 devices)
[32m[04/15 06:28:53 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 06:28:53 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 06:28:54 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.03s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=4.52s).
Accumulating evaluation results...
DONE (t=0.89s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.196
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.379
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.173
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.120
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.242
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.439
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.101
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.161
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.299
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.471
[32m[04/15 06:29:02 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.571 | 37.919 | 17.273 | 11.977 | 24.185 | 43.930 |
[32m[04/15 06:29:02 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 28.240 | bicycle       | 8.349 | car            | 41.398 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.297 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  8  *  2000  iterations ============
4 channel input
[32m[04/15 06:29:03 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 06:29:04 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.35 seconds.
[5m[31mWARNING[0m [32m[04/15 06:29:04 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:29:04 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 06:29:05 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 06:29:05 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 06:29:05 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 06:29:06 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 06:29:19 d2.utils.events]: [0m eta: 0:19:01  iter: 19  total_loss: 0.642  loss_cls: 0.212  loss_box_reg: 0.346  loss_rpn_cls: 0.016  loss_rpn_loc: 0.066  time: 0.5997  data_time: 0.0598  lr: 0.000100  max_mem: 3067M
[32m[04/15 06:29:29 d2.utils.events]: [0m eta: 0:17:48  iter: 39  total_loss: 0.730  loss_cls: 0.234  loss_box_reg: 0.370  loss_rpn_cls: 0.020  loss_rpn_loc: 0.062  time: 0.5654  data_time: 0.0075  lr: 0.000200  max_mem: 3067M
[32m[04/15 06:29:41 d2.utils.events]: [0m eta: 0:17:45  iter: 59  total_loss: 0.826  loss_cls: 0.266  loss_box_reg: 0.434  loss_rpn_cls: 0.031  loss_rpn_loc: 0.079  time: 0.5667  data_time: 0.0084  lr: 0.000300  max_mem: 3067M
[32m[04/15 06:29:52 d2.utils.events]: [0m eta: 0:17:34  iter: 79  total_loss: 0.571  loss_cls: 0.191  loss_box_reg: 0.316  loss_rpn_cls: 0.016  loss_rpn_loc: 0.047  time: 0.5633  data_time: 0.0067  lr: 0.000400  max_mem: 3067M
[32m[04/15 06:30:04 d2.utils.events]: [0m eta: 0:17:29  iter: 99  total_loss: 0.701  loss_cls: 0.215  loss_box_reg: 0.368  loss_rpn_cls: 0.027  loss_rpn_loc: 0.059  time: 0.5683  data_time: 0.0073  lr: 0.000500  max_mem: 3067M
[32m[04/15 06:30:15 d2.utils.events]: [0m eta: 0:17:24  iter: 119  total_loss: 0.640  loss_cls: 0.198  loss_box_reg: 0.357  loss_rpn_cls: 0.017  loss_rpn_loc: 0.049  time: 0.5663  data_time: 0.0090  lr: 0.000599  max_mem: 3067M
[32m[04/15 06:30:27 d2.utils.events]: [0m eta: 0:17:15  iter: 139  total_loss: 0.581  loss_cls: 0.204  loss_box_reg: 0.309  loss_rpn_cls: 0.015  loss_rpn_loc: 0.047  time: 0.5669  data_time: 0.0065  lr: 0.000699  max_mem: 3067M
[32m[04/15 06:30:39 d2.utils.events]: [0m eta: 0:17:10  iter: 159  total_loss: 0.680  loss_cls: 0.238  loss_box_reg: 0.365  loss_rpn_cls: 0.022  loss_rpn_loc: 0.059  time: 0.5720  data_time: 0.0071  lr: 0.000799  max_mem: 3067M
[32m[04/15 06:30:50 d2.utils.events]: [0m eta: 0:16:53  iter: 179  total_loss: 0.567  loss_cls: 0.182  loss_box_reg: 0.315  loss_rpn_cls: 0.016  loss_rpn_loc: 0.035  time: 0.5687  data_time: 0.0085  lr: 0.000899  max_mem: 3067M
[32m[04/15 06:31:02 d2.utils.events]: [0m eta: 0:16:49  iter: 199  total_loss: 0.652  loss_cls: 0.209  loss_box_reg: 0.345  loss_rpn_cls: 0.018  loss_rpn_loc: 0.040  time: 0.5719  data_time: 0.0101  lr: 0.000999  max_mem: 3067M
[32m[04/15 06:31:13 d2.utils.events]: [0m eta: 0:16:35  iter: 219  total_loss: 0.638  loss_cls: 0.192  loss_box_reg: 0.365  loss_rpn_cls: 0.019  loss_rpn_loc: 0.069  time: 0.5698  data_time: 0.0133  lr: 0.001099  max_mem: 3067M
[32m[04/15 06:31:25 d2.utils.events]: [0m eta: 0:16:26  iter: 239  total_loss: 0.579  loss_cls: 0.194  loss_box_reg: 0.325  loss_rpn_cls: 0.019  loss_rpn_loc: 0.043  time: 0.5716  data_time: 0.0108  lr: 0.001199  max_mem: 3067M
[32m[04/15 06:31:36 d2.utils.events]: [0m eta: 0:16:14  iter: 259  total_loss: 0.554  loss_cls: 0.203  loss_box_reg: 0.299  loss_rpn_cls: 0.016  loss_rpn_loc: 0.044  time: 0.5707  data_time: 0.0080  lr: 0.001299  max_mem: 3067M
[32m[04/15 06:31:47 d2.utils.events]: [0m eta: 0:16:01  iter: 279  total_loss: 0.668  loss_cls: 0.215  loss_box_reg: 0.365  loss_rpn_cls: 0.024  loss_rpn_loc: 0.063  time: 0.5687  data_time: 0.0073  lr: 0.001399  max_mem: 3067M
[32m[04/15 06:31:59 d2.utils.events]: [0m eta: 0:15:51  iter: 299  total_loss: 0.719  loss_cls: 0.235  loss_box_reg: 0.370  loss_rpn_cls: 0.020  loss_rpn_loc: 0.063  time: 0.5702  data_time: 0.0086  lr: 0.001499  max_mem: 3067M
[32m[04/15 06:32:10 d2.utils.events]: [0m eta: 0:15:34  iter: 319  total_loss: 0.720  loss_cls: 0.226  loss_box_reg: 0.384  loss_rpn_cls: 0.022  loss_rpn_loc: 0.078  time: 0.5682  data_time: 0.0083  lr: 0.001598  max_mem: 3067M
[32m[04/15 06:32:21 d2.utils.events]: [0m eta: 0:15:22  iter: 339  total_loss: 0.723  loss_cls: 0.232  loss_box_reg: 0.388  loss_rpn_cls: 0.028  loss_rpn_loc: 0.058  time: 0.5676  data_time: 0.0069  lr: 0.001698  max_mem: 3067M
[32m[04/15 06:32:33 d2.utils.events]: [0m eta: 0:15:11  iter: 359  total_loss: 0.576  loss_cls: 0.197  loss_box_reg: 0.330  loss_rpn_cls: 0.017  loss_rpn_loc: 0.047  time: 0.5674  data_time: 0.0064  lr: 0.001798  max_mem: 3067M
[32m[04/15 06:32:44 d2.utils.events]: [0m eta: 0:14:57  iter: 379  total_loss: 0.705  loss_cls: 0.231  loss_box_reg: 0.347  loss_rpn_cls: 0.024  loss_rpn_loc: 0.058  time: 0.5664  data_time: 0.0066  lr: 0.001898  max_mem: 3067M
[32m[04/15 06:32:55 d2.utils.events]: [0m eta: 0:14:43  iter: 399  total_loss: 0.608  loss_cls: 0.199  loss_box_reg: 0.320  loss_rpn_cls: 0.025  loss_rpn_loc: 0.059  time: 0.5659  data_time: 0.0068  lr: 0.001998  max_mem: 3067M
[32m[04/15 06:33:06 d2.utils.events]: [0m eta: 0:14:30  iter: 419  total_loss: 0.692  loss_cls: 0.217  loss_box_reg: 0.388  loss_rpn_cls: 0.018  loss_rpn_loc: 0.061  time: 0.5653  data_time: 0.0067  lr: 0.002098  max_mem: 3067M
[32m[04/15 06:33:17 d2.utils.events]: [0m eta: 0:14:18  iter: 439  total_loss: 0.717  loss_cls: 0.228  loss_box_reg: 0.403  loss_rpn_cls: 0.020  loss_rpn_loc: 0.059  time: 0.5651  data_time: 0.0068  lr: 0.002198  max_mem: 3067M
[32m[04/15 06:33:28 d2.utils.events]: [0m eta: 0:14:07  iter: 459  total_loss: 0.586  loss_cls: 0.195  loss_box_reg: 0.321  loss_rpn_cls: 0.018  loss_rpn_loc: 0.046  time: 0.5646  data_time: 0.0064  lr: 0.002298  max_mem: 3067M
[32m[04/15 06:33:41 d2.utils.events]: [0m eta: 0:13:58  iter: 479  total_loss: 0.720  loss_cls: 0.228  loss_box_reg: 0.395  loss_rpn_cls: 0.018  loss_rpn_loc: 0.060  time: 0.5663  data_time: 0.0049  lr: 0.002398  max_mem: 3067M
[32m[04/15 06:33:52 d2.utils.events]: [0m eta: 0:13:47  iter: 499  total_loss: 0.602  loss_cls: 0.189  loss_box_reg: 0.318  loss_rpn_cls: 0.021  loss_rpn_loc: 0.051  time: 0.5656  data_time: 0.0070  lr: 0.002498  max_mem: 3067M
[32m[04/15 06:34:02 d2.utils.events]: [0m eta: 0:13:35  iter: 519  total_loss: 0.748  loss_cls: 0.234  loss_box_reg: 0.398  loss_rpn_cls: 0.015  loss_rpn_loc: 0.056  time: 0.5642  data_time: 0.0064  lr: 0.002597  max_mem: 3067M
[32m[04/15 06:34:13 d2.utils.events]: [0m eta: 0:13:24  iter: 539  total_loss: 0.667  loss_cls: 0.205  loss_box_reg: 0.346  loss_rpn_cls: 0.020  loss_rpn_loc: 0.053  time: 0.5635  data_time: 0.0069  lr: 0.002697  max_mem: 3067M
[32m[04/15 06:34:25 d2.utils.events]: [0m eta: 0:13:15  iter: 559  total_loss: 0.630  loss_cls: 0.224  loss_box_reg: 0.351  loss_rpn_cls: 0.020  loss_rpn_loc: 0.046  time: 0.5641  data_time: 0.0068  lr: 0.002797  max_mem: 3067M
[32m[04/15 06:34:38 d2.utils.events]: [0m eta: 0:13:07  iter: 579  total_loss: 0.683  loss_cls: 0.226  loss_box_reg: 0.374  loss_rpn_cls: 0.017  loss_rpn_loc: 0.048  time: 0.5663  data_time: 0.0073  lr: 0.002897  max_mem: 3067M
[32m[04/15 06:34:48 d2.utils.events]: [0m eta: 0:12:55  iter: 599  total_loss: 0.590  loss_cls: 0.210  loss_box_reg: 0.330  loss_rpn_cls: 0.014  loss_rpn_loc: 0.055  time: 0.5650  data_time: 0.0076  lr: 0.002997  max_mem: 3067M
[32m[04/15 06:35:00 d2.utils.events]: [0m eta: 0:12:44  iter: 619  total_loss: 0.749  loss_cls: 0.239  loss_box_reg: 0.401  loss_rpn_cls: 0.023  loss_rpn_loc: 0.066  time: 0.5652  data_time: 0.0071  lr: 0.003097  max_mem: 3067M
[32m[04/15 06:35:12 d2.utils.events]: [0m eta: 0:12:34  iter: 639  total_loss: 0.665  loss_cls: 0.203  loss_box_reg: 0.332  loss_rpn_cls: 0.027  loss_rpn_loc: 0.059  time: 0.5670  data_time: 0.0073  lr: 0.003197  max_mem: 3067M
[32m[04/15 06:35:23 d2.utils.events]: [0m eta: 0:12:23  iter: 659  total_loss: 0.659  loss_cls: 0.194  loss_box_reg: 0.370  loss_rpn_cls: 0.017  loss_rpn_loc: 0.049  time: 0.5663  data_time: 0.0084  lr: 0.003297  max_mem: 3067M
[32m[04/15 06:35:34 d2.utils.events]: [0m eta: 0:12:11  iter: 679  total_loss: 0.698  loss_cls: 0.216  loss_box_reg: 0.378  loss_rpn_cls: 0.026  loss_rpn_loc: 0.062  time: 0.5659  data_time: 0.0085  lr: 0.003397  max_mem: 3067M
[32m[04/15 06:35:47 d2.utils.events]: [0m eta: 0:12:01  iter: 699  total_loss: 0.682  loss_cls: 0.224  loss_box_reg: 0.399  loss_rpn_cls: 0.014  loss_rpn_loc: 0.049  time: 0.5680  data_time: 0.0099  lr: 0.003497  max_mem: 3067M
[32m[04/15 06:35:58 d2.utils.events]: [0m eta: 0:11:50  iter: 719  total_loss: 0.653  loss_cls: 0.230  loss_box_reg: 0.367  loss_rpn_cls: 0.021  loss_rpn_loc: 0.065  time: 0.5672  data_time: 0.0066  lr: 0.003596  max_mem: 3067M
[32m[04/15 06:36:09 d2.utils.events]: [0m eta: 0:11:38  iter: 739  total_loss: 0.710  loss_cls: 0.237  loss_box_reg: 0.372  loss_rpn_cls: 0.019  loss_rpn_loc: 0.072  time: 0.5667  data_time: 0.0082  lr: 0.003696  max_mem: 3067M
[32m[04/15 06:36:23 d2.utils.events]: [0m eta: 0:11:28  iter: 759  total_loss: 0.516  loss_cls: 0.181  loss_box_reg: 0.301  loss_rpn_cls: 0.018  loss_rpn_loc: 0.038  time: 0.5695  data_time: 0.1361  lr: 0.003796  max_mem: 3067M
[32m[04/15 06:36:35 d2.utils.events]: [0m eta: 0:11:18  iter: 779  total_loss: 0.611  loss_cls: 0.188  loss_box_reg: 0.313  loss_rpn_cls: 0.019  loss_rpn_loc: 0.044  time: 0.5696  data_time: 0.0070  lr: 0.003896  max_mem: 3067M
[32m[04/15 06:36:46 d2.utils.events]: [0m eta: 0:11:07  iter: 799  total_loss: 0.645  loss_cls: 0.209  loss_box_reg: 0.330  loss_rpn_cls: 0.024  loss_rpn_loc: 0.070  time: 0.5699  data_time: 0.0085  lr: 0.003996  max_mem: 3067M
[32m[04/15 06:36:58 d2.utils.events]: [0m eta: 0:10:56  iter: 819  total_loss: 0.682  loss_cls: 0.211  loss_box_reg: 0.360  loss_rpn_cls: 0.025  loss_rpn_loc: 0.065  time: 0.5702  data_time: 0.0081  lr: 0.004096  max_mem: 3067M
[32m[04/15 06:37:10 d2.utils.events]: [0m eta: 0:10:45  iter: 839  total_loss: 0.685  loss_cls: 0.223  loss_box_reg: 0.373  loss_rpn_cls: 0.024  loss_rpn_loc: 0.060  time: 0.5706  data_time: 0.0074  lr: 0.004196  max_mem: 3067M
[32m[04/15 06:37:21 d2.utils.events]: [0m eta: 0:10:34  iter: 859  total_loss: 0.672  loss_cls: 0.214  loss_box_reg: 0.361  loss_rpn_cls: 0.022  loss_rpn_loc: 0.060  time: 0.5703  data_time: 0.0072  lr: 0.004296  max_mem: 3067M
[32m[04/15 06:37:32 d2.utils.events]: [0m eta: 0:10:22  iter: 879  total_loss: 0.595  loss_cls: 0.188  loss_box_reg: 0.373  loss_rpn_cls: 0.015  loss_rpn_loc: 0.052  time: 0.5694  data_time: 0.0071  lr: 0.004396  max_mem: 3067M
[32m[04/15 06:37:44 d2.utils.events]: [0m eta: 0:10:11  iter: 899  total_loss: 0.672  loss_cls: 0.216  loss_box_reg: 0.356  loss_rpn_cls: 0.025  loss_rpn_loc: 0.047  time: 0.5699  data_time: 0.0078  lr: 0.004496  max_mem: 3067M
[32m[04/15 06:37:56 d2.utils.events]: [0m eta: 0:10:00  iter: 919  total_loss: 0.652  loss_cls: 0.212  loss_box_reg: 0.339  loss_rpn_cls: 0.026  loss_rpn_loc: 0.063  time: 0.5701  data_time: 0.0072  lr: 0.004595  max_mem: 3067M
[32m[04/15 06:38:06 d2.utils.events]: [0m eta: 0:09:48  iter: 939  total_loss: 0.626  loss_cls: 0.210  loss_box_reg: 0.352  loss_rpn_cls: 0.023  loss_rpn_loc: 0.051  time: 0.5695  data_time: 0.0069  lr: 0.004695  max_mem: 3067M
[32m[04/15 06:38:17 d2.utils.events]: [0m eta: 0:09:37  iter: 959  total_loss: 0.670  loss_cls: 0.210  loss_box_reg: 0.378  loss_rpn_cls: 0.018  loss_rpn_loc: 0.060  time: 0.5685  data_time: 0.0090  lr: 0.004795  max_mem: 3067M
[32m[04/15 06:38:30 d2.utils.events]: [0m eta: 0:09:26  iter: 979  total_loss: 0.526  loss_cls: 0.177  loss_box_reg: 0.295  loss_rpn_cls: 0.016  loss_rpn_loc: 0.041  time: 0.5699  data_time: 0.0072  lr: 0.004895  max_mem: 3067M
[32m[04/15 06:38:40 d2.utils.events]: [0m eta: 0:09:15  iter: 999  total_loss: 0.689  loss_cls: 0.229  loss_box_reg: 0.374  loss_rpn_cls: 0.027  loss_rpn_loc: 0.072  time: 0.5690  data_time: 0.0082  lr: 0.004995  max_mem: 3067M
[32m[04/15 06:38:52 d2.utils.events]: [0m eta: 0:09:03  iter: 1019  total_loss: 0.657  loss_cls: 0.230  loss_box_reg: 0.332  loss_rpn_cls: 0.025  loss_rpn_loc: 0.052  time: 0.5687  data_time: 0.0098  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:39:04 d2.utils.events]: [0m eta: 0:08:53  iter: 1039  total_loss: 0.557  loss_cls: 0.191  loss_box_reg: 0.280  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5698  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:39:15 d2.utils.events]: [0m eta: 0:08:42  iter: 1059  total_loss: 0.623  loss_cls: 0.195  loss_box_reg: 0.311  loss_rpn_cls: 0.019  loss_rpn_loc: 0.056  time: 0.5695  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:39:26 d2.utils.events]: [0m eta: 0:08:31  iter: 1079  total_loss: 0.794  loss_cls: 0.276  loss_box_reg: 0.401  loss_rpn_cls: 0.024  loss_rpn_loc: 0.083  time: 0.5688  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:39:38 d2.utils.events]: [0m eta: 0:08:20  iter: 1099  total_loss: 0.727  loss_cls: 0.222  loss_box_reg: 0.413  loss_rpn_cls: 0.023  loss_rpn_loc: 0.069  time: 0.5695  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:39:49 d2.utils.events]: [0m eta: 0:08:08  iter: 1119  total_loss: 0.732  loss_cls: 0.217  loss_box_reg: 0.398  loss_rpn_cls: 0.021  loss_rpn_loc: 0.060  time: 0.5691  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:40:00 d2.utils.events]: [0m eta: 0:07:56  iter: 1139  total_loss: 0.531  loss_cls: 0.175  loss_box_reg: 0.307  loss_rpn_cls: 0.021  loss_rpn_loc: 0.056  time: 0.5683  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:40:12 d2.utils.events]: [0m eta: 0:07:45  iter: 1159  total_loss: 0.531  loss_cls: 0.171  loss_box_reg: 0.279  loss_rpn_cls: 0.037  loss_rpn_loc: 0.060  time: 0.5686  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:40:24 d2.utils.events]: [0m eta: 0:07:34  iter: 1179  total_loss: 0.727  loss_cls: 0.222  loss_box_reg: 0.407  loss_rpn_cls: 0.020  loss_rpn_loc: 0.055  time: 0.5690  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:40:34 d2.utils.events]: [0m eta: 0:07:22  iter: 1199  total_loss: 0.599  loss_cls: 0.211  loss_box_reg: 0.278  loss_rpn_cls: 0.017  loss_rpn_loc: 0.049  time: 0.5683  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:40:46 d2.utils.events]: [0m eta: 0:07:12  iter: 1219  total_loss: 0.586  loss_cls: 0.198  loss_box_reg: 0.342  loss_rpn_cls: 0.026  loss_rpn_loc: 0.050  time: 0.5682  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:40:58 d2.utils.events]: [0m eta: 0:07:00  iter: 1239  total_loss: 0.738  loss_cls: 0.241  loss_box_reg: 0.406  loss_rpn_cls: 0.021  loss_rpn_loc: 0.051  time: 0.5686  data_time: 0.0096  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:41:09 d2.utils.events]: [0m eta: 0:06:50  iter: 1259  total_loss: 0.713  loss_cls: 0.240  loss_box_reg: 0.381  loss_rpn_cls: 0.025  loss_rpn_loc: 0.050  time: 0.5685  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:41:20 d2.utils.events]: [0m eta: 0:06:39  iter: 1279  total_loss: 0.608  loss_cls: 0.217  loss_box_reg: 0.314  loss_rpn_cls: 0.021  loss_rpn_loc: 0.058  time: 0.5682  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:41:32 d2.utils.events]: [0m eta: 0:06:28  iter: 1299  total_loss: 0.750  loss_cls: 0.254  loss_box_reg: 0.380  loss_rpn_cls: 0.021  loss_rpn_loc: 0.062  time: 0.5687  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:41:42 d2.utils.events]: [0m eta: 0:06:17  iter: 1319  total_loss: 0.736  loss_cls: 0.249  loss_box_reg: 0.406  loss_rpn_cls: 0.020  loss_rpn_loc: 0.074  time: 0.5678  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:41:53 d2.utils.events]: [0m eta: 0:06:05  iter: 1339  total_loss: 0.777  loss_cls: 0.259  loss_box_reg: 0.417  loss_rpn_cls: 0.023  loss_rpn_loc: 0.069  time: 0.5674  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:42:06 d2.utils.events]: [0m eta: 0:05:55  iter: 1359  total_loss: 0.709  loss_cls: 0.220  loss_box_reg: 0.389  loss_rpn_cls: 0.019  loss_rpn_loc: 0.069  time: 0.5683  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:42:17 d2.utils.events]: [0m eta: 0:05:44  iter: 1379  total_loss: 0.821  loss_cls: 0.267  loss_box_reg: 0.407  loss_rpn_cls: 0.023  loss_rpn_loc: 0.060  time: 0.5682  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:42:28 d2.utils.events]: [0m eta: 0:05:32  iter: 1399  total_loss: 0.634  loss_cls: 0.216  loss_box_reg: 0.349  loss_rpn_cls: 0.027  loss_rpn_loc: 0.056  time: 0.5674  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:42:40 d2.utils.events]: [0m eta: 0:05:21  iter: 1419  total_loss: 0.660  loss_cls: 0.213  loss_box_reg: 0.317  loss_rpn_cls: 0.025  loss_rpn_loc: 0.050  time: 0.5677  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:42:51 d2.utils.events]: [0m eta: 0:05:11  iter: 1439  total_loss: 0.615  loss_cls: 0.189  loss_box_reg: 0.321  loss_rpn_cls: 0.019  loss_rpn_loc: 0.055  time: 0.5679  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:43:02 d2.utils.events]: [0m eta: 0:04:59  iter: 1459  total_loss: 0.746  loss_cls: 0.246  loss_box_reg: 0.376  loss_rpn_cls: 0.032  loss_rpn_loc: 0.063  time: 0.5676  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:43:14 d2.utils.events]: [0m eta: 0:04:48  iter: 1479  total_loss: 0.715  loss_cls: 0.215  loss_box_reg: 0.379  loss_rpn_cls: 0.026  loss_rpn_loc: 0.067  time: 0.5676  data_time: 0.0136  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:43:26 d2.utils.events]: [0m eta: 0:04:37  iter: 1499  total_loss: 0.667  loss_cls: 0.213  loss_box_reg: 0.351  loss_rpn_cls: 0.023  loss_rpn_loc: 0.072  time: 0.5679  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:43:36 d2.utils.events]: [0m eta: 0:04:26  iter: 1519  total_loss: 0.666  loss_cls: 0.211  loss_box_reg: 0.361  loss_rpn_cls: 0.020  loss_rpn_loc: 0.069  time: 0.5674  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:43:48 d2.utils.events]: [0m eta: 0:04:15  iter: 1539  total_loss: 0.638  loss_cls: 0.206  loss_box_reg: 0.338  loss_rpn_cls: 0.029  loss_rpn_loc: 0.058  time: 0.5676  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:44:00 d2.utils.events]: [0m eta: 0:04:04  iter: 1559  total_loss: 0.657  loss_cls: 0.203  loss_box_reg: 0.356  loss_rpn_cls: 0.025  loss_rpn_loc: 0.062  time: 0.5679  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:44:11 d2.utils.events]: [0m eta: 0:03:52  iter: 1579  total_loss: 0.694  loss_cls: 0.244  loss_box_reg: 0.311  loss_rpn_cls: 0.019  loss_rpn_loc: 0.054  time: 0.5678  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:44:22 d2.utils.events]: [0m eta: 0:03:42  iter: 1599  total_loss: 0.725  loss_cls: 0.228  loss_box_reg: 0.396  loss_rpn_cls: 0.021  loss_rpn_loc: 0.084  time: 0.5678  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:44:35 d2.utils.events]: [0m eta: 0:03:31  iter: 1619  total_loss: 0.602  loss_cls: 0.205  loss_box_reg: 0.314  loss_rpn_cls: 0.015  loss_rpn_loc: 0.054  time: 0.5682  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:44:46 d2.utils.events]: [0m eta: 0:03:19  iter: 1639  total_loss: 0.646  loss_cls: 0.208  loss_box_reg: 0.356  loss_rpn_cls: 0.021  loss_rpn_loc: 0.053  time: 0.5682  data_time: 0.0096  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:44:57 d2.utils.events]: [0m eta: 0:03:08  iter: 1659  total_loss: 0.677  loss_cls: 0.194  loss_box_reg: 0.381  loss_rpn_cls: 0.023  loss_rpn_loc: 0.064  time: 0.5680  data_time: 0.0108  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:45:09 d2.utils.events]: [0m eta: 0:02:57  iter: 1679  total_loss: 0.756  loss_cls: 0.264  loss_box_reg: 0.382  loss_rpn_cls: 0.031  loss_rpn_loc: 0.052  time: 0.5683  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:45:21 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.690  loss_cls: 0.233  loss_box_reg: 0.367  loss_rpn_cls: 0.023  loss_rpn_loc: 0.051  time: 0.5684  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:45:31 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.725  loss_cls: 0.240  loss_box_reg: 0.399  loss_rpn_cls: 0.029  loss_rpn_loc: 0.057  time: 0.5677  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:45:43 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.703  loss_cls: 0.213  loss_box_reg: 0.370  loss_rpn_cls: 0.028  loss_rpn_loc: 0.068  time: 0.5677  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:45:54 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.656  loss_cls: 0.225  loss_box_reg: 0.361  loss_rpn_cls: 0.022  loss_rpn_loc: 0.058  time: 0.5675  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:46:05 d2.utils.events]: [0m eta: 0:02:01  iter: 1779  total_loss: 0.708  loss_cls: 0.232  loss_box_reg: 0.336  loss_rpn_cls: 0.030  loss_rpn_loc: 0.061  time: 0.5673  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:46:17 d2.utils.events]: [0m eta: 0:01:50  iter: 1799  total_loss: 0.688  loss_cls: 0.218  loss_box_reg: 0.350  loss_rpn_cls: 0.029  loss_rpn_loc: 0.056  time: 0.5677  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:46:28 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.671  loss_cls: 0.220  loss_box_reg: 0.381  loss_rpn_cls: 0.019  loss_rpn_loc: 0.051  time: 0.5677  data_time: 0.0132  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:46:39 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.677  loss_cls: 0.199  loss_box_reg: 0.389  loss_rpn_cls: 0.022  loss_rpn_loc: 0.061  time: 0.5674  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:46:51 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.630  loss_cls: 0.192  loss_box_reg: 0.335  loss_rpn_cls: 0.029  loss_rpn_loc: 0.059  time: 0.5675  data_time: 0.0102  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:47:02 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.653  loss_cls: 0.219  loss_box_reg: 0.340  loss_rpn_cls: 0.024  loss_rpn_loc: 0.084  time: 0.5677  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:47:14 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.529  loss_cls: 0.189  loss_box_reg: 0.288  loss_rpn_cls: 0.018  loss_rpn_loc: 0.059  time: 0.5677  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:47:25 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.622  loss_cls: 0.196  loss_box_reg: 0.328  loss_rpn_cls: 0.015  loss_rpn_loc: 0.051  time: 0.5677  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:47:37 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.596  loss_cls: 0.213  loss_box_reg: 0.314  loss_rpn_cls: 0.020  loss_rpn_loc: 0.057  time: 0.5680  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:47:49 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.687  loss_cls: 0.208  loss_box_reg: 0.354  loss_rpn_cls: 0.017  loss_rpn_loc: 0.051  time: 0.5680  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:48:00 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.683  loss_cls: 0.220  loss_box_reg: 0.382  loss_rpn_cls: 0.030  loss_rpn_loc: 0.052  time: 0.5677  data_time: 0.0095  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 06:48:23 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:48:23 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 06:48:23 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 06:48:23 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.662  loss_cls: 0.228  loss_box_reg: 0.382  loss_rpn_cls: 0.020  loss_rpn_loc: 0.073  time: 0.5681  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 06:48:25 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:55 (0.5684 s / it)
[32m[04/15 06:48:25 d2.engine.hooks]: [0mTotal training time: 0:19:17 (0:00:22 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 06:48:31 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:48:31 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 06:48:32 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 06:48:34 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1200 s / img. ETA=0:03:17
[32m[04/15 06:48:39 d2.evaluation.evaluator]: [0mInference done 48/1257. 0.1302 s / img. ETA=0:02:50
[32m[04/15 06:48:44 d2.evaluation.evaluator]: [0mInference done 84/1257. 0.1307 s / img. ETA=0:02:44
[32m[04/15 06:48:49 d2.evaluation.evaluator]: [0mInference done 118/1257. 0.1347 s / img. ETA=0:02:42
[32m[04/15 06:48:54 d2.evaluation.evaluator]: [0mInference done 147/1257. 0.1321 s / img. ETA=0:02:46
[32m[04/15 06:48:59 d2.evaluation.evaluator]: [0mInference done 183/1257. 0.1298 s / img. ETA=0:02:38
[32m[04/15 06:49:04 d2.evaluation.evaluator]: [0mInference done 217/1257. 0.1293 s / img. ETA=0:02:34
[32m[04/15 06:49:09 d2.evaluation.evaluator]: [0mInference done 247/1257. 0.1291 s / img. ETA=0:02:32
[32m[04/15 06:49:15 d2.evaluation.evaluator]: [0mInference done 287/1257. 0.1282 s / img. ETA=0:02:23
[32m[04/15 06:49:20 d2.evaluation.evaluator]: [0mInference done 324/1257. 0.1285 s / img. ETA=0:02:16
[32m[04/15 06:49:25 d2.evaluation.evaluator]: [0mInference done 359/1257. 0.1297 s / img. ETA=0:02:11
[32m[04/15 06:49:30 d2.evaluation.evaluator]: [0mInference done 396/1257. 0.1299 s / img. ETA=0:02:05
[32m[04/15 06:49:35 d2.evaluation.evaluator]: [0mInference done 430/1257. 0.1293 s / img. ETA=0:02:00
[32m[04/15 06:49:40 d2.evaluation.evaluator]: [0mInference done 459/1257. 0.1287 s / img. ETA=0:01:57
[32m[04/15 06:49:45 d2.evaluation.evaluator]: [0mInference done 490/1257. 0.1288 s / img. ETA=0:01:53
[32m[04/15 06:49:50 d2.evaluation.evaluator]: [0mInference done 524/1257. 0.1285 s / img. ETA=0:01:48
[32m[04/15 06:49:55 d2.evaluation.evaluator]: [0mInference done 559/1257. 0.1281 s / img. ETA=0:01:43
[32m[04/15 06:50:00 d2.evaluation.evaluator]: [0mInference done 596/1257. 0.1284 s / img. ETA=0:01:37
[32m[04/15 06:50:06 d2.evaluation.evaluator]: [0mInference done 629/1257. 0.1290 s / img. ETA=0:01:33
[32m[04/15 06:50:11 d2.evaluation.evaluator]: [0mInference done 663/1257. 0.1291 s / img. ETA=0:01:28
[32m[04/15 06:50:16 d2.evaluation.evaluator]: [0mInference done 700/1257. 0.1287 s / img. ETA=0:01:22
[32m[04/15 06:50:21 d2.evaluation.evaluator]: [0mInference done 737/1257. 0.1284 s / img. ETA=0:01:16
[32m[04/15 06:50:26 d2.evaluation.evaluator]: [0mInference done 771/1257. 0.1283 s / img. ETA=0:01:11
[32m[04/15 06:50:31 d2.evaluation.evaluator]: [0mInference done 809/1257. 0.1283 s / img. ETA=0:01:05
[32m[04/15 06:50:36 d2.evaluation.evaluator]: [0mInference done 848/1257. 0.1281 s / img. ETA=0:00:59
[32m[04/15 06:50:41 d2.evaluation.evaluator]: [0mInference done 885/1257. 0.1283 s / img. ETA=0:00:54
[32m[04/15 06:50:46 d2.evaluation.evaluator]: [0mInference done 918/1257. 0.1290 s / img. ETA=0:00:49
[32m[04/15 06:50:51 d2.evaluation.evaluator]: [0mInference done 952/1257. 0.1290 s / img. ETA=0:00:44
[32m[04/15 06:50:56 d2.evaluation.evaluator]: [0mInference done 991/1257. 0.1288 s / img. ETA=0:00:38
[32m[04/15 06:51:01 d2.evaluation.evaluator]: [0mInference done 1030/1257. 0.1286 s / img. ETA=0:00:32
[32m[04/15 06:51:07 d2.evaluation.evaluator]: [0mInference done 1069/1257. 0.1287 s / img. ETA=0:00:27
[32m[04/15 06:51:12 d2.evaluation.evaluator]: [0mInference done 1106/1257. 0.1284 s / img. ETA=0:00:21
[32m[04/15 06:51:17 d2.evaluation.evaluator]: [0mInference done 1139/1257. 0.1282 s / img. ETA=0:00:17
[32m[04/15 06:51:22 d2.evaluation.evaluator]: [0mInference done 1172/1257. 0.1286 s / img. ETA=0:00:12
[32m[04/15 06:51:27 d2.evaluation.evaluator]: [0mInference done 1210/1257. 0.1286 s / img. ETA=0:00:06
[32m[04/15 06:51:32 d2.evaluation.evaluator]: [0mInference done 1248/1257. 0.1283 s / img. ETA=0:00:01
[32m[04/15 06:51:34 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:00.673477 (0.144308 s / img per device, on 1 devices)
[32m[04/15 06:51:34 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:40 (0.128278 s / img per device, on 1 devices)
[32m[04/15 06:51:35 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 06:51:35 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 06:51:36 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.03s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=4.33s).
Accumulating evaluation results...
DONE (t=0.74s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.184
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.372
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.156
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.109
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.229
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.110
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.154
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.288
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.460
[32m[04/15 06:51:42 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 18.368 | 37.156 | 15.594 | 10.902 | 22.906 | 40.840 |
[32m[04/15 06:51:42 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 24.045 | bicycle       | 9.301 | car            | 39.508 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.616 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  9  *  2000  iterations ============
4 channel input
[32m[04/15 06:51:44 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 06:51:45 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.42 seconds.
[5m[31mWARNING[0m [32m[04/15 06:51:45 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 06:51:45 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 06:51:46 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 06:51:46 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 06:51:46 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 06:51:47 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 06:51:59 d2.utils.events]: [0m eta: 0:18:29  iter: 19  total_loss: 0.646  loss_cls: 0.199  loss_box_reg: 0.347  loss_rpn_cls: 0.017  loss_rpn_loc: 0.046  time: 0.5758  data_time: 0.0502  lr: 0.000100  max_mem: 3067M
[32m[04/15 06:52:13 d2.utils.events]: [0m eta: 0:18:56  iter: 39  total_loss: 0.593  loss_cls: 0.193  loss_box_reg: 0.336  loss_rpn_cls: 0.015  loss_rpn_loc: 0.042  time: 0.6329  data_time: 0.2006  lr: 0.000200  max_mem: 3067M
[32m[04/15 06:52:24 d2.utils.events]: [0m eta: 0:18:27  iter: 59  total_loss: 0.633  loss_cls: 0.209  loss_box_reg: 0.349  loss_rpn_cls: 0.017  loss_rpn_loc: 0.056  time: 0.6092  data_time: 0.0072  lr: 0.000300  max_mem: 3067M
[32m[04/15 06:52:36 d2.utils.events]: [0m eta: 0:18:29  iter: 79  total_loss: 0.622  loss_cls: 0.233  loss_box_reg: 0.365  loss_rpn_cls: 0.018  loss_rpn_loc: 0.051  time: 0.6085  data_time: 0.0075  lr: 0.000400  max_mem: 3067M
[32m[04/15 06:52:48 d2.utils.events]: [0m eta: 0:18:05  iter: 99  total_loss: 0.756  loss_cls: 0.240  loss_box_reg: 0.425  loss_rpn_cls: 0.020  loss_rpn_loc: 0.077  time: 0.6018  data_time: 0.0082  lr: 0.000500  max_mem: 3067M
[32m[04/15 06:53:00 d2.utils.events]: [0m eta: 0:17:54  iter: 119  total_loss: 0.614  loss_cls: 0.197  loss_box_reg: 0.334  loss_rpn_cls: 0.029  loss_rpn_loc: 0.057  time: 0.6024  data_time: 0.0083  lr: 0.000599  max_mem: 3067M
[32m[04/15 06:53:11 d2.utils.events]: [0m eta: 0:17:36  iter: 139  total_loss: 0.624  loss_cls: 0.203  loss_box_reg: 0.360  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5944  data_time: 0.0075  lr: 0.000699  max_mem: 3067M
[32m[04/15 06:53:22 d2.utils.events]: [0m eta: 0:17:20  iter: 159  total_loss: 0.719  loss_cls: 0.221  loss_box_reg: 0.396  loss_rpn_cls: 0.019  loss_rpn_loc: 0.077  time: 0.5883  data_time: 0.0078  lr: 0.000799  max_mem: 3067M
[32m[04/15 06:53:35 d2.utils.events]: [0m eta: 0:17:12  iter: 179  total_loss: 0.553  loss_cls: 0.179  loss_box_reg: 0.306  loss_rpn_cls: 0.013  loss_rpn_loc: 0.048  time: 0.5903  data_time: 0.0080  lr: 0.000899  max_mem: 3067M
[32m[04/15 06:53:46 d2.utils.events]: [0m eta: 0:16:58  iter: 199  total_loss: 0.642  loss_cls: 0.209  loss_box_reg: 0.351  loss_rpn_cls: 0.020  loss_rpn_loc: 0.050  time: 0.5865  data_time: 0.0079  lr: 0.000999  max_mem: 3067M
[32m[04/15 06:53:56 d2.utils.events]: [0m eta: 0:16:42  iter: 219  total_loss: 0.673  loss_cls: 0.202  loss_box_reg: 0.349  loss_rpn_cls: 0.022  loss_rpn_loc: 0.040  time: 0.5803  data_time: 0.0079  lr: 0.001099  max_mem: 3067M
[32m[04/15 06:54:08 d2.utils.events]: [0m eta: 0:16:32  iter: 239  total_loss: 0.738  loss_cls: 0.244  loss_box_reg: 0.411  loss_rpn_cls: 0.020  loss_rpn_loc: 0.058  time: 0.5811  data_time: 0.0083  lr: 0.001199  max_mem: 3067M
[32m[04/15 06:54:20 d2.utils.events]: [0m eta: 0:16:21  iter: 259  total_loss: 0.641  loss_cls: 0.200  loss_box_reg: 0.344  loss_rpn_cls: 0.022  loss_rpn_loc: 0.048  time: 0.5825  data_time: 0.0089  lr: 0.001299  max_mem: 3067M
[32m[04/15 06:54:31 d2.utils.events]: [0m eta: 0:16:05  iter: 279  total_loss: 0.633  loss_cls: 0.197  loss_box_reg: 0.353  loss_rpn_cls: 0.012  loss_rpn_loc: 0.048  time: 0.5790  data_time: 0.0068  lr: 0.001399  max_mem: 3067M
[32m[04/15 06:54:42 d2.utils.events]: [0m eta: 0:15:51  iter: 299  total_loss: 0.594  loss_cls: 0.193  loss_box_reg: 0.346  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5766  data_time: 0.0080  lr: 0.001499  max_mem: 3067M
[32m[04/15 06:54:55 d2.utils.events]: [0m eta: 0:15:45  iter: 319  total_loss: 0.601  loss_cls: 0.201  loss_box_reg: 0.323  loss_rpn_cls: 0.020  loss_rpn_loc: 0.054  time: 0.5795  data_time: 0.0081  lr: 0.001598  max_mem: 3067M
[32m[04/15 06:55:06 d2.utils.events]: [0m eta: 0:15:34  iter: 339  total_loss: 0.693  loss_cls: 0.209  loss_box_reg: 0.361  loss_rpn_cls: 0.019  loss_rpn_loc: 0.081  time: 0.5795  data_time: 0.0350  lr: 0.001698  max_mem: 3067M
[32m[04/15 06:55:17 d2.utils.events]: [0m eta: 0:15:21  iter: 359  total_loss: 0.653  loss_cls: 0.213  loss_box_reg: 0.326  loss_rpn_cls: 0.019  loss_rpn_loc: 0.058  time: 0.5777  data_time: 0.0204  lr: 0.001798  max_mem: 3067M
[32m[04/15 06:55:29 d2.utils.events]: [0m eta: 0:15:09  iter: 379  total_loss: 0.684  loss_cls: 0.233  loss_box_reg: 0.383  loss_rpn_cls: 0.020  loss_rpn_loc: 0.063  time: 0.5772  data_time: 0.0081  lr: 0.001898  max_mem: 3067M
[32m[04/15 06:55:40 d2.utils.events]: [0m eta: 0:14:56  iter: 399  total_loss: 0.713  loss_cls: 0.222  loss_box_reg: 0.394  loss_rpn_cls: 0.022  loss_rpn_loc: 0.065  time: 0.5769  data_time: 0.0071  lr: 0.001998  max_mem: 3067M
[32m[04/15 06:55:51 d2.utils.events]: [0m eta: 0:14:42  iter: 419  total_loss: 0.636  loss_cls: 0.199  loss_box_reg: 0.332  loss_rpn_cls: 0.016  loss_rpn_loc: 0.051  time: 0.5749  data_time: 0.0093  lr: 0.002098  max_mem: 3067M
[32m[04/15 06:56:02 d2.utils.events]: [0m eta: 0:14:30  iter: 439  total_loss: 0.651  loss_cls: 0.207  loss_box_reg: 0.350  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5743  data_time: 0.0073  lr: 0.002198  max_mem: 3067M
[32m[04/15 06:56:15 d2.utils.events]: [0m eta: 0:14:19  iter: 459  total_loss: 0.621  loss_cls: 0.208  loss_box_reg: 0.349  loss_rpn_cls: 0.010  loss_rpn_loc: 0.047  time: 0.5756  data_time: 0.0079  lr: 0.002298  max_mem: 3067M
[32m[04/15 06:56:26 d2.utils.events]: [0m eta: 0:14:08  iter: 479  total_loss: 0.544  loss_cls: 0.196  loss_box_reg: 0.300  loss_rpn_cls: 0.018  loss_rpn_loc: 0.034  time: 0.5741  data_time: 0.0065  lr: 0.002398  max_mem: 3067M
[32m[04/15 06:56:37 d2.utils.events]: [0m eta: 0:13:57  iter: 499  total_loss: 0.621  loss_cls: 0.205  loss_box_reg: 0.330  loss_rpn_cls: 0.014  loss_rpn_loc: 0.040  time: 0.5742  data_time: 0.0078  lr: 0.002498  max_mem: 3067M
[32m[04/15 06:56:49 d2.utils.events]: [0m eta: 0:13:46  iter: 519  total_loss: 0.536  loss_cls: 0.179  loss_box_reg: 0.304  loss_rpn_cls: 0.021  loss_rpn_loc: 0.041  time: 0.5747  data_time: 0.0069  lr: 0.002597  max_mem: 3067M
[32m[04/15 06:57:00 d2.utils.events]: [0m eta: 0:13:34  iter: 539  total_loss: 0.651  loss_cls: 0.214  loss_box_reg: 0.347  loss_rpn_cls: 0.015  loss_rpn_loc: 0.056  time: 0.5733  data_time: 0.0076  lr: 0.002697  max_mem: 3067M
[32m[04/15 06:57:10 d2.utils.events]: [0m eta: 0:13:22  iter: 559  total_loss: 0.526  loss_cls: 0.160  loss_box_reg: 0.281  loss_rpn_cls: 0.015  loss_rpn_loc: 0.049  time: 0.5715  data_time: 0.0069  lr: 0.002797  max_mem: 3067M
[32m[04/15 06:57:23 d2.utils.events]: [0m eta: 0:13:11  iter: 579  total_loss: 0.683  loss_cls: 0.217  loss_box_reg: 0.398  loss_rpn_cls: 0.019  loss_rpn_loc: 0.073  time: 0.5728  data_time: 0.0068  lr: 0.002897  max_mem: 3067M
[32m[04/15 06:57:34 d2.utils.events]: [0m eta: 0:13:00  iter: 599  total_loss: 0.652  loss_cls: 0.217  loss_box_reg: 0.333  loss_rpn_cls: 0.023  loss_rpn_loc: 0.060  time: 0.5726  data_time: 0.0098  lr: 0.002997  max_mem: 3067M
[32m[04/15 06:57:45 d2.utils.events]: [0m eta: 0:12:49  iter: 619  total_loss: 0.556  loss_cls: 0.173  loss_box_reg: 0.317  loss_rpn_cls: 0.021  loss_rpn_loc: 0.035  time: 0.5716  data_time: 0.0075  lr: 0.003097  max_mem: 3067M
[32m[04/15 06:57:57 d2.utils.events]: [0m eta: 0:12:38  iter: 639  total_loss: 0.630  loss_cls: 0.216  loss_box_reg: 0.330  loss_rpn_cls: 0.017  loss_rpn_loc: 0.053  time: 0.5723  data_time: 0.0069  lr: 0.003197  max_mem: 3067M
[32m[04/15 06:58:09 d2.utils.events]: [0m eta: 0:12:27  iter: 659  total_loss: 0.574  loss_cls: 0.202  loss_box_reg: 0.301  loss_rpn_cls: 0.013  loss_rpn_loc: 0.045  time: 0.5726  data_time: 0.0076  lr: 0.003297  max_mem: 3067M
[32m[04/15 06:58:20 d2.utils.events]: [0m eta: 0:12:15  iter: 679  total_loss: 0.454  loss_cls: 0.181  loss_box_reg: 0.251  loss_rpn_cls: 0.012  loss_rpn_loc: 0.033  time: 0.5715  data_time: 0.0071  lr: 0.003397  max_mem: 3067M
[32m[04/15 06:58:31 d2.utils.events]: [0m eta: 0:12:04  iter: 699  total_loss: 0.712  loss_cls: 0.228  loss_box_reg: 0.367  loss_rpn_cls: 0.018  loss_rpn_loc: 0.065  time: 0.5712  data_time: 0.0063  lr: 0.003497  max_mem: 3067M
[32m[04/15 06:58:43 d2.utils.events]: [0m eta: 0:11:54  iter: 719  total_loss: 0.648  loss_cls: 0.217  loss_box_reg: 0.373  loss_rpn_cls: 0.019  loss_rpn_loc: 0.063  time: 0.5722  data_time: 0.0090  lr: 0.003596  max_mem: 3067M
[32m[04/15 06:58:54 d2.utils.events]: [0m eta: 0:11:42  iter: 739  total_loss: 0.731  loss_cls: 0.228  loss_box_reg: 0.374  loss_rpn_cls: 0.028  loss_rpn_loc: 0.074  time: 0.5713  data_time: 0.0093  lr: 0.003696  max_mem: 3067M
[32m[04/15 06:59:05 d2.utils.events]: [0m eta: 0:11:30  iter: 759  total_loss: 0.615  loss_cls: 0.200  loss_box_reg: 0.335  loss_rpn_cls: 0.019  loss_rpn_loc: 0.054  time: 0.5703  data_time: 0.0083  lr: 0.003796  max_mem: 3067M
[32m[04/15 06:59:17 d2.utils.events]: [0m eta: 0:11:20  iter: 779  total_loss: 0.654  loss_cls: 0.216  loss_box_reg: 0.369  loss_rpn_cls: 0.017  loss_rpn_loc: 0.051  time: 0.5718  data_time: 0.0102  lr: 0.003896  max_mem: 3067M
[32m[04/15 06:59:28 d2.utils.events]: [0m eta: 0:11:08  iter: 799  total_loss: 0.708  loss_cls: 0.246  loss_box_reg: 0.391  loss_rpn_cls: 0.020  loss_rpn_loc: 0.058  time: 0.5709  data_time: 0.0083  lr: 0.003996  max_mem: 3067M
[32m[04/15 06:59:39 d2.utils.events]: [0m eta: 0:10:56  iter: 819  total_loss: 0.639  loss_cls: 0.218  loss_box_reg: 0.331  loss_rpn_cls: 0.018  loss_rpn_loc: 0.050  time: 0.5701  data_time: 0.0075  lr: 0.004096  max_mem: 3067M
[32m[04/15 06:59:52 d2.utils.events]: [0m eta: 0:10:46  iter: 839  total_loss: 0.628  loss_cls: 0.192  loss_box_reg: 0.344  loss_rpn_cls: 0.025  loss_rpn_loc: 0.065  time: 0.5717  data_time: 0.0116  lr: 0.004196  max_mem: 3067M
[32m[04/15 07:00:03 d2.utils.events]: [0m eta: 0:10:35  iter: 859  total_loss: 0.656  loss_cls: 0.213  loss_box_reg: 0.347  loss_rpn_cls: 0.022  loss_rpn_loc: 0.055  time: 0.5715  data_time: 0.0080  lr: 0.004296  max_mem: 3067M
[32m[04/15 07:00:14 d2.utils.events]: [0m eta: 0:10:23  iter: 879  total_loss: 0.688  loss_cls: 0.211  loss_box_reg: 0.356  loss_rpn_cls: 0.028  loss_rpn_loc: 0.062  time: 0.5707  data_time: 0.0078  lr: 0.004396  max_mem: 3067M
[32m[04/15 07:00:26 d2.utils.events]: [0m eta: 0:10:13  iter: 899  total_loss: 0.718  loss_cls: 0.225  loss_box_reg: 0.401  loss_rpn_cls: 0.024  loss_rpn_loc: 0.074  time: 0.5714  data_time: 0.0083  lr: 0.004496  max_mem: 3067M
[32m[04/15 07:00:38 d2.utils.events]: [0m eta: 0:10:01  iter: 919  total_loss: 0.688  loss_cls: 0.240  loss_box_reg: 0.378  loss_rpn_cls: 0.021  loss_rpn_loc: 0.057  time: 0.5714  data_time: 0.0076  lr: 0.004595  max_mem: 3067M
[32m[04/15 07:00:49 d2.utils.events]: [0m eta: 0:09:50  iter: 939  total_loss: 0.657  loss_cls: 0.219  loss_box_reg: 0.361  loss_rpn_cls: 0.021  loss_rpn_loc: 0.057  time: 0.5706  data_time: 0.0078  lr: 0.004695  max_mem: 3067M
[32m[04/15 07:01:00 d2.utils.events]: [0m eta: 0:09:39  iter: 959  total_loss: 0.527  loss_cls: 0.169  loss_box_reg: 0.271  loss_rpn_cls: 0.015  loss_rpn_loc: 0.054  time: 0.5705  data_time: 0.0069  lr: 0.004795  max_mem: 3067M
[32m[04/15 07:01:12 d2.utils.events]: [0m eta: 0:09:28  iter: 979  total_loss: 0.619  loss_cls: 0.191  loss_box_reg: 0.338  loss_rpn_cls: 0.015  loss_rpn_loc: 0.063  time: 0.5715  data_time: 0.0091  lr: 0.004895  max_mem: 3067M
[32m[04/15 07:01:23 d2.utils.events]: [0m eta: 0:09:17  iter: 999  total_loss: 0.653  loss_cls: 0.222  loss_box_reg: 0.351  loss_rpn_cls: 0.019  loss_rpn_loc: 0.059  time: 0.5706  data_time: 0.0097  lr: 0.004995  max_mem: 3067M
[32m[04/15 07:01:34 d2.utils.events]: [0m eta: 0:09:05  iter: 1019  total_loss: 0.629  loss_cls: 0.202  loss_box_reg: 0.332  loss_rpn_cls: 0.038  loss_rpn_loc: 0.053  time: 0.5698  data_time: 0.0117  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:01:46 d2.utils.events]: [0m eta: 0:08:54  iter: 1039  total_loss: 0.655  loss_cls: 0.226  loss_box_reg: 0.336  loss_rpn_cls: 0.027  loss_rpn_loc: 0.058  time: 0.5711  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:01:57 d2.utils.events]: [0m eta: 0:08:42  iter: 1059  total_loss: 0.692  loss_cls: 0.225  loss_box_reg: 0.363  loss_rpn_cls: 0.024  loss_rpn_loc: 0.047  time: 0.5704  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:02:08 d2.utils.events]: [0m eta: 0:08:30  iter: 1079  total_loss: 0.580  loss_cls: 0.179  loss_box_reg: 0.312  loss_rpn_cls: 0.021  loss_rpn_loc: 0.060  time: 0.5699  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:02:20 d2.utils.events]: [0m eta: 0:08:19  iter: 1099  total_loss: 0.691  loss_cls: 0.204  loss_box_reg: 0.376  loss_rpn_cls: 0.026  loss_rpn_loc: 0.062  time: 0.5698  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:02:31 d2.utils.events]: [0m eta: 0:08:08  iter: 1119  total_loss: 0.797  loss_cls: 0.238  loss_box_reg: 0.411  loss_rpn_cls: 0.028  loss_rpn_loc: 0.052  time: 0.5698  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:02:42 d2.utils.events]: [0m eta: 0:07:56  iter: 1139  total_loss: 0.665  loss_cls: 0.203  loss_box_reg: 0.353  loss_rpn_cls: 0.024  loss_rpn_loc: 0.070  time: 0.5693  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:02:53 d2.utils.events]: [0m eta: 0:07:44  iter: 1159  total_loss: 0.689  loss_cls: 0.240  loss_box_reg: 0.348  loss_rpn_cls: 0.022  loss_rpn_loc: 0.054  time: 0.5690  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:03:05 d2.utils.events]: [0m eta: 0:07:33  iter: 1179  total_loss: 0.654  loss_cls: 0.230  loss_box_reg: 0.346  loss_rpn_cls: 0.021  loss_rpn_loc: 0.070  time: 0.5695  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:03:16 d2.utils.events]: [0m eta: 0:07:22  iter: 1199  total_loss: 0.643  loss_cls: 0.208  loss_box_reg: 0.327  loss_rpn_cls: 0.019  loss_rpn_loc: 0.067  time: 0.5690  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:03:27 d2.utils.events]: [0m eta: 0:07:11  iter: 1219  total_loss: 0.656  loss_cls: 0.202  loss_box_reg: 0.370  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5683  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:03:39 d2.utils.events]: [0m eta: 0:07:00  iter: 1239  total_loss: 0.654  loss_cls: 0.230  loss_box_reg: 0.335  loss_rpn_cls: 0.017  loss_rpn_loc: 0.065  time: 0.5693  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:03:50 d2.utils.events]: [0m eta: 0:06:49  iter: 1259  total_loss: 0.739  loss_cls: 0.205  loss_box_reg: 0.404  loss_rpn_cls: 0.020  loss_rpn_loc: 0.086  time: 0.5689  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:04:01 d2.utils.events]: [0m eta: 0:06:38  iter: 1279  total_loss: 0.607  loss_cls: 0.188  loss_box_reg: 0.352  loss_rpn_cls: 0.022  loss_rpn_loc: 0.054  time: 0.5685  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:04:14 d2.utils.events]: [0m eta: 0:06:27  iter: 1299  total_loss: 0.663  loss_cls: 0.224  loss_box_reg: 0.372  loss_rpn_cls: 0.037  loss_rpn_loc: 0.069  time: 0.5693  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:04:25 d2.utils.events]: [0m eta: 0:06:16  iter: 1319  total_loss: 0.670  loss_cls: 0.219  loss_box_reg: 0.377  loss_rpn_cls: 0.017  loss_rpn_loc: 0.081  time: 0.5692  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:04:36 d2.utils.events]: [0m eta: 0:06:04  iter: 1339  total_loss: 0.571  loss_cls: 0.169  loss_box_reg: 0.326  loss_rpn_cls: 0.015  loss_rpn_loc: 0.036  time: 0.5684  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:04:47 d2.utils.events]: [0m eta: 0:05:52  iter: 1359  total_loss: 0.771  loss_cls: 0.267  loss_box_reg: 0.403  loss_rpn_cls: 0.024  loss_rpn_loc: 0.062  time: 0.5684  data_time: 0.0130  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:04:59 d2.utils.events]: [0m eta: 0:05:41  iter: 1379  total_loss: 0.691  loss_cls: 0.225  loss_box_reg: 0.365  loss_rpn_cls: 0.017  loss_rpn_loc: 0.053  time: 0.5690  data_time: 0.0097  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:05:10 d2.utils.events]: [0m eta: 0:05:30  iter: 1399  total_loss: 0.735  loss_cls: 0.227  loss_box_reg: 0.407  loss_rpn_cls: 0.025  loss_rpn_loc: 0.065  time: 0.5683  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:05:21 d2.utils.events]: [0m eta: 0:05:19  iter: 1419  total_loss: 0.720  loss_cls: 0.229  loss_box_reg: 0.347  loss_rpn_cls: 0.020  loss_rpn_loc: 0.076  time: 0.5683  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:05:33 d2.utils.events]: [0m eta: 0:05:09  iter: 1439  total_loss: 0.587  loss_cls: 0.200  loss_box_reg: 0.308  loss_rpn_cls: 0.025  loss_rpn_loc: 0.057  time: 0.5688  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:05:44 d2.utils.events]: [0m eta: 0:04:57  iter: 1459  total_loss: 0.744  loss_cls: 0.223  loss_box_reg: 0.391  loss_rpn_cls: 0.025  loss_rpn_loc: 0.076  time: 0.5682  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:05:55 d2.utils.events]: [0m eta: 0:04:46  iter: 1479  total_loss: 0.606  loss_cls: 0.190  loss_box_reg: 0.308  loss_rpn_cls: 0.024  loss_rpn_loc: 0.047  time: 0.5678  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:06:07 d2.utils.events]: [0m eta: 0:04:35  iter: 1499  total_loss: 0.708  loss_cls: 0.224  loss_box_reg: 0.388  loss_rpn_cls: 0.023  loss_rpn_loc: 0.064  time: 0.5685  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:06:18 d2.utils.events]: [0m eta: 0:04:24  iter: 1519  total_loss: 0.636  loss_cls: 0.211  loss_box_reg: 0.351  loss_rpn_cls: 0.020  loss_rpn_loc: 0.046  time: 0.5682  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:06:29 d2.utils.events]: [0m eta: 0:04:13  iter: 1539  total_loss: 0.669  loss_cls: 0.213  loss_box_reg: 0.365  loss_rpn_cls: 0.016  loss_rpn_loc: 0.065  time: 0.5678  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:06:41 d2.utils.events]: [0m eta: 0:04:02  iter: 1559  total_loss: 0.609  loss_cls: 0.192  loss_box_reg: 0.328  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5678  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:06:53 d2.utils.events]: [0m eta: 0:03:51  iter: 1579  total_loss: 0.645  loss_cls: 0.202  loss_box_reg: 0.348  loss_rpn_cls: 0.019  loss_rpn_loc: 0.055  time: 0.5682  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:07:03 d2.utils.events]: [0m eta: 0:03:40  iter: 1599  total_loss: 0.656  loss_cls: 0.208  loss_box_reg: 0.336  loss_rpn_cls: 0.022  loss_rpn_loc: 0.062  time: 0.5678  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:07:15 d2.utils.events]: [0m eta: 0:03:29  iter: 1619  total_loss: 0.584  loss_cls: 0.212  loss_box_reg: 0.333  loss_rpn_cls: 0.021  loss_rpn_loc: 0.060  time: 0.5676  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:07:27 d2.utils.events]: [0m eta: 0:03:18  iter: 1639  total_loss: 0.646  loss_cls: 0.231  loss_box_reg: 0.339  loss_rpn_cls: 0.023  loss_rpn_loc: 0.058  time: 0.5680  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:07:38 d2.utils.events]: [0m eta: 0:03:07  iter: 1659  total_loss: 0.628  loss_cls: 0.196  loss_box_reg: 0.305  loss_rpn_cls: 0.022  loss_rpn_loc: 0.054  time: 0.5676  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:07:49 d2.utils.events]: [0m eta: 0:02:56  iter: 1679  total_loss: 0.803  loss_cls: 0.250  loss_box_reg: 0.404  loss_rpn_cls: 0.024  loss_rpn_loc: 0.075  time: 0.5675  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:08:00 d2.utils.events]: [0m eta: 0:02:45  iter: 1699  total_loss: 0.652  loss_cls: 0.218  loss_box_reg: 0.339  loss_rpn_cls: 0.024  loss_rpn_loc: 0.063  time: 0.5676  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:08:12 d2.utils.events]: [0m eta: 0:02:34  iter: 1719  total_loss: 0.727  loss_cls: 0.238  loss_box_reg: 0.384  loss_rpn_cls: 0.025  loss_rpn_loc: 0.063  time: 0.5679  data_time: 0.0815  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:08:24 d2.utils.events]: [0m eta: 0:02:23  iter: 1739  total_loss: 0.594  loss_cls: 0.197  loss_box_reg: 0.329  loss_rpn_cls: 0.013  loss_rpn_loc: 0.044  time: 0.5679  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:08:36 d2.utils.events]: [0m eta: 0:02:12  iter: 1759  total_loss: 0.615  loss_cls: 0.193  loss_box_reg: 0.291  loss_rpn_cls: 0.021  loss_rpn_loc: 0.059  time: 0.5680  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:08:48 d2.utils.events]: [0m eta: 0:02:01  iter: 1779  total_loss: 0.705  loss_cls: 0.228  loss_box_reg: 0.370  loss_rpn_cls: 0.021  loss_rpn_loc: 0.070  time: 0.5687  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:08:59 d2.utils.events]: [0m eta: 0:01:50  iter: 1799  total_loss: 0.635  loss_cls: 0.202  loss_box_reg: 0.326  loss_rpn_cls: 0.024  loss_rpn_loc: 0.060  time: 0.5684  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:09:09 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.724  loss_cls: 0.229  loss_box_reg: 0.368  loss_rpn_cls: 0.021  loss_rpn_loc: 0.058  time: 0.5678  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:09:21 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.636  loss_cls: 0.219  loss_box_reg: 0.333  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5678  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:09:32 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.479  loss_cls: 0.160  loss_box_reg: 0.248  loss_rpn_cls: 0.017  loss_rpn_loc: 0.047  time: 0.5678  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:09:43 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.674  loss_cls: 0.251  loss_box_reg: 0.346  loss_rpn_cls: 0.020  loss_rpn_loc: 0.050  time: 0.5672  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:09:53 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.657  loss_cls: 0.214  loss_box_reg: 0.341  loss_rpn_cls: 0.025  loss_rpn_loc: 0.062  time: 0.5667  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:10:06 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.617  loss_cls: 0.199  loss_box_reg: 0.335  loss_rpn_cls: 0.021  loss_rpn_loc: 0.048  time: 0.5675  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:10:17 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.636  loss_cls: 0.210  loss_box_reg: 0.344  loss_rpn_cls: 0.022  loss_rpn_loc: 0.052  time: 0.5673  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:10:28 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.658  loss_cls: 0.220  loss_box_reg: 0.351  loss_rpn_cls: 0.022  loss_rpn_loc: 0.057  time: 0.5671  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:10:41 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.761  loss_cls: 0.231  loss_box_reg: 0.402  loss_rpn_cls: 0.017  loss_rpn_loc: 0.072  time: 0.5676  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:11:07 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json takes 1.11 seconds.
[5m[31mWARNING[0m [32m[04/15 07:11:07 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 07:11:07 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 07:11:08 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 07:11:08 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.620  loss_cls: 0.217  loss_box_reg: 0.357  loss_rpn_cls: 0.018  loss_rpn_loc: 0.054  time: 0.5676  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:11:11 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:54 (0.5679 s / it)
[32m[04/15 07:11:11 d2.engine.hooks]: [0mTotal training time: 0:19:22 (0:00:28 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 07:11:18 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 07:11:18 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 07:11:19 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 07:11:26 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1202 s / img. ETA=0:03:08
[32m[04/15 07:11:31 d2.evaluation.evaluator]: [0mInference done 43/1257. 0.1218 s / img. ETA=0:03:13
[32m[04/15 07:11:36 d2.evaluation.evaluator]: [0mInference done 73/1257. 0.1216 s / img. ETA=0:03:13
[32m[04/15 07:11:41 d2.evaluation.evaluator]: [0mInference done 109/1257. 0.1239 s / img. ETA=0:02:59
[32m[04/15 07:11:47 d2.evaluation.evaluator]: [0mInference done 135/1257. 0.1234 s / img. ETA=0:03:08
[32m[04/15 07:11:52 d2.evaluation.evaluator]: [0mInference done 162/1257. 0.1266 s / img. ETA=0:03:08
[32m[04/15 07:11:57 d2.evaluation.evaluator]: [0mInference done 196/1257. 0.1292 s / img. ETA=0:02:57
[32m[04/15 07:12:02 d2.evaluation.evaluator]: [0mInference done 231/1257. 0.1278 s / img. ETA=0:02:48
[32m[04/15 07:12:07 d2.evaluation.evaluator]: [0mInference done 264/1257. 0.1270 s / img. ETA=0:02:41
[32m[04/15 07:12:12 d2.evaluation.evaluator]: [0mInference done 296/1257. 0.1263 s / img. ETA=0:02:35
[32m[04/15 07:12:17 d2.evaluation.evaluator]: [0mInference done 334/1257. 0.1256 s / img. ETA=0:02:26
[32m[04/15 07:12:22 d2.evaluation.evaluator]: [0mInference done 371/1257. 0.1253 s / img. ETA=0:02:18
[32m[04/15 07:12:27 d2.evaluation.evaluator]: [0mInference done 403/1257. 0.1255 s / img. ETA=0:02:13
[32m[04/15 07:12:32 d2.evaluation.evaluator]: [0mInference done 434/1257. 0.1274 s / img. ETA=0:02:09
[32m[04/15 07:12:38 d2.evaluation.evaluator]: [0mInference done 461/1257. 0.1278 s / img. ETA=0:02:06
[32m[04/15 07:12:43 d2.evaluation.evaluator]: [0mInference done 494/1257. 0.1273 s / img. ETA=0:02:01
[32m[04/15 07:12:48 d2.evaluation.evaluator]: [0mInference done 520/1257. 0.1270 s / img. ETA=0:01:58
[32m[04/15 07:12:53 d2.evaluation.evaluator]: [0mInference done 545/1257. 0.1268 s / img. ETA=0:01:55
[32m[04/15 07:12:58 d2.evaluation.evaluator]: [0mInference done 574/1257. 0.1266 s / img. ETA=0:01:51
[32m[04/15 07:13:03 d2.evaluation.evaluator]: [0mInference done 607/1257. 0.1263 s / img. ETA=0:01:45
[32m[04/15 07:13:08 d2.evaluation.evaluator]: [0mInference done 635/1257. 0.1264 s / img. ETA=0:01:41
[32m[04/15 07:13:13 d2.evaluation.evaluator]: [0mInference done 665/1257. 0.1272 s / img. ETA=0:01:37
[32m[04/15 07:13:18 d2.evaluation.evaluator]: [0mInference done 696/1257. 0.1275 s / img. ETA=0:01:31
[32m[04/15 07:13:23 d2.evaluation.evaluator]: [0mInference done 729/1257. 0.1272 s / img. ETA=0:01:26
[32m[04/15 07:13:28 d2.evaluation.evaluator]: [0mInference done 762/1257. 0.1269 s / img. ETA=0:01:20
[32m[04/15 07:13:33 d2.evaluation.evaluator]: [0mInference done 802/1257. 0.1267 s / img. ETA=0:01:13
[32m[04/15 07:13:38 d2.evaluation.evaluator]: [0mInference done 841/1257. 0.1264 s / img. ETA=0:01:06
[32m[04/15 07:13:43 d2.evaluation.evaluator]: [0mInference done 881/1257. 0.1262 s / img. ETA=0:00:59
[32m[04/15 07:13:48 d2.evaluation.evaluator]: [0mInference done 918/1257. 0.1262 s / img. ETA=0:00:53
[32m[04/15 07:13:53 d2.evaluation.evaluator]: [0mInference done 951/1257. 0.1270 s / img. ETA=0:00:48
[32m[04/15 07:13:58 d2.evaluation.evaluator]: [0mInference done 986/1257. 0.1276 s / img. ETA=0:00:42
[32m[04/15 07:14:03 d2.evaluation.evaluator]: [0mInference done 1025/1257. 0.1274 s / img. ETA=0:00:36
[32m[04/15 07:14:09 d2.evaluation.evaluator]: [0mInference done 1062/1257. 0.1272 s / img. ETA=0:00:30
[32m[04/15 07:14:14 d2.evaluation.evaluator]: [0mInference done 1096/1257. 0.1270 s / img. ETA=0:00:24
[32m[04/15 07:14:19 d2.evaluation.evaluator]: [0mInference done 1127/1257. 0.1268 s / img. ETA=0:00:20
[32m[04/15 07:14:24 d2.evaluation.evaluator]: [0mInference done 1161/1257. 0.1266 s / img. ETA=0:00:14
[32m[04/15 07:14:29 d2.evaluation.evaluator]: [0mInference done 1193/1257. 0.1271 s / img. ETA=0:00:09
[32m[04/15 07:14:34 d2.evaluation.evaluator]: [0mInference done 1223/1257. 0.1276 s / img. ETA=0:00:05
[32m[04/15 07:14:39 d2.evaluation.evaluator]: [0mInference done 1248/1257. 0.1274 s / img. ETA=0:00:01
[32m[04/15 07:14:41 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:15.748497 (0.156349 s / img per device, on 1 devices)
[32m[04/15 07:14:41 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:39 (0.127382 s / img per device, on 1 devices)
[32m[04/15 07:14:41 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 07:14:41 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 07:14:42 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.60s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=4.96s).
Accumulating evaluation results...
DONE (t=1.27s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.195
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.387
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.174
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.123
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.245
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.387
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.239
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.241
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.173
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.288
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.421
[32m[04/15 07:14:50 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.536 | 38.712 | 17.442 | 12.276 | 24.526 | 38.696 |
[32m[04/15 07:14:50 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 28.156 | bicycle       | 8.464 | car            | 41.525 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  10  *  2000  iterations ============
4 channel input
[32m[04/15 07:14:51 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 07:14:53 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.69 seconds.
[5m[31mWARNING[0m [32m[04/15 07:14:53 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 07:14:53 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 07:14:53 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 07:14:53 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 07:14:53 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 07:15:02 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 07:15:17 d2.utils.events]: [0m eta: 0:19:05  iter: 19  total_loss: 0.526  loss_cls: 0.190  loss_box_reg: 0.302  loss_rpn_cls: 0.020  loss_rpn_loc: 0.036  time: 0.5901  data_time: 0.1599  lr: 0.000100  max_mem: 3067M
[32m[04/15 07:15:28 d2.utils.events]: [0m eta: 0:18:02  iter: 39  total_loss: 0.659  loss_cls: 0.216  loss_box_reg: 0.305  loss_rpn_cls: 0.018  loss_rpn_loc: 0.051  time: 0.5615  data_time: 0.0076  lr: 0.000200  max_mem: 3067M
[32m[04/15 07:15:38 d2.utils.events]: [0m eta: 0:17:28  iter: 59  total_loss: 0.597  loss_cls: 0.203  loss_box_reg: 0.301  loss_rpn_cls: 0.013  loss_rpn_loc: 0.059  time: 0.5500  data_time: 0.0067  lr: 0.000300  max_mem: 3067M
[32m[04/15 07:15:51 d2.utils.events]: [0m eta: 0:17:32  iter: 79  total_loss: 0.670  loss_cls: 0.214  loss_box_reg: 0.370  loss_rpn_cls: 0.019  loss_rpn_loc: 0.059  time: 0.5654  data_time: 0.0083  lr: 0.000400  max_mem: 3067M
[32m[04/15 07:16:02 d2.utils.events]: [0m eta: 0:17:20  iter: 99  total_loss: 0.639  loss_cls: 0.200  loss_box_reg: 0.368  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5673  data_time: 0.0088  lr: 0.000500  max_mem: 3067M
[32m[04/15 07:16:13 d2.utils.events]: [0m eta: 0:17:00  iter: 119  total_loss: 0.594  loss_cls: 0.186  loss_box_reg: 0.319  loss_rpn_cls: 0.016  loss_rpn_loc: 0.054  time: 0.5597  data_time: 0.0069  lr: 0.000599  max_mem: 3067M
[32m[04/15 07:16:24 d2.utils.events]: [0m eta: 0:16:49  iter: 139  total_loss: 0.649  loss_cls: 0.218  loss_box_reg: 0.325  loss_rpn_cls: 0.019  loss_rpn_loc: 0.050  time: 0.5558  data_time: 0.0064  lr: 0.000699  max_mem: 3067M
[32m[04/15 07:16:36 d2.utils.events]: [0m eta: 0:16:43  iter: 159  total_loss: 0.629  loss_cls: 0.199  loss_box_reg: 0.350  loss_rpn_cls: 0.017  loss_rpn_loc: 0.042  time: 0.5634  data_time: 0.0091  lr: 0.000799  max_mem: 3067M
[32m[04/15 07:16:47 d2.utils.events]: [0m eta: 0:16:27  iter: 179  total_loss: 0.581  loss_cls: 0.187  loss_box_reg: 0.324  loss_rpn_cls: 0.018  loss_rpn_loc: 0.046  time: 0.5608  data_time: 0.0109  lr: 0.000899  max_mem: 3067M
[32m[04/15 07:16:58 d2.utils.events]: [0m eta: 0:16:13  iter: 199  total_loss: 0.626  loss_cls: 0.183  loss_box_reg: 0.363  loss_rpn_cls: 0.017  loss_rpn_loc: 0.057  time: 0.5562  data_time: 0.0068  lr: 0.000999  max_mem: 3067M
[32m[04/15 07:17:08 d2.utils.events]: [0m eta: 0:15:53  iter: 219  total_loss: 0.620  loss_cls: 0.188  loss_box_reg: 0.324  loss_rpn_cls: 0.013  loss_rpn_loc: 0.055  time: 0.5524  data_time: 0.0070  lr: 0.001099  max_mem: 3067M
[32m[04/15 07:17:21 d2.utils.events]: [0m eta: 0:15:54  iter: 239  total_loss: 0.666  loss_cls: 0.211  loss_box_reg: 0.377  loss_rpn_cls: 0.013  loss_rpn_loc: 0.065  time: 0.5602  data_time: 0.0076  lr: 0.001199  max_mem: 3067M
[32m[04/15 07:17:32 d2.utils.events]: [0m eta: 0:15:39  iter: 259  total_loss: 0.636  loss_cls: 0.205  loss_box_reg: 0.370  loss_rpn_cls: 0.012  loss_rpn_loc: 0.046  time: 0.5577  data_time: 0.0089  lr: 0.001299  max_mem: 3067M
[32m[04/15 07:17:43 d2.utils.events]: [0m eta: 0:15:31  iter: 279  total_loss: 0.533  loss_cls: 0.183  loss_box_reg: 0.305  loss_rpn_cls: 0.011  loss_rpn_loc: 0.050  time: 0.5563  data_time: 0.0074  lr: 0.001399  max_mem: 3067M
[32m[04/15 07:17:54 d2.utils.events]: [0m eta: 0:15:21  iter: 299  total_loss: 0.680  loss_cls: 0.225  loss_box_reg: 0.360  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5583  data_time: 0.0069  lr: 0.001499  max_mem: 3067M
[32m[04/15 07:18:06 d2.utils.events]: [0m eta: 0:15:12  iter: 319  total_loss: 0.594  loss_cls: 0.189  loss_box_reg: 0.335  loss_rpn_cls: 0.012  loss_rpn_loc: 0.047  time: 0.5598  data_time: 0.0081  lr: 0.001598  max_mem: 3067M
[32m[04/15 07:18:17 d2.utils.events]: [0m eta: 0:14:57  iter: 339  total_loss: 0.652  loss_cls: 0.195  loss_box_reg: 0.320  loss_rpn_cls: 0.015  loss_rpn_loc: 0.071  time: 0.5574  data_time: 0.0069  lr: 0.001698  max_mem: 3067M
[32m[04/15 07:18:27 d2.utils.events]: [0m eta: 0:14:43  iter: 359  total_loss: 0.669  loss_cls: 0.213  loss_box_reg: 0.387  loss_rpn_cls: 0.021  loss_rpn_loc: 0.070  time: 0.5560  data_time: 0.0440  lr: 0.001798  max_mem: 3067M
[32m[04/15 07:18:40 d2.utils.events]: [0m eta: 0:14:37  iter: 379  total_loss: 0.576  loss_cls: 0.191  loss_box_reg: 0.325  loss_rpn_cls: 0.017  loss_rpn_loc: 0.051  time: 0.5596  data_time: 0.0094  lr: 0.001898  max_mem: 3067M
[32m[04/15 07:18:51 d2.utils.events]: [0m eta: 0:14:28  iter: 399  total_loss: 0.617  loss_cls: 0.189  loss_box_reg: 0.339  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5596  data_time: 0.0075  lr: 0.001998  max_mem: 3067M
[32m[04/15 07:19:02 d2.utils.events]: [0m eta: 0:14:17  iter: 419  total_loss: 0.598  loss_cls: 0.179  loss_box_reg: 0.337  loss_rpn_cls: 0.015  loss_rpn_loc: 0.045  time: 0.5583  data_time: 0.0062  lr: 0.002098  max_mem: 3067M
[32m[04/15 07:19:15 d2.utils.events]: [0m eta: 0:14:08  iter: 439  total_loss: 0.663  loss_cls: 0.206  loss_box_reg: 0.345  loss_rpn_cls: 0.009  loss_rpn_loc: 0.060  time: 0.5612  data_time: 0.0080  lr: 0.002198  max_mem: 3067M
[32m[04/15 07:19:26 d2.utils.events]: [0m eta: 0:13:58  iter: 459  total_loss: 0.684  loss_cls: 0.212  loss_box_reg: 0.392  loss_rpn_cls: 0.026  loss_rpn_loc: 0.055  time: 0.5615  data_time: 0.0074  lr: 0.002298  max_mem: 3067M
[32m[04/15 07:19:37 d2.utils.events]: [0m eta: 0:13:47  iter: 479  total_loss: 0.546  loss_cls: 0.168  loss_box_reg: 0.288  loss_rpn_cls: 0.017  loss_rpn_loc: 0.048  time: 0.5604  data_time: 0.0074  lr: 0.002398  max_mem: 3067M
[32m[04/15 07:19:49 d2.utils.events]: [0m eta: 0:13:37  iter: 499  total_loss: 0.612  loss_cls: 0.204  loss_box_reg: 0.347  loss_rpn_cls: 0.015  loss_rpn_loc: 0.048  time: 0.5616  data_time: 0.0096  lr: 0.002498  max_mem: 3067M
[32m[04/15 07:20:01 d2.utils.events]: [0m eta: 0:13:26  iter: 519  total_loss: 0.657  loss_cls: 0.224  loss_box_reg: 0.365  loss_rpn_cls: 0.023  loss_rpn_loc: 0.059  time: 0.5629  data_time: 0.0077  lr: 0.002597  max_mem: 3067M
[32m[04/15 07:20:11 d2.utils.events]: [0m eta: 0:13:15  iter: 539  total_loss: 0.697  loss_cls: 0.231  loss_box_reg: 0.347  loss_rpn_cls: 0.021  loss_rpn_loc: 0.055  time: 0.5617  data_time: 0.0073  lr: 0.002697  max_mem: 3067M
[32m[04/15 07:20:22 d2.utils.events]: [0m eta: 0:13:02  iter: 559  total_loss: 0.655  loss_cls: 0.222  loss_box_reg: 0.340  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5606  data_time: 0.0091  lr: 0.002797  max_mem: 3067M
[32m[04/15 07:20:35 d2.utils.events]: [0m eta: 0:12:53  iter: 579  total_loss: 0.590  loss_cls: 0.202  loss_box_reg: 0.321  loss_rpn_cls: 0.012  loss_rpn_loc: 0.053  time: 0.5627  data_time: 0.0098  lr: 0.002897  max_mem: 3067M
[32m[04/15 07:20:45 d2.utils.events]: [0m eta: 0:12:42  iter: 599  total_loss: 0.573  loss_cls: 0.162  loss_box_reg: 0.308  loss_rpn_cls: 0.021  loss_rpn_loc: 0.051  time: 0.5615  data_time: 0.0062  lr: 0.002997  max_mem: 3067M
[32m[04/15 07:20:57 d2.utils.events]: [0m eta: 0:12:31  iter: 619  total_loss: 0.653  loss_cls: 0.203  loss_box_reg: 0.353  loss_rpn_cls: 0.019  loss_rpn_loc: 0.058  time: 0.5612  data_time: 0.0064  lr: 0.003097  max_mem: 3067M
[32m[04/15 07:21:08 d2.utils.events]: [0m eta: 0:12:21  iter: 639  total_loss: 0.669  loss_cls: 0.201  loss_box_reg: 0.350  loss_rpn_cls: 0.023  loss_rpn_loc: 0.063  time: 0.5621  data_time: 0.0065  lr: 0.003197  max_mem: 3067M
[32m[04/15 07:21:20 d2.utils.events]: [0m eta: 0:12:10  iter: 659  total_loss: 0.627  loss_cls: 0.224  loss_box_reg: 0.347  loss_rpn_cls: 0.015  loss_rpn_loc: 0.064  time: 0.5620  data_time: 0.0085  lr: 0.003297  max_mem: 3067M
[32m[04/15 07:21:31 d2.utils.events]: [0m eta: 0:11:59  iter: 679  total_loss: 0.660  loss_cls: 0.215  loss_box_reg: 0.378  loss_rpn_cls: 0.014  loss_rpn_loc: 0.054  time: 0.5614  data_time: 0.0066  lr: 0.003397  max_mem: 3067M
[32m[04/15 07:21:42 d2.utils.events]: [0m eta: 0:11:50  iter: 699  total_loss: 0.624  loss_cls: 0.163  loss_box_reg: 0.330  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5622  data_time: 0.0103  lr: 0.003497  max_mem: 3067M
[32m[04/15 07:21:54 d2.utils.events]: [0m eta: 0:11:39  iter: 719  total_loss: 0.644  loss_cls: 0.204  loss_box_reg: 0.369  loss_rpn_cls: 0.018  loss_rpn_loc: 0.059  time: 0.5628  data_time: 0.0087  lr: 0.003596  max_mem: 3067M
[32m[04/15 07:22:05 d2.utils.events]: [0m eta: 0:11:28  iter: 739  total_loss: 0.661  loss_cls: 0.219  loss_box_reg: 0.347  loss_rpn_cls: 0.015  loss_rpn_loc: 0.060  time: 0.5623  data_time: 0.0066  lr: 0.003696  max_mem: 3067M
[32m[04/15 07:22:16 d2.utils.events]: [0m eta: 0:11:17  iter: 759  total_loss: 0.775  loss_cls: 0.233  loss_box_reg: 0.393  loss_rpn_cls: 0.031  loss_rpn_loc: 0.065  time: 0.5617  data_time: 0.0080  lr: 0.003796  max_mem: 3067M
[32m[04/15 07:22:29 d2.utils.events]: [0m eta: 0:11:07  iter: 779  total_loss: 0.729  loss_cls: 0.229  loss_box_reg: 0.399  loss_rpn_cls: 0.020  loss_rpn_loc: 0.071  time: 0.5634  data_time: 0.0076  lr: 0.003896  max_mem: 3067M
[32m[04/15 07:22:40 d2.utils.events]: [0m eta: 0:10:57  iter: 799  total_loss: 0.717  loss_cls: 0.195  loss_box_reg: 0.395  loss_rpn_cls: 0.020  loss_rpn_loc: 0.091  time: 0.5629  data_time: 0.0081  lr: 0.003996  max_mem: 3067M
[32m[04/15 07:22:50 d2.utils.events]: [0m eta: 0:10:45  iter: 819  total_loss: 0.653  loss_cls: 0.202  loss_box_reg: 0.313  loss_rpn_cls: 0.026  loss_rpn_loc: 0.069  time: 0.5621  data_time: 0.0063  lr: 0.004096  max_mem: 3067M
[32m[04/15 07:23:03 d2.utils.events]: [0m eta: 0:10:36  iter: 839  total_loss: 0.598  loss_cls: 0.184  loss_box_reg: 0.319  loss_rpn_cls: 0.016  loss_rpn_loc: 0.058  time: 0.5631  data_time: 0.0082  lr: 0.004196  max_mem: 3067M
[32m[04/15 07:23:14 d2.utils.events]: [0m eta: 0:10:24  iter: 859  total_loss: 0.643  loss_cls: 0.206  loss_box_reg: 0.380  loss_rpn_cls: 0.020  loss_rpn_loc: 0.056  time: 0.5630  data_time: 0.0073  lr: 0.004296  max_mem: 3067M
[32m[04/15 07:23:25 d2.utils.events]: [0m eta: 0:10:13  iter: 879  total_loss: 0.615  loss_cls: 0.185  loss_box_reg: 0.334  loss_rpn_cls: 0.021  loss_rpn_loc: 0.048  time: 0.5625  data_time: 0.0081  lr: 0.004396  max_mem: 3067M
[32m[04/15 07:23:37 d2.utils.events]: [0m eta: 0:10:02  iter: 899  total_loss: 0.536  loss_cls: 0.193  loss_box_reg: 0.262  loss_rpn_cls: 0.015  loss_rpn_loc: 0.059  time: 0.5629  data_time: 0.0077  lr: 0.004496  max_mem: 3067M
[32m[04/15 07:23:48 d2.utils.events]: [0m eta: 0:09:52  iter: 919  total_loss: 0.615  loss_cls: 0.190  loss_box_reg: 0.350  loss_rpn_cls: 0.020  loss_rpn_loc: 0.053  time: 0.5634  data_time: 0.0070  lr: 0.004595  max_mem: 3067M
[32m[04/15 07:23:59 d2.utils.events]: [0m eta: 0:09:40  iter: 939  total_loss: 0.657  loss_cls: 0.220  loss_box_reg: 0.352  loss_rpn_cls: 0.021  loss_rpn_loc: 0.047  time: 0.5628  data_time: 0.0067  lr: 0.004695  max_mem: 3067M
[32m[04/15 07:24:11 d2.utils.events]: [0m eta: 0:09:29  iter: 959  total_loss: 0.799  loss_cls: 0.262  loss_box_reg: 0.441  loss_rpn_cls: 0.023  loss_rpn_loc: 0.060  time: 0.5628  data_time: 0.0081  lr: 0.004795  max_mem: 3067M
[32m[04/15 07:24:24 d2.utils.events]: [0m eta: 0:09:19  iter: 979  total_loss: 0.679  loss_cls: 0.240  loss_box_reg: 0.354  loss_rpn_cls: 0.016  loss_rpn_loc: 0.049  time: 0.5651  data_time: 0.1334  lr: 0.004895  max_mem: 3067M
[32m[04/15 07:24:37 d2.utils.events]: [0m eta: 0:09:08  iter: 999  total_loss: 0.587  loss_cls: 0.194  loss_box_reg: 0.312  loss_rpn_cls: 0.023  loss_rpn_loc: 0.044  time: 0.5668  data_time: 0.1215  lr: 0.004995  max_mem: 3067M
[32m[04/15 07:24:49 d2.utils.events]: [0m eta: 0:08:58  iter: 1019  total_loss: 0.674  loss_cls: 0.225  loss_box_reg: 0.348  loss_rpn_cls: 0.029  loss_rpn_loc: 0.040  time: 0.5673  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:25:01 d2.utils.events]: [0m eta: 0:08:48  iter: 1039  total_loss: 0.555  loss_cls: 0.176  loss_box_reg: 0.287  loss_rpn_cls: 0.017  loss_rpn_loc: 0.045  time: 0.5676  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:25:14 d2.utils.events]: [0m eta: 0:08:40  iter: 1059  total_loss: 0.609  loss_cls: 0.199  loss_box_reg: 0.330  loss_rpn_cls: 0.018  loss_rpn_loc: 0.060  time: 0.5685  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:25:24 d2.utils.events]: [0m eta: 0:08:27  iter: 1079  total_loss: 0.687  loss_cls: 0.235  loss_box_reg: 0.390  loss_rpn_cls: 0.017  loss_rpn_loc: 0.061  time: 0.5679  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:25:35 d2.utils.events]: [0m eta: 0:08:16  iter: 1099  total_loss: 0.576  loss_cls: 0.197  loss_box_reg: 0.290  loss_rpn_cls: 0.013  loss_rpn_loc: 0.047  time: 0.5672  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:25:48 d2.utils.events]: [0m eta: 0:08:08  iter: 1119  total_loss: 0.625  loss_cls: 0.207  loss_box_reg: 0.340  loss_rpn_cls: 0.016  loss_rpn_loc: 0.048  time: 0.5684  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:25:59 d2.utils.events]: [0m eta: 0:07:56  iter: 1139  total_loss: 0.592  loss_cls: 0.170  loss_box_reg: 0.305  loss_rpn_cls: 0.020  loss_rpn_loc: 0.050  time: 0.5678  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:26:09 d2.utils.events]: [0m eta: 0:07:43  iter: 1159  total_loss: 0.658  loss_cls: 0.220  loss_box_reg: 0.349  loss_rpn_cls: 0.026  loss_rpn_loc: 0.048  time: 0.5669  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:26:21 d2.utils.events]: [0m eta: 0:07:34  iter: 1179  total_loss: 0.563  loss_cls: 0.199  loss_box_reg: 0.312  loss_rpn_cls: 0.025  loss_rpn_loc: 0.054  time: 0.5669  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:26:33 d2.utils.events]: [0m eta: 0:07:23  iter: 1199  total_loss: 0.695  loss_cls: 0.228  loss_box_reg: 0.391  loss_rpn_cls: 0.020  loss_rpn_loc: 0.050  time: 0.5674  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:26:44 d2.utils.events]: [0m eta: 0:07:13  iter: 1219  total_loss: 0.611  loss_cls: 0.176  loss_box_reg: 0.342  loss_rpn_cls: 0.024  loss_rpn_loc: 0.053  time: 0.5672  data_time: 0.0047  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:26:55 d2.utils.events]: [0m eta: 0:07:01  iter: 1239  total_loss: 0.697  loss_cls: 0.218  loss_box_reg: 0.370  loss_rpn_cls: 0.020  loss_rpn_loc: 0.061  time: 0.5674  data_time: 0.0054  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:27:07 d2.utils.events]: [0m eta: 0:06:50  iter: 1259  total_loss: 0.676  loss_cls: 0.224  loss_box_reg: 0.354  loss_rpn_cls: 0.024  loss_rpn_loc: 0.065  time: 0.5671  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:27:17 d2.utils.events]: [0m eta: 0:06:39  iter: 1279  total_loss: 0.733  loss_cls: 0.230  loss_box_reg: 0.386  loss_rpn_cls: 0.023  loss_rpn_loc: 0.071  time: 0.5665  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:27:28 d2.utils.events]: [0m eta: 0:06:28  iter: 1299  total_loss: 0.671  loss_cls: 0.221  loss_box_reg: 0.355  loss_rpn_cls: 0.013  loss_rpn_loc: 0.057  time: 0.5660  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:27:41 d2.utils.events]: [0m eta: 0:06:17  iter: 1319  total_loss: 0.686  loss_cls: 0.252  loss_box_reg: 0.375  loss_rpn_cls: 0.018  loss_rpn_loc: 0.060  time: 0.5669  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:27:52 d2.utils.events]: [0m eta: 0:06:07  iter: 1339  total_loss: 0.780  loss_cls: 0.246  loss_box_reg: 0.434  loss_rpn_cls: 0.019  loss_rpn_loc: 0.065  time: 0.5667  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:28:03 d2.utils.events]: [0m eta: 0:05:56  iter: 1359  total_loss: 0.712  loss_cls: 0.227  loss_box_reg: 0.393  loss_rpn_cls: 0.029  loss_rpn_loc: 0.068  time: 0.5665  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:28:15 d2.utils.events]: [0m eta: 0:05:45  iter: 1379  total_loss: 0.762  loss_cls: 0.241  loss_box_reg: 0.370  loss_rpn_cls: 0.019  loss_rpn_loc: 0.065  time: 0.5670  data_time: 0.0098  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:28:27 d2.utils.events]: [0m eta: 0:05:34  iter: 1399  total_loss: 0.572  loss_cls: 0.199  loss_box_reg: 0.308  loss_rpn_cls: 0.020  loss_rpn_loc: 0.043  time: 0.5670  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:28:37 d2.utils.events]: [0m eta: 0:05:22  iter: 1419  total_loss: 0.702  loss_cls: 0.222  loss_box_reg: 0.364  loss_rpn_cls: 0.025  loss_rpn_loc: 0.075  time: 0.5664  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:28:49 d2.utils.events]: [0m eta: 0:05:11  iter: 1439  total_loss: 0.613  loss_cls: 0.197  loss_box_reg: 0.366  loss_rpn_cls: 0.018  loss_rpn_loc: 0.046  time: 0.5666  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:29:01 d2.utils.events]: [0m eta: 0:05:00  iter: 1459  total_loss: 0.738  loss_cls: 0.233  loss_box_reg: 0.389  loss_rpn_cls: 0.026  loss_rpn_loc: 0.051  time: 0.5668  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:29:11 d2.utils.events]: [0m eta: 0:04:49  iter: 1479  total_loss: 0.628  loss_cls: 0.210  loss_box_reg: 0.327  loss_rpn_cls: 0.021  loss_rpn_loc: 0.058  time: 0.5663  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:29:23 d2.utils.events]: [0m eta: 0:04:37  iter: 1499  total_loss: 0.633  loss_cls: 0.213  loss_box_reg: 0.328  loss_rpn_cls: 0.021  loss_rpn_loc: 0.050  time: 0.5661  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:29:35 d2.utils.events]: [0m eta: 0:04:27  iter: 1519  total_loss: 0.599  loss_cls: 0.187  loss_box_reg: 0.337  loss_rpn_cls: 0.022  loss_rpn_loc: 0.053  time: 0.5667  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:29:46 d2.utils.events]: [0m eta: 0:04:16  iter: 1539  total_loss: 0.623  loss_cls: 0.214  loss_box_reg: 0.334  loss_rpn_cls: 0.015  loss_rpn_loc: 0.051  time: 0.5662  data_time: 0.0062  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:29:57 d2.utils.events]: [0m eta: 0:04:05  iter: 1559  total_loss: 0.653  loss_cls: 0.209  loss_box_reg: 0.335  loss_rpn_cls: 0.018  loss_rpn_loc: 0.053  time: 0.5659  data_time: 0.0061  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:30:09 d2.utils.events]: [0m eta: 0:03:54  iter: 1579  total_loss: 0.742  loss_cls: 0.226  loss_box_reg: 0.377  loss_rpn_cls: 0.030  loss_rpn_loc: 0.069  time: 0.5665  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:30:20 d2.utils.events]: [0m eta: 0:03:43  iter: 1599  total_loss: 0.659  loss_cls: 0.209  loss_box_reg: 0.383  loss_rpn_cls: 0.015  loss_rpn_loc: 0.053  time: 0.5663  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:30:30 d2.utils.events]: [0m eta: 0:03:31  iter: 1619  total_loss: 0.762  loss_cls: 0.233  loss_box_reg: 0.349  loss_rpn_cls: 0.036  loss_rpn_loc: 0.058  time: 0.5657  data_time: 0.0062  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:30:43 d2.utils.events]: [0m eta: 0:03:20  iter: 1639  total_loss: 0.602  loss_cls: 0.210  loss_box_reg: 0.313  loss_rpn_cls: 0.025  loss_rpn_loc: 0.056  time: 0.5662  data_time: 0.0096  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:30:55 d2.utils.events]: [0m eta: 0:03:09  iter: 1659  total_loss: 0.600  loss_cls: 0.198  loss_box_reg: 0.337  loss_rpn_cls: 0.020  loss_rpn_loc: 0.049  time: 0.5664  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:31:05 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.597  loss_cls: 0.187  loss_box_reg: 0.332  loss_rpn_cls: 0.017  loss_rpn_loc: 0.053  time: 0.5659  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:31:17 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.730  loss_cls: 0.227  loss_box_reg: 0.371  loss_rpn_cls: 0.033  loss_rpn_loc: 0.066  time: 0.5659  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:31:29 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.704  loss_cls: 0.236  loss_box_reg: 0.374  loss_rpn_cls: 0.035  loss_rpn_loc: 0.062  time: 0.5662  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:31:40 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.726  loss_cls: 0.236  loss_box_reg: 0.383  loss_rpn_cls: 0.022  loss_rpn_loc: 0.067  time: 0.5661  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:31:51 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.661  loss_cls: 0.206  loss_box_reg: 0.390  loss_rpn_cls: 0.020  loss_rpn_loc: 0.063  time: 0.5662  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:32:04 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.651  loss_cls: 0.204  loss_box_reg: 0.316  loss_rpn_cls: 0.023  loss_rpn_loc: 0.045  time: 0.5666  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:32:14 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.738  loss_cls: 0.238  loss_box_reg: 0.419  loss_rpn_cls: 0.026  loss_rpn_loc: 0.056  time: 0.5662  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:32:25 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.530  loss_cls: 0.160  loss_box_reg: 0.286  loss_rpn_cls: 0.017  loss_rpn_loc: 0.056  time: 0.5660  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:32:37 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.629  loss_cls: 0.201  loss_box_reg: 0.354  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5664  data_time: 0.0206  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:32:49 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.655  loss_cls: 0.221  loss_box_reg: 0.374  loss_rpn_cls: 0.018  loss_rpn_loc: 0.048  time: 0.5667  data_time: 0.0104  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:33:00 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.728  loss_cls: 0.229  loss_box_reg: 0.394  loss_rpn_cls: 0.019  loss_rpn_loc: 0.061  time: 0.5663  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:33:12 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.720  loss_cls: 0.210  loss_box_reg: 0.369  loss_rpn_cls: 0.026  loss_rpn_loc: 0.089  time: 0.5667  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:33:23 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.690  loss_cls: 0.243  loss_box_reg: 0.361  loss_rpn_cls: 0.023  loss_rpn_loc: 0.050  time: 0.5666  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:33:34 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.701  loss_cls: 0.239  loss_box_reg: 0.380  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5661  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:33:46 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.597  loss_cls: 0.193  loss_box_reg: 0.323  loss_rpn_cls: 0.016  loss_rpn_loc: 0.049  time: 0.5664  data_time: 0.0062  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:33:57 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.863  loss_cls: 0.270  loss_box_reg: 0.455  loss_rpn_cls: 0.032  loss_rpn_loc: 0.069  time: 0.5664  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:34:18 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json takes 1.46 seconds.
[5m[31mWARNING[0m [32m[04/15 07:34:18 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 07:34:18 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 07:34:19 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 07:34:19 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.752  loss_cls: 0.219  loss_box_reg: 0.420  loss_rpn_cls: 0.016  loss_rpn_loc: 0.060  time: 0.5663  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:34:19 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:51 (0.5666 s / it)
[32m[04/15 07:34:19 d2.engine.hooks]: [0mTotal training time: 0:19:12 (0:00:21 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 07:34:26 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 07:34:26 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 07:34:26 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 07:34:29 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1238 s / img. ETA=0:03:12
[32m[04/15 07:34:34 d2.evaluation.evaluator]: [0mInference done 47/1257. 0.1230 s / img. ETA=0:02:55
[32m[04/15 07:34:39 d2.evaluation.evaluator]: [0mInference done 84/1257. 0.1224 s / img. ETA=0:02:45
[32m[04/15 07:34:44 d2.evaluation.evaluator]: [0mInference done 118/1257. 0.1250 s / img. ETA=0:02:43
[32m[04/15 07:34:49 d2.evaluation.evaluator]: [0mInference done 150/1257. 0.1239 s / img. ETA=0:02:42
[32m[04/15 07:34:54 d2.evaluation.evaluator]: [0mInference done 189/1257. 0.1243 s / img. ETA=0:02:32
[32m[04/15 07:34:59 d2.evaluation.evaluator]: [0mInference done 222/1257. 0.1255 s / img. ETA=0:02:29
[32m[04/15 07:35:04 d2.evaluation.evaluator]: [0mInference done 255/1257. 0.1283 s / img. ETA=0:02:26
[32m[04/15 07:35:09 d2.evaluation.evaluator]: [0mInference done 291/1257. 0.1289 s / img. ETA=0:02:20
[32m[04/15 07:35:14 d2.evaluation.evaluator]: [0mInference done 327/1257. 0.1279 s / img. ETA=0:02:14
[32m[04/15 07:35:19 d2.evaluation.evaluator]: [0mInference done 367/1257. 0.1272 s / img. ETA=0:02:06
[32m[04/15 07:35:25 d2.evaluation.evaluator]: [0mInference done 408/1257. 0.1265 s / img. ETA=0:01:59
[32m[04/15 07:35:30 d2.evaluation.evaluator]: [0mInference done 441/1257. 0.1261 s / img. ETA=0:01:55
[32m[04/15 07:35:35 d2.evaluation.evaluator]: [0mInference done 470/1257. 0.1259 s / img. ETA=0:01:53
[32m[04/15 07:35:40 d2.evaluation.evaluator]: [0mInference done 502/1257. 0.1260 s / img. ETA=0:01:49
[32m[04/15 07:35:45 d2.evaluation.evaluator]: [0mInference done 535/1257. 0.1270 s / img. ETA=0:01:45
[32m[04/15 07:35:50 d2.evaluation.evaluator]: [0mInference done 568/1257. 0.1276 s / img. ETA=0:01:40
[32m[04/15 07:35:55 d2.evaluation.evaluator]: [0mInference done 602/1257. 0.1273 s / img. ETA=0:01:35
[32m[04/15 07:36:01 d2.evaluation.evaluator]: [0mInference done 633/1257. 0.1269 s / img. ETA=0:01:32
[32m[04/15 07:36:06 d2.evaluation.evaluator]: [0mInference done 663/1257. 0.1267 s / img. ETA=0:01:28
[32m[04/15 07:36:11 d2.evaluation.evaluator]: [0mInference done 698/1257. 0.1268 s / img. ETA=0:01:23
[32m[04/15 07:36:16 d2.evaluation.evaluator]: [0mInference done 737/1257. 0.1268 s / img. ETA=0:01:16
[32m[04/15 07:36:21 d2.evaluation.evaluator]: [0mInference done 771/1257. 0.1268 s / img. ETA=0:01:11
[32m[04/15 07:36:26 d2.evaluation.evaluator]: [0mInference done 804/1257. 0.1277 s / img. ETA=0:01:06
[32m[04/15 07:36:31 d2.evaluation.evaluator]: [0mInference done 841/1257. 0.1278 s / img. ETA=0:01:01
[32m[04/15 07:36:36 d2.evaluation.evaluator]: [0mInference done 881/1257. 0.1275 s / img. ETA=0:00:55
[32m[04/15 07:36:41 d2.evaluation.evaluator]: [0mInference done 920/1257. 0.1273 s / img. ETA=0:00:49
[32m[04/15 07:36:46 d2.evaluation.evaluator]: [0mInference done 960/1257. 0.1270 s / img. ETA=0:00:43
[32m[04/15 07:36:51 d2.evaluation.evaluator]: [0mInference done 997/1257. 0.1272 s / img. ETA=0:00:37
[32m[04/15 07:36:56 d2.evaluation.evaluator]: [0mInference done 1038/1257. 0.1270 s / img. ETA=0:00:31
[32m[04/15 07:37:01 d2.evaluation.evaluator]: [0mInference done 1075/1257. 0.1272 s / img. ETA=0:00:26
[32m[04/15 07:37:07 d2.evaluation.evaluator]: [0mInference done 1109/1257. 0.1277 s / img. ETA=0:00:21
[32m[04/15 07:37:12 d2.evaluation.evaluator]: [0mInference done 1144/1257. 0.1278 s / img. ETA=0:00:16
[32m[04/15 07:37:17 d2.evaluation.evaluator]: [0mInference done 1182/1257. 0.1276 s / img. ETA=0:00:10
[32m[04/15 07:37:22 d2.evaluation.evaluator]: [0mInference done 1216/1257. 0.1275 s / img. ETA=0:00:05
[32m[04/15 07:37:27 d2.evaluation.evaluator]: [0mInference done 1246/1257. 0.1274 s / img. ETA=0:00:01
[32m[04/15 07:37:28 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:00.562956 (0.144220 s / img per device, on 1 devices)
[32m[04/15 07:37:28 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:39 (0.127483 s / img per device, on 1 devices)
[32m[04/15 07:37:30 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 07:37:30 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 07:37:31 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.64s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.21s).
Accumulating evaluation results...
DONE (t=0.83s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.406
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.188
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.254
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.434
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.107
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.190
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.312
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.467
[32m[04/15 07:37:39 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 20.803 | 40.593 | 18.811 | 13.021 | 25.357 | 43.433 |
[32m[04/15 07:37:39 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP     | category       | AP     |
|:--------------|:-------|:--------------|:-------|:---------------|:-------|
| person        | 29.650 | bicycle       | 10.274 | car            | 43.203 |
| motorcycle    | nan    | airplane      | nan    | bus            | nan    |
| train         | nan    | truck         | nan    | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan    | stop sign      | nan    |
| parking meter | nan    | bench         | nan    | bird           | nan    |
| cat           | nan    | dog           | 0.086  | horse          | nan    |
| sheep         | nan    | cow           | nan    | elephant       | nan    |
| bear          | nan    | zebra         | nan    | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan    | handbag        | nan    |
| tie           | nan    | suitcase      | nan    | frisbee        | nan    |
| skis          | nan    | snowboard     | nan    | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan    | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan    | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan    | cup            | nan    |
| fork          | nan    | knife         | nan    | spoon          | nan    |
| bowl          | nan    | banana        | nan    | apple          | nan    |
| sandwich      | nan    | orange        | nan    | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan    | pizza          | nan    |
| donut         | nan    | cake          | nan    | chair          | nan    |
| couch         | nan    | potted plant  | nan    | bed            | nan    |
| dining table  | nan    | toilet        | nan    | tv             | nan    |
| laptop        | nan    | mouse         | nan    | remote         | nan    |
| keyboard      | nan    | cell phone    | nan    | microwave      | nan    |
| oven          | nan    | bike          | nan    | hydrant        | nan    |
| motor         | nan    | rider         | nan    | light          | nan    |
| sign          | nan    | motor vehicle | nan    | human face     | nan    |
| hair drier    | nan    | license plate | nan    |                |        |
============== The  11  *  2000  iterations ============
4 channel input
[32m[04/15 07:37:40 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 07:37:41 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.43 seconds.
[5m[31mWARNING[0m [32m[04/15 07:37:41 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 07:37:42 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 07:37:42 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 07:37:42 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 07:37:42 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 07:37:45 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 07:37:57 d2.utils.events]: [0m eta: 0:16:42  iter: 19  total_loss: 0.574  loss_cls: 0.187  loss_box_reg: 0.319  loss_rpn_cls: 0.017  loss_rpn_loc: 0.050  time: 0.5067  data_time: 0.0779  lr: 0.000100  max_mem: 3067M
[32m[04/15 07:38:08 d2.utils.events]: [0m eta: 0:17:22  iter: 39  total_loss: 0.593  loss_cls: 0.197  loss_box_reg: 0.339  loss_rpn_cls: 0.020  loss_rpn_loc: 0.055  time: 0.5399  data_time: 0.0070  lr: 0.000200  max_mem: 3067M
[32m[04/15 07:38:20 d2.utils.events]: [0m eta: 0:17:27  iter: 59  total_loss: 0.711  loss_cls: 0.236  loss_box_reg: 0.389  loss_rpn_cls: 0.025  loss_rpn_loc: 0.060  time: 0.5512  data_time: 0.0076  lr: 0.000300  max_mem: 3067M
[32m[04/15 07:38:32 d2.utils.events]: [0m eta: 0:17:35  iter: 79  total_loss: 0.652  loss_cls: 0.197  loss_box_reg: 0.328  loss_rpn_cls: 0.016  loss_rpn_loc: 0.063  time: 0.5638  data_time: 0.0087  lr: 0.000400  max_mem: 3067M
[32m[04/15 07:38:42 d2.utils.events]: [0m eta: 0:17:08  iter: 99  total_loss: 0.693  loss_cls: 0.223  loss_box_reg: 0.377  loss_rpn_cls: 0.028  loss_rpn_loc: 0.063  time: 0.5550  data_time: 0.0094  lr: 0.000500  max_mem: 3067M
[32m[04/15 07:38:54 d2.utils.events]: [0m eta: 0:16:55  iter: 119  total_loss: 0.551  loss_cls: 0.184  loss_box_reg: 0.313  loss_rpn_cls: 0.017  loss_rpn_loc: 0.051  time: 0.5542  data_time: 0.0118  lr: 0.000599  max_mem: 3067M
[32m[04/15 07:39:06 d2.utils.events]: [0m eta: 0:17:02  iter: 139  total_loss: 0.666  loss_cls: 0.214  loss_box_reg: 0.353  loss_rpn_cls: 0.018  loss_rpn_loc: 0.062  time: 0.5637  data_time: 0.0075  lr: 0.000699  max_mem: 3067M
[32m[04/15 07:39:17 d2.utils.events]: [0m eta: 0:16:50  iter: 159  total_loss: 0.657  loss_cls: 0.230  loss_box_reg: 0.359  loss_rpn_cls: 0.020  loss_rpn_loc: 0.074  time: 0.5601  data_time: 0.0089  lr: 0.000799  max_mem: 3067M
[32m[04/15 07:39:28 d2.utils.events]: [0m eta: 0:16:39  iter: 179  total_loss: 0.570  loss_cls: 0.190  loss_box_reg: 0.318  loss_rpn_cls: 0.022  loss_rpn_loc: 0.049  time: 0.5589  data_time: 0.0070  lr: 0.000899  max_mem: 3067M
[32m[04/15 07:39:40 d2.utils.events]: [0m eta: 0:16:30  iter: 199  total_loss: 0.590  loss_cls: 0.181  loss_box_reg: 0.333  loss_rpn_cls: 0.017  loss_rpn_loc: 0.052  time: 0.5611  data_time: 0.0078  lr: 0.000999  max_mem: 3067M
[32m[04/15 07:39:51 d2.utils.events]: [0m eta: 0:16:19  iter: 219  total_loss: 0.729  loss_cls: 0.241  loss_box_reg: 0.397  loss_rpn_cls: 0.026  loss_rpn_loc: 0.059  time: 0.5608  data_time: 0.0083  lr: 0.001099  max_mem: 3067M
[32m[04/15 07:40:02 d2.utils.events]: [0m eta: 0:16:06  iter: 239  total_loss: 0.591  loss_cls: 0.186  loss_box_reg: 0.341  loss_rpn_cls: 0.010  loss_rpn_loc: 0.053  time: 0.5584  data_time: 0.0067  lr: 0.001199  max_mem: 3067M
[32m[04/15 07:40:13 d2.utils.events]: [0m eta: 0:15:54  iter: 259  total_loss: 0.655  loss_cls: 0.202  loss_box_reg: 0.353  loss_rpn_cls: 0.022  loss_rpn_loc: 0.051  time: 0.5579  data_time: 0.0090  lr: 0.001299  max_mem: 3067M
[32m[04/15 07:40:27 d2.utils.events]: [0m eta: 0:15:43  iter: 279  total_loss: 0.665  loss_cls: 0.229  loss_box_reg: 0.374  loss_rpn_cls: 0.025  loss_rpn_loc: 0.060  time: 0.5704  data_time: 0.2240  lr: 0.001399  max_mem: 3067M
[32m[04/15 07:40:39 d2.utils.events]: [0m eta: 0:15:33  iter: 299  total_loss: 0.624  loss_cls: 0.214  loss_box_reg: 0.342  loss_rpn_cls: 0.016  loss_rpn_loc: 0.041  time: 0.5700  data_time: 0.0099  lr: 0.001499  max_mem: 3067M
[32m[04/15 07:40:50 d2.utils.events]: [0m eta: 0:15:23  iter: 319  total_loss: 0.610  loss_cls: 0.203  loss_box_reg: 0.352  loss_rpn_cls: 0.023  loss_rpn_loc: 0.053  time: 0.5698  data_time: 0.0082  lr: 0.001598  max_mem: 3067M
[32m[04/15 07:41:05 d2.utils.events]: [0m eta: 0:15:10  iter: 339  total_loss: 0.537  loss_cls: 0.171  loss_box_reg: 0.297  loss_rpn_cls: 0.014  loss_rpn_loc: 0.039  time: 0.5683  data_time: 0.0093  lr: 0.001698  max_mem: 3067M
[32m[04/15 07:41:17 d2.utils.events]: [0m eta: 0:15:02  iter: 359  total_loss: 0.615  loss_cls: 0.204  loss_box_reg: 0.370  loss_rpn_cls: 0.013  loss_rpn_loc: 0.043  time: 0.5712  data_time: 0.0077  lr: 0.001798  max_mem: 3067M
[32m[04/15 07:41:28 d2.utils.events]: [0m eta: 0:14:48  iter: 379  total_loss: 0.629  loss_cls: 0.209  loss_box_reg: 0.327  loss_rpn_cls: 0.017  loss_rpn_loc: 0.058  time: 0.5680  data_time: 0.0077  lr: 0.001898  max_mem: 3067M
[32m[04/15 07:41:39 d2.utils.events]: [0m eta: 0:14:37  iter: 399  total_loss: 0.575  loss_cls: 0.192  loss_box_reg: 0.297  loss_rpn_cls: 0.019  loss_rpn_loc: 0.056  time: 0.5674  data_time: 0.0069  lr: 0.001998  max_mem: 3067M
[32m[04/15 07:41:51 d2.utils.events]: [0m eta: 0:14:27  iter: 419  total_loss: 0.557  loss_cls: 0.171  loss_box_reg: 0.325  loss_rpn_cls: 0.018  loss_rpn_loc: 0.049  time: 0.5689  data_time: 0.0085  lr: 0.002098  max_mem: 3067M
[32m[04/15 07:42:02 d2.utils.events]: [0m eta: 0:14:16  iter: 439  total_loss: 0.673  loss_cls: 0.201  loss_box_reg: 0.367  loss_rpn_cls: 0.019  loss_rpn_loc: 0.079  time: 0.5673  data_time: 0.0079  lr: 0.002198  max_mem: 3067M
[32m[04/15 07:42:13 d2.utils.events]: [0m eta: 0:14:05  iter: 459  total_loss: 0.725  loss_cls: 0.238  loss_box_reg: 0.376  loss_rpn_cls: 0.018  loss_rpn_loc: 0.060  time: 0.5662  data_time: 0.0083  lr: 0.002298  max_mem: 3067M
[32m[04/15 07:42:24 d2.utils.events]: [0m eta: 0:13:55  iter: 479  total_loss: 0.603  loss_cls: 0.201  loss_box_reg: 0.324  loss_rpn_cls: 0.017  loss_rpn_loc: 0.051  time: 0.5668  data_time: 0.0079  lr: 0.002398  max_mem: 3067M
[32m[04/15 07:42:36 d2.utils.events]: [0m eta: 0:13:44  iter: 499  total_loss: 0.627  loss_cls: 0.193  loss_box_reg: 0.368  loss_rpn_cls: 0.018  loss_rpn_loc: 0.058  time: 0.5675  data_time: 0.0070  lr: 0.002498  max_mem: 3067M
[32m[04/15 07:42:47 d2.utils.events]: [0m eta: 0:13:33  iter: 519  total_loss: 0.661  loss_cls: 0.207  loss_box_reg: 0.345  loss_rpn_cls: 0.014  loss_rpn_loc: 0.050  time: 0.5667  data_time: 0.0087  lr: 0.002597  max_mem: 3067M
[32m[04/15 07:42:59 d2.utils.events]: [0m eta: 0:13:22  iter: 539  total_loss: 0.512  loss_cls: 0.181  loss_box_reg: 0.290  loss_rpn_cls: 0.012  loss_rpn_loc: 0.028  time: 0.5666  data_time: 0.0072  lr: 0.002697  max_mem: 3067M
[32m[04/15 07:43:11 d2.utils.events]: [0m eta: 0:13:12  iter: 559  total_loss: 0.654  loss_cls: 0.211  loss_box_reg: 0.339  loss_rpn_cls: 0.022  loss_rpn_loc: 0.064  time: 0.5681  data_time: 0.0076  lr: 0.002797  max_mem: 3067M
[32m[04/15 07:43:22 d2.utils.events]: [0m eta: 0:13:01  iter: 579  total_loss: 0.642  loss_cls: 0.200  loss_box_reg: 0.346  loss_rpn_cls: 0.013  loss_rpn_loc: 0.060  time: 0.5667  data_time: 0.0076  lr: 0.002897  max_mem: 3067M
[32m[04/15 07:43:33 d2.utils.events]: [0m eta: 0:12:49  iter: 599  total_loss: 0.684  loss_cls: 0.246  loss_box_reg: 0.369  loss_rpn_cls: 0.018  loss_rpn_loc: 0.062  time: 0.5665  data_time: 0.0081  lr: 0.002997  max_mem: 3067M
[32m[04/15 07:43:45 d2.utils.events]: [0m eta: 0:12:39  iter: 619  total_loss: 0.475  loss_cls: 0.132  loss_box_reg: 0.285  loss_rpn_cls: 0.009  loss_rpn_loc: 0.043  time: 0.5680  data_time: 0.0079  lr: 0.003097  max_mem: 3067M
[32m[04/15 07:43:56 d2.utils.events]: [0m eta: 0:12:27  iter: 639  total_loss: 0.644  loss_cls: 0.210  loss_box_reg: 0.374  loss_rpn_cls: 0.016  loss_rpn_loc: 0.056  time: 0.5669  data_time: 0.0080  lr: 0.003197  max_mem: 3067M
[32m[04/15 07:44:07 d2.utils.events]: [0m eta: 0:12:16  iter: 659  total_loss: 0.672  loss_cls: 0.198  loss_box_reg: 0.383  loss_rpn_cls: 0.017  loss_rpn_loc: 0.065  time: 0.5663  data_time: 0.0074  lr: 0.003297  max_mem: 3067M
[32m[04/15 07:44:19 d2.utils.events]: [0m eta: 0:12:05  iter: 679  total_loss: 0.597  loss_cls: 0.210  loss_box_reg: 0.331  loss_rpn_cls: 0.026  loss_rpn_loc: 0.056  time: 0.5671  data_time: 0.0070  lr: 0.003397  max_mem: 3067M
[32m[04/15 07:44:31 d2.utils.events]: [0m eta: 0:11:55  iter: 699  total_loss: 0.689  loss_cls: 0.233  loss_box_reg: 0.391  loss_rpn_cls: 0.026  loss_rpn_loc: 0.051  time: 0.5680  data_time: 0.0082  lr: 0.003497  max_mem: 3067M
[32m[04/15 07:44:42 d2.utils.events]: [0m eta: 0:11:44  iter: 719  total_loss: 0.594  loss_cls: 0.195  loss_box_reg: 0.325  loss_rpn_cls: 0.012  loss_rpn_loc: 0.049  time: 0.5675  data_time: 0.0065  lr: 0.003596  max_mem: 3067M
[32m[04/15 07:44:54 d2.utils.events]: [0m eta: 0:11:32  iter: 739  total_loss: 0.636  loss_cls: 0.203  loss_box_reg: 0.356  loss_rpn_cls: 0.020  loss_rpn_loc: 0.059  time: 0.5675  data_time: 0.0087  lr: 0.003696  max_mem: 3067M
[32m[04/15 07:45:06 d2.utils.events]: [0m eta: 0:11:22  iter: 759  total_loss: 0.596  loss_cls: 0.176  loss_box_reg: 0.342  loss_rpn_cls: 0.019  loss_rpn_loc: 0.040  time: 0.5679  data_time: 0.0098  lr: 0.003796  max_mem: 3067M
[32m[04/15 07:45:17 d2.utils.events]: [0m eta: 0:11:11  iter: 779  total_loss: 0.767  loss_cls: 0.228  loss_box_reg: 0.420  loss_rpn_cls: 0.018  loss_rpn_loc: 0.061  time: 0.5676  data_time: 0.0067  lr: 0.003896  max_mem: 3067M
[32m[04/15 07:45:28 d2.utils.events]: [0m eta: 0:11:00  iter: 799  total_loss: 0.590  loss_cls: 0.179  loss_box_reg: 0.311  loss_rpn_cls: 0.014  loss_rpn_loc: 0.045  time: 0.5674  data_time: 0.0069  lr: 0.003996  max_mem: 3067M
[32m[04/15 07:45:40 d2.utils.events]: [0m eta: 0:10:49  iter: 819  total_loss: 0.631  loss_cls: 0.200  loss_box_reg: 0.355  loss_rpn_cls: 0.021  loss_rpn_loc: 0.055  time: 0.5681  data_time: 0.0085  lr: 0.004096  max_mem: 3067M
[32m[04/15 07:45:51 d2.utils.events]: [0m eta: 0:10:38  iter: 839  total_loss: 0.584  loss_cls: 0.180  loss_box_reg: 0.311  loss_rpn_cls: 0.018  loss_rpn_loc: 0.042  time: 0.5674  data_time: 0.0081  lr: 0.004196  max_mem: 3067M
[32m[04/15 07:46:02 d2.utils.events]: [0m eta: 0:10:26  iter: 859  total_loss: 0.542  loss_cls: 0.178  loss_box_reg: 0.311  loss_rpn_cls: 0.016  loss_rpn_loc: 0.037  time: 0.5667  data_time: 0.0071  lr: 0.004296  max_mem: 3067M
[32m[04/15 07:46:14 d2.utils.events]: [0m eta: 0:10:16  iter: 879  total_loss: 0.643  loss_cls: 0.199  loss_box_reg: 0.382  loss_rpn_cls: 0.016  loss_rpn_loc: 0.065  time: 0.5680  data_time: 0.0072  lr: 0.004396  max_mem: 3067M
[32m[04/15 07:46:25 d2.utils.events]: [0m eta: 0:10:05  iter: 899  total_loss: 0.558  loss_cls: 0.188  loss_box_reg: 0.307  loss_rpn_cls: 0.015  loss_rpn_loc: 0.051  time: 0.5676  data_time: 0.0080  lr: 0.004496  max_mem: 3067M
[32m[04/15 07:46:36 d2.utils.events]: [0m eta: 0:09:54  iter: 919  total_loss: 0.544  loss_cls: 0.178  loss_box_reg: 0.305  loss_rpn_cls: 0.016  loss_rpn_loc: 0.051  time: 0.5669  data_time: 0.0071  lr: 0.004595  max_mem: 3067M
[32m[04/15 07:46:48 d2.utils.events]: [0m eta: 0:09:43  iter: 939  total_loss: 0.661  loss_cls: 0.206  loss_box_reg: 0.359  loss_rpn_cls: 0.024  loss_rpn_loc: 0.059  time: 0.5674  data_time: 0.0088  lr: 0.004695  max_mem: 3067M
[32m[04/15 07:46:59 d2.utils.events]: [0m eta: 0:09:31  iter: 959  total_loss: 0.588  loss_cls: 0.193  loss_box_reg: 0.309  loss_rpn_cls: 0.017  loss_rpn_loc: 0.059  time: 0.5670  data_time: 0.0078  lr: 0.004795  max_mem: 3067M
[32m[04/15 07:47:10 d2.utils.events]: [0m eta: 0:09:20  iter: 979  total_loss: 0.668  loss_cls: 0.210  loss_box_reg: 0.373  loss_rpn_cls: 0.014  loss_rpn_loc: 0.072  time: 0.5668  data_time: 0.0082  lr: 0.004895  max_mem: 3067M
[32m[04/15 07:47:21 d2.utils.events]: [0m eta: 0:09:09  iter: 999  total_loss: 0.592  loss_cls: 0.205  loss_box_reg: 0.325  loss_rpn_cls: 0.018  loss_rpn_loc: 0.044  time: 0.5665  data_time: 0.0070  lr: 0.004995  max_mem: 3067M
[32m[04/15 07:47:34 d2.utils.events]: [0m eta: 0:08:59  iter: 1019  total_loss: 0.579  loss_cls: 0.201  loss_box_reg: 0.338  loss_rpn_cls: 0.019  loss_rpn_loc: 0.052  time: 0.5674  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:47:45 d2.utils.events]: [0m eta: 0:08:48  iter: 1039  total_loss: 0.624  loss_cls: 0.229  loss_box_reg: 0.338  loss_rpn_cls: 0.017  loss_rpn_loc: 0.056  time: 0.5669  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:47:55 d2.utils.events]: [0m eta: 0:08:37  iter: 1059  total_loss: 0.489  loss_cls: 0.164  loss_box_reg: 0.247  loss_rpn_cls: 0.014  loss_rpn_loc: 0.041  time: 0.5661  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:48:08 d2.utils.events]: [0m eta: 0:08:26  iter: 1079  total_loss: 0.764  loss_cls: 0.246  loss_box_reg: 0.429  loss_rpn_cls: 0.023  loss_rpn_loc: 0.072  time: 0.5669  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:48:19 d2.utils.events]: [0m eta: 0:08:15  iter: 1099  total_loss: 0.615  loss_cls: 0.201  loss_box_reg: 0.326  loss_rpn_cls: 0.015  loss_rpn_loc: 0.070  time: 0.5666  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:48:30 d2.utils.events]: [0m eta: 0:08:04  iter: 1119  total_loss: 0.663  loss_cls: 0.223  loss_box_reg: 0.366  loss_rpn_cls: 0.017  loss_rpn_loc: 0.053  time: 0.5664  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:48:41 d2.utils.events]: [0m eta: 0:07:53  iter: 1139  total_loss: 0.770  loss_cls: 0.221  loss_box_reg: 0.403  loss_rpn_cls: 0.028  loss_rpn_loc: 0.074  time: 0.5665  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:48:53 d2.utils.events]: [0m eta: 0:07:42  iter: 1159  total_loss: 0.639  loss_cls: 0.209  loss_box_reg: 0.372  loss_rpn_cls: 0.021  loss_rpn_loc: 0.067  time: 0.5665  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:49:04 d2.utils.events]: [0m eta: 0:07:31  iter: 1179  total_loss: 0.669  loss_cls: 0.227  loss_box_reg: 0.330  loss_rpn_cls: 0.021  loss_rpn_loc: 0.067  time: 0.5663  data_time: 0.0102  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:49:16 d2.utils.events]: [0m eta: 0:07:20  iter: 1199  total_loss: 0.684  loss_cls: 0.216  loss_box_reg: 0.380  loss_rpn_cls: 0.019  loss_rpn_loc: 0.066  time: 0.5665  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:49:27 d2.utils.events]: [0m eta: 0:07:09  iter: 1219  total_loss: 0.651  loss_cls: 0.194  loss_box_reg: 0.337  loss_rpn_cls: 0.023  loss_rpn_loc: 0.071  time: 0.5667  data_time: 0.0092  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:49:38 d2.utils.events]: [0m eta: 0:06:58  iter: 1239  total_loss: 0.565  loss_cls: 0.190  loss_box_reg: 0.303  loss_rpn_cls: 0.015  loss_rpn_loc: 0.036  time: 0.5663  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:49:49 d2.utils.events]: [0m eta: 0:06:47  iter: 1259  total_loss: 0.635  loss_cls: 0.199  loss_box_reg: 0.338  loss_rpn_cls: 0.017  loss_rpn_loc: 0.067  time: 0.5659  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:50:01 d2.utils.events]: [0m eta: 0:06:36  iter: 1279  total_loss: 0.715  loss_cls: 0.248  loss_box_reg: 0.365  loss_rpn_cls: 0.020  loss_rpn_loc: 0.059  time: 0.5665  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:50:13 d2.utils.events]: [0m eta: 0:06:25  iter: 1299  total_loss: 0.689  loss_cls: 0.231  loss_box_reg: 0.400  loss_rpn_cls: 0.014  loss_rpn_loc: 0.058  time: 0.5666  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:50:24 d2.utils.events]: [0m eta: 0:06:14  iter: 1319  total_loss: 0.675  loss_cls: 0.218  loss_box_reg: 0.361  loss_rpn_cls: 0.018  loss_rpn_loc: 0.059  time: 0.5663  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:50:36 d2.utils.events]: [0m eta: 0:06:04  iter: 1339  total_loss: 0.658  loss_cls: 0.226  loss_box_reg: 0.362  loss_rpn_cls: 0.023  loss_rpn_loc: 0.067  time: 0.5671  data_time: 0.0103  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:50:48 d2.utils.events]: [0m eta: 0:05:52  iter: 1359  total_loss: 0.520  loss_cls: 0.173  loss_box_reg: 0.269  loss_rpn_cls: 0.023  loss_rpn_loc: 0.053  time: 0.5670  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:50:59 d2.utils.events]: [0m eta: 0:05:42  iter: 1379  total_loss: 0.551  loss_cls: 0.183  loss_box_reg: 0.296  loss_rpn_cls: 0.023  loss_rpn_loc: 0.042  time: 0.5667  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:51:11 d2.utils.events]: [0m eta: 0:05:32  iter: 1399  total_loss: 0.682  loss_cls: 0.212  loss_box_reg: 0.368  loss_rpn_cls: 0.014  loss_rpn_loc: 0.060  time: 0.5673  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:51:22 d2.utils.events]: [0m eta: 0:05:20  iter: 1419  total_loss: 0.656  loss_cls: 0.221  loss_box_reg: 0.358  loss_rpn_cls: 0.018  loss_rpn_loc: 0.060  time: 0.5672  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:51:33 d2.utils.events]: [0m eta: 0:05:09  iter: 1439  total_loss: 0.649  loss_cls: 0.214  loss_box_reg: 0.356  loss_rpn_cls: 0.022  loss_rpn_loc: 0.056  time: 0.5669  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:51:45 d2.utils.events]: [0m eta: 0:04:58  iter: 1459  total_loss: 0.673  loss_cls: 0.218  loss_box_reg: 0.372  loss_rpn_cls: 0.017  loss_rpn_loc: 0.062  time: 0.5672  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:51:56 d2.utils.events]: [0m eta: 0:04:47  iter: 1479  total_loss: 0.602  loss_cls: 0.216  loss_box_reg: 0.325  loss_rpn_cls: 0.025  loss_rpn_loc: 0.056  time: 0.5672  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:52:08 d2.utils.events]: [0m eta: 0:04:36  iter: 1499  total_loss: 0.657  loss_cls: 0.199  loss_box_reg: 0.363  loss_rpn_cls: 0.019  loss_rpn_loc: 0.070  time: 0.5670  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:52:19 d2.utils.events]: [0m eta: 0:04:25  iter: 1519  total_loss: 0.732  loss_cls: 0.231  loss_box_reg: 0.394  loss_rpn_cls: 0.021  loss_rpn_loc: 0.060  time: 0.5671  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:52:31 d2.utils.events]: [0m eta: 0:04:14  iter: 1539  total_loss: 0.685  loss_cls: 0.219  loss_box_reg: 0.392  loss_rpn_cls: 0.018  loss_rpn_loc: 0.066  time: 0.5672  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:52:42 d2.utils.events]: [0m eta: 0:04:02  iter: 1559  total_loss: 0.580  loss_cls: 0.191  loss_box_reg: 0.323  loss_rpn_cls: 0.020  loss_rpn_loc: 0.043  time: 0.5669  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:52:52 d2.utils.events]: [0m eta: 0:03:51  iter: 1579  total_loss: 0.691  loss_cls: 0.242  loss_box_reg: 0.351  loss_rpn_cls: 0.032  loss_rpn_loc: 0.049  time: 0.5664  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:53:04 d2.utils.events]: [0m eta: 0:03:40  iter: 1599  total_loss: 0.628  loss_cls: 0.190  loss_box_reg: 0.335  loss_rpn_cls: 0.022  loss_rpn_loc: 0.059  time: 0.5667  data_time: 0.0100  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:53:16 d2.utils.events]: [0m eta: 0:03:29  iter: 1619  total_loss: 0.684  loss_cls: 0.220  loss_box_reg: 0.357  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5670  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:53:27 d2.utils.events]: [0m eta: 0:03:18  iter: 1639  total_loss: 0.606  loss_cls: 0.189  loss_box_reg: 0.325  loss_rpn_cls: 0.014  loss_rpn_loc: 0.056  time: 0.5665  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:53:39 d2.utils.events]: [0m eta: 0:03:07  iter: 1659  total_loss: 0.656  loss_cls: 0.217  loss_box_reg: 0.372  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5667  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:53:50 d2.utils.events]: [0m eta: 0:02:56  iter: 1679  total_loss: 0.715  loss_cls: 0.242  loss_box_reg: 0.364  loss_rpn_cls: 0.019  loss_rpn_loc: 0.047  time: 0.5667  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:54:01 d2.utils.events]: [0m eta: 0:02:45  iter: 1699  total_loss: 0.623  loss_cls: 0.190  loss_box_reg: 0.366  loss_rpn_cls: 0.020  loss_rpn_loc: 0.065  time: 0.5663  data_time: 0.0062  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:54:13 d2.utils.events]: [0m eta: 0:02:34  iter: 1719  total_loss: 0.737  loss_cls: 0.249  loss_box_reg: 0.392  loss_rpn_cls: 0.016  loss_rpn_loc: 0.048  time: 0.5668  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:54:25 d2.utils.events]: [0m eta: 0:02:23  iter: 1739  total_loss: 0.567  loss_cls: 0.210  loss_box_reg: 0.294  loss_rpn_cls: 0.018  loss_rpn_loc: 0.051  time: 0.5670  data_time: 0.0092  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:54:36 d2.utils.events]: [0m eta: 0:02:12  iter: 1759  total_loss: 0.616  loss_cls: 0.205  loss_box_reg: 0.348  loss_rpn_cls: 0.025  loss_rpn_loc: 0.054  time: 0.5669  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:54:47 d2.utils.events]: [0m eta: 0:02:01  iter: 1779  total_loss: 0.621  loss_cls: 0.195  loss_box_reg: 0.328  loss_rpn_cls: 0.024  loss_rpn_loc: 0.046  time: 0.5667  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:54:59 d2.utils.events]: [0m eta: 0:01:50  iter: 1799  total_loss: 0.573  loss_cls: 0.179  loss_box_reg: 0.331  loss_rpn_cls: 0.018  loss_rpn_loc: 0.045  time: 0.5670  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:55:10 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.718  loss_cls: 0.225  loss_box_reg: 0.324  loss_rpn_cls: 0.030  loss_rpn_loc: 0.073  time: 0.5668  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:55:21 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.687  loss_cls: 0.202  loss_box_reg: 0.361  loss_rpn_cls: 0.024  loss_rpn_loc: 0.056  time: 0.5668  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:55:34 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.616  loss_cls: 0.204  loss_box_reg: 0.337  loss_rpn_cls: 0.020  loss_rpn_loc: 0.050  time: 0.5673  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:55:45 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.661  loss_cls: 0.200  loss_box_reg: 0.350  loss_rpn_cls: 0.017  loss_rpn_loc: 0.056  time: 0.5672  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:55:56 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.721  loss_cls: 0.236  loss_box_reg: 0.386  loss_rpn_cls: 0.025  loss_rpn_loc: 0.058  time: 0.5671  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:56:08 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.606  loss_cls: 0.192  loss_box_reg: 0.313  loss_rpn_cls: 0.021  loss_rpn_loc: 0.076  time: 0.5670  data_time: 0.0113  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:56:19 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.789  loss_cls: 0.234  loss_box_reg: 0.422  loss_rpn_cls: 0.021  loss_rpn_loc: 0.077  time: 0.5670  data_time: 0.0494  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:56:30 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.642  loss_cls: 0.204  loss_box_reg: 0.340  loss_rpn_cls: 0.019  loss_rpn_loc: 0.040  time: 0.5668  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:56:42 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.759  loss_cls: 0.255  loss_box_reg: 0.411  loss_rpn_cls: 0.016  loss_rpn_loc: 0.059  time: 0.5670  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:57:13 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json takes 1.30 seconds.
[5m[31mWARNING[0m [32m[04/15 07:57:13 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 07:57:13 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 07:57:13 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 07:57:13 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.639  loss_cls: 0.207  loss_box_reg: 0.366  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5672  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 07:57:15 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:53 (0.5675 s / it)
[32m[04/15 07:57:15 d2.engine.hooks]: [0mTotal training time: 0:19:27 (0:00:34 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 07:57:24 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 07:57:24 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 07:57:24 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 07:57:30 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1229 s / img. ETA=0:03:29
[32m[04/15 07:57:36 d2.evaluation.evaluator]: [0mInference done 39/1257. 0.1200 s / img. ETA=0:03:36
[32m[04/15 07:57:41 d2.evaluation.evaluator]: [0mInference done 65/1257. 0.1216 s / img. ETA=0:03:47
[32m[04/15 07:57:46 d2.evaluation.evaluator]: [0mInference done 99/1257. 0.1208 s / img. ETA=0:03:23
[32m[04/15 07:57:51 d2.evaluation.evaluator]: [0mInference done 129/1257. 0.1239 s / img. ETA=0:03:19
[32m[04/15 07:57:57 d2.evaluation.evaluator]: [0mInference done 151/1257. 0.1230 s / img. ETA=0:03:27
[32m[04/15 07:58:02 d2.evaluation.evaluator]: [0mInference done 178/1257. 0.1223 s / img. ETA=0:03:22
[32m[04/15 07:58:07 d2.evaluation.evaluator]: [0mInference done 213/1257. 0.1249 s / img. ETA=0:03:08
[32m[04/15 07:58:12 d2.evaluation.evaluator]: [0mInference done 238/1257. 0.1265 s / img. ETA=0:03:06
[32m[04/15 07:58:17 d2.evaluation.evaluator]: [0mInference done 268/1257. 0.1257 s / img. ETA=0:02:59
[32m[04/15 07:58:22 d2.evaluation.evaluator]: [0mInference done 305/1257. 0.1257 s / img. ETA=0:02:47
[32m[04/15 07:58:27 d2.evaluation.evaluator]: [0mInference done 340/1257. 0.1262 s / img. ETA=0:02:38
[32m[04/15 07:58:32 d2.evaluation.evaluator]: [0mInference done 378/1257. 0.1256 s / img. ETA=0:02:28
[32m[04/15 07:58:37 d2.evaluation.evaluator]: [0mInference done 415/1257. 0.1251 s / img. ETA=0:02:19
[32m[04/15 07:58:42 d2.evaluation.evaluator]: [0mInference done 443/1257. 0.1246 s / img. ETA=0:02:15
[32m[04/15 07:58:47 d2.evaluation.evaluator]: [0mInference done 470/1257. 0.1250 s / img. ETA=0:02:12
[32m[04/15 07:58:53 d2.evaluation.evaluator]: [0mInference done 502/1257. 0.1253 s / img. ETA=0:02:06
[32m[04/15 07:58:58 d2.evaluation.evaluator]: [0mInference done 534/1257. 0.1251 s / img. ETA=0:02:00
[32m[04/15 07:59:03 d2.evaluation.evaluator]: [0mInference done 566/1257. 0.1253 s / img. ETA=0:01:55
[32m[04/15 07:59:08 d2.evaluation.evaluator]: [0mInference done 601/1257. 0.1255 s / img. ETA=0:01:48
[32m[04/15 07:59:13 d2.evaluation.evaluator]: [0mInference done 628/1257. 0.1253 s / img. ETA=0:01:44
[32m[04/15 07:59:18 d2.evaluation.evaluator]: [0mInference done 653/1257. 0.1250 s / img. ETA=0:01:41
[32m[04/15 07:59:23 d2.evaluation.evaluator]: [0mInference done 683/1257. 0.1249 s / img. ETA=0:01:36
[32m[04/15 07:59:28 d2.evaluation.evaluator]: [0mInference done 717/1257. 0.1257 s / img. ETA=0:01:30
[32m[04/15 07:59:33 d2.evaluation.evaluator]: [0mInference done 751/1257. 0.1262 s / img. ETA=0:01:24
[32m[04/15 07:59:38 d2.evaluation.evaluator]: [0mInference done 791/1257. 0.1259 s / img. ETA=0:01:16
[32m[04/15 07:59:43 d2.evaluation.evaluator]: [0mInference done 830/1257. 0.1260 s / img. ETA=0:01:09
[32m[04/15 07:59:48 d2.evaluation.evaluator]: [0mInference done 867/1257. 0.1262 s / img. ETA=0:01:02
[32m[04/15 07:59:54 d2.evaluation.evaluator]: [0mInference done 906/1257. 0.1260 s / img. ETA=0:00:56
[32m[04/15 07:59:59 d2.evaluation.evaluator]: [0mInference done 945/1257. 0.1258 s / img. ETA=0:00:49
[32m[04/15 08:00:04 d2.evaluation.evaluator]: [0mInference done 976/1257. 0.1261 s / img. ETA=0:00:44
[32m[04/15 08:00:09 d2.evaluation.evaluator]: [0mInference done 1010/1257. 0.1267 s / img. ETA=0:00:39
[32m[04/15 08:00:14 d2.evaluation.evaluator]: [0mInference done 1046/1257. 0.1268 s / img. ETA=0:00:33
[32m[04/15 08:00:19 d2.evaluation.evaluator]: [0mInference done 1082/1257. 0.1267 s / img. ETA=0:00:27
[32m[04/15 08:00:24 d2.evaluation.evaluator]: [0mInference done 1115/1257. 0.1267 s / img. ETA=0:00:22
[32m[04/15 08:00:29 d2.evaluation.evaluator]: [0mInference done 1142/1257. 0.1265 s / img. ETA=0:00:18
[32m[04/15 08:00:34 d2.evaluation.evaluator]: [0mInference done 1175/1257. 0.1263 s / img. ETA=0:00:12
[32m[04/15 08:00:39 d2.evaluation.evaluator]: [0mInference done 1202/1257. 0.1261 s / img. ETA=0:00:08
[32m[04/15 08:00:45 d2.evaluation.evaluator]: [0mInference done 1231/1257. 0.1264 s / img. ETA=0:00:04
[32m[04/15 08:00:50 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:20.434352 (0.160091 s / img per device, on 1 devices)
[32m[04/15 08:00:50 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:38 (0.126364 s / img per device, on 1 devices)
[32m[04/15 08:00:50 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 08:00:50 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 08:00:50 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.05s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.89s).
Accumulating evaluation results...
DONE (t=1.03s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.205
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.418
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.169
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.137
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.245
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.439
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.104
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.478
[32m[04/15 08:00:58 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 20.468 | 41.842 | 16.934 | 13.676 | 24.528 | 43.861 |
[32m[04/15 08:00:58 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP     | category       | AP     |
|:--------------|:-------|:--------------|:-------|:---------------|:-------|
| person        | 28.494 | bicycle       | 10.501 | car            | 42.876 |
| motorcycle    | nan    | airplane      | nan    | bus            | nan    |
| train         | nan    | truck         | nan    | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan    | stop sign      | nan    |
| parking meter | nan    | bench         | nan    | bird           | nan    |
| cat           | nan    | dog           | 0.000  | horse          | nan    |
| sheep         | nan    | cow           | nan    | elephant       | nan    |
| bear          | nan    | zebra         | nan    | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan    | handbag        | nan    |
| tie           | nan    | suitcase      | nan    | frisbee        | nan    |
| skis          | nan    | snowboard     | nan    | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan    | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan    | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan    | cup            | nan    |
| fork          | nan    | knife         | nan    | spoon          | nan    |
| bowl          | nan    | banana        | nan    | apple          | nan    |
| sandwich      | nan    | orange        | nan    | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan    | pizza          | nan    |
| donut         | nan    | cake          | nan    | chair          | nan    |
| couch         | nan    | potted plant  | nan    | bed            | nan    |
| dining table  | nan    | toilet        | nan    | tv             | nan    |
| laptop        | nan    | mouse         | nan    | remote         | nan    |
| keyboard      | nan    | cell phone    | nan    | microwave      | nan    |
| oven          | nan    | bike          | nan    | hydrant        | nan    |
| motor         | nan    | rider         | nan    | light          | nan    |
| sign          | nan    | motor vehicle | nan    | human face     | nan    |
| hair drier    | nan    | license plate | nan    |                |        |
============== The  12  *  2000  iterations ============
4 channel input
[32m[04/15 08:01:00 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 08:01:01 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.76 seconds.
[5m[31mWARNING[0m [32m[04/15 08:01:01 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:01:01 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 08:01:02 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 08:01:03 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 08:01:03 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 08:01:03 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 08:01:15 d2.utils.events]: [0m eta: 0:17:54  iter: 19  total_loss: 0.667  loss_cls: 0.217  loss_box_reg: 0.345  loss_rpn_cls: 0.021  loss_rpn_loc: 0.068  time: 0.5389  data_time: 0.0463  lr: 0.000100  max_mem: 3067M
[32m[04/15 08:01:27 d2.utils.events]: [0m eta: 0:18:15  iter: 39  total_loss: 0.584  loss_cls: 0.200  loss_box_reg: 0.343  loss_rpn_cls: 0.011  loss_rpn_loc: 0.044  time: 0.5744  data_time: 0.0110  lr: 0.000200  max_mem: 3067M
[32m[04/15 08:01:38 d2.utils.events]: [0m eta: 0:17:59  iter: 59  total_loss: 0.609  loss_cls: 0.205  loss_box_reg: 0.324  loss_rpn_cls: 0.019  loss_rpn_loc: 0.044  time: 0.5713  data_time: 0.0075  lr: 0.000300  max_mem: 3067M
[32m[04/15 08:01:50 d2.utils.events]: [0m eta: 0:17:55  iter: 79  total_loss: 0.519  loss_cls: 0.184  loss_box_reg: 0.288  loss_rpn_cls: 0.022  loss_rpn_loc: 0.045  time: 0.5703  data_time: 0.0068  lr: 0.000400  max_mem: 3067M
[32m[04/15 08:02:00 d2.utils.events]: [0m eta: 0:17:35  iter: 99  total_loss: 0.502  loss_cls: 0.160  loss_box_reg: 0.287  loss_rpn_cls: 0.013  loss_rpn_loc: 0.045  time: 0.5644  data_time: 0.0074  lr: 0.000500  max_mem: 3067M
[32m[04/15 08:02:13 d2.utils.events]: [0m eta: 0:17:31  iter: 119  total_loss: 0.759  loss_cls: 0.230  loss_box_reg: 0.430  loss_rpn_cls: 0.018  loss_rpn_loc: 0.058  time: 0.5705  data_time: 0.0116  lr: 0.000599  max_mem: 3067M
[32m[04/15 08:02:24 d2.utils.events]: [0m eta: 0:17:20  iter: 139  total_loss: 0.638  loss_cls: 0.224  loss_box_reg: 0.333  loss_rpn_cls: 0.014  loss_rpn_loc: 0.046  time: 0.5717  data_time: 0.0067  lr: 0.000699  max_mem: 3067M
[32m[04/15 08:02:35 d2.utils.events]: [0m eta: 0:17:11  iter: 159  total_loss: 0.617  loss_cls: 0.218  loss_box_reg: 0.315  loss_rpn_cls: 0.017  loss_rpn_loc: 0.049  time: 0.5703  data_time: 0.0075  lr: 0.000799  max_mem: 3067M
[32m[04/15 08:02:48 d2.utils.events]: [0m eta: 0:17:05  iter: 179  total_loss: 0.619  loss_cls: 0.198  loss_box_reg: 0.355  loss_rpn_cls: 0.016  loss_rpn_loc: 0.057  time: 0.5738  data_time: 0.0130  lr: 0.000899  max_mem: 3067M
[32m[04/15 08:02:58 d2.utils.events]: [0m eta: 0:16:47  iter: 199  total_loss: 0.589  loss_cls: 0.196  loss_box_reg: 0.321  loss_rpn_cls: 0.014  loss_rpn_loc: 0.053  time: 0.5699  data_time: 0.0075  lr: 0.000999  max_mem: 3067M
[32m[04/15 08:03:09 d2.utils.events]: [0m eta: 0:16:32  iter: 219  total_loss: 0.654  loss_cls: 0.197  loss_box_reg: 0.373  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5680  data_time: 0.0069  lr: 0.001099  max_mem: 3067M
[32m[04/15 08:03:21 d2.utils.events]: [0m eta: 0:16:20  iter: 239  total_loss: 0.745  loss_cls: 0.244  loss_box_reg: 0.402  loss_rpn_cls: 0.028  loss_rpn_loc: 0.061  time: 0.5671  data_time: 0.0090  lr: 0.001199  max_mem: 3067M
[32m[04/15 08:03:32 d2.utils.events]: [0m eta: 0:16:10  iter: 259  total_loss: 0.571  loss_cls: 0.183  loss_box_reg: 0.305  loss_rpn_cls: 0.014  loss_rpn_loc: 0.038  time: 0.5682  data_time: 0.0091  lr: 0.001299  max_mem: 3067M
[32m[04/15 08:03:44 d2.utils.events]: [0m eta: 0:15:59  iter: 279  total_loss: 0.720  loss_cls: 0.228  loss_box_reg: 0.424  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5681  data_time: 0.0065  lr: 0.001399  max_mem: 3067M
[32m[04/15 08:03:55 d2.utils.events]: [0m eta: 0:15:47  iter: 299  total_loss: 0.532  loss_cls: 0.172  loss_box_reg: 0.297  loss_rpn_cls: 0.013  loss_rpn_loc: 0.056  time: 0.5665  data_time: 0.0071  lr: 0.001499  max_mem: 3067M
[32m[04/15 08:04:07 d2.utils.events]: [0m eta: 0:15:40  iter: 319  total_loss: 0.501  loss_cls: 0.150  loss_box_reg: 0.287  loss_rpn_cls: 0.013  loss_rpn_loc: 0.037  time: 0.5694  data_time: 0.0071  lr: 0.001598  max_mem: 3067M
[32m[04/15 08:04:19 d2.utils.events]: [0m eta: 0:15:28  iter: 339  total_loss: 0.671  loss_cls: 0.225  loss_box_reg: 0.384  loss_rpn_cls: 0.014  loss_rpn_loc: 0.065  time: 0.5695  data_time: 0.0075  lr: 0.001698  max_mem: 3067M
[32m[04/15 08:04:29 d2.utils.events]: [0m eta: 0:15:13  iter: 359  total_loss: 0.664  loss_cls: 0.194  loss_box_reg: 0.361  loss_rpn_cls: 0.016  loss_rpn_loc: 0.062  time: 0.5668  data_time: 0.0057  lr: 0.001798  max_mem: 3067M
[32m[04/15 08:04:41 d2.utils.events]: [0m eta: 0:15:03  iter: 379  total_loss: 0.599  loss_cls: 0.186  loss_box_reg: 0.323  loss_rpn_cls: 0.013  loss_rpn_loc: 0.051  time: 0.5685  data_time: 0.0076  lr: 0.001898  max_mem: 3067M
[32m[04/15 08:04:53 d2.utils.events]: [0m eta: 0:14:52  iter: 399  total_loss: 0.581  loss_cls: 0.190  loss_box_reg: 0.340  loss_rpn_cls: 0.014  loss_rpn_loc: 0.050  time: 0.5682  data_time: 0.0125  lr: 0.001998  max_mem: 3067M
[32m[04/15 08:05:04 d2.utils.events]: [0m eta: 0:14:41  iter: 419  total_loss: 0.737  loss_cls: 0.226  loss_box_reg: 0.403  loss_rpn_cls: 0.015  loss_rpn_loc: 0.053  time: 0.5677  data_time: 0.0062  lr: 0.002098  max_mem: 3067M
[32m[04/15 08:05:16 d2.utils.events]: [0m eta: 0:14:32  iter: 439  total_loss: 0.525  loss_cls: 0.175  loss_box_reg: 0.293  loss_rpn_cls: 0.012  loss_rpn_loc: 0.044  time: 0.5688  data_time: 0.0076  lr: 0.002198  max_mem: 3067M
[32m[04/15 08:05:27 d2.utils.events]: [0m eta: 0:14:21  iter: 459  total_loss: 0.647  loss_cls: 0.212  loss_box_reg: 0.354  loss_rpn_cls: 0.010  loss_rpn_loc: 0.043  time: 0.5692  data_time: 0.0078  lr: 0.002298  max_mem: 3067M
[32m[04/15 08:05:39 d2.utils.events]: [0m eta: 0:14:11  iter: 479  total_loss: 0.642  loss_cls: 0.197  loss_box_reg: 0.351  loss_rpn_cls: 0.019  loss_rpn_loc: 0.052  time: 0.5695  data_time: 0.0079  lr: 0.002398  max_mem: 3067M
[32m[04/15 08:05:51 d2.utils.events]: [0m eta: 0:14:00  iter: 499  total_loss: 0.653  loss_cls: 0.204  loss_box_reg: 0.369  loss_rpn_cls: 0.017  loss_rpn_loc: 0.048  time: 0.5699  data_time: 0.0090  lr: 0.002498  max_mem: 3067M
[32m[04/15 08:06:02 d2.utils.events]: [0m eta: 0:13:48  iter: 519  total_loss: 0.697  loss_cls: 0.201  loss_box_reg: 0.382  loss_rpn_cls: 0.015  loss_rpn_loc: 0.061  time: 0.5692  data_time: 0.0089  lr: 0.002597  max_mem: 3067M
[32m[04/15 08:06:13 d2.utils.events]: [0m eta: 0:13:36  iter: 539  total_loss: 0.594  loss_cls: 0.186  loss_box_reg: 0.336  loss_rpn_cls: 0.018  loss_rpn_loc: 0.053  time: 0.5690  data_time: 0.0067  lr: 0.002697  max_mem: 3067M
[32m[04/15 08:06:24 d2.utils.events]: [0m eta: 0:13:25  iter: 559  total_loss: 0.710  loss_cls: 0.221  loss_box_reg: 0.381  loss_rpn_cls: 0.011  loss_rpn_loc: 0.054  time: 0.5680  data_time: 0.0093  lr: 0.002797  max_mem: 3067M
[32m[04/15 08:06:36 d2.utils.events]: [0m eta: 0:13:13  iter: 579  total_loss: 0.705  loss_cls: 0.208  loss_box_reg: 0.386  loss_rpn_cls: 0.013  loss_rpn_loc: 0.065  time: 0.5686  data_time: 0.0083  lr: 0.002897  max_mem: 3067M
[32m[04/15 08:06:47 d2.utils.events]: [0m eta: 0:13:02  iter: 599  total_loss: 0.600  loss_cls: 0.182  loss_box_reg: 0.319  loss_rpn_cls: 0.020  loss_rpn_loc: 0.041  time: 0.5682  data_time: 0.0066  lr: 0.002997  max_mem: 3067M
[32m[04/15 08:06:58 d2.utils.events]: [0m eta: 0:12:48  iter: 619  total_loss: 0.601  loss_cls: 0.197  loss_box_reg: 0.356  loss_rpn_cls: 0.020  loss_rpn_loc: 0.062  time: 0.5665  data_time: 0.0066  lr: 0.003097  max_mem: 3067M
[32m[04/15 08:07:10 d2.utils.events]: [0m eta: 0:12:37  iter: 639  total_loss: 0.669  loss_cls: 0.196  loss_box_reg: 0.367  loss_rpn_cls: 0.020  loss_rpn_loc: 0.064  time: 0.5676  data_time: 0.0080  lr: 0.003197  max_mem: 3067M
[32m[04/15 08:07:21 d2.utils.events]: [0m eta: 0:12:25  iter: 659  total_loss: 0.647  loss_cls: 0.211  loss_box_reg: 0.351  loss_rpn_cls: 0.027  loss_rpn_loc: 0.057  time: 0.5671  data_time: 0.0064  lr: 0.003297  max_mem: 3067M
[32m[04/15 08:07:32 d2.utils.events]: [0m eta: 0:12:14  iter: 679  total_loss: 0.587  loss_cls: 0.183  loss_box_reg: 0.328  loss_rpn_cls: 0.012  loss_rpn_loc: 0.053  time: 0.5665  data_time: 0.0071  lr: 0.003397  max_mem: 3067M
[32m[04/15 08:07:44 d2.utils.events]: [0m eta: 0:12:03  iter: 699  total_loss: 0.562  loss_cls: 0.185  loss_box_reg: 0.322  loss_rpn_cls: 0.016  loss_rpn_loc: 0.054  time: 0.5672  data_time: 0.0069  lr: 0.003497  max_mem: 3067M
[32m[04/15 08:07:55 d2.utils.events]: [0m eta: 0:11:52  iter: 719  total_loss: 0.587  loss_cls: 0.179  loss_box_reg: 0.328  loss_rpn_cls: 0.016  loss_rpn_loc: 0.055  time: 0.5670  data_time: 0.0080  lr: 0.003596  max_mem: 3067M
[32m[04/15 08:08:07 d2.utils.events]: [0m eta: 0:11:42  iter: 739  total_loss: 0.558  loss_cls: 0.185  loss_box_reg: 0.300  loss_rpn_cls: 0.015  loss_rpn_loc: 0.043  time: 0.5674  data_time: 0.0064  lr: 0.003696  max_mem: 3067M
[32m[04/15 08:08:18 d2.utils.events]: [0m eta: 0:11:31  iter: 759  total_loss: 0.538  loss_cls: 0.169  loss_box_reg: 0.320  loss_rpn_cls: 0.013  loss_rpn_loc: 0.059  time: 0.5677  data_time: 0.0069  lr: 0.003796  max_mem: 3067M
[32m[04/15 08:08:30 d2.utils.events]: [0m eta: 0:11:21  iter: 779  total_loss: 0.727  loss_cls: 0.229  loss_box_reg: 0.398  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5679  data_time: 0.0072  lr: 0.003896  max_mem: 3067M
[32m[04/15 08:08:41 d2.utils.events]: [0m eta: 0:11:08  iter: 799  total_loss: 0.644  loss_cls: 0.219  loss_box_reg: 0.329  loss_rpn_cls: 0.015  loss_rpn_loc: 0.056  time: 0.5679  data_time: 0.0073  lr: 0.003996  max_mem: 3067M
[32m[04/15 08:08:53 d2.utils.events]: [0m eta: 0:10:57  iter: 819  total_loss: 0.710  loss_cls: 0.220  loss_box_reg: 0.385  loss_rpn_cls: 0.018  loss_rpn_loc: 0.060  time: 0.5678  data_time: 0.0072  lr: 0.004096  max_mem: 3067M
[32m[04/15 08:09:04 d2.utils.events]: [0m eta: 0:10:46  iter: 839  total_loss: 0.716  loss_cls: 0.214  loss_box_reg: 0.401  loss_rpn_cls: 0.020  loss_rpn_loc: 0.063  time: 0.5681  data_time: 0.0068  lr: 0.004196  max_mem: 3067M
[32m[04/15 08:09:16 d2.utils.events]: [0m eta: 0:10:35  iter: 859  total_loss: 0.575  loss_cls: 0.184  loss_box_reg: 0.309  loss_rpn_cls: 0.010  loss_rpn_loc: 0.039  time: 0.5677  data_time: 0.0078  lr: 0.004296  max_mem: 3067M
[32m[04/15 08:09:27 d2.utils.events]: [0m eta: 0:10:23  iter: 879  total_loss: 0.601  loss_cls: 0.189  loss_box_reg: 0.321  loss_rpn_cls: 0.020  loss_rpn_loc: 0.046  time: 0.5676  data_time: 0.0087  lr: 0.004396  max_mem: 3067M
[32m[04/15 08:09:38 d2.utils.events]: [0m eta: 0:10:12  iter: 899  total_loss: 0.681  loss_cls: 0.211  loss_box_reg: 0.373  loss_rpn_cls: 0.025  loss_rpn_loc: 0.065  time: 0.5676  data_time: 0.0069  lr: 0.004496  max_mem: 3067M
[32m[04/15 08:09:49 d2.utils.events]: [0m eta: 0:10:01  iter: 919  total_loss: 0.686  loss_cls: 0.216  loss_box_reg: 0.355  loss_rpn_cls: 0.021  loss_rpn_loc: 0.065  time: 0.5670  data_time: 0.0066  lr: 0.004595  max_mem: 3067M
[32m[04/15 08:10:01 d2.utils.events]: [0m eta: 0:09:50  iter: 939  total_loss: 0.639  loss_cls: 0.199  loss_box_reg: 0.372  loss_rpn_cls: 0.018  loss_rpn_loc: 0.057  time: 0.5671  data_time: 0.0075  lr: 0.004695  max_mem: 3067M
[32m[04/15 08:10:13 d2.utils.events]: [0m eta: 0:09:39  iter: 959  total_loss: 0.657  loss_cls: 0.216  loss_box_reg: 0.355  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5681  data_time: 0.0087  lr: 0.004795  max_mem: 3067M
[32m[04/15 08:10:24 d2.utils.events]: [0m eta: 0:09:28  iter: 979  total_loss: 0.652  loss_cls: 0.201  loss_box_reg: 0.351  loss_rpn_cls: 0.017  loss_rpn_loc: 0.063  time: 0.5677  data_time: 0.0076  lr: 0.004895  max_mem: 3067M
[32m[04/15 08:10:36 d2.utils.events]: [0m eta: 0:09:17  iter: 999  total_loss: 0.561  loss_cls: 0.188  loss_box_reg: 0.316  loss_rpn_cls: 0.021  loss_rpn_loc: 0.040  time: 0.5675  data_time: 0.0068  lr: 0.004995  max_mem: 3067M
[32m[04/15 08:10:48 d2.utils.events]: [0m eta: 0:09:07  iter: 1019  total_loss: 0.567  loss_cls: 0.206  loss_box_reg: 0.322  loss_rpn_cls: 0.016  loss_rpn_loc: 0.047  time: 0.5682  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:10:59 d2.utils.events]: [0m eta: 0:08:54  iter: 1039  total_loss: 0.694  loss_cls: 0.205  loss_box_reg: 0.402  loss_rpn_cls: 0.014  loss_rpn_loc: 0.069  time: 0.5676  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:11:10 d2.utils.events]: [0m eta: 0:08:43  iter: 1059  total_loss: 0.565  loss_cls: 0.187  loss_box_reg: 0.317  loss_rpn_cls: 0.019  loss_rpn_loc: 0.052  time: 0.5671  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:11:22 d2.utils.events]: [0m eta: 0:08:32  iter: 1079  total_loss: 0.656  loss_cls: 0.214  loss_box_reg: 0.373  loss_rpn_cls: 0.022  loss_rpn_loc: 0.051  time: 0.5679  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:11:33 d2.utils.events]: [0m eta: 0:08:21  iter: 1099  total_loss: 0.562  loss_cls: 0.168  loss_box_reg: 0.301  loss_rpn_cls: 0.022  loss_rpn_loc: 0.059  time: 0.5675  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:11:44 d2.utils.events]: [0m eta: 0:08:09  iter: 1119  total_loss: 0.618  loss_cls: 0.181  loss_box_reg: 0.368  loss_rpn_cls: 0.022  loss_rpn_loc: 0.045  time: 0.5673  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:11:56 d2.utils.events]: [0m eta: 0:07:58  iter: 1139  total_loss: 0.622  loss_cls: 0.205  loss_box_reg: 0.373  loss_rpn_cls: 0.015  loss_rpn_loc: 0.048  time: 0.5678  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:12:08 d2.utils.events]: [0m eta: 0:07:46  iter: 1159  total_loss: 0.690  loss_cls: 0.228  loss_box_reg: 0.333  loss_rpn_cls: 0.022  loss_rpn_loc: 0.053  time: 0.5682  data_time: 0.0738  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:12:19 d2.utils.events]: [0m eta: 0:07:35  iter: 1179  total_loss: 0.615  loss_cls: 0.179  loss_box_reg: 0.333  loss_rpn_cls: 0.026  loss_rpn_loc: 0.054  time: 0.5677  data_time: 0.0202  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:12:31 d2.utils.events]: [0m eta: 0:07:25  iter: 1199  total_loss: 0.574  loss_cls: 0.196  loss_box_reg: 0.301  loss_rpn_cls: 0.023  loss_rpn_loc: 0.046  time: 0.5680  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:12:43 d2.utils.events]: [0m eta: 0:07:14  iter: 1219  total_loss: 0.621  loss_cls: 0.189  loss_box_reg: 0.338  loss_rpn_cls: 0.026  loss_rpn_loc: 0.055  time: 0.5686  data_time: 0.0097  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:12:55 d2.utils.events]: [0m eta: 0:07:04  iter: 1239  total_loss: 0.612  loss_cls: 0.212  loss_box_reg: 0.323  loss_rpn_cls: 0.019  loss_rpn_loc: 0.044  time: 0.5690  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:13:06 d2.utils.events]: [0m eta: 0:06:52  iter: 1259  total_loss: 0.614  loss_cls: 0.194  loss_box_reg: 0.342  loss_rpn_cls: 0.012  loss_rpn_loc: 0.054  time: 0.5686  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:13:18 d2.utils.events]: [0m eta: 0:06:41  iter: 1279  total_loss: 0.722  loss_cls: 0.221  loss_box_reg: 0.400  loss_rpn_cls: 0.019  loss_rpn_loc: 0.059  time: 0.5687  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:13:30 d2.utils.events]: [0m eta: 0:06:31  iter: 1299  total_loss: 0.762  loss_cls: 0.258  loss_box_reg: 0.403  loss_rpn_cls: 0.016  loss_rpn_loc: 0.062  time: 0.5694  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:13:41 d2.utils.events]: [0m eta: 0:06:18  iter: 1319  total_loss: 0.665  loss_cls: 0.195  loss_box_reg: 0.361  loss_rpn_cls: 0.021  loss_rpn_loc: 0.050  time: 0.5689  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:13:52 d2.utils.events]: [0m eta: 0:06:07  iter: 1339  total_loss: 0.640  loss_cls: 0.206  loss_box_reg: 0.359  loss_rpn_cls: 0.015  loss_rpn_loc: 0.057  time: 0.5688  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:14:05 d2.utils.events]: [0m eta: 0:05:57  iter: 1359  total_loss: 0.611  loss_cls: 0.207  loss_box_reg: 0.348  loss_rpn_cls: 0.018  loss_rpn_loc: 0.050  time: 0.5693  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:14:15 d2.utils.events]: [0m eta: 0:05:45  iter: 1379  total_loss: 0.695  loss_cls: 0.207  loss_box_reg: 0.373  loss_rpn_cls: 0.012  loss_rpn_loc: 0.066  time: 0.5688  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:14:26 d2.utils.events]: [0m eta: 0:05:34  iter: 1399  total_loss: 0.764  loss_cls: 0.239  loss_box_reg: 0.426  loss_rpn_cls: 0.021  loss_rpn_loc: 0.071  time: 0.5684  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:14:39 d2.utils.events]: [0m eta: 0:05:23  iter: 1419  total_loss: 0.728  loss_cls: 0.230  loss_box_reg: 0.418  loss_rpn_cls: 0.032  loss_rpn_loc: 0.060  time: 0.5690  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:14:50 d2.utils.events]: [0m eta: 0:05:12  iter: 1439  total_loss: 0.589  loss_cls: 0.185  loss_box_reg: 0.294  loss_rpn_cls: 0.021  loss_rpn_loc: 0.045  time: 0.5688  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:15:00 d2.utils.events]: [0m eta: 0:05:00  iter: 1459  total_loss: 0.700  loss_cls: 0.222  loss_box_reg: 0.380  loss_rpn_cls: 0.021  loss_rpn_loc: 0.050  time: 0.5682  data_time: 0.0062  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:15:12 d2.utils.events]: [0m eta: 0:04:49  iter: 1479  total_loss: 0.705  loss_cls: 0.221  loss_box_reg: 0.333  loss_rpn_cls: 0.017  loss_rpn_loc: 0.070  time: 0.5683  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:15:24 d2.utils.events]: [0m eta: 0:04:38  iter: 1499  total_loss: 0.559  loss_cls: 0.176  loss_box_reg: 0.286  loss_rpn_cls: 0.017  loss_rpn_loc: 0.041  time: 0.5688  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:15:35 d2.utils.events]: [0m eta: 0:04:26  iter: 1519  total_loss: 0.667  loss_cls: 0.219  loss_box_reg: 0.405  loss_rpn_cls: 0.023  loss_rpn_loc: 0.055  time: 0.5682  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:15:46 d2.utils.events]: [0m eta: 0:04:15  iter: 1539  total_loss: 0.578  loss_cls: 0.185  loss_box_reg: 0.299  loss_rpn_cls: 0.022  loss_rpn_loc: 0.051  time: 0.5681  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:15:58 d2.utils.events]: [0m eta: 0:04:05  iter: 1559  total_loss: 0.602  loss_cls: 0.190  loss_box_reg: 0.325  loss_rpn_cls: 0.024  loss_rpn_loc: 0.064  time: 0.5684  data_time: 0.0096  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:16:09 d2.utils.events]: [0m eta: 0:03:54  iter: 1579  total_loss: 0.687  loss_cls: 0.214  loss_box_reg: 0.365  loss_rpn_cls: 0.019  loss_rpn_loc: 0.053  time: 0.5681  data_time: 0.0101  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:16:20 d2.utils.events]: [0m eta: 0:03:42  iter: 1599  total_loss: 0.586  loss_cls: 0.177  loss_box_reg: 0.310  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5680  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:16:32 d2.utils.events]: [0m eta: 0:03:32  iter: 1619  total_loss: 0.689  loss_cls: 0.229  loss_box_reg: 0.392  loss_rpn_cls: 0.018  loss_rpn_loc: 0.058  time: 0.5683  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:16:43 d2.utils.events]: [0m eta: 0:03:20  iter: 1639  total_loss: 0.611  loss_cls: 0.185  loss_box_reg: 0.360  loss_rpn_cls: 0.021  loss_rpn_loc: 0.047  time: 0.5678  data_time: 0.0061  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:16:54 d2.utils.events]: [0m eta: 0:03:09  iter: 1659  total_loss: 0.643  loss_cls: 0.222  loss_box_reg: 0.316  loss_rpn_cls: 0.024  loss_rpn_loc: 0.055  time: 0.5677  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:17:06 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.725  loss_cls: 0.221  loss_box_reg: 0.419  loss_rpn_cls: 0.019  loss_rpn_loc: 0.058  time: 0.5681  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:17:18 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.667  loss_cls: 0.215  loss_box_reg: 0.356  loss_rpn_cls: 0.017  loss_rpn_loc: 0.056  time: 0.5681  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:17:28 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.595  loss_cls: 0.194  loss_box_reg: 0.322  loss_rpn_cls: 0.019  loss_rpn_loc: 0.041  time: 0.5676  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:17:40 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.589  loss_cls: 0.195  loss_box_reg: 0.334  loss_rpn_cls: 0.024  loss_rpn_loc: 0.061  time: 0.5678  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:17:52 d2.utils.events]: [0m eta: 0:02:14  iter: 1759  total_loss: 0.635  loss_cls: 0.220  loss_box_reg: 0.334  loss_rpn_cls: 0.021  loss_rpn_loc: 0.052  time: 0.5682  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:18:03 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.657  loss_cls: 0.202  loss_box_reg: 0.334  loss_rpn_cls: 0.020  loss_rpn_loc: 0.047  time: 0.5678  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:18:14 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.656  loss_cls: 0.204  loss_box_reg: 0.375  loss_rpn_cls: 0.021  loss_rpn_loc: 0.064  time: 0.5679  data_time: 0.0098  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:18:26 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.629  loss_cls: 0.177  loss_box_reg: 0.332  loss_rpn_cls: 0.025  loss_rpn_loc: 0.055  time: 0.5679  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:18:37 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.742  loss_cls: 0.228  loss_box_reg: 0.378  loss_rpn_cls: 0.017  loss_rpn_loc: 0.065  time: 0.5675  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:18:48 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.566  loss_cls: 0.174  loss_box_reg: 0.297  loss_rpn_cls: 0.018  loss_rpn_loc: 0.046  time: 0.5674  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:19:00 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.716  loss_cls: 0.227  loss_box_reg: 0.390  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5679  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:19:11 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.686  loss_cls: 0.221  loss_box_reg: 0.387  loss_rpn_cls: 0.021  loss_rpn_loc: 0.057  time: 0.5675  data_time: 0.0058  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:19:24 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.666  loss_cls: 0.230  loss_box_reg: 0.355  loss_rpn_cls: 0.028  loss_rpn_loc: 0.060  time: 0.5681  data_time: 0.0043  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:19:34 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.662  loss_cls: 0.211  loss_box_reg: 0.353  loss_rpn_cls: 0.022  loss_rpn_loc: 0.050  time: 0.5676  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:19:45 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.594  loss_cls: 0.179  loss_box_reg: 0.304  loss_rpn_cls: 0.019  loss_rpn_loc: 0.056  time: 0.5672  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:19:55 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.744  loss_cls: 0.224  loss_box_reg: 0.391  loss_rpn_cls: 0.020  loss_rpn_loc: 0.070  time: 0.5668  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 08:20:16 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:20:16 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 08:20:17 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 08:20:17 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.724  loss_cls: 0.239  loss_box_reg: 0.381  loss_rpn_cls: 0.031  loss_rpn_loc: 0.054  time: 0.5667  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:20:19 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:52 (0.5670 s / it)
[32m[04/15 08:20:19 d2.engine.hooks]: [0mTotal training time: 0:19:14 (0:00:21 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 08:20:25 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:20:26 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 08:20:27 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 08:20:29 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1201 s / img. ETA=0:03:06
[32m[04/15 08:20:34 d2.evaluation.evaluator]: [0mInference done 45/1257. 0.1222 s / img. ETA=0:03:00
[32m[04/15 08:20:39 d2.evaluation.evaluator]: [0mInference done 84/1257. 0.1225 s / img. ETA=0:02:43
[32m[04/15 08:20:44 d2.evaluation.evaluator]: [0mInference done 118/1257. 0.1294 s / img. ETA=0:02:42
[32m[04/15 08:20:49 d2.evaluation.evaluator]: [0mInference done 151/1257. 0.1325 s / img. ETA=0:02:40
[32m[04/15 08:20:54 d2.evaluation.evaluator]: [0mInference done 190/1257. 0.1310 s / img. ETA=0:02:31
[32m[04/15 08:20:59 d2.evaluation.evaluator]: [0mInference done 225/1257. 0.1291 s / img. ETA=0:02:26
[32m[04/15 08:21:05 d2.evaluation.evaluator]: [0mInference done 257/1257. 0.1297 s / img. ETA=0:02:24
[32m[04/15 08:21:10 d2.evaluation.evaluator]: [0mInference done 296/1257. 0.1287 s / img. ETA=0:02:17
[32m[04/15 08:21:15 d2.evaluation.evaluator]: [0mInference done 336/1257. 0.1279 s / img. ETA=0:02:09
[32m[04/15 08:21:20 d2.evaluation.evaluator]: [0mInference done 373/1257. 0.1282 s / img. ETA=0:02:03
[32m[04/15 08:21:25 d2.evaluation.evaluator]: [0mInference done 406/1257. 0.1299 s / img. ETA=0:02:00
[32m[04/15 08:21:30 d2.evaluation.evaluator]: [0mInference done 439/1257. 0.1304 s / img. ETA=0:01:56
[32m[04/15 08:21:35 d2.evaluation.evaluator]: [0mInference done 469/1257. 0.1296 s / img. ETA=0:01:53
[32m[04/15 08:21:40 d2.evaluation.evaluator]: [0mInference done 504/1257. 0.1290 s / img. ETA=0:01:48
[32m[04/15 08:21:45 d2.evaluation.evaluator]: [0mInference done 534/1257. 0.1287 s / img. ETA=0:01:45
[32m[04/15 08:21:50 d2.evaluation.evaluator]: [0mInference done 567/1257. 0.1283 s / img. ETA=0:01:40
[32m[04/15 08:21:55 d2.evaluation.evaluator]: [0mInference done 605/1257. 0.1278 s / img. ETA=0:01:34
[32m[04/15 08:22:00 d2.evaluation.evaluator]: [0mInference done 640/1257. 0.1283 s / img. ETA=0:01:29
[32m[04/15 08:22:05 d2.evaluation.evaluator]: [0mInference done 672/1257. 0.1291 s / img. ETA=0:01:25
[32m[04/15 08:22:10 d2.evaluation.evaluator]: [0mInference done 708/1257. 0.1292 s / img. ETA=0:01:19
[32m[04/15 08:22:15 d2.evaluation.evaluator]: [0mInference done 747/1257. 0.1288 s / img. ETA=0:01:13
[32m[04/15 08:22:20 d2.evaluation.evaluator]: [0mInference done 786/1257. 0.1285 s / img. ETA=0:01:07
[32m[04/15 08:22:25 d2.evaluation.evaluator]: [0mInference done 826/1257. 0.1281 s / img. ETA=0:01:01
[32m[04/15 08:22:30 d2.evaluation.evaluator]: [0mInference done 866/1257. 0.1279 s / img. ETA=0:00:55
[32m[04/15 08:22:36 d2.evaluation.evaluator]: [0mInference done 905/1257. 0.1278 s / img. ETA=0:00:49
[32m[04/15 08:22:41 d2.evaluation.evaluator]: [0mInference done 940/1257. 0.1282 s / img. ETA=0:00:44
[32m[04/15 08:22:46 d2.evaluation.evaluator]: [0mInference done 974/1257. 0.1290 s / img. ETA=0:00:40
[32m[04/15 08:22:51 d2.evaluation.evaluator]: [0mInference done 1014/1257. 0.1286 s / img. ETA=0:00:34
[32m[04/15 08:22:56 d2.evaluation.evaluator]: [0mInference done 1053/1257. 0.1283 s / img. ETA=0:00:28
[32m[04/15 08:23:01 d2.evaluation.evaluator]: [0mInference done 1092/1257. 0.1279 s / img. ETA=0:00:23
[32m[04/15 08:23:06 d2.evaluation.evaluator]: [0mInference done 1129/1257. 0.1277 s / img. ETA=0:00:18
[32m[04/15 08:23:11 d2.evaluation.evaluator]: [0mInference done 1158/1257. 0.1276 s / img. ETA=0:00:14
[32m[04/15 08:23:16 d2.evaluation.evaluator]: [0mInference done 1181/1257. 0.1278 s / img. ETA=0:00:10
[32m[04/15 08:23:21 d2.evaluation.evaluator]: [0mInference done 1214/1257. 0.1282 s / img. ETA=0:00:06
[32m[04/15 08:23:26 d2.evaluation.evaluator]: [0mInference done 1252/1257. 0.1282 s / img. ETA=0:00:00
[32m[04/15 08:23:27 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:59.034933 (0.142999 s / img per device, on 1 devices)
[32m[04/15 08:23:27 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:40 (0.128185 s / img per device, on 1 devices)
[32m[04/15 08:23:27 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 08:23:27 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 08:23:27 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.33s).
Accumulating evaluation results...
DONE (t=0.82s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.194
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.389
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.169
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.114
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.240
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.470
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.095
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.235
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.160
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.284
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.511
[32m[04/15 08:23:35 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.405 | 38.926 | 16.919 | 11.440 | 23.988 | 47.024 |
[32m[04/15 08:23:35 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 28.537 | bicycle       | 8.366 | car            | 40.717 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  13  *  2000  iterations ============
4 channel input
[32m[04/15 08:23:36 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 08:23:37 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.35 seconds.
[5m[31mWARNING[0m [32m[04/15 08:23:37 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:23:38 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 08:23:38 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 08:23:39 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 08:23:39 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 08:23:41 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 08:23:52 d2.utils.events]: [0m eta: 0:18:20  iter: 19  total_loss: 0.640  loss_cls: 0.205  loss_box_reg: 0.344  loss_rpn_cls: 0.026  loss_rpn_loc: 0.061  time: 0.5505  data_time: 0.0533  lr: 0.000100  max_mem: 3067M
[32m[04/15 08:24:05 d2.utils.events]: [0m eta: 0:19:20  iter: 39  total_loss: 0.619  loss_cls: 0.204  loss_box_reg: 0.330  loss_rpn_cls: 0.029  loss_rpn_loc: 0.052  time: 0.5973  data_time: 0.0069  lr: 0.000200  max_mem: 3067M
[32m[04/15 08:24:16 d2.utils.events]: [0m eta: 0:18:11  iter: 59  total_loss: 0.700  loss_cls: 0.203  loss_box_reg: 0.413  loss_rpn_cls: 0.022  loss_rpn_loc: 0.065  time: 0.5790  data_time: 0.0108  lr: 0.000300  max_mem: 3067M
[32m[04/15 08:24:27 d2.utils.events]: [0m eta: 0:17:52  iter: 79  total_loss: 0.648  loss_cls: 0.213  loss_box_reg: 0.350  loss_rpn_cls: 0.026  loss_rpn_loc: 0.054  time: 0.5702  data_time: 0.0076  lr: 0.000400  max_mem: 3067M
[32m[04/15 08:24:38 d2.utils.events]: [0m eta: 0:17:34  iter: 99  total_loss: 0.673  loss_cls: 0.212  loss_box_reg: 0.347  loss_rpn_cls: 0.023  loss_rpn_loc: 0.059  time: 0.5649  data_time: 0.0072  lr: 0.000500  max_mem: 3067M
[32m[04/15 08:24:50 d2.utils.events]: [0m eta: 0:17:29  iter: 119  total_loss: 0.635  loss_cls: 0.193  loss_box_reg: 0.373  loss_rpn_cls: 0.019  loss_rpn_loc: 0.055  time: 0.5718  data_time: 0.0092  lr: 0.000599  max_mem: 3067M
[32m[04/15 08:25:01 d2.utils.events]: [0m eta: 0:17:12  iter: 139  total_loss: 0.689  loss_cls: 0.209  loss_box_reg: 0.382  loss_rpn_cls: 0.017  loss_rpn_loc: 0.044  time: 0.5665  data_time: 0.0072  lr: 0.000699  max_mem: 3067M
[32m[04/15 08:25:12 d2.utils.events]: [0m eta: 0:16:55  iter: 159  total_loss: 0.583  loss_cls: 0.177  loss_box_reg: 0.306  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5624  data_time: 0.0077  lr: 0.000799  max_mem: 3067M
[32m[04/15 08:25:24 d2.utils.events]: [0m eta: 0:16:46  iter: 179  total_loss: 0.651  loss_cls: 0.196  loss_box_reg: 0.359  loss_rpn_cls: 0.017  loss_rpn_loc: 0.043  time: 0.5659  data_time: 0.0096  lr: 0.000899  max_mem: 3067M
[32m[04/15 08:25:36 d2.utils.events]: [0m eta: 0:16:41  iter: 199  total_loss: 0.687  loss_cls: 0.199  loss_box_reg: 0.380  loss_rpn_cls: 0.028  loss_rpn_loc: 0.045  time: 0.5686  data_time: 0.0074  lr: 0.000999  max_mem: 3067M
[32m[04/15 08:25:47 d2.utils.events]: [0m eta: 0:16:30  iter: 219  total_loss: 0.495  loss_cls: 0.177  loss_box_reg: 0.279  loss_rpn_cls: 0.012  loss_rpn_loc: 0.058  time: 0.5677  data_time: 0.0074  lr: 0.001099  max_mem: 3067M
[32m[04/15 08:25:58 d2.utils.events]: [0m eta: 0:16:19  iter: 239  total_loss: 0.649  loss_cls: 0.207  loss_box_reg: 0.350  loss_rpn_cls: 0.016  loss_rpn_loc: 0.064  time: 0.5669  data_time: 0.0065  lr: 0.001199  max_mem: 3067M
[32m[04/15 08:26:10 d2.utils.events]: [0m eta: 0:16:08  iter: 259  total_loss: 0.648  loss_cls: 0.217  loss_box_reg: 0.364  loss_rpn_cls: 0.013  loss_rpn_loc: 0.062  time: 0.5697  data_time: 0.0080  lr: 0.001299  max_mem: 3067M
[32m[04/15 08:26:22 d2.utils.events]: [0m eta: 0:15:51  iter: 279  total_loss: 0.594  loss_cls: 0.174  loss_box_reg: 0.341  loss_rpn_cls: 0.013  loss_rpn_loc: 0.053  time: 0.5686  data_time: 0.0452  lr: 0.001399  max_mem: 3067M
[32m[04/15 08:26:34 d2.utils.events]: [0m eta: 0:15:44  iter: 299  total_loss: 0.567  loss_cls: 0.186  loss_box_reg: 0.313  loss_rpn_cls: 0.015  loss_rpn_loc: 0.052  time: 0.5693  data_time: 0.0075  lr: 0.001499  max_mem: 3067M
[32m[04/15 08:26:46 d2.utils.events]: [0m eta: 0:15:35  iter: 319  total_loss: 0.661  loss_cls: 0.203  loss_box_reg: 0.386  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5731  data_time: 0.0099  lr: 0.001598  max_mem: 3067M
[32m[04/15 08:26:57 d2.utils.events]: [0m eta: 0:15:23  iter: 339  total_loss: 0.595  loss_cls: 0.166  loss_box_reg: 0.333  loss_rpn_cls: 0.014  loss_rpn_loc: 0.053  time: 0.5713  data_time: 0.0085  lr: 0.001698  max_mem: 3067M
[32m[04/15 08:27:08 d2.utils.events]: [0m eta: 0:15:10  iter: 359  total_loss: 0.638  loss_cls: 0.201  loss_box_reg: 0.354  loss_rpn_cls: 0.018  loss_rpn_loc: 0.058  time: 0.5685  data_time: 0.0074  lr: 0.001798  max_mem: 3067M
[32m[04/15 08:27:20 d2.utils.events]: [0m eta: 0:14:59  iter: 379  total_loss: 0.561  loss_cls: 0.189  loss_box_reg: 0.321  loss_rpn_cls: 0.015  loss_rpn_loc: 0.047  time: 0.5702  data_time: 0.0066  lr: 0.001898  max_mem: 3067M
[32m[04/15 08:27:31 d2.utils.events]: [0m eta: 0:14:46  iter: 399  total_loss: 0.577  loss_cls: 0.198  loss_box_reg: 0.344  loss_rpn_cls: 0.012  loss_rpn_loc: 0.036  time: 0.5690  data_time: 0.0086  lr: 0.001998  max_mem: 3067M
[32m[04/15 08:27:42 d2.utils.events]: [0m eta: 0:14:35  iter: 419  total_loss: 0.499  loss_cls: 0.181  loss_box_reg: 0.296  loss_rpn_cls: 0.013  loss_rpn_loc: 0.036  time: 0.5679  data_time: 0.0068  lr: 0.002098  max_mem: 3067M
[32m[04/15 08:27:53 d2.utils.events]: [0m eta: 0:14:25  iter: 439  total_loss: 0.545  loss_cls: 0.191  loss_box_reg: 0.324  loss_rpn_cls: 0.013  loss_rpn_loc: 0.049  time: 0.5680  data_time: 0.0074  lr: 0.002198  max_mem: 3067M
[32m[04/15 08:28:06 d2.utils.events]: [0m eta: 0:14:15  iter: 459  total_loss: 0.629  loss_cls: 0.196  loss_box_reg: 0.365  loss_rpn_cls: 0.020  loss_rpn_loc: 0.052  time: 0.5699  data_time: 0.0723  lr: 0.002298  max_mem: 3067M
[32m[04/15 08:28:17 d2.utils.events]: [0m eta: 0:14:03  iter: 479  total_loss: 0.618  loss_cls: 0.195  loss_box_reg: 0.321  loss_rpn_cls: 0.020  loss_rpn_loc: 0.048  time: 0.5687  data_time: 0.0206  lr: 0.002398  max_mem: 3067M
[32m[04/15 08:28:27 d2.utils.events]: [0m eta: 0:13:51  iter: 499  total_loss: 0.707  loss_cls: 0.202  loss_box_reg: 0.382  loss_rpn_cls: 0.018  loss_rpn_loc: 0.065  time: 0.5674  data_time: 0.0071  lr: 0.002498  max_mem: 3067M
[32m[04/15 08:28:40 d2.utils.events]: [0m eta: 0:13:39  iter: 519  total_loss: 0.543  loss_cls: 0.169  loss_box_reg: 0.287  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5667  data_time: 0.0078  lr: 0.002597  max_mem: 3067M
[32m[04/15 08:28:52 d2.utils.events]: [0m eta: 0:13:28  iter: 539  total_loss: 0.686  loss_cls: 0.213  loss_box_reg: 0.399  loss_rpn_cls: 0.016  loss_rpn_loc: 0.063  time: 0.5680  data_time: 0.0075  lr: 0.002697  max_mem: 3067M
[32m[04/15 08:29:03 d2.utils.events]: [0m eta: 0:13:16  iter: 559  total_loss: 0.596  loss_cls: 0.192  loss_box_reg: 0.339  loss_rpn_cls: 0.011  loss_rpn_loc: 0.049  time: 0.5672  data_time: 0.0076  lr: 0.002797  max_mem: 3067M
[32m[04/15 08:29:15 d2.utils.events]: [0m eta: 0:13:06  iter: 579  total_loss: 0.575  loss_cls: 0.161  loss_box_reg: 0.334  loss_rpn_cls: 0.012  loss_rpn_loc: 0.047  time: 0.5670  data_time: 0.0096  lr: 0.002897  max_mem: 3067M
[32m[04/15 08:29:27 d2.utils.events]: [0m eta: 0:12:57  iter: 599  total_loss: 0.576  loss_cls: 0.187  loss_box_reg: 0.336  loss_rpn_cls: 0.012  loss_rpn_loc: 0.055  time: 0.5687  data_time: 0.0089  lr: 0.002997  max_mem: 3067M
[32m[04/15 08:29:38 d2.utils.events]: [0m eta: 0:12:46  iter: 619  total_loss: 0.601  loss_cls: 0.171  loss_box_reg: 0.364  loss_rpn_cls: 0.021  loss_rpn_loc: 0.058  time: 0.5682  data_time: 0.0064  lr: 0.003097  max_mem: 3067M
[32m[04/15 08:29:50 d2.utils.events]: [0m eta: 0:12:35  iter: 639  total_loss: 0.594  loss_cls: 0.187  loss_box_reg: 0.334  loss_rpn_cls: 0.011  loss_rpn_loc: 0.035  time: 0.5681  data_time: 0.0072  lr: 0.003197  max_mem: 3067M
[32m[04/15 08:30:02 d2.utils.events]: [0m eta: 0:12:24  iter: 659  total_loss: 0.677  loss_cls: 0.219  loss_box_reg: 0.373  loss_rpn_cls: 0.021  loss_rpn_loc: 0.068  time: 0.5686  data_time: 0.0071  lr: 0.003297  max_mem: 3067M
[32m[04/15 08:30:13 d2.utils.events]: [0m eta: 0:12:13  iter: 679  total_loss: 0.642  loss_cls: 0.213  loss_box_reg: 0.335  loss_rpn_cls: 0.020  loss_rpn_loc: 0.048  time: 0.5684  data_time: 0.0070  lr: 0.003397  max_mem: 3067M
[32m[04/15 08:30:24 d2.utils.events]: [0m eta: 0:12:01  iter: 699  total_loss: 0.649  loss_cls: 0.221  loss_box_reg: 0.357  loss_rpn_cls: 0.018  loss_rpn_loc: 0.043  time: 0.5677  data_time: 0.0072  lr: 0.003497  max_mem: 3067M
[32m[04/15 08:30:35 d2.utils.events]: [0m eta: 0:11:50  iter: 719  total_loss: 0.691  loss_cls: 0.223  loss_box_reg: 0.363  loss_rpn_cls: 0.013  loss_rpn_loc: 0.059  time: 0.5675  data_time: 0.0094  lr: 0.003596  max_mem: 3067M
[32m[04/15 08:30:47 d2.utils.events]: [0m eta: 0:11:39  iter: 739  total_loss: 0.669  loss_cls: 0.198  loss_box_reg: 0.340  loss_rpn_cls: 0.017  loss_rpn_loc: 0.063  time: 0.5680  data_time: 0.0069  lr: 0.003696  max_mem: 3067M
[32m[04/15 08:30:58 d2.utils.events]: [0m eta: 0:11:28  iter: 759  total_loss: 0.574  loss_cls: 0.191  loss_box_reg: 0.308  loss_rpn_cls: 0.014  loss_rpn_loc: 0.048  time: 0.5675  data_time: 0.0087  lr: 0.003796  max_mem: 3067M
[32m[04/15 08:31:09 d2.utils.events]: [0m eta: 0:11:15  iter: 779  total_loss: 0.633  loss_cls: 0.194  loss_box_reg: 0.358  loss_rpn_cls: 0.011  loss_rpn_loc: 0.056  time: 0.5667  data_time: 0.0080  lr: 0.003896  max_mem: 3067M
[32m[04/15 08:31:21 d2.utils.events]: [0m eta: 0:11:06  iter: 799  total_loss: 0.527  loss_cls: 0.189  loss_box_reg: 0.287  loss_rpn_cls: 0.018  loss_rpn_loc: 0.040  time: 0.5683  data_time: 0.0072  lr: 0.003996  max_mem: 3067M
[32m[04/15 08:31:32 d2.utils.events]: [0m eta: 0:10:54  iter: 819  total_loss: 0.664  loss_cls: 0.217  loss_box_reg: 0.355  loss_rpn_cls: 0.017  loss_rpn_loc: 0.057  time: 0.5673  data_time: 0.0066  lr: 0.004096  max_mem: 3067M
[32m[04/15 08:31:43 d2.utils.events]: [0m eta: 0:10:42  iter: 839  total_loss: 0.629  loss_cls: 0.196  loss_box_reg: 0.356  loss_rpn_cls: 0.013  loss_rpn_loc: 0.058  time: 0.5668  data_time: 0.0068  lr: 0.004196  max_mem: 3067M
[32m[04/15 08:31:55 d2.utils.events]: [0m eta: 0:10:32  iter: 859  total_loss: 0.672  loss_cls: 0.216  loss_box_reg: 0.392  loss_rpn_cls: 0.018  loss_rpn_loc: 0.059  time: 0.5677  data_time: 0.0078  lr: 0.004296  max_mem: 3067M
[32m[04/15 08:32:07 d2.utils.events]: [0m eta: 0:10:21  iter: 879  total_loss: 0.609  loss_cls: 0.193  loss_box_reg: 0.330  loss_rpn_cls: 0.016  loss_rpn_loc: 0.047  time: 0.5673  data_time: 0.0117  lr: 0.004396  max_mem: 3067M
[32m[04/15 08:32:18 d2.utils.events]: [0m eta: 0:10:09  iter: 899  total_loss: 0.652  loss_cls: 0.199  loss_box_reg: 0.371  loss_rpn_cls: 0.013  loss_rpn_loc: 0.058  time: 0.5668  data_time: 0.0072  lr: 0.004496  max_mem: 3067M
[32m[04/15 08:32:29 d2.utils.events]: [0m eta: 0:09:58  iter: 919  total_loss: 0.580  loss_cls: 0.203  loss_box_reg: 0.297  loss_rpn_cls: 0.011  loss_rpn_loc: 0.047  time: 0.5665  data_time: 0.0078  lr: 0.004595  max_mem: 3067M
[32m[04/15 08:32:41 d2.utils.events]: [0m eta: 0:09:47  iter: 939  total_loss: 0.663  loss_cls: 0.212  loss_box_reg: 0.339  loss_rpn_cls: 0.020  loss_rpn_loc: 0.061  time: 0.5673  data_time: 0.0069  lr: 0.004695  max_mem: 3067M
[32m[04/15 08:32:52 d2.utils.events]: [0m eta: 0:09:36  iter: 959  total_loss: 0.588  loss_cls: 0.202  loss_box_reg: 0.314  loss_rpn_cls: 0.014  loss_rpn_loc: 0.043  time: 0.5670  data_time: 0.0079  lr: 0.004795  max_mem: 3067M
[32m[04/15 08:33:02 d2.utils.events]: [0m eta: 0:09:24  iter: 979  total_loss: 0.757  loss_cls: 0.234  loss_box_reg: 0.359  loss_rpn_cls: 0.029  loss_rpn_loc: 0.068  time: 0.5658  data_time: 0.0068  lr: 0.004895  max_mem: 3067M
[32m[04/15 08:33:15 d2.utils.events]: [0m eta: 0:09:14  iter: 999  total_loss: 0.709  loss_cls: 0.214  loss_box_reg: 0.376  loss_rpn_cls: 0.019  loss_rpn_loc: 0.055  time: 0.5667  data_time: 0.0078  lr: 0.004995  max_mem: 3067M
[32m[04/15 08:33:26 d2.utils.events]: [0m eta: 0:09:03  iter: 1019  total_loss: 0.640  loss_cls: 0.206  loss_box_reg: 0.357  loss_rpn_cls: 0.021  loss_rpn_loc: 0.067  time: 0.5662  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:33:37 d2.utils.events]: [0m eta: 0:08:50  iter: 1039  total_loss: 0.492  loss_cls: 0.157  loss_box_reg: 0.274  loss_rpn_cls: 0.024  loss_rpn_loc: 0.037  time: 0.5659  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:33:49 d2.utils.events]: [0m eta: 0:08:40  iter: 1059  total_loss: 0.745  loss_cls: 0.247  loss_box_reg: 0.388  loss_rpn_cls: 0.025  loss_rpn_loc: 0.060  time: 0.5663  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:34:00 d2.utils.events]: [0m eta: 0:08:29  iter: 1079  total_loss: 0.615  loss_cls: 0.211  loss_box_reg: 0.329  loss_rpn_cls: 0.020  loss_rpn_loc: 0.058  time: 0.5662  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:34:11 d2.utils.events]: [0m eta: 0:08:19  iter: 1099  total_loss: 0.734  loss_cls: 0.221  loss_box_reg: 0.383  loss_rpn_cls: 0.018  loss_rpn_loc: 0.064  time: 0.5660  data_time: 0.0147  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:34:23 d2.utils.events]: [0m eta: 0:08:06  iter: 1119  total_loss: 0.625  loss_cls: 0.208  loss_box_reg: 0.319  loss_rpn_cls: 0.020  loss_rpn_loc: 0.055  time: 0.5656  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:34:35 d2.utils.events]: [0m eta: 0:07:56  iter: 1139  total_loss: 0.608  loss_cls: 0.188  loss_box_reg: 0.338  loss_rpn_cls: 0.018  loss_rpn_loc: 0.061  time: 0.5662  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:34:46 d2.utils.events]: [0m eta: 0:07:46  iter: 1159  total_loss: 0.668  loss_cls: 0.216  loss_box_reg: 0.392  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5659  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:34:57 d2.utils.events]: [0m eta: 0:07:33  iter: 1179  total_loss: 0.667  loss_cls: 0.215  loss_box_reg: 0.359  loss_rpn_cls: 0.022  loss_rpn_loc: 0.057  time: 0.5655  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:35:08 d2.utils.events]: [0m eta: 0:07:22  iter: 1199  total_loss: 0.699  loss_cls: 0.227  loss_box_reg: 0.358  loss_rpn_cls: 0.028  loss_rpn_loc: 0.064  time: 0.5657  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:35:19 d2.utils.events]: [0m eta: 0:07:11  iter: 1219  total_loss: 0.715  loss_cls: 0.219  loss_box_reg: 0.364  loss_rpn_cls: 0.020  loss_rpn_loc: 0.063  time: 0.5655  data_time: 0.0101  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:35:30 d2.utils.events]: [0m eta: 0:06:58  iter: 1239  total_loss: 0.695  loss_cls: 0.222  loss_box_reg: 0.397  loss_rpn_cls: 0.018  loss_rpn_loc: 0.066  time: 0.5646  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:35:42 d2.utils.events]: [0m eta: 0:06:47  iter: 1259  total_loss: 0.599  loss_cls: 0.171  loss_box_reg: 0.327  loss_rpn_cls: 0.016  loss_rpn_loc: 0.050  time: 0.5651  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:35:53 d2.utils.events]: [0m eta: 0:06:37  iter: 1279  total_loss: 0.501  loss_cls: 0.170  loss_box_reg: 0.244  loss_rpn_cls: 0.018  loss_rpn_loc: 0.041  time: 0.5650  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:36:04 d2.utils.events]: [0m eta: 0:06:25  iter: 1299  total_loss: 0.688  loss_cls: 0.232  loss_box_reg: 0.379  loss_rpn_cls: 0.022  loss_rpn_loc: 0.060  time: 0.5644  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:36:15 d2.utils.events]: [0m eta: 0:06:14  iter: 1319  total_loss: 0.630  loss_cls: 0.184  loss_box_reg: 0.359  loss_rpn_cls: 0.016  loss_rpn_loc: 0.051  time: 0.5647  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:36:27 d2.utils.events]: [0m eta: 0:06:04  iter: 1339  total_loss: 0.628  loss_cls: 0.203  loss_box_reg: 0.330  loss_rpn_cls: 0.015  loss_rpn_loc: 0.055  time: 0.5650  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:36:38 d2.utils.events]: [0m eta: 0:05:53  iter: 1359  total_loss: 0.774  loss_cls: 0.230  loss_box_reg: 0.386  loss_rpn_cls: 0.026  loss_rpn_loc: 0.076  time: 0.5645  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:36:49 d2.utils.events]: [0m eta: 0:05:41  iter: 1379  total_loss: 0.619  loss_cls: 0.214  loss_box_reg: 0.351  loss_rpn_cls: 0.020  loss_rpn_loc: 0.061  time: 0.5643  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:37:01 d2.utils.events]: [0m eta: 0:05:31  iter: 1399  total_loss: 0.713  loss_cls: 0.214  loss_box_reg: 0.393  loss_rpn_cls: 0.022  loss_rpn_loc: 0.063  time: 0.5647  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:37:12 d2.utils.events]: [0m eta: 0:05:20  iter: 1419  total_loss: 0.674  loss_cls: 0.205  loss_box_reg: 0.373  loss_rpn_cls: 0.019  loss_rpn_loc: 0.061  time: 0.5644  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:37:23 d2.utils.events]: [0m eta: 0:05:08  iter: 1439  total_loss: 0.689  loss_cls: 0.228  loss_box_reg: 0.379  loss_rpn_cls: 0.018  loss_rpn_loc: 0.045  time: 0.5641  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:37:36 d2.utils.events]: [0m eta: 0:04:57  iter: 1459  total_loss: 0.691  loss_cls: 0.228  loss_box_reg: 0.381  loss_rpn_cls: 0.025  loss_rpn_loc: 0.066  time: 0.5651  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:37:46 d2.utils.events]: [0m eta: 0:04:46  iter: 1479  total_loss: 0.565  loss_cls: 0.197  loss_box_reg: 0.325  loss_rpn_cls: 0.017  loss_rpn_loc: 0.050  time: 0.5647  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:37:58 d2.utils.events]: [0m eta: 0:04:35  iter: 1499  total_loss: 0.588  loss_cls: 0.189  loss_box_reg: 0.309  loss_rpn_cls: 0.016  loss_rpn_loc: 0.051  time: 0.5645  data_time: 0.0059  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:38:10 d2.utils.events]: [0m eta: 0:04:24  iter: 1519  total_loss: 0.651  loss_cls: 0.212  loss_box_reg: 0.352  loss_rpn_cls: 0.021  loss_rpn_loc: 0.068  time: 0.5649  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:38:21 d2.utils.events]: [0m eta: 0:04:13  iter: 1539  total_loss: 0.713  loss_cls: 0.225  loss_box_reg: 0.379  loss_rpn_cls: 0.025  loss_rpn_loc: 0.066  time: 0.5649  data_time: 0.0096  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:38:32 d2.utils.events]: [0m eta: 0:04:03  iter: 1559  total_loss: 0.594  loss_cls: 0.206  loss_box_reg: 0.333  loss_rpn_cls: 0.017  loss_rpn_loc: 0.050  time: 0.5647  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:38:44 d2.utils.events]: [0m eta: 0:03:52  iter: 1579  total_loss: 0.565  loss_cls: 0.186  loss_box_reg: 0.304  loss_rpn_cls: 0.019  loss_rpn_loc: 0.043  time: 0.5650  data_time: 0.0096  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:38:56 d2.utils.events]: [0m eta: 0:03:40  iter: 1599  total_loss: 0.600  loss_cls: 0.208  loss_box_reg: 0.326  loss_rpn_cls: 0.017  loss_rpn_loc: 0.072  time: 0.5653  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:39:07 d2.utils.events]: [0m eta: 0:03:29  iter: 1619  total_loss: 0.669  loss_cls: 0.227  loss_box_reg: 0.368  loss_rpn_cls: 0.015  loss_rpn_loc: 0.059  time: 0.5651  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:39:18 d2.utils.events]: [0m eta: 0:03:18  iter: 1639  total_loss: 0.580  loss_cls: 0.198  loss_box_reg: 0.311  loss_rpn_cls: 0.017  loss_rpn_loc: 0.043  time: 0.5651  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:39:30 d2.utils.events]: [0m eta: 0:03:07  iter: 1659  total_loss: 0.559  loss_cls: 0.186  loss_box_reg: 0.311  loss_rpn_cls: 0.020  loss_rpn_loc: 0.059  time: 0.5652  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:39:41 d2.utils.events]: [0m eta: 0:02:56  iter: 1679  total_loss: 0.593  loss_cls: 0.186  loss_box_reg: 0.325  loss_rpn_cls: 0.018  loss_rpn_loc: 0.037  time: 0.5649  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:39:52 d2.utils.events]: [0m eta: 0:02:45  iter: 1699  total_loss: 0.618  loss_cls: 0.191  loss_box_reg: 0.324  loss_rpn_cls: 0.019  loss_rpn_loc: 0.064  time: 0.5647  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:40:05 d2.utils.events]: [0m eta: 0:02:34  iter: 1719  total_loss: 0.629  loss_cls: 0.197  loss_box_reg: 0.359  loss_rpn_cls: 0.018  loss_rpn_loc: 0.050  time: 0.5655  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:40:15 d2.utils.events]: [0m eta: 0:02:23  iter: 1739  total_loss: 0.754  loss_cls: 0.243  loss_box_reg: 0.410  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5652  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:40:26 d2.utils.events]: [0m eta: 0:02:12  iter: 1759  total_loss: 0.608  loss_cls: 0.201  loss_box_reg: 0.328  loss_rpn_cls: 0.021  loss_rpn_loc: 0.054  time: 0.5648  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:40:38 d2.utils.events]: [0m eta: 0:02:01  iter: 1779  total_loss: 0.562  loss_cls: 0.180  loss_box_reg: 0.313  loss_rpn_cls: 0.019  loss_rpn_loc: 0.050  time: 0.5652  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:40:50 d2.utils.events]: [0m eta: 0:01:50  iter: 1799  total_loss: 0.648  loss_cls: 0.211  loss_box_reg: 0.392  loss_rpn_cls: 0.023  loss_rpn_loc: 0.062  time: 0.5651  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:41:01 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.524  loss_cls: 0.169  loss_box_reg: 0.302  loss_rpn_cls: 0.018  loss_rpn_loc: 0.045  time: 0.5649  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:41:13 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.679  loss_cls: 0.214  loss_box_reg: 0.379  loss_rpn_cls: 0.015  loss_rpn_loc: 0.068  time: 0.5653  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:41:24 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.695  loss_cls: 0.229  loss_box_reg: 0.385  loss_rpn_cls: 0.025  loss_rpn_loc: 0.074  time: 0.5652  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:41:35 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.629  loss_cls: 0.195  loss_box_reg: 0.366  loss_rpn_cls: 0.021  loss_rpn_loc: 0.043  time: 0.5651  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:41:47 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.610  loss_cls: 0.199  loss_box_reg: 0.316  loss_rpn_cls: 0.020  loss_rpn_loc: 0.044  time: 0.5654  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:41:59 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.631  loss_cls: 0.206  loss_box_reg: 0.326  loss_rpn_cls: 0.023  loss_rpn_loc: 0.058  time: 0.5657  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:42:10 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.685  loss_cls: 0.221  loss_box_reg: 0.407  loss_rpn_cls: 0.021  loss_rpn_loc: 0.046  time: 0.5655  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:42:21 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.561  loss_cls: 0.189  loss_box_reg: 0.292  loss_rpn_cls: 0.021  loss_rpn_loc: 0.045  time: 0.5652  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:42:33 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.660  loss_cls: 0.203  loss_box_reg: 0.382  loss_rpn_cls: 0.016  loss_rpn_loc: 0.059  time: 0.5656  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 08:42:57 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:42:57 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 08:42:57 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 08:42:57 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.637  loss_cls: 0.206  loss_box_reg: 0.351  loss_rpn_cls: 0.014  loss_rpn_loc: 0.048  time: 0.5654  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:43:00 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:49 (0.5657 s / it)
[32m[04/15 08:43:00 d2.engine.hooks]: [0mTotal training time: 0:19:17 (0:00:28 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 08:43:07 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:43:07 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 08:43:07 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 08:43:12 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1196 s / img. ETA=0:03:17
[32m[04/15 08:43:18 d2.evaluation.evaluator]: [0mInference done 47/1257. 0.1205 s / img. ETA=0:03:02
[32m[04/15 08:43:23 d2.evaluation.evaluator]: [0mInference done 80/1257. 0.1220 s / img. ETA=0:02:58
[32m[04/15 08:43:28 d2.evaluation.evaluator]: [0mInference done 113/1257. 0.1224 s / img. ETA=0:02:53
[32m[04/15 08:43:33 d2.evaluation.evaluator]: [0mInference done 147/1257. 0.1221 s / img. ETA=0:02:47
[32m[04/15 08:43:38 d2.evaluation.evaluator]: [0mInference done 183/1257. 0.1221 s / img. ETA=0:02:39
[32m[04/15 08:43:43 d2.evaluation.evaluator]: [0mInference done 217/1257. 0.1260 s / img. ETA=0:02:35
[32m[04/15 08:43:48 d2.evaluation.evaluator]: [0mInference done 248/1257. 0.1260 s / img. ETA=0:02:32
[32m[04/15 08:43:54 d2.evaluation.evaluator]: [0mInference done 267/1257. 0.1257 s / img. ETA=0:02:40
[32m[04/15 08:43:59 d2.evaluation.evaluator]: [0mInference done 287/1257. 0.1258 s / img. ETA=0:02:43
[32m[04/15 08:44:04 d2.evaluation.evaluator]: [0mInference done 320/1257. 0.1257 s / img. ETA=0:02:36
[32m[04/15 08:44:09 d2.evaluation.evaluator]: [0mInference done 357/1257. 0.1258 s / img. ETA=0:02:27
[32m[04/15 08:44:14 d2.evaluation.evaluator]: [0mInference done 396/1257. 0.1258 s / img. ETA=0:02:18
[32m[04/15 08:44:19 d2.evaluation.evaluator]: [0mInference done 432/1257. 0.1256 s / img. ETA=0:02:11
[32m[04/15 08:44:24 d2.evaluation.evaluator]: [0mInference done 460/1257. 0.1256 s / img. ETA=0:02:08
[32m[04/15 08:44:30 d2.evaluation.evaluator]: [0mInference done 490/1257. 0.1258 s / img. ETA=0:02:03
[32m[04/15 08:44:35 d2.evaluation.evaluator]: [0mInference done 520/1257. 0.1268 s / img. ETA=0:01:59
[32m[04/15 08:44:40 d2.evaluation.evaluator]: [0mInference done 547/1257. 0.1265 s / img. ETA=0:01:56
[32m[04/15 08:44:45 d2.evaluation.evaluator]: [0mInference done 577/1257. 0.1262 s / img. ETA=0:01:51
[32m[04/15 08:44:50 d2.evaluation.evaluator]: [0mInference done 608/1257. 0.1262 s / img. ETA=0:01:46
[32m[04/15 08:44:55 d2.evaluation.evaluator]: [0mInference done 638/1257. 0.1259 s / img. ETA=0:01:41
[32m[04/15 08:45:00 d2.evaluation.evaluator]: [0mInference done 661/1257. 0.1257 s / img. ETA=0:01:39
[32m[04/15 08:45:05 d2.evaluation.evaluator]: [0mInference done 691/1257. 0.1255 s / img. ETA=0:01:34
[32m[04/15 08:45:10 d2.evaluation.evaluator]: [0mInference done 725/1257. 0.1257 s / img. ETA=0:01:28
[32m[04/15 08:45:16 d2.evaluation.evaluator]: [0mInference done 758/1257. 0.1266 s / img. ETA=0:01:22
[32m[04/15 08:45:21 d2.evaluation.evaluator]: [0mInference done 794/1257. 0.1270 s / img. ETA=0:01:15
[32m[04/15 08:45:26 d2.evaluation.evaluator]: [0mInference done 833/1257. 0.1268 s / img. ETA=0:01:08
[32m[04/15 08:45:31 d2.evaluation.evaluator]: [0mInference done 868/1257. 0.1267 s / img. ETA=0:01:02
[32m[04/15 08:45:36 d2.evaluation.evaluator]: [0mInference done 906/1257. 0.1264 s / img. ETA=0:00:56
[32m[04/15 08:45:41 d2.evaluation.evaluator]: [0mInference done 942/1257. 0.1261 s / img. ETA=0:00:50
[32m[04/15 08:45:46 d2.evaluation.evaluator]: [0mInference done 979/1257. 0.1259 s / img. ETA=0:00:44
[32m[04/15 08:45:51 d2.evaluation.evaluator]: [0mInference done 1014/1257. 0.1262 s / img. ETA=0:00:38
[32m[04/15 08:45:56 d2.evaluation.evaluator]: [0mInference done 1046/1257. 0.1270 s / img. ETA=0:00:33
[32m[04/15 08:46:01 d2.evaluation.evaluator]: [0mInference done 1084/1257. 0.1268 s / img. ETA=0:00:27
[32m[04/15 08:46:06 d2.evaluation.evaluator]: [0mInference done 1114/1257. 0.1268 s / img. ETA=0:00:22
[32m[04/15 08:46:11 d2.evaluation.evaluator]: [0mInference done 1138/1257. 0.1266 s / img. ETA=0:00:18
[32m[04/15 08:46:17 d2.evaluation.evaluator]: [0mInference done 1160/1257. 0.1265 s / img. ETA=0:00:15
[32m[04/15 08:46:22 d2.evaluation.evaluator]: [0mInference done 1184/1257. 0.1263 s / img. ETA=0:00:11
[32m[04/15 08:46:27 d2.evaluation.evaluator]: [0mInference done 1206/1257. 0.1264 s / img. ETA=0:00:08
[32m[04/15 08:46:32 d2.evaluation.evaluator]: [0mInference done 1235/1257. 0.1269 s / img. ETA=0:00:03
[32m[04/15 08:46:35 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:23.904007 (0.162863 s / img per device, on 1 devices)
[32m[04/15 08:46:35 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:38 (0.126792 s / img per device, on 1 devices)
[32m[04/15 08:46:36 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 08:46:36 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 08:46:36 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=6.37s).
Accumulating evaluation results...
DONE (t=1.52s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.196
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.399
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.168
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.241
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.401
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.098
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.249
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.193
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.291
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.436
[32m[04/15 08:46:46 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.558 | 39.923 | 16.793 | 12.946 | 24.123 | 40.134 |
[32m[04/15 08:46:46 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 28.959 | bicycle       | 9.358 | car            | 39.915 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  14  *  2000  iterations ============
4 channel input
[32m[04/15 08:46:47 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 08:46:49 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.14 seconds.
[5m[31mWARNING[0m [32m[04/15 08:46:49 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 08:46:49 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 08:46:50 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 08:46:50 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 08:46:50 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 08:46:55 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 08:47:08 d2.utils.events]: [0m eta: 0:17:39  iter: 19  total_loss: 0.576  loss_cls: 0.181  loss_box_reg: 0.332  loss_rpn_cls: 0.011  loss_rpn_loc: 0.042  time: 0.5373  data_time: 0.0880  lr: 0.000100  max_mem: 3067M
[32m[04/15 08:47:21 d2.utils.events]: [0m eta: 0:19:02  iter: 39  total_loss: 0.673  loss_cls: 0.209  loss_box_reg: 0.389  loss_rpn_cls: 0.013  loss_rpn_loc: 0.047  time: 0.5900  data_time: 0.0105  lr: 0.000200  max_mem: 3067M
[32m[04/15 08:47:33 d2.utils.events]: [0m eta: 0:18:40  iter: 59  total_loss: 0.473  loss_cls: 0.156  loss_box_reg: 0.272  loss_rpn_cls: 0.014  loss_rpn_loc: 0.035  time: 0.5844  data_time: 0.0074  lr: 0.000300  max_mem: 3067M
[32m[04/15 08:47:44 d2.utils.events]: [0m eta: 0:18:06  iter: 79  total_loss: 0.555  loss_cls: 0.198  loss_box_reg: 0.284  loss_rpn_cls: 0.018  loss_rpn_loc: 0.056  time: 0.5757  data_time: 0.0063  lr: 0.000400  max_mem: 3067M
[32m[04/15 08:47:56 d2.utils.events]: [0m eta: 0:17:58  iter: 99  total_loss: 0.555  loss_cls: 0.174  loss_box_reg: 0.314  loss_rpn_cls: 0.014  loss_rpn_loc: 0.052  time: 0.5801  data_time: 0.0077  lr: 0.000500  max_mem: 3067M
[32m[04/15 08:48:08 d2.utils.events]: [0m eta: 0:17:50  iter: 119  total_loss: 0.561  loss_cls: 0.179  loss_box_reg: 0.322  loss_rpn_cls: 0.012  loss_rpn_loc: 0.049  time: 0.5824  data_time: 0.0075  lr: 0.000599  max_mem: 3067M
[32m[04/15 08:48:19 d2.utils.events]: [0m eta: 0:17:31  iter: 139  total_loss: 0.691  loss_cls: 0.232  loss_box_reg: 0.357  loss_rpn_cls: 0.014  loss_rpn_loc: 0.067  time: 0.5774  data_time: 0.0076  lr: 0.000699  max_mem: 3067M
[32m[04/15 08:48:30 d2.utils.events]: [0m eta: 0:17:21  iter: 159  total_loss: 0.558  loss_cls: 0.193  loss_box_reg: 0.310  loss_rpn_cls: 0.014  loss_rpn_loc: 0.051  time: 0.5755  data_time: 0.0079  lr: 0.000799  max_mem: 3067M
[32m[04/15 08:48:43 d2.utils.events]: [0m eta: 0:17:18  iter: 179  total_loss: 0.621  loss_cls: 0.206  loss_box_reg: 0.332  loss_rpn_cls: 0.017  loss_rpn_loc: 0.048  time: 0.5817  data_time: 0.0117  lr: 0.000899  max_mem: 3067M
[32m[04/15 08:48:54 d2.utils.events]: [0m eta: 0:17:05  iter: 199  total_loss: 0.647  loss_cls: 0.225  loss_box_reg: 0.359  loss_rpn_cls: 0.015  loss_rpn_loc: 0.049  time: 0.5788  data_time: 0.0070  lr: 0.000999  max_mem: 3067M
[32m[04/15 08:49:05 d2.utils.events]: [0m eta: 0:16:47  iter: 219  total_loss: 0.729  loss_cls: 0.221  loss_box_reg: 0.392  loss_rpn_cls: 0.012  loss_rpn_loc: 0.050  time: 0.5748  data_time: 0.0062  lr: 0.001099  max_mem: 3067M
[32m[04/15 08:49:17 d2.utils.events]: [0m eta: 0:16:36  iter: 239  total_loss: 0.709  loss_cls: 0.239  loss_box_reg: 0.391  loss_rpn_cls: 0.017  loss_rpn_loc: 0.062  time: 0.5788  data_time: 0.0127  lr: 0.001199  max_mem: 3067M
[32m[04/15 08:49:29 d2.utils.events]: [0m eta: 0:16:25  iter: 259  total_loss: 0.656  loss_cls: 0.225  loss_box_reg: 0.382  loss_rpn_cls: 0.020  loss_rpn_loc: 0.058  time: 0.5780  data_time: 0.0083  lr: 0.001299  max_mem: 3067M
[32m[04/15 08:49:39 d2.utils.events]: [0m eta: 0:16:12  iter: 279  total_loss: 0.611  loss_cls: 0.198  loss_box_reg: 0.336  loss_rpn_cls: 0.012  loss_rpn_loc: 0.043  time: 0.5752  data_time: 0.0071  lr: 0.001399  max_mem: 3067M
[32m[04/15 08:49:51 d2.utils.events]: [0m eta: 0:16:02  iter: 299  total_loss: 0.573  loss_cls: 0.187  loss_box_reg: 0.329  loss_rpn_cls: 0.011  loss_rpn_loc: 0.047  time: 0.5754  data_time: 0.0078  lr: 0.001499  max_mem: 3067M
[32m[04/15 08:50:03 d2.utils.events]: [0m eta: 0:15:42  iter: 319  total_loss: 0.635  loss_cls: 0.203  loss_box_reg: 0.345  loss_rpn_cls: 0.016  loss_rpn_loc: 0.047  time: 0.5759  data_time: 0.0078  lr: 0.001598  max_mem: 3067M
[32m[04/15 08:50:14 d2.utils.events]: [0m eta: 0:15:27  iter: 339  total_loss: 0.521  loss_cls: 0.173  loss_box_reg: 0.308  loss_rpn_cls: 0.011  loss_rpn_loc: 0.040  time: 0.5739  data_time: 0.0070  lr: 0.001698  max_mem: 3067M
[32m[04/15 08:50:25 d2.utils.events]: [0m eta: 0:15:16  iter: 359  total_loss: 0.494  loss_cls: 0.157  loss_box_reg: 0.295  loss_rpn_cls: 0.009  loss_rpn_loc: 0.031  time: 0.5725  data_time: 0.0069  lr: 0.001798  max_mem: 3067M
[32m[04/15 08:50:37 d2.utils.events]: [0m eta: 0:15:07  iter: 379  total_loss: 0.564  loss_cls: 0.192  loss_box_reg: 0.320  loss_rpn_cls: 0.014  loss_rpn_loc: 0.043  time: 0.5744  data_time: 0.0070  lr: 0.001898  max_mem: 3067M
[32m[04/15 08:50:48 d2.utils.events]: [0m eta: 0:14:54  iter: 399  total_loss: 0.622  loss_cls: 0.213  loss_box_reg: 0.353  loss_rpn_cls: 0.014  loss_rpn_loc: 0.050  time: 0.5733  data_time: 0.0067  lr: 0.001998  max_mem: 3067M
[32m[04/15 08:50:59 d2.utils.events]: [0m eta: 0:14:42  iter: 419  total_loss: 0.595  loss_cls: 0.173  loss_box_reg: 0.335  loss_rpn_cls: 0.018  loss_rpn_loc: 0.041  time: 0.5715  data_time: 0.0065  lr: 0.002098  max_mem: 3067M
[32m[04/15 08:51:12 d2.utils.events]: [0m eta: 0:14:32  iter: 439  total_loss: 0.654  loss_cls: 0.204  loss_box_reg: 0.364  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5745  data_time: 0.0078  lr: 0.002198  max_mem: 3067M
[32m[04/15 08:51:23 d2.utils.events]: [0m eta: 0:14:20  iter: 459  total_loss: 0.556  loss_cls: 0.189  loss_box_reg: 0.297  loss_rpn_cls: 0.016  loss_rpn_loc: 0.045  time: 0.5731  data_time: 0.0066  lr: 0.002298  max_mem: 3067M
[32m[04/15 08:51:34 d2.utils.events]: [0m eta: 0:14:09  iter: 479  total_loss: 0.637  loss_cls: 0.206  loss_box_reg: 0.354  loss_rpn_cls: 0.007  loss_rpn_loc: 0.041  time: 0.5721  data_time: 0.0096  lr: 0.002398  max_mem: 3067M
[32m[04/15 08:51:46 d2.utils.events]: [0m eta: 0:14:00  iter: 499  total_loss: 0.666  loss_cls: 0.211  loss_box_reg: 0.359  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5741  data_time: 0.0079  lr: 0.002498  max_mem: 3067M
[32m[04/15 08:51:58 d2.utils.events]: [0m eta: 0:13:48  iter: 519  total_loss: 0.535  loss_cls: 0.182  loss_box_reg: 0.298  loss_rpn_cls: 0.016  loss_rpn_loc: 0.034  time: 0.5739  data_time: 0.0079  lr: 0.002597  max_mem: 3067M
[32m[04/15 08:52:09 d2.utils.events]: [0m eta: 0:13:36  iter: 539  total_loss: 0.627  loss_cls: 0.194  loss_box_reg: 0.357  loss_rpn_cls: 0.016  loss_rpn_loc: 0.051  time: 0.5730  data_time: 0.0069  lr: 0.002697  max_mem: 3067M
[32m[04/15 08:52:21 d2.utils.events]: [0m eta: 0:13:25  iter: 559  total_loss: 0.576  loss_cls: 0.191  loss_box_reg: 0.294  loss_rpn_cls: 0.012  loss_rpn_loc: 0.058  time: 0.5735  data_time: 0.0075  lr: 0.002797  max_mem: 3067M
[32m[04/15 08:52:32 d2.utils.events]: [0m eta: 0:13:13  iter: 579  total_loss: 0.618  loss_cls: 0.201  loss_box_reg: 0.321  loss_rpn_cls: 0.015  loss_rpn_loc: 0.053  time: 0.5732  data_time: 0.0082  lr: 0.002897  max_mem: 3067M
[32m[04/15 08:52:43 d2.utils.events]: [0m eta: 0:13:02  iter: 599  total_loss: 0.665  loss_cls: 0.215  loss_box_reg: 0.380  loss_rpn_cls: 0.014  loss_rpn_loc: 0.054  time: 0.5725  data_time: 0.0083  lr: 0.002997  max_mem: 3067M
[32m[04/15 08:52:54 d2.utils.events]: [0m eta: 0:12:50  iter: 619  total_loss: 0.748  loss_cls: 0.250  loss_box_reg: 0.402  loss_rpn_cls: 0.022  loss_rpn_loc: 0.062  time: 0.5714  data_time: 0.0091  lr: 0.003097  max_mem: 3067M
[32m[04/15 08:53:07 d2.utils.events]: [0m eta: 0:12:40  iter: 639  total_loss: 0.608  loss_cls: 0.188  loss_box_reg: 0.343  loss_rpn_cls: 0.013  loss_rpn_loc: 0.041  time: 0.5731  data_time: 0.0071  lr: 0.003197  max_mem: 3067M
[32m[04/15 08:53:18 d2.utils.events]: [0m eta: 0:12:28  iter: 659  total_loss: 0.518  loss_cls: 0.173  loss_box_reg: 0.304  loss_rpn_cls: 0.012  loss_rpn_loc: 0.048  time: 0.5721  data_time: 0.0069  lr: 0.003297  max_mem: 3067M
[32m[04/15 08:53:28 d2.utils.events]: [0m eta: 0:12:17  iter: 679  total_loss: 0.678  loss_cls: 0.204  loss_box_reg: 0.381  loss_rpn_cls: 0.015  loss_rpn_loc: 0.075  time: 0.5704  data_time: 0.0080  lr: 0.003397  max_mem: 3067M
[32m[04/15 08:53:41 d2.utils.events]: [0m eta: 0:12:06  iter: 699  total_loss: 0.585  loss_cls: 0.207  loss_box_reg: 0.326  loss_rpn_cls: 0.015  loss_rpn_loc: 0.037  time: 0.5722  data_time: 0.0090  lr: 0.003497  max_mem: 3067M
[32m[04/15 08:53:52 d2.utils.events]: [0m eta: 0:11:54  iter: 719  total_loss: 0.520  loss_cls: 0.152  loss_box_reg: 0.293  loss_rpn_cls: 0.019  loss_rpn_loc: 0.045  time: 0.5709  data_time: 0.0066  lr: 0.003596  max_mem: 3067M
[32m[04/15 08:54:02 d2.utils.events]: [0m eta: 0:11:43  iter: 739  total_loss: 0.534  loss_cls: 0.162  loss_box_reg: 0.265  loss_rpn_cls: 0.015  loss_rpn_loc: 0.044  time: 0.5701  data_time: 0.0063  lr: 0.003696  max_mem: 3067M
[32m[04/15 08:54:14 d2.utils.events]: [0m eta: 0:11:31  iter: 759  total_loss: 0.551  loss_cls: 0.184  loss_box_reg: 0.292  loss_rpn_cls: 0.014  loss_rpn_loc: 0.042  time: 0.5700  data_time: 0.0063  lr: 0.003796  max_mem: 3067M
[32m[04/15 08:54:26 d2.utils.events]: [0m eta: 0:11:20  iter: 779  total_loss: 0.660  loss_cls: 0.215  loss_box_reg: 0.373  loss_rpn_cls: 0.018  loss_rpn_loc: 0.042  time: 0.5705  data_time: 0.0073  lr: 0.003896  max_mem: 3067M
[32m[04/15 08:54:37 d2.utils.events]: [0m eta: 0:11:09  iter: 799  total_loss: 0.664  loss_cls: 0.210  loss_box_reg: 0.355  loss_rpn_cls: 0.016  loss_rpn_loc: 0.060  time: 0.5697  data_time: 0.0063  lr: 0.003996  max_mem: 3067M
[32m[04/15 08:54:48 d2.utils.events]: [0m eta: 0:10:56  iter: 819  total_loss: 0.620  loss_cls: 0.198  loss_box_reg: 0.355  loss_rpn_cls: 0.012  loss_rpn_loc: 0.046  time: 0.5693  data_time: 0.0067  lr: 0.004096  max_mem: 3067M
[32m[04/15 08:55:01 d2.utils.events]: [0m eta: 0:10:46  iter: 839  total_loss: 0.625  loss_cls: 0.212  loss_box_reg: 0.344  loss_rpn_cls: 0.020  loss_rpn_loc: 0.071  time: 0.5708  data_time: 0.0120  lr: 0.004196  max_mem: 3067M
[32m[04/15 08:55:11 d2.utils.events]: [0m eta: 0:10:35  iter: 859  total_loss: 0.660  loss_cls: 0.207  loss_box_reg: 0.365  loss_rpn_cls: 0.015  loss_rpn_loc: 0.056  time: 0.5701  data_time: 0.0069  lr: 0.004296  max_mem: 3067M
[32m[04/15 08:55:22 d2.utils.events]: [0m eta: 0:10:22  iter: 879  total_loss: 0.660  loss_cls: 0.192  loss_box_reg: 0.375  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5690  data_time: 0.0072  lr: 0.004396  max_mem: 3067M
[32m[04/15 08:55:35 d2.utils.events]: [0m eta: 0:10:13  iter: 899  total_loss: 0.648  loss_cls: 0.213  loss_box_reg: 0.361  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5706  data_time: 0.0073  lr: 0.004496  max_mem: 3067M
[32m[04/15 08:55:46 d2.utils.events]: [0m eta: 0:10:01  iter: 919  total_loss: 0.589  loss_cls: 0.194  loss_box_reg: 0.331  loss_rpn_cls: 0.012  loss_rpn_loc: 0.041  time: 0.5703  data_time: 0.0072  lr: 0.004595  max_mem: 3067M
[32m[04/15 08:55:57 d2.utils.events]: [0m eta: 0:09:50  iter: 939  total_loss: 0.642  loss_cls: 0.201  loss_box_reg: 0.369  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5698  data_time: 0.0063  lr: 0.004695  max_mem: 3067M
[32m[04/15 08:56:10 d2.utils.events]: [0m eta: 0:09:40  iter: 959  total_loss: 0.741  loss_cls: 0.230  loss_box_reg: 0.401  loss_rpn_cls: 0.018  loss_rpn_loc: 0.067  time: 0.5710  data_time: 0.0082  lr: 0.004795  max_mem: 3067M
[32m[04/15 08:56:21 d2.utils.events]: [0m eta: 0:09:28  iter: 979  total_loss: 0.620  loss_cls: 0.201  loss_box_reg: 0.367  loss_rpn_cls: 0.014  loss_rpn_loc: 0.036  time: 0.5705  data_time: 0.0083  lr: 0.004895  max_mem: 3067M
[32m[04/15 08:56:32 d2.utils.events]: [0m eta: 0:09:16  iter: 999  total_loss: 0.584  loss_cls: 0.198  loss_box_reg: 0.332  loss_rpn_cls: 0.015  loss_rpn_loc: 0.048  time: 0.5699  data_time: 0.0075  lr: 0.004995  max_mem: 3067M
[32m[04/15 08:56:44 d2.utils.events]: [0m eta: 0:09:07  iter: 1019  total_loss: 0.750  loss_cls: 0.238  loss_box_reg: 0.408  loss_rpn_cls: 0.023  loss_rpn_loc: 0.060  time: 0.5702  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:56:55 d2.utils.events]: [0m eta: 0:08:55  iter: 1039  total_loss: 0.602  loss_cls: 0.202  loss_box_reg: 0.379  loss_rpn_cls: 0.026  loss_rpn_loc: 0.052  time: 0.5706  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:57:06 d2.utils.events]: [0m eta: 0:08:43  iter: 1059  total_loss: 0.582  loss_cls: 0.192  loss_box_reg: 0.328  loss_rpn_cls: 0.013  loss_rpn_loc: 0.052  time: 0.5699  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:57:17 d2.utils.events]: [0m eta: 0:08:31  iter: 1079  total_loss: 0.583  loss_cls: 0.204  loss_box_reg: 0.335  loss_rpn_cls: 0.018  loss_rpn_loc: 0.048  time: 0.5692  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:57:30 d2.utils.events]: [0m eta: 0:08:20  iter: 1099  total_loss: 0.661  loss_cls: 0.214  loss_box_reg: 0.360  loss_rpn_cls: 0.021  loss_rpn_loc: 0.057  time: 0.5707  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:57:41 d2.utils.events]: [0m eta: 0:08:08  iter: 1119  total_loss: 0.729  loss_cls: 0.240  loss_box_reg: 0.408  loss_rpn_cls: 0.013  loss_rpn_loc: 0.083  time: 0.5700  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:57:51 d2.utils.events]: [0m eta: 0:07:56  iter: 1139  total_loss: 0.707  loss_cls: 0.232  loss_box_reg: 0.381  loss_rpn_cls: 0.024  loss_rpn_loc: 0.067  time: 0.5692  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:58:04 d2.utils.events]: [0m eta: 0:07:45  iter: 1159  total_loss: 0.633  loss_cls: 0.209  loss_box_reg: 0.368  loss_rpn_cls: 0.023  loss_rpn_loc: 0.046  time: 0.5698  data_time: 0.0136  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:58:15 d2.utils.events]: [0m eta: 0:07:34  iter: 1179  total_loss: 0.653  loss_cls: 0.240  loss_box_reg: 0.354  loss_rpn_cls: 0.027  loss_rpn_loc: 0.066  time: 0.5694  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:58:25 d2.utils.events]: [0m eta: 0:07:22  iter: 1199  total_loss: 0.744  loss_cls: 0.228  loss_box_reg: 0.397  loss_rpn_cls: 0.019  loss_rpn_loc: 0.077  time: 0.5687  data_time: 0.0059  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:58:37 d2.utils.events]: [0m eta: 0:07:12  iter: 1219  total_loss: 0.608  loss_cls: 0.193  loss_box_reg: 0.323  loss_rpn_cls: 0.010  loss_rpn_loc: 0.052  time: 0.5690  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:58:49 d2.utils.events]: [0m eta: 0:07:00  iter: 1239  total_loss: 0.686  loss_cls: 0.204  loss_box_reg: 0.335  loss_rpn_cls: 0.028  loss_rpn_loc: 0.077  time: 0.5692  data_time: 0.0113  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:58:59 d2.utils.events]: [0m eta: 0:06:49  iter: 1259  total_loss: 0.569  loss_cls: 0.179  loss_box_reg: 0.306  loss_rpn_cls: 0.021  loss_rpn_loc: 0.051  time: 0.5685  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:59:11 d2.utils.events]: [0m eta: 0:06:38  iter: 1279  total_loss: 0.675  loss_cls: 0.202  loss_box_reg: 0.385  loss_rpn_cls: 0.018  loss_rpn_loc: 0.079  time: 0.5683  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:59:23 d2.utils.events]: [0m eta: 0:06:27  iter: 1299  total_loss: 0.649  loss_cls: 0.213  loss_box_reg: 0.348  loss_rpn_cls: 0.020  loss_rpn_loc: 0.052  time: 0.5693  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:59:34 d2.utils.events]: [0m eta: 0:06:16  iter: 1319  total_loss: 0.604  loss_cls: 0.183  loss_box_reg: 0.336  loss_rpn_cls: 0.022  loss_rpn_loc: 0.048  time: 0.5690  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:59:46 d2.utils.events]: [0m eta: 0:06:05  iter: 1339  total_loss: 0.656  loss_cls: 0.211  loss_box_reg: 0.338  loss_rpn_cls: 0.017  loss_rpn_loc: 0.053  time: 0.5687  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 08:59:57 d2.utils.events]: [0m eta: 0:05:54  iter: 1359  total_loss: 0.708  loss_cls: 0.209  loss_box_reg: 0.384  loss_rpn_cls: 0.014  loss_rpn_loc: 0.053  time: 0.5689  data_time: 0.0097  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:00:10 d2.utils.events]: [0m eta: 0:05:43  iter: 1379  total_loss: 0.688  loss_cls: 0.220  loss_box_reg: 0.379  loss_rpn_cls: 0.019  loss_rpn_loc: 0.072  time: 0.5697  data_time: 0.1103  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:00:21 d2.utils.events]: [0m eta: 0:05:32  iter: 1399  total_loss: 0.679  loss_cls: 0.213  loss_box_reg: 0.377  loss_rpn_cls: 0.014  loss_rpn_loc: 0.073  time: 0.5691  data_time: 0.0094  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:00:33 d2.utils.events]: [0m eta: 0:05:21  iter: 1419  total_loss: 0.567  loss_cls: 0.173  loss_box_reg: 0.317  loss_rpn_cls: 0.017  loss_rpn_loc: 0.053  time: 0.5695  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:00:45 d2.utils.events]: [0m eta: 0:05:10  iter: 1439  total_loss: 0.621  loss_cls: 0.209  loss_box_reg: 0.373  loss_rpn_cls: 0.015  loss_rpn_loc: 0.065  time: 0.5704  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:00:56 d2.utils.events]: [0m eta: 0:04:59  iter: 1459  total_loss: 0.635  loss_cls: 0.205  loss_box_reg: 0.358  loss_rpn_cls: 0.019  loss_rpn_loc: 0.067  time: 0.5695  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:01:07 d2.utils.events]: [0m eta: 0:04:48  iter: 1479  total_loss: 0.673  loss_cls: 0.213  loss_box_reg: 0.354  loss_rpn_cls: 0.019  loss_rpn_loc: 0.061  time: 0.5690  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:01:18 d2.utils.events]: [0m eta: 0:04:37  iter: 1499  total_loss: 0.665  loss_cls: 0.202  loss_box_reg: 0.383  loss_rpn_cls: 0.014  loss_rpn_loc: 0.055  time: 0.5693  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:01:30 d2.utils.events]: [0m eta: 0:04:25  iter: 1519  total_loss: 0.701  loss_cls: 0.233  loss_box_reg: 0.382  loss_rpn_cls: 0.026  loss_rpn_loc: 0.067  time: 0.5693  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:01:40 d2.utils.events]: [0m eta: 0:04:13  iter: 1539  total_loss: 0.632  loss_cls: 0.195  loss_box_reg: 0.321  loss_rpn_cls: 0.018  loss_rpn_loc: 0.047  time: 0.5686  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:01:51 d2.utils.events]: [0m eta: 0:04:02  iter: 1559  total_loss: 0.566  loss_cls: 0.206  loss_box_reg: 0.313  loss_rpn_cls: 0.020  loss_rpn_loc: 0.050  time: 0.5681  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:02:04 d2.utils.events]: [0m eta: 0:03:51  iter: 1579  total_loss: 0.678  loss_cls: 0.224  loss_box_reg: 0.368  loss_rpn_cls: 0.014  loss_rpn_loc: 0.042  time: 0.5688  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:02:14 d2.utils.events]: [0m eta: 0:03:40  iter: 1599  total_loss: 0.631  loss_cls: 0.197  loss_box_reg: 0.307  loss_rpn_cls: 0.020  loss_rpn_loc: 0.062  time: 0.5682  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:02:25 d2.utils.events]: [0m eta: 0:03:29  iter: 1619  total_loss: 0.766  loss_cls: 0.247  loss_box_reg: 0.428  loss_rpn_cls: 0.025  loss_rpn_loc: 0.062  time: 0.5679  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:02:38 d2.utils.events]: [0m eta: 0:03:18  iter: 1639  total_loss: 0.586  loss_cls: 0.189  loss_box_reg: 0.315  loss_rpn_cls: 0.013  loss_rpn_loc: 0.052  time: 0.5685  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:02:49 d2.utils.events]: [0m eta: 0:03:07  iter: 1659  total_loss: 0.630  loss_cls: 0.221  loss_box_reg: 0.356  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5685  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:03:00 d2.utils.events]: [0m eta: 0:02:57  iter: 1679  total_loss: 0.650  loss_cls: 0.201  loss_box_reg: 0.377  loss_rpn_cls: 0.017  loss_rpn_loc: 0.048  time: 0.5681  data_time: 0.0096  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:03:12 d2.utils.events]: [0m eta: 0:02:45  iter: 1699  total_loss: 0.564  loss_cls: 0.181  loss_box_reg: 0.318  loss_rpn_cls: 0.021  loss_rpn_loc: 0.052  time: 0.5683  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:03:24 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.562  loss_cls: 0.193  loss_box_reg: 0.306  loss_rpn_cls: 0.027  loss_rpn_loc: 0.060  time: 0.5686  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:03:34 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.727  loss_cls: 0.215  loss_box_reg: 0.406  loss_rpn_cls: 0.021  loss_rpn_loc: 0.072  time: 0.5679  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:03:45 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.693  loss_cls: 0.219  loss_box_reg: 0.380  loss_rpn_cls: 0.022  loss_rpn_loc: 0.062  time: 0.5678  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:03:57 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.570  loss_cls: 0.168  loss_box_reg: 0.332  loss_rpn_cls: 0.013  loss_rpn_loc: 0.062  time: 0.5681  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:04:08 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.580  loss_cls: 0.175  loss_box_reg: 0.321  loss_rpn_cls: 0.022  loss_rpn_loc: 0.045  time: 0.5678  data_time: 0.0092  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:04:19 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.684  loss_cls: 0.219  loss_box_reg: 0.366  loss_rpn_cls: 0.015  loss_rpn_loc: 0.048  time: 0.5674  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:04:31 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.734  loss_cls: 0.236  loss_box_reg: 0.387  loss_rpn_cls: 0.026  loss_rpn_loc: 0.074  time: 0.5680  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:04:42 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.628  loss_cls: 0.202  loss_box_reg: 0.343  loss_rpn_cls: 0.019  loss_rpn_loc: 0.073  time: 0.5677  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:04:53 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.704  loss_cls: 0.227  loss_box_reg: 0.363  loss_rpn_cls: 0.025  loss_rpn_loc: 0.064  time: 0.5674  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:05:05 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.654  loss_cls: 0.234  loss_box_reg: 0.363  loss_rpn_cls: 0.020  loss_rpn_loc: 0.054  time: 0.5679  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:05:17 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.599  loss_cls: 0.203  loss_box_reg: 0.358  loss_rpn_cls: 0.019  loss_rpn_loc: 0.047  time: 0.5681  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:05:28 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.646  loss_cls: 0.221  loss_box_reg: 0.338  loss_rpn_cls: 0.022  loss_rpn_loc: 0.052  time: 0.5677  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:05:40 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.667  loss_cls: 0.217  loss_box_reg: 0.352  loss_rpn_cls: 0.021  loss_rpn_loc: 0.048  time: 0.5680  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:05:52 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.617  loss_cls: 0.195  loss_box_reg: 0.328  loss_rpn_cls: 0.019  loss_rpn_loc: 0.056  time: 0.5683  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 09:06:16 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:06:16 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 09:06:17 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 09:06:17 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.792  loss_cls: 0.255  loss_box_reg: 0.444  loss_rpn_cls: 0.023  loss_rpn_loc: 0.067  time: 0.5680  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:06:20 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:54 (0.5683 s / it)
[32m[04/15 09:06:20 d2.engine.hooks]: [0mTotal training time: 0:19:21 (0:00:26 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 09:06:27 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:06:27 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 09:06:28 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 09:06:31 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1217 s / img. ETA=0:04:45
[32m[04/15 09:06:36 d2.evaluation.evaluator]: [0mInference done 37/1257. 0.1210 s / img. ETA=0:04:05
[32m[04/15 09:06:41 d2.evaluation.evaluator]: [0mInference done 70/1257. 0.1207 s / img. ETA=0:03:30
[32m[04/15 09:06:47 d2.evaluation.evaluator]: [0mInference done 100/1257. 0.1253 s / img. ETA=0:03:23
[32m[04/15 09:06:52 d2.evaluation.evaluator]: [0mInference done 138/1257. 0.1246 s / img. ETA=0:03:04
[32m[04/15 09:06:57 d2.evaluation.evaluator]: [0mInference done 169/1257. 0.1291 s / img. ETA=0:02:58
[32m[04/15 09:07:02 d2.evaluation.evaluator]: [0mInference done 204/1257. 0.1312 s / img. ETA=0:02:49
[32m[04/15 09:07:07 d2.evaluation.evaluator]: [0mInference done 239/1257. 0.1296 s / img. ETA=0:02:41
[32m[04/15 09:07:12 d2.evaluation.evaluator]: [0mInference done 273/1257. 0.1284 s / img. ETA=0:02:34
[32m[04/15 09:07:17 d2.evaluation.evaluator]: [0mInference done 309/1257. 0.1274 s / img. ETA=0:02:27
[32m[04/15 09:07:22 d2.evaluation.evaluator]: [0mInference done 348/1257. 0.1267 s / img. ETA=0:02:18
[32m[04/15 09:07:27 d2.evaluation.evaluator]: [0mInference done 387/1257. 0.1262 s / img. ETA=0:02:10
[32m[04/15 09:07:32 d2.evaluation.evaluator]: [0mInference done 424/1257. 0.1266 s / img. ETA=0:02:04
[32m[04/15 09:07:37 d2.evaluation.evaluator]: [0mInference done 448/1257. 0.1278 s / img. ETA=0:02:03
[32m[04/15 09:07:42 d2.evaluation.evaluator]: [0mInference done 478/1257. 0.1277 s / img. ETA=0:01:59
[32m[04/15 09:07:47 d2.evaluation.evaluator]: [0mInference done 506/1257. 0.1272 s / img. ETA=0:01:56
[32m[04/15 09:07:53 d2.evaluation.evaluator]: [0mInference done 540/1257. 0.1270 s / img. ETA=0:01:50
[32m[04/15 09:07:58 d2.evaluation.evaluator]: [0mInference done 573/1257. 0.1277 s / img. ETA=0:01:45
[32m[04/15 09:08:03 d2.evaluation.evaluator]: [0mInference done 604/1257. 0.1274 s / img. ETA=0:01:41
[32m[04/15 09:08:08 d2.evaluation.evaluator]: [0mInference done 635/1257. 0.1271 s / img. ETA=0:01:36
[32m[04/15 09:08:13 d2.evaluation.evaluator]: [0mInference done 664/1257. 0.1277 s / img. ETA=0:01:32
[32m[04/15 09:08:18 d2.evaluation.evaluator]: [0mInference done 696/1257. 0.1283 s / img. ETA=0:01:27
[32m[04/15 09:08:23 d2.evaluation.evaluator]: [0mInference done 732/1257. 0.1280 s / img. ETA=0:01:21
[32m[04/15 09:08:28 d2.evaluation.evaluator]: [0mInference done 763/1257. 0.1276 s / img. ETA=0:01:17
[32m[04/15 09:08:33 d2.evaluation.evaluator]: [0mInference done 797/1257. 0.1273 s / img. ETA=0:01:11
[32m[04/15 09:08:38 d2.evaluation.evaluator]: [0mInference done 830/1257. 0.1270 s / img. ETA=0:01:06
[32m[04/15 09:08:43 d2.evaluation.evaluator]: [0mInference done 859/1257. 0.1268 s / img. ETA=0:01:02
[32m[04/15 09:08:49 d2.evaluation.evaluator]: [0mInference done 893/1257. 0.1273 s / img. ETA=0:00:56
[32m[04/15 09:08:54 d2.evaluation.evaluator]: [0mInference done 926/1257. 0.1281 s / img. ETA=0:00:51
[32m[04/15 09:08:59 d2.evaluation.evaluator]: [0mInference done 964/1257. 0.1279 s / img. ETA=0:00:45
[32m[04/15 09:09:04 d2.evaluation.evaluator]: [0mInference done 994/1257. 0.1277 s / img. ETA=0:00:40
[32m[04/15 09:09:09 d2.evaluation.evaluator]: [0mInference done 1019/1257. 0.1275 s / img. ETA=0:00:37
[32m[04/15 09:09:14 d2.evaluation.evaluator]: [0mInference done 1050/1257. 0.1273 s / img. ETA=0:00:32
[32m[04/15 09:09:19 d2.evaluation.evaluator]: [0mInference done 1087/1257. 0.1271 s / img. ETA=0:00:26
[32m[04/15 09:09:24 d2.evaluation.evaluator]: [0mInference done 1116/1257. 0.1269 s / img. ETA=0:00:22
[32m[04/15 09:09:29 d2.evaluation.evaluator]: [0mInference done 1149/1257. 0.1274 s / img. ETA=0:00:16
[32m[04/15 09:09:34 d2.evaluation.evaluator]: [0mInference done 1182/1257. 0.1281 s / img. ETA=0:00:11
[32m[04/15 09:09:40 d2.evaluation.evaluator]: [0mInference done 1215/1257. 0.1280 s / img. ETA=0:00:06
[32m[04/15 09:09:45 d2.evaluation.evaluator]: [0mInference done 1255/1257. 0.1278 s / img. ETA=0:00:00
[32m[04/15 09:09:45 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:15.003564 (0.155754 s / img per device, on 1 devices)
[32m[04/15 09:09:45 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:40 (0.127815 s / img per device, on 1 devices)
[32m[04/15 09:09:45 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 09:09:45 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 09:09:45 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.30s).
Accumulating evaluation results...
DONE (t=1.31s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.188
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.373
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.167
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.119
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.233
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.229
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.232
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.271
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.428
[32m[04/15 09:09:54 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 18.832 | 37.330 | 16.702 | 11.945 | 23.291 | 39.102 |
[32m[04/15 09:09:54 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 24.946 | bicycle       | 8.469 | car            | 41.914 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  15  *  2000  iterations ============
4 channel input
[32m[04/15 09:09:55 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 09:09:57 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.54 seconds.
[5m[31mWARNING[0m [32m[04/15 09:09:57 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:09:57 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 09:09:58 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 09:09:58 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 09:09:58 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 09:10:01 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 09:10:14 d2.utils.events]: [0m eta: 0:19:21  iter: 19  total_loss: 0.600  loss_cls: 0.198  loss_box_reg: 0.303  loss_rpn_cls: 0.014  loss_rpn_loc: 0.047  time: 0.6113  data_time: 0.0781  lr: 0.000100  max_mem: 3067M
[32m[04/15 09:10:26 d2.utils.events]: [0m eta: 0:18:55  iter: 39  total_loss: 0.710  loss_cls: 0.207  loss_box_reg: 0.375  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5987  data_time: 0.0077  lr: 0.000200  max_mem: 3067M
[32m[04/15 09:10:37 d2.utils.events]: [0m eta: 0:17:59  iter: 59  total_loss: 0.704  loss_cls: 0.236  loss_box_reg: 0.405  loss_rpn_cls: 0.018  loss_rpn_loc: 0.070  time: 0.5741  data_time: 0.0092  lr: 0.000300  max_mem: 3067M
[32m[04/15 09:10:48 d2.utils.events]: [0m eta: 0:17:37  iter: 79  total_loss: 0.653  loss_cls: 0.192  loss_box_reg: 0.326  loss_rpn_cls: 0.023  loss_rpn_loc: 0.059  time: 0.5631  data_time: 0.0097  lr: 0.000400  max_mem: 3067M
[32m[04/15 09:11:00 d2.utils.events]: [0m eta: 0:17:34  iter: 99  total_loss: 0.586  loss_cls: 0.195  loss_box_reg: 0.334  loss_rpn_cls: 0.020  loss_rpn_loc: 0.047  time: 0.5748  data_time: 0.0085  lr: 0.000500  max_mem: 3067M
[32m[04/15 09:11:11 d2.utils.events]: [0m eta: 0:17:22  iter: 119  total_loss: 0.708  loss_cls: 0.226  loss_box_reg: 0.393  loss_rpn_cls: 0.015  loss_rpn_loc: 0.063  time: 0.5702  data_time: 0.0087  lr: 0.000599  max_mem: 3067M
[32m[04/15 09:11:22 d2.utils.events]: [0m eta: 0:17:03  iter: 139  total_loss: 0.518  loss_cls: 0.174  loss_box_reg: 0.290  loss_rpn_cls: 0.011  loss_rpn_loc: 0.056  time: 0.5638  data_time: 0.0101  lr: 0.000699  max_mem: 3067M
[32m[04/15 09:11:34 d2.utils.events]: [0m eta: 0:17:01  iter: 159  total_loss: 0.610  loss_cls: 0.182  loss_box_reg: 0.351  loss_rpn_cls: 0.015  loss_rpn_loc: 0.053  time: 0.5697  data_time: 0.0096  lr: 0.000799  max_mem: 3067M
[32m[04/15 09:11:46 d2.utils.events]: [0m eta: 0:16:50  iter: 179  total_loss: 0.548  loss_cls: 0.175  loss_box_reg: 0.320  loss_rpn_cls: 0.014  loss_rpn_loc: 0.047  time: 0.5703  data_time: 0.0071  lr: 0.000899  max_mem: 3067M
[32m[04/15 09:11:56 d2.utils.events]: [0m eta: 0:16:37  iter: 199  total_loss: 0.645  loss_cls: 0.211  loss_box_reg: 0.363  loss_rpn_cls: 0.012  loss_rpn_loc: 0.054  time: 0.5668  data_time: 0.0083  lr: 0.000999  max_mem: 3067M
[32m[04/15 09:12:08 d2.utils.events]: [0m eta: 0:16:28  iter: 219  total_loss: 0.524  loss_cls: 0.177  loss_box_reg: 0.285  loss_rpn_cls: 0.016  loss_rpn_loc: 0.046  time: 0.5695  data_time: 0.0065  lr: 0.001099  max_mem: 3067M
[32m[04/15 09:12:19 d2.utils.events]: [0m eta: 0:16:17  iter: 239  total_loss: 0.588  loss_cls: 0.195  loss_box_reg: 0.348  loss_rpn_cls: 0.016  loss_rpn_loc: 0.045  time: 0.5679  data_time: 0.0044  lr: 0.001199  max_mem: 3067M
[32m[04/15 09:12:31 d2.utils.events]: [0m eta: 0:16:05  iter: 259  total_loss: 0.617  loss_cls: 0.176  loss_box_reg: 0.337  loss_rpn_cls: 0.014  loss_rpn_loc: 0.054  time: 0.5696  data_time: 0.0041  lr: 0.001299  max_mem: 3067M
[32m[04/15 09:12:43 d2.utils.events]: [0m eta: 0:15:53  iter: 279  total_loss: 0.554  loss_cls: 0.178  loss_box_reg: 0.317  loss_rpn_cls: 0.013  loss_rpn_loc: 0.040  time: 0.5676  data_time: 0.0073  lr: 0.001399  max_mem: 3067M
[32m[04/15 09:12:53 d2.utils.events]: [0m eta: 0:15:40  iter: 299  total_loss: 0.561  loss_cls: 0.185  loss_box_reg: 0.339  loss_rpn_cls: 0.012  loss_rpn_loc: 0.043  time: 0.5655  data_time: 0.0086  lr: 0.001499  max_mem: 3067M
[32m[04/15 09:13:04 d2.utils.events]: [0m eta: 0:15:28  iter: 319  total_loss: 0.609  loss_cls: 0.196  loss_box_reg: 0.292  loss_rpn_cls: 0.011  loss_rpn_loc: 0.066  time: 0.5632  data_time: 0.0096  lr: 0.001598  max_mem: 3067M
[32m[04/15 09:13:17 d2.utils.events]: [0m eta: 0:15:18  iter: 339  total_loss: 0.521  loss_cls: 0.175  loss_box_reg: 0.304  loss_rpn_cls: 0.017  loss_rpn_loc: 0.031  time: 0.5652  data_time: 0.0079  lr: 0.001698  max_mem: 3067M
[32m[04/15 09:13:28 d2.utils.events]: [0m eta: 0:15:06  iter: 359  total_loss: 0.492  loss_cls: 0.142  loss_box_reg: 0.288  loss_rpn_cls: 0.012  loss_rpn_loc: 0.047  time: 0.5648  data_time: 0.0075  lr: 0.001798  max_mem: 3067M
[32m[04/15 09:13:39 d2.utils.events]: [0m eta: 0:14:55  iter: 379  total_loss: 0.616  loss_cls: 0.215  loss_box_reg: 0.333  loss_rpn_cls: 0.020  loss_rpn_loc: 0.056  time: 0.5641  data_time: 0.0064  lr: 0.001898  max_mem: 3067M
[32m[04/15 09:13:51 d2.utils.events]: [0m eta: 0:14:44  iter: 399  total_loss: 0.533  loss_cls: 0.169  loss_box_reg: 0.284  loss_rpn_cls: 0.014  loss_rpn_loc: 0.044  time: 0.5639  data_time: 0.0075  lr: 0.001998  max_mem: 3067M
[32m[04/15 09:14:03 d2.utils.events]: [0m eta: 0:14:35  iter: 419  total_loss: 0.576  loss_cls: 0.192  loss_box_reg: 0.338  loss_rpn_cls: 0.014  loss_rpn_loc: 0.036  time: 0.5659  data_time: 0.0070  lr: 0.002098  max_mem: 3067M
[32m[04/15 09:14:14 d2.utils.events]: [0m eta: 0:14:22  iter: 439  total_loss: 0.618  loss_cls: 0.204  loss_box_reg: 0.346  loss_rpn_cls: 0.013  loss_rpn_loc: 0.050  time: 0.5646  data_time: 0.0059  lr: 0.002198  max_mem: 3067M
[32m[04/15 09:14:25 d2.utils.events]: [0m eta: 0:14:10  iter: 459  total_loss: 0.599  loss_cls: 0.177  loss_box_reg: 0.311  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5642  data_time: 0.0094  lr: 0.002298  max_mem: 3067M
[32m[04/15 09:14:37 d2.utils.events]: [0m eta: 0:14:01  iter: 479  total_loss: 0.592  loss_cls: 0.194  loss_box_reg: 0.298  loss_rpn_cls: 0.019  loss_rpn_loc: 0.048  time: 0.5665  data_time: 0.0070  lr: 0.002398  max_mem: 3067M
[32m[04/15 09:14:48 d2.utils.events]: [0m eta: 0:13:49  iter: 499  total_loss: 0.600  loss_cls: 0.199  loss_box_reg: 0.294  loss_rpn_cls: 0.011  loss_rpn_loc: 0.040  time: 0.5654  data_time: 0.0064  lr: 0.002498  max_mem: 3067M
[32m[04/15 09:14:59 d2.utils.events]: [0m eta: 0:13:36  iter: 519  total_loss: 0.632  loss_cls: 0.202  loss_box_reg: 0.334  loss_rpn_cls: 0.014  loss_rpn_loc: 0.054  time: 0.5645  data_time: 0.0073  lr: 0.002597  max_mem: 3067M
[32m[04/15 09:15:12 d2.utils.events]: [0m eta: 0:13:27  iter: 539  total_loss: 0.598  loss_cls: 0.187  loss_box_reg: 0.293  loss_rpn_cls: 0.014  loss_rpn_loc: 0.054  time: 0.5665  data_time: 0.0113  lr: 0.002697  max_mem: 3067M
[32m[04/15 09:15:22 d2.utils.events]: [0m eta: 0:13:16  iter: 559  total_loss: 0.571  loss_cls: 0.170  loss_box_reg: 0.288  loss_rpn_cls: 0.013  loss_rpn_loc: 0.049  time: 0.5652  data_time: 0.0079  lr: 0.002797  max_mem: 3067M
[32m[04/15 09:15:33 d2.utils.events]: [0m eta: 0:13:05  iter: 579  total_loss: 0.728  loss_cls: 0.221  loss_box_reg: 0.399  loss_rpn_cls: 0.017  loss_rpn_loc: 0.067  time: 0.5641  data_time: 0.0084  lr: 0.002897  max_mem: 3067M
[32m[04/15 09:15:46 d2.utils.events]: [0m eta: 0:12:55  iter: 599  total_loss: 0.667  loss_cls: 0.222  loss_box_reg: 0.363  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5659  data_time: 0.0081  lr: 0.002997  max_mem: 3067M
[32m[04/15 09:15:58 d2.utils.events]: [0m eta: 0:12:45  iter: 619  total_loss: 0.651  loss_cls: 0.207  loss_box_reg: 0.370  loss_rpn_cls: 0.017  loss_rpn_loc: 0.052  time: 0.5684  data_time: 0.1222  lr: 0.003097  max_mem: 3067M
[32m[04/15 09:16:10 d2.utils.events]: [0m eta: 0:12:33  iter: 639  total_loss: 0.633  loss_cls: 0.210  loss_box_reg: 0.335  loss_rpn_cls: 0.011  loss_rpn_loc: 0.049  time: 0.5678  data_time: 0.0112  lr: 0.003197  max_mem: 3067M
[32m[04/15 09:16:21 d2.utils.events]: [0m eta: 0:12:22  iter: 659  total_loss: 0.659  loss_cls: 0.210  loss_box_reg: 0.357  loss_rpn_cls: 0.018  loss_rpn_loc: 0.056  time: 0.5673  data_time: 0.0163  lr: 0.003297  max_mem: 3067M
[32m[04/15 09:16:33 d2.utils.events]: [0m eta: 0:12:12  iter: 679  total_loss: 0.548  loss_cls: 0.169  loss_box_reg: 0.314  loss_rpn_cls: 0.015  loss_rpn_loc: 0.048  time: 0.5681  data_time: 0.0074  lr: 0.003397  max_mem: 3067M
[32m[04/15 09:16:44 d2.utils.events]: [0m eta: 0:12:02  iter: 699  total_loss: 0.649  loss_cls: 0.226  loss_box_reg: 0.360  loss_rpn_cls: 0.021  loss_rpn_loc: 0.062  time: 0.5682  data_time: 0.0067  lr: 0.003497  max_mem: 3067M
[32m[04/15 09:16:55 d2.utils.events]: [0m eta: 0:11:51  iter: 719  total_loss: 0.560  loss_cls: 0.179  loss_box_reg: 0.323  loss_rpn_cls: 0.016  loss_rpn_loc: 0.051  time: 0.5678  data_time: 0.0070  lr: 0.003596  max_mem: 3067M
[32m[04/15 09:17:07 d2.utils.events]: [0m eta: 0:11:40  iter: 739  total_loss: 0.578  loss_cls: 0.194  loss_box_reg: 0.310  loss_rpn_cls: 0.012  loss_rpn_loc: 0.065  time: 0.5677  data_time: 0.0074  lr: 0.003696  max_mem: 3067M
[32m[04/15 09:17:19 d2.utils.events]: [0m eta: 0:11:30  iter: 759  total_loss: 0.627  loss_cls: 0.199  loss_box_reg: 0.350  loss_rpn_cls: 0.015  loss_rpn_loc: 0.057  time: 0.5690  data_time: 0.0110  lr: 0.003796  max_mem: 3067M
[32m[04/15 09:17:30 d2.utils.events]: [0m eta: 0:11:18  iter: 779  total_loss: 0.547  loss_cls: 0.179  loss_box_reg: 0.292  loss_rpn_cls: 0.017  loss_rpn_loc: 0.044  time: 0.5680  data_time: 0.0076  lr: 0.003896  max_mem: 3067M
[32m[04/15 09:17:41 d2.utils.events]: [0m eta: 0:11:06  iter: 799  total_loss: 0.542  loss_cls: 0.185  loss_box_reg: 0.285  loss_rpn_cls: 0.016  loss_rpn_loc: 0.051  time: 0.5671  data_time: 0.0067  lr: 0.003996  max_mem: 3067M
[32m[04/15 09:17:53 d2.utils.events]: [0m eta: 0:10:56  iter: 819  total_loss: 0.649  loss_cls: 0.217  loss_box_reg: 0.337  loss_rpn_cls: 0.016  loss_rpn_loc: 0.050  time: 0.5684  data_time: 0.0083  lr: 0.004096  max_mem: 3067M
[32m[04/15 09:18:04 d2.utils.events]: [0m eta: 0:10:43  iter: 839  total_loss: 0.584  loss_cls: 0.201  loss_box_reg: 0.281  loss_rpn_cls: 0.017  loss_rpn_loc: 0.059  time: 0.5675  data_time: 0.0078  lr: 0.004196  max_mem: 3067M
[32m[04/15 09:18:15 d2.utils.events]: [0m eta: 0:10:32  iter: 859  total_loss: 0.647  loss_cls: 0.202  loss_box_reg: 0.367  loss_rpn_cls: 0.021  loss_rpn_loc: 0.048  time: 0.5667  data_time: 0.0089  lr: 0.004296  max_mem: 3067M
[32m[04/15 09:18:26 d2.utils.events]: [0m eta: 0:10:21  iter: 879  total_loss: 0.623  loss_cls: 0.205  loss_box_reg: 0.345  loss_rpn_cls: 0.015  loss_rpn_loc: 0.065  time: 0.5672  data_time: 0.0082  lr: 0.004396  max_mem: 3067M
[32m[04/15 09:18:38 d2.utils.events]: [0m eta: 0:10:10  iter: 899  total_loss: 0.712  loss_cls: 0.222  loss_box_reg: 0.363  loss_rpn_cls: 0.017  loss_rpn_loc: 0.087  time: 0.5675  data_time: 0.0069  lr: 0.004496  max_mem: 3067M
[32m[04/15 09:18:49 d2.utils.events]: [0m eta: 0:09:58  iter: 919  total_loss: 0.584  loss_cls: 0.196  loss_box_reg: 0.335  loss_rpn_cls: 0.013  loss_rpn_loc: 0.039  time: 0.5668  data_time: 0.0073  lr: 0.004595  max_mem: 3067M
[32m[04/15 09:19:01 d2.utils.events]: [0m eta: 0:09:48  iter: 939  total_loss: 0.681  loss_cls: 0.216  loss_box_reg: 0.372  loss_rpn_cls: 0.019  loss_rpn_loc: 0.055  time: 0.5674  data_time: 0.0073  lr: 0.004695  max_mem: 3067M
[32m[04/15 09:19:13 d2.utils.events]: [0m eta: 0:09:37  iter: 959  total_loss: 0.645  loss_cls: 0.205  loss_box_reg: 0.310  loss_rpn_cls: 0.013  loss_rpn_loc: 0.042  time: 0.5683  data_time: 0.0071  lr: 0.004795  max_mem: 3067M
[32m[04/15 09:19:24 d2.utils.events]: [0m eta: 0:09:26  iter: 979  total_loss: 0.528  loss_cls: 0.174  loss_box_reg: 0.287  loss_rpn_cls: 0.017  loss_rpn_loc: 0.039  time: 0.5674  data_time: 0.0074  lr: 0.004895  max_mem: 3067M
[32m[04/15 09:19:35 d2.utils.events]: [0m eta: 0:09:15  iter: 999  total_loss: 0.559  loss_cls: 0.179  loss_box_reg: 0.325  loss_rpn_cls: 0.018  loss_rpn_loc: 0.051  time: 0.5675  data_time: 0.0085  lr: 0.004995  max_mem: 3067M
[32m[04/15 09:19:48 d2.utils.events]: [0m eta: 0:09:04  iter: 1019  total_loss: 0.717  loss_cls: 0.225  loss_box_reg: 0.383  loss_rpn_cls: 0.018  loss_rpn_loc: 0.067  time: 0.5684  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:19:58 d2.utils.events]: [0m eta: 0:08:52  iter: 1039  total_loss: 0.687  loss_cls: 0.197  loss_box_reg: 0.361  loss_rpn_cls: 0.025  loss_rpn_loc: 0.073  time: 0.5675  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:20:09 d2.utils.events]: [0m eta: 0:08:41  iter: 1059  total_loss: 0.621  loss_cls: 0.189  loss_box_reg: 0.354  loss_rpn_cls: 0.021  loss_rpn_loc: 0.056  time: 0.5667  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:20:21 d2.utils.events]: [0m eta: 0:08:31  iter: 1079  total_loss: 0.640  loss_cls: 0.196  loss_box_reg: 0.335  loss_rpn_cls: 0.022  loss_rpn_loc: 0.068  time: 0.5672  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:20:32 d2.utils.events]: [0m eta: 0:08:19  iter: 1099  total_loss: 0.671  loss_cls: 0.217  loss_box_reg: 0.379  loss_rpn_cls: 0.020  loss_rpn_loc: 0.056  time: 0.5669  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:20:43 d2.utils.events]: [0m eta: 0:08:08  iter: 1119  total_loss: 0.555  loss_cls: 0.190  loss_box_reg: 0.314  loss_rpn_cls: 0.019  loss_rpn_loc: 0.053  time: 0.5663  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:20:54 d2.utils.events]: [0m eta: 0:07:57  iter: 1139  total_loss: 0.659  loss_cls: 0.215  loss_box_reg: 0.344  loss_rpn_cls: 0.015  loss_rpn_loc: 0.063  time: 0.5665  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:21:07 d2.utils.events]: [0m eta: 0:07:45  iter: 1159  total_loss: 0.796  loss_cls: 0.246  loss_box_reg: 0.432  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5671  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:21:17 d2.utils.events]: [0m eta: 0:07:34  iter: 1179  total_loss: 0.479  loss_cls: 0.147  loss_box_reg: 0.260  loss_rpn_cls: 0.022  loss_rpn_loc: 0.053  time: 0.5664  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:21:29 d2.utils.events]: [0m eta: 0:07:23  iter: 1199  total_loss: 0.705  loss_cls: 0.211  loss_box_reg: 0.398  loss_rpn_cls: 0.011  loss_rpn_loc: 0.053  time: 0.5664  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:21:41 d2.utils.events]: [0m eta: 0:07:12  iter: 1219  total_loss: 0.617  loss_cls: 0.179  loss_box_reg: 0.327  loss_rpn_cls: 0.012  loss_rpn_loc: 0.051  time: 0.5671  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:21:52 d2.utils.events]: [0m eta: 0:07:01  iter: 1239  total_loss: 0.601  loss_cls: 0.216  loss_box_reg: 0.314  loss_rpn_cls: 0.016  loss_rpn_loc: 0.068  time: 0.5664  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:22:03 d2.utils.events]: [0m eta: 0:06:49  iter: 1259  total_loss: 0.680  loss_cls: 0.233  loss_box_reg: 0.396  loss_rpn_cls: 0.025  loss_rpn_loc: 0.056  time: 0.5660  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:22:15 d2.utils.events]: [0m eta: 0:06:39  iter: 1279  total_loss: 0.727  loss_cls: 0.237  loss_box_reg: 0.381  loss_rpn_cls: 0.022  loss_rpn_loc: 0.064  time: 0.5669  data_time: 0.0092  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:22:27 d2.utils.events]: [0m eta: 0:06:29  iter: 1299  total_loss: 0.639  loss_cls: 0.192  loss_box_reg: 0.348  loss_rpn_cls: 0.012  loss_rpn_loc: 0.071  time: 0.5668  data_time: 0.0092  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:22:37 d2.utils.events]: [0m eta: 0:06:17  iter: 1319  total_loss: 0.619  loss_cls: 0.193  loss_box_reg: 0.352  loss_rpn_cls: 0.017  loss_rpn_loc: 0.052  time: 0.5663  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:22:50 d2.utils.events]: [0m eta: 0:06:06  iter: 1339  total_loss: 0.619  loss_cls: 0.186  loss_box_reg: 0.365  loss_rpn_cls: 0.010  loss_rpn_loc: 0.052  time: 0.5669  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:23:01 d2.utils.events]: [0m eta: 0:05:56  iter: 1359  total_loss: 0.652  loss_cls: 0.214  loss_box_reg: 0.352  loss_rpn_cls: 0.014  loss_rpn_loc: 0.050  time: 0.5670  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:23:12 d2.utils.events]: [0m eta: 0:05:44  iter: 1379  total_loss: 0.654  loss_cls: 0.206  loss_box_reg: 0.363  loss_rpn_cls: 0.013  loss_rpn_loc: 0.059  time: 0.5668  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:23:23 d2.utils.events]: [0m eta: 0:05:33  iter: 1399  total_loss: 0.647  loss_cls: 0.206  loss_box_reg: 0.335  loss_rpn_cls: 0.015  loss_rpn_loc: 0.047  time: 0.5666  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:23:35 d2.utils.events]: [0m eta: 0:05:22  iter: 1419  total_loss: 0.622  loss_cls: 0.196  loss_box_reg: 0.355  loss_rpn_cls: 0.024  loss_rpn_loc: 0.053  time: 0.5671  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:23:46 d2.utils.events]: [0m eta: 0:05:11  iter: 1439  total_loss: 0.517  loss_cls: 0.164  loss_box_reg: 0.273  loss_rpn_cls: 0.018  loss_rpn_loc: 0.046  time: 0.5666  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:23:57 d2.utils.events]: [0m eta: 0:05:00  iter: 1459  total_loss: 0.766  loss_cls: 0.244  loss_box_reg: 0.425  loss_rpn_cls: 0.019  loss_rpn_loc: 0.059  time: 0.5664  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:24:10 d2.utils.events]: [0m eta: 0:04:49  iter: 1479  total_loss: 0.668  loss_cls: 0.215  loss_box_reg: 0.402  loss_rpn_cls: 0.020  loss_rpn_loc: 0.057  time: 0.5673  data_time: 0.0107  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:24:21 d2.utils.events]: [0m eta: 0:04:38  iter: 1499  total_loss: 0.566  loss_cls: 0.166  loss_box_reg: 0.313  loss_rpn_cls: 0.016  loss_rpn_loc: 0.044  time: 0.5666  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:24:32 d2.utils.events]: [0m eta: 0:04:27  iter: 1519  total_loss: 0.526  loss_cls: 0.168  loss_box_reg: 0.301  loss_rpn_cls: 0.014  loss_rpn_loc: 0.033  time: 0.5666  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:24:44 d2.utils.events]: [0m eta: 0:04:16  iter: 1539  total_loss: 0.570  loss_cls: 0.192  loss_box_reg: 0.293  loss_rpn_cls: 0.014  loss_rpn_loc: 0.043  time: 0.5673  data_time: 0.0102  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:24:55 d2.utils.events]: [0m eta: 0:04:05  iter: 1559  total_loss: 0.677  loss_cls: 0.216  loss_box_reg: 0.390  loss_rpn_cls: 0.022  loss_rpn_loc: 0.063  time: 0.5670  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:25:06 d2.utils.events]: [0m eta: 0:03:53  iter: 1579  total_loss: 0.611  loss_cls: 0.199  loss_box_reg: 0.341  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5665  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:25:18 d2.utils.events]: [0m eta: 0:03:42  iter: 1599  total_loss: 0.522  loss_cls: 0.181  loss_box_reg: 0.297  loss_rpn_cls: 0.021  loss_rpn_loc: 0.060  time: 0.5669  data_time: 0.0106  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:25:29 d2.utils.events]: [0m eta: 0:03:31  iter: 1619  total_loss: 0.712  loss_cls: 0.226  loss_box_reg: 0.373  loss_rpn_cls: 0.024  loss_rpn_loc: 0.062  time: 0.5667  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:25:40 d2.utils.events]: [0m eta: 0:03:20  iter: 1639  total_loss: 0.742  loss_cls: 0.195  loss_box_reg: 0.416  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5665  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:25:52 d2.utils.events]: [0m eta: 0:03:09  iter: 1659  total_loss: 0.661  loss_cls: 0.219  loss_box_reg: 0.348  loss_rpn_cls: 0.017  loss_rpn_loc: 0.051  time: 0.5667  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:26:04 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.610  loss_cls: 0.190  loss_box_reg: 0.329  loss_rpn_cls: 0.016  loss_rpn_loc: 0.060  time: 0.5668  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:26:15 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.763  loss_cls: 0.249  loss_box_reg: 0.415  loss_rpn_cls: 0.019  loss_rpn_loc: 0.072  time: 0.5666  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:26:26 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.716  loss_cls: 0.222  loss_box_reg: 0.372  loss_rpn_cls: 0.020  loss_rpn_loc: 0.068  time: 0.5665  data_time: 0.0098  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:26:38 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.603  loss_cls: 0.202  loss_box_reg: 0.337  loss_rpn_cls: 0.018  loss_rpn_loc: 0.060  time: 0.5667  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:26:48 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.677  loss_cls: 0.216  loss_box_reg: 0.392  loss_rpn_cls: 0.018  loss_rpn_loc: 0.057  time: 0.5662  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:26:59 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.544  loss_cls: 0.179  loss_box_reg: 0.316  loss_rpn_cls: 0.015  loss_rpn_loc: 0.043  time: 0.5660  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:27:12 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.707  loss_cls: 0.223  loss_box_reg: 0.412  loss_rpn_cls: 0.023  loss_rpn_loc: 0.062  time: 0.5667  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:27:23 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.657  loss_cls: 0.190  loss_box_reg: 0.361  loss_rpn_cls: 0.018  loss_rpn_loc: 0.057  time: 0.5663  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:27:33 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.644  loss_cls: 0.200  loss_box_reg: 0.357  loss_rpn_cls: 0.014  loss_rpn_loc: 0.050  time: 0.5658  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:27:46 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.532  loss_cls: 0.148  loss_box_reg: 0.318  loss_rpn_cls: 0.015  loss_rpn_loc: 0.048  time: 0.5663  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:27:57 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.591  loss_cls: 0.177  loss_box_reg: 0.360  loss_rpn_cls: 0.018  loss_rpn_loc: 0.048  time: 0.5661  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:28:07 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.680  loss_cls: 0.224  loss_box_reg: 0.371  loss_rpn_cls: 0.022  loss_rpn_loc: 0.058  time: 0.5657  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:28:19 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.604  loss_cls: 0.195  loss_box_reg: 0.357  loss_rpn_cls: 0.016  loss_rpn_loc: 0.058  time: 0.5660  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:28:31 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.520  loss_cls: 0.156  loss_box_reg: 0.279  loss_rpn_cls: 0.015  loss_rpn_loc: 0.060  time: 0.5659  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:28:41 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.659  loss_cls: 0.183  loss_box_reg: 0.360  loss_rpn_cls: 0.016  loss_rpn_loc: 0.049  time: 0.5656  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:28:53 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.745  loss_cls: 0.270  loss_box_reg: 0.405  loss_rpn_cls: 0.024  loss_rpn_loc: 0.070  time: 0.5659  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 09:29:18 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:29:18 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 09:29:18 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 09:29:18 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.669  loss_cls: 0.227  loss_box_reg: 0.366  loss_rpn_cls: 0.026  loss_rpn_loc: 0.071  time: 0.5660  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:29:21 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:50 (0.5663 s / it)
[32m[04/15 09:29:21 d2.engine.hooks]: [0mTotal training time: 0:19:17 (0:00:26 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 09:29:27 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:29:27 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 09:29:28 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 09:29:35 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1203 s / img. ETA=0:03:17
[32m[04/15 09:29:40 d2.evaluation.evaluator]: [0mInference done 43/1257. 0.1217 s / img. ETA=0:03:11
[32m[04/15 09:29:45 d2.evaluation.evaluator]: [0mInference done 73/1257. 0.1217 s / img. ETA=0:03:18
[32m[04/15 09:29:51 d2.evaluation.evaluator]: [0mInference done 104/1257. 0.1229 s / img. ETA=0:03:11
[32m[04/15 09:29:56 d2.evaluation.evaluator]: [0mInference done 137/1257. 0.1223 s / img. ETA=0:03:03
[32m[04/15 09:30:01 d2.evaluation.evaluator]: [0mInference done 171/1257. 0.1230 s / img. ETA=0:02:54
[32m[04/15 09:30:06 d2.evaluation.evaluator]: [0mInference done 205/1257. 0.1249 s / img. ETA=0:02:46
[32m[04/15 09:30:11 d2.evaluation.evaluator]: [0mInference done 237/1257. 0.1273 s / img. ETA=0:02:41
[32m[04/15 09:30:16 d2.evaluation.evaluator]: [0mInference done 275/1257. 0.1271 s / img. ETA=0:02:31
[32m[04/15 09:30:21 d2.evaluation.evaluator]: [0mInference done 314/1257. 0.1265 s / img. ETA=0:02:22
[32m[04/15 09:30:26 d2.evaluation.evaluator]: [0mInference done 352/1257. 0.1259 s / img. ETA=0:02:15
[32m[04/15 09:30:31 d2.evaluation.evaluator]: [0mInference done 391/1257. 0.1256 s / img. ETA=0:02:07
[32m[04/15 09:30:36 d2.evaluation.evaluator]: [0mInference done 427/1257. 0.1257 s / img. ETA=0:02:02
[32m[04/15 09:30:41 d2.evaluation.evaluator]: [0mInference done 455/1257. 0.1254 s / img. ETA=0:01:59
[32m[04/15 09:30:47 d2.evaluation.evaluator]: [0mInference done 486/1257. 0.1259 s / img. ETA=0:01:56
[32m[04/15 09:30:52 d2.evaluation.evaluator]: [0mInference done 518/1257. 0.1271 s / img. ETA=0:01:51
[32m[04/15 09:30:57 d2.evaluation.evaluator]: [0mInference done 548/1257. 0.1274 s / img. ETA=0:01:47
[32m[04/15 09:31:02 d2.evaluation.evaluator]: [0mInference done 579/1257. 0.1271 s / img. ETA=0:01:43
[32m[04/15 09:31:07 d2.evaluation.evaluator]: [0mInference done 608/1257. 0.1269 s / img. ETA=0:01:39
[32m[04/15 09:31:12 d2.evaluation.evaluator]: [0mInference done 641/1257. 0.1266 s / img. ETA=0:01:35
[32m[04/15 09:31:17 d2.evaluation.evaluator]: [0mInference done 669/1257. 0.1263 s / img. ETA=0:01:31
[32m[04/15 09:31:23 d2.evaluation.evaluator]: [0mInference done 700/1257. 0.1261 s / img. ETA=0:01:26
[32m[04/15 09:31:28 d2.evaluation.evaluator]: [0mInference done 729/1257. 0.1269 s / img. ETA=0:01:22
[32m[04/15 09:31:33 d2.evaluation.evaluator]: [0mInference done 756/1257. 0.1271 s / img. ETA=0:01:19
[32m[04/15 09:31:38 d2.evaluation.evaluator]: [0mInference done 770/1257. 0.1270 s / img. ETA=0:01:18
[32m[04/15 09:31:43 d2.evaluation.evaluator]: [0mInference done 789/1257. 0.1269 s / img. ETA=0:01:17
[32m[04/15 09:31:48 d2.evaluation.evaluator]: [0mInference done 816/1257. 0.1269 s / img. ETA=0:01:12
[32m[04/15 09:31:54 d2.evaluation.evaluator]: [0mInference done 852/1257. 0.1268 s / img. ETA=0:01:06
[32m[04/15 09:31:59 d2.evaluation.evaluator]: [0mInference done 889/1257. 0.1268 s / img. ETA=0:01:00
[32m[04/15 09:32:04 d2.evaluation.evaluator]: [0mInference done 916/1257. 0.1270 s / img. ETA=0:00:56
[32m[04/15 09:32:09 d2.evaluation.evaluator]: [0mInference done 948/1257. 0.1267 s / img. ETA=0:00:50
[32m[04/15 09:32:14 d2.evaluation.evaluator]: [0mInference done 986/1257. 0.1265 s / img. ETA=0:00:44
[32m[04/15 09:32:19 d2.evaluation.evaluator]: [0mInference done 1018/1257. 0.1273 s / img. ETA=0:00:38
[32m[04/15 09:32:24 d2.evaluation.evaluator]: [0mInference done 1053/1257. 0.1278 s / img. ETA=0:00:33
[32m[04/15 09:32:29 d2.evaluation.evaluator]: [0mInference done 1093/1257. 0.1275 s / img. ETA=0:00:26
[32m[04/15 09:32:34 d2.evaluation.evaluator]: [0mInference done 1126/1257. 0.1273 s / img. ETA=0:00:21
[32m[04/15 09:32:39 d2.evaluation.evaluator]: [0mInference done 1160/1257. 0.1272 s / img. ETA=0:00:15
[32m[04/15 09:32:45 d2.evaluation.evaluator]: [0mInference done 1199/1257. 0.1271 s / img. ETA=0:00:09
[32m[04/15 09:32:50 d2.evaluation.evaluator]: [0mInference done 1237/1257. 0.1271 s / img. ETA=0:00:03
[32m[04/15 09:32:52 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:18.369256 (0.158442 s / img per device, on 1 devices)
[32m[04/15 09:32:52 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:38 (0.126993 s / img per device, on 1 devices)
[32m[04/15 09:32:53 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 09:32:53 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 09:32:53 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=6.28s).
Accumulating evaluation results...
DONE (t=1.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.437
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.193
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.126
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.433
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.117
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.265
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.268
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.171
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.329
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.476
[32m[04/15 09:33:02 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 21.254 | 43.676 | 19.326 | 12.638 | 26.366 | 43.323 |
[32m[04/15 09:33:02 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP     | category       | AP     |
|:--------------|:-------|:--------------|:-------|:---------------|:-------|
| person        | 30.534 | bicycle       | 10.213 | car            | 42.372 |
| motorcycle    | nan    | airplane      | nan    | bus            | nan    |
| train         | nan    | truck         | nan    | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan    | stop sign      | nan    |
| parking meter | nan    | bench         | nan    | bird           | nan    |
| cat           | nan    | dog           | 1.897  | horse          | nan    |
| sheep         | nan    | cow           | nan    | elephant       | nan    |
| bear          | nan    | zebra         | nan    | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan    | handbag        | nan    |
| tie           | nan    | suitcase      | nan    | frisbee        | nan    |
| skis          | nan    | snowboard     | nan    | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan    | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan    | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan    | cup            | nan    |
| fork          | nan    | knife         | nan    | spoon          | nan    |
| bowl          | nan    | banana        | nan    | apple          | nan    |
| sandwich      | nan    | orange        | nan    | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan    | pizza          | nan    |
| donut         | nan    | cake          | nan    | chair          | nan    |
| couch         | nan    | potted plant  | nan    | bed            | nan    |
| dining table  | nan    | toilet        | nan    | tv             | nan    |
| laptop        | nan    | mouse         | nan    | remote         | nan    |
| keyboard      | nan    | cell phone    | nan    | microwave      | nan    |
| oven          | nan    | bike          | nan    | hydrant        | nan    |
| motor         | nan    | rider         | nan    | light          | nan    |
| sign          | nan    | motor vehicle | nan    | human face     | nan    |
| hair drier    | nan    | license plate | nan    |                |        |
============== The  16  *  2000  iterations ============
4 channel input
[32m[04/15 09:33:03 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 09:33:05 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.72 seconds.
[5m[31mWARNING[0m [32m[04/15 09:33:05 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:33:05 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 09:33:06 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 09:33:06 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 09:33:06 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 09:33:08 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 09:33:20 d2.utils.events]: [0m eta: 0:18:28  iter: 19  total_loss: 0.591  loss_cls: 0.211  loss_box_reg: 0.299  loss_rpn_cls: 0.022  loss_rpn_loc: 0.049  time: 0.5548  data_time: 0.0500  lr: 0.000100  max_mem: 3067M
[32m[04/15 09:33:31 d2.utils.events]: [0m eta: 0:18:17  iter: 39  total_loss: 0.767  loss_cls: 0.239  loss_box_reg: 0.412  loss_rpn_cls: 0.024  loss_rpn_loc: 0.060  time: 0.5526  data_time: 0.0066  lr: 0.000200  max_mem: 3067M
[32m[04/15 09:33:43 d2.utils.events]: [0m eta: 0:18:04  iter: 59  total_loss: 0.627  loss_cls: 0.198  loss_box_reg: 0.370  loss_rpn_cls: 0.026  loss_rpn_loc: 0.041  time: 0.5617  data_time: 0.0075  lr: 0.000300  max_mem: 3067M
[32m[04/15 09:33:55 d2.utils.events]: [0m eta: 0:18:04  iter: 79  total_loss: 0.548  loss_cls: 0.169  loss_box_reg: 0.300  loss_rpn_cls: 0.022  loss_rpn_loc: 0.041  time: 0.5685  data_time: 0.0088  lr: 0.000400  max_mem: 3067M
[32m[04/15 09:34:06 d2.utils.events]: [0m eta: 0:17:51  iter: 99  total_loss: 0.656  loss_cls: 0.224  loss_box_reg: 0.363  loss_rpn_cls: 0.022  loss_rpn_loc: 0.063  time: 0.5662  data_time: 0.0074  lr: 0.000500  max_mem: 3067M
[32m[04/15 09:34:17 d2.utils.events]: [0m eta: 0:17:36  iter: 119  total_loss: 0.606  loss_cls: 0.186  loss_box_reg: 0.341  loss_rpn_cls: 0.017  loss_rpn_loc: 0.057  time: 0.5637  data_time: 0.0077  lr: 0.000599  max_mem: 3067M
[32m[04/15 09:34:30 d2.utils.events]: [0m eta: 0:17:30  iter: 139  total_loss: 0.480  loss_cls: 0.140  loss_box_reg: 0.294  loss_rpn_cls: 0.013  loss_rpn_loc: 0.031  time: 0.5716  data_time: 0.0080  lr: 0.000699  max_mem: 3067M
[32m[04/15 09:34:40 d2.utils.events]: [0m eta: 0:17:08  iter: 159  total_loss: 0.508  loss_cls: 0.159  loss_box_reg: 0.263  loss_rpn_cls: 0.018  loss_rpn_loc: 0.034  time: 0.5668  data_time: 0.0068  lr: 0.000799  max_mem: 3067M
[32m[04/15 09:34:52 d2.utils.events]: [0m eta: 0:16:51  iter: 179  total_loss: 0.622  loss_cls: 0.190  loss_box_reg: 0.356  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5653  data_time: 0.0071  lr: 0.000899  max_mem: 3067M
[32m[04/15 09:35:04 d2.utils.events]: [0m eta: 0:16:46  iter: 199  total_loss: 0.607  loss_cls: 0.209  loss_box_reg: 0.330  loss_rpn_cls: 0.016  loss_rpn_loc: 0.050  time: 0.5681  data_time: 0.0081  lr: 0.000999  max_mem: 3067M
[32m[04/15 09:35:15 d2.utils.events]: [0m eta: 0:16:33  iter: 219  total_loss: 0.563  loss_cls: 0.183  loss_box_reg: 0.311  loss_rpn_cls: 0.017  loss_rpn_loc: 0.042  time: 0.5661  data_time: 0.0096  lr: 0.001099  max_mem: 3067M
[32m[04/15 09:35:25 d2.utils.events]: [0m eta: 0:16:19  iter: 239  total_loss: 0.521  loss_cls: 0.168  loss_box_reg: 0.295  loss_rpn_cls: 0.017  loss_rpn_loc: 0.046  time: 0.5640  data_time: 0.0080  lr: 0.001199  max_mem: 3067M
[32m[04/15 09:35:37 d2.utils.events]: [0m eta: 0:16:14  iter: 259  total_loss: 0.626  loss_cls: 0.198  loss_box_reg: 0.341  loss_rpn_cls: 0.024  loss_rpn_loc: 0.054  time: 0.5654  data_time: 0.0064  lr: 0.001299  max_mem: 3067M
[32m[04/15 09:35:50 d2.utils.events]: [0m eta: 0:16:07  iter: 279  total_loss: 0.525  loss_cls: 0.164  loss_box_reg: 0.275  loss_rpn_cls: 0.012  loss_rpn_loc: 0.038  time: 0.5692  data_time: 0.0064  lr: 0.001399  max_mem: 3067M
[32m[04/15 09:36:01 d2.utils.events]: [0m eta: 0:15:54  iter: 299  total_loss: 0.558  loss_cls: 0.180  loss_box_reg: 0.314  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5682  data_time: 0.0068  lr: 0.001499  max_mem: 3067M
[32m[04/15 09:36:12 d2.utils.events]: [0m eta: 0:15:45  iter: 319  total_loss: 0.535  loss_cls: 0.170  loss_box_reg: 0.306  loss_rpn_cls: 0.011  loss_rpn_loc: 0.042  time: 0.5686  data_time: 0.0076  lr: 0.001598  max_mem: 3067M
[32m[04/15 09:36:25 d2.utils.events]: [0m eta: 0:15:36  iter: 339  total_loss: 0.589  loss_cls: 0.188  loss_box_reg: 0.350  loss_rpn_cls: 0.011  loss_rpn_loc: 0.045  time: 0.5707  data_time: 0.0068  lr: 0.001698  max_mem: 3067M
[32m[04/15 09:36:35 d2.utils.events]: [0m eta: 0:15:22  iter: 359  total_loss: 0.602  loss_cls: 0.187  loss_box_reg: 0.321  loss_rpn_cls: 0.016  loss_rpn_loc: 0.048  time: 0.5688  data_time: 0.0108  lr: 0.001798  max_mem: 3067M
[32m[04/15 09:36:46 d2.utils.events]: [0m eta: 0:15:08  iter: 379  total_loss: 0.625  loss_cls: 0.198  loss_box_reg: 0.362  loss_rpn_cls: 0.010  loss_rpn_loc: 0.038  time: 0.5676  data_time: 0.0084  lr: 0.001898  max_mem: 3067M
[32m[04/15 09:36:59 d2.utils.events]: [0m eta: 0:14:58  iter: 399  total_loss: 0.554  loss_cls: 0.171  loss_box_reg: 0.313  loss_rpn_cls: 0.015  loss_rpn_loc: 0.037  time: 0.5693  data_time: 0.0089  lr: 0.001998  max_mem: 3067M
[32m[04/15 09:37:10 d2.utils.events]: [0m eta: 0:14:45  iter: 419  total_loss: 0.716  loss_cls: 0.212  loss_box_reg: 0.423  loss_rpn_cls: 0.014  loss_rpn_loc: 0.067  time: 0.5688  data_time: 0.0080  lr: 0.002098  max_mem: 3067M
[32m[04/15 09:37:21 d2.utils.events]: [0m eta: 0:14:32  iter: 439  total_loss: 0.622  loss_cls: 0.190  loss_box_reg: 0.362  loss_rpn_cls: 0.014  loss_rpn_loc: 0.055  time: 0.5673  data_time: 0.0062  lr: 0.002198  max_mem: 3067M
[32m[04/15 09:37:32 d2.utils.events]: [0m eta: 0:14:20  iter: 459  total_loss: 0.608  loss_cls: 0.189  loss_box_reg: 0.345  loss_rpn_cls: 0.016  loss_rpn_loc: 0.064  time: 0.5672  data_time: 0.0068  lr: 0.002298  max_mem: 3067M
[32m[04/15 09:37:44 d2.utils.events]: [0m eta: 0:14:08  iter: 479  total_loss: 0.663  loss_cls: 0.210  loss_box_reg: 0.355  loss_rpn_cls: 0.018  loss_rpn_loc: 0.062  time: 0.5672  data_time: 0.0069  lr: 0.002398  max_mem: 3067M
[32m[04/15 09:37:54 d2.utils.events]: [0m eta: 0:13:55  iter: 499  total_loss: 0.678  loss_cls: 0.187  loss_box_reg: 0.388  loss_rpn_cls: 0.015  loss_rpn_loc: 0.055  time: 0.5660  data_time: 0.0092  lr: 0.002498  max_mem: 3067M
[32m[04/15 09:38:06 d2.utils.events]: [0m eta: 0:13:43  iter: 519  total_loss: 0.665  loss_cls: 0.203  loss_box_reg: 0.381  loss_rpn_cls: 0.011  loss_rpn_loc: 0.071  time: 0.5659  data_time: 0.0066  lr: 0.002597  max_mem: 3067M
[32m[04/15 09:38:18 d2.utils.events]: [0m eta: 0:13:33  iter: 539  total_loss: 0.620  loss_cls: 0.211  loss_box_reg: 0.334  loss_rpn_cls: 0.015  loss_rpn_loc: 0.058  time: 0.5676  data_time: 0.0071  lr: 0.002697  max_mem: 3067M
[32m[04/15 09:38:29 d2.utils.events]: [0m eta: 0:13:23  iter: 559  total_loss: 0.655  loss_cls: 0.202  loss_box_reg: 0.371  loss_rpn_cls: 0.012  loss_rpn_loc: 0.049  time: 0.5675  data_time: 0.0082  lr: 0.002797  max_mem: 3067M
[32m[04/15 09:38:41 d2.utils.events]: [0m eta: 0:13:12  iter: 579  total_loss: 0.675  loss_cls: 0.216  loss_box_reg: 0.392  loss_rpn_cls: 0.017  loss_rpn_loc: 0.044  time: 0.5671  data_time: 0.0080  lr: 0.002897  max_mem: 3067M
[32m[04/15 09:38:53 d2.utils.events]: [0m eta: 0:13:03  iter: 599  total_loss: 0.547  loss_cls: 0.173  loss_box_reg: 0.298  loss_rpn_cls: 0.011  loss_rpn_loc: 0.043  time: 0.5686  data_time: 0.0087  lr: 0.002997  max_mem: 3067M
[32m[04/15 09:39:04 d2.utils.events]: [0m eta: 0:12:50  iter: 619  total_loss: 0.570  loss_cls: 0.186  loss_box_reg: 0.336  loss_rpn_cls: 0.016  loss_rpn_loc: 0.040  time: 0.5680  data_time: 0.0096  lr: 0.003097  max_mem: 3067M
[32m[04/15 09:39:15 d2.utils.events]: [0m eta: 0:12:37  iter: 639  total_loss: 0.595  loss_cls: 0.182  loss_box_reg: 0.352  loss_rpn_cls: 0.011  loss_rpn_loc: 0.061  time: 0.5669  data_time: 0.0085  lr: 0.003197  max_mem: 3067M
[32m[04/15 09:39:27 d2.utils.events]: [0m eta: 0:12:26  iter: 659  total_loss: 0.581  loss_cls: 0.186  loss_box_reg: 0.333  loss_rpn_cls: 0.014  loss_rpn_loc: 0.034  time: 0.5677  data_time: 0.0074  lr: 0.003297  max_mem: 3067M
[32m[04/15 09:39:38 d2.utils.events]: [0m eta: 0:12:14  iter: 679  total_loss: 0.565  loss_cls: 0.180  loss_box_reg: 0.331  loss_rpn_cls: 0.012  loss_rpn_loc: 0.038  time: 0.5668  data_time: 0.0076  lr: 0.003397  max_mem: 3067M
[32m[04/15 09:39:49 d2.utils.events]: [0m eta: 0:12:02  iter: 699  total_loss: 0.629  loss_cls: 0.195  loss_box_reg: 0.340  loss_rpn_cls: 0.018  loss_rpn_loc: 0.045  time: 0.5661  data_time: 0.0071  lr: 0.003497  max_mem: 3067M
[32m[04/15 09:40:00 d2.utils.events]: [0m eta: 0:11:51  iter: 719  total_loss: 0.608  loss_cls: 0.184  loss_box_reg: 0.347  loss_rpn_cls: 0.011  loss_rpn_loc: 0.047  time: 0.5665  data_time: 0.0075  lr: 0.003596  max_mem: 3067M
[32m[04/15 09:40:12 d2.utils.events]: [0m eta: 0:11:41  iter: 739  total_loss: 0.679  loss_cls: 0.196  loss_box_reg: 0.322  loss_rpn_cls: 0.015  loss_rpn_loc: 0.089  time: 0.5673  data_time: 0.0098  lr: 0.003696  max_mem: 3067M
[32m[04/15 09:40:23 d2.utils.events]: [0m eta: 0:11:29  iter: 759  total_loss: 0.732  loss_cls: 0.237  loss_box_reg: 0.434  loss_rpn_cls: 0.016  loss_rpn_loc: 0.059  time: 0.5668  data_time: 0.0079  lr: 0.003796  max_mem: 3067M
[32m[04/15 09:40:35 d2.utils.events]: [0m eta: 0:11:18  iter: 779  total_loss: 0.674  loss_cls: 0.213  loss_box_reg: 0.398  loss_rpn_cls: 0.013  loss_rpn_loc: 0.057  time: 0.5666  data_time: 0.0065  lr: 0.003896  max_mem: 3067M
[32m[04/15 09:40:47 d2.utils.events]: [0m eta: 0:11:08  iter: 799  total_loss: 0.628  loss_cls: 0.204  loss_box_reg: 0.346  loss_rpn_cls: 0.016  loss_rpn_loc: 0.058  time: 0.5672  data_time: 0.0080  lr: 0.003996  max_mem: 3067M
[32m[04/15 09:40:58 d2.utils.events]: [0m eta: 0:10:57  iter: 819  total_loss: 0.704  loss_cls: 0.227  loss_box_reg: 0.393  loss_rpn_cls: 0.016  loss_rpn_loc: 0.050  time: 0.5668  data_time: 0.0072  lr: 0.004096  max_mem: 3067M
[32m[04/15 09:41:08 d2.utils.events]: [0m eta: 0:10:45  iter: 839  total_loss: 0.578  loss_cls: 0.187  loss_box_reg: 0.339  loss_rpn_cls: 0.018  loss_rpn_loc: 0.048  time: 0.5661  data_time: 0.0085  lr: 0.004196  max_mem: 3067M
[32m[04/15 09:41:21 d2.utils.events]: [0m eta: 0:10:35  iter: 859  total_loss: 0.694  loss_cls: 0.239  loss_box_reg: 0.370  loss_rpn_cls: 0.023  loss_rpn_loc: 0.059  time: 0.5674  data_time: 0.0081  lr: 0.004296  max_mem: 3067M
[32m[04/15 09:41:32 d2.utils.events]: [0m eta: 0:10:23  iter: 879  total_loss: 0.610  loss_cls: 0.204  loss_box_reg: 0.343  loss_rpn_cls: 0.013  loss_rpn_loc: 0.045  time: 0.5668  data_time: 0.0081  lr: 0.004396  max_mem: 3067M
[32m[04/15 09:41:43 d2.utils.events]: [0m eta: 0:10:12  iter: 899  total_loss: 0.585  loss_cls: 0.163  loss_box_reg: 0.335  loss_rpn_cls: 0.016  loss_rpn_loc: 0.039  time: 0.5665  data_time: 0.0068  lr: 0.004496  max_mem: 3067M
[32m[04/15 09:41:55 d2.utils.events]: [0m eta: 0:10:01  iter: 919  total_loss: 0.712  loss_cls: 0.228  loss_box_reg: 0.396  loss_rpn_cls: 0.024  loss_rpn_loc: 0.064  time: 0.5675  data_time: 0.0076  lr: 0.004595  max_mem: 3067M
[32m[04/15 09:42:07 d2.utils.events]: [0m eta: 0:09:49  iter: 939  total_loss: 0.673  loss_cls: 0.219  loss_box_reg: 0.368  loss_rpn_cls: 0.018  loss_rpn_loc: 0.048  time: 0.5670  data_time: 0.0078  lr: 0.004695  max_mem: 3067M
[32m[04/15 09:42:17 d2.utils.events]: [0m eta: 0:09:38  iter: 959  total_loss: 0.642  loss_cls: 0.196  loss_box_reg: 0.367  loss_rpn_cls: 0.017  loss_rpn_loc: 0.053  time: 0.5664  data_time: 0.0075  lr: 0.004795  max_mem: 3067M
[32m[04/15 09:42:29 d2.utils.events]: [0m eta: 0:09:27  iter: 979  total_loss: 0.565  loss_cls: 0.175  loss_box_reg: 0.324  loss_rpn_cls: 0.023  loss_rpn_loc: 0.046  time: 0.5668  data_time: 0.0072  lr: 0.004895  max_mem: 3067M
[32m[04/15 09:42:41 d2.utils.events]: [0m eta: 0:09:16  iter: 999  total_loss: 0.548  loss_cls: 0.191  loss_box_reg: 0.301  loss_rpn_cls: 0.015  loss_rpn_loc: 0.053  time: 0.5669  data_time: 0.0076  lr: 0.004995  max_mem: 3067M
[32m[04/15 09:42:52 d2.utils.events]: [0m eta: 0:09:04  iter: 1019  total_loss: 0.712  loss_cls: 0.240  loss_box_reg: 0.389  loss_rpn_cls: 0.016  loss_rpn_loc: 0.057  time: 0.5666  data_time: 0.0095  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:43:04 d2.utils.events]: [0m eta: 0:08:53  iter: 1039  total_loss: 0.684  loss_cls: 0.241  loss_box_reg: 0.366  loss_rpn_cls: 0.024  loss_rpn_loc: 0.057  time: 0.5669  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:43:15 d2.utils.events]: [0m eta: 0:08:42  iter: 1059  total_loss: 0.566  loss_cls: 0.158  loss_box_reg: 0.323  loss_rpn_cls: 0.014  loss_rpn_loc: 0.049  time: 0.5669  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:43:26 d2.utils.events]: [0m eta: 0:08:31  iter: 1079  total_loss: 0.592  loss_cls: 0.188  loss_box_reg: 0.343  loss_rpn_cls: 0.015  loss_rpn_loc: 0.051  time: 0.5664  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:43:37 d2.utils.events]: [0m eta: 0:08:19  iter: 1099  total_loss: 0.689  loss_cls: 0.225  loss_box_reg: 0.357  loss_rpn_cls: 0.017  loss_rpn_loc: 0.059  time: 0.5660  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:43:49 d2.utils.events]: [0m eta: 0:08:08  iter: 1119  total_loss: 0.617  loss_cls: 0.204  loss_box_reg: 0.357  loss_rpn_cls: 0.019  loss_rpn_loc: 0.048  time: 0.5663  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:44:00 d2.utils.events]: [0m eta: 0:07:57  iter: 1139  total_loss: 0.681  loss_cls: 0.224  loss_box_reg: 0.375  loss_rpn_cls: 0.014  loss_rpn_loc: 0.077  time: 0.5662  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:44:12 d2.utils.events]: [0m eta: 0:07:47  iter: 1159  total_loss: 0.671  loss_cls: 0.220  loss_box_reg: 0.383  loss_rpn_cls: 0.010  loss_rpn_loc: 0.049  time: 0.5664  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:44:24 d2.utils.events]: [0m eta: 0:07:36  iter: 1179  total_loss: 0.711  loss_cls: 0.213  loss_box_reg: 0.403  loss_rpn_cls: 0.011  loss_rpn_loc: 0.055  time: 0.5671  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:44:35 d2.utils.events]: [0m eta: 0:07:24  iter: 1199  total_loss: 0.609  loss_cls: 0.192  loss_box_reg: 0.337  loss_rpn_cls: 0.012  loss_rpn_loc: 0.040  time: 0.5666  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:44:46 d2.utils.events]: [0m eta: 0:07:13  iter: 1219  total_loss: 0.643  loss_cls: 0.217  loss_box_reg: 0.335  loss_rpn_cls: 0.022  loss_rpn_loc: 0.044  time: 0.5664  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:44:59 d2.utils.events]: [0m eta: 0:07:03  iter: 1239  total_loss: 0.630  loss_cls: 0.203  loss_box_reg: 0.353  loss_rpn_cls: 0.015  loss_rpn_loc: 0.042  time: 0.5673  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:45:09 d2.utils.events]: [0m eta: 0:06:50  iter: 1259  total_loss: 0.674  loss_cls: 0.213  loss_box_reg: 0.379  loss_rpn_cls: 0.015  loss_rpn_loc: 0.054  time: 0.5663  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:45:20 d2.utils.events]: [0m eta: 0:06:38  iter: 1279  total_loss: 0.633  loss_cls: 0.196  loss_box_reg: 0.334  loss_rpn_cls: 0.021  loss_rpn_loc: 0.059  time: 0.5662  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:45:32 d2.utils.events]: [0m eta: 0:06:27  iter: 1299  total_loss: 0.694  loss_cls: 0.220  loss_box_reg: 0.404  loss_rpn_cls: 0.016  loss_rpn_loc: 0.064  time: 0.5668  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:45:43 d2.utils.events]: [0m eta: 0:06:15  iter: 1319  total_loss: 0.607  loss_cls: 0.206  loss_box_reg: 0.309  loss_rpn_cls: 0.015  loss_rpn_loc: 0.042  time: 0.5666  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:45:54 d2.utils.events]: [0m eta: 0:06:03  iter: 1339  total_loss: 0.712  loss_cls: 0.236  loss_box_reg: 0.401  loss_rpn_cls: 0.015  loss_rpn_loc: 0.054  time: 0.5662  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:46:06 d2.utils.events]: [0m eta: 0:05:52  iter: 1359  total_loss: 0.589  loss_cls: 0.188  loss_box_reg: 0.328  loss_rpn_cls: 0.023  loss_rpn_loc: 0.054  time: 0.5662  data_time: 0.0111  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:46:17 d2.utils.events]: [0m eta: 0:05:41  iter: 1379  total_loss: 0.671  loss_cls: 0.213  loss_box_reg: 0.378  loss_rpn_cls: 0.016  loss_rpn_loc: 0.049  time: 0.5662  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:46:28 d2.utils.events]: [0m eta: 0:05:30  iter: 1399  total_loss: 0.612  loss_cls: 0.204  loss_box_reg: 0.345  loss_rpn_cls: 0.021  loss_rpn_loc: 0.056  time: 0.5659  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:46:40 d2.utils.events]: [0m eta: 0:05:19  iter: 1419  total_loss: 0.700  loss_cls: 0.224  loss_box_reg: 0.412  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5664  data_time: 0.0099  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:46:52 d2.utils.events]: [0m eta: 0:05:09  iter: 1439  total_loss: 0.606  loss_cls: 0.179  loss_box_reg: 0.317  loss_rpn_cls: 0.015  loss_rpn_loc: 0.040  time: 0.5667  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:47:03 d2.utils.events]: [0m eta: 0:04:58  iter: 1459  total_loss: 0.656  loss_cls: 0.229  loss_box_reg: 0.371  loss_rpn_cls: 0.019  loss_rpn_loc: 0.062  time: 0.5664  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:47:15 d2.utils.events]: [0m eta: 0:04:47  iter: 1479  total_loss: 0.741  loss_cls: 0.215  loss_box_reg: 0.387  loss_rpn_cls: 0.023  loss_rpn_loc: 0.061  time: 0.5665  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:47:27 d2.utils.events]: [0m eta: 0:04:37  iter: 1499  total_loss: 0.685  loss_cls: 0.216  loss_box_reg: 0.393  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5670  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:47:40 d2.utils.events]: [0m eta: 0:04:27  iter: 1519  total_loss: 0.572  loss_cls: 0.182  loss_box_reg: 0.315  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5678  data_time: 0.0910  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:47:51 d2.utils.events]: [0m eta: 0:04:15  iter: 1539  total_loss: 0.641  loss_cls: 0.200  loss_box_reg: 0.343  loss_rpn_cls: 0.026  loss_rpn_loc: 0.047  time: 0.5676  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:48:03 d2.utils.events]: [0m eta: 0:04:04  iter: 1559  total_loss: 0.672  loss_cls: 0.211  loss_box_reg: 0.369  loss_rpn_cls: 0.022  loss_rpn_loc: 0.061  time: 0.5679  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:48:15 d2.utils.events]: [0m eta: 0:03:53  iter: 1579  total_loss: 0.657  loss_cls: 0.216  loss_box_reg: 0.355  loss_rpn_cls: 0.016  loss_rpn_loc: 0.056  time: 0.5686  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:48:26 d2.utils.events]: [0m eta: 0:03:41  iter: 1599  total_loss: 0.629  loss_cls: 0.188  loss_box_reg: 0.351  loss_rpn_cls: 0.017  loss_rpn_loc: 0.048  time: 0.5681  data_time: 0.0113  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:48:37 d2.utils.events]: [0m eta: 0:03:30  iter: 1619  total_loss: 0.607  loss_cls: 0.190  loss_box_reg: 0.351  loss_rpn_cls: 0.019  loss_rpn_loc: 0.066  time: 0.5677  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:48:49 d2.utils.events]: [0m eta: 0:03:20  iter: 1639  total_loss: 0.672  loss_cls: 0.227  loss_box_reg: 0.340  loss_rpn_cls: 0.021  loss_rpn_loc: 0.059  time: 0.5680  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:49:00 d2.utils.events]: [0m eta: 0:03:09  iter: 1659  total_loss: 0.569  loss_cls: 0.188  loss_box_reg: 0.331  loss_rpn_cls: 0.015  loss_rpn_loc: 0.044  time: 0.5678  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:49:11 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.568  loss_cls: 0.173  loss_box_reg: 0.328  loss_rpn_cls: 0.019  loss_rpn_loc: 0.056  time: 0.5676  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:49:23 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.519  loss_cls: 0.158  loss_box_reg: 0.285  loss_rpn_cls: 0.016  loss_rpn_loc: 0.056  time: 0.5678  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:49:34 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.531  loss_cls: 0.177  loss_box_reg: 0.286  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5678  data_time: 0.0120  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:49:45 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.499  loss_cls: 0.166  loss_box_reg: 0.270  loss_rpn_cls: 0.015  loss_rpn_loc: 0.047  time: 0.5675  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:49:57 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.738  loss_cls: 0.237  loss_box_reg: 0.393  loss_rpn_cls: 0.023  loss_rpn_loc: 0.062  time: 0.5676  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:50:09 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.577  loss_cls: 0.183  loss_box_reg: 0.299  loss_rpn_cls: 0.022  loss_rpn_loc: 0.055  time: 0.5682  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:50:20 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.595  loss_cls: 0.196  loss_box_reg: 0.313  loss_rpn_cls: 0.015  loss_rpn_loc: 0.055  time: 0.5679  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:50:31 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.667  loss_cls: 0.200  loss_box_reg: 0.368  loss_rpn_cls: 0.014  loss_rpn_loc: 0.064  time: 0.5676  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:50:44 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.598  loss_cls: 0.202  loss_box_reg: 0.336  loss_rpn_cls: 0.018  loss_rpn_loc: 0.046  time: 0.5683  data_time: 0.0116  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:50:55 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.668  loss_cls: 0.218  loss_box_reg: 0.343  loss_rpn_cls: 0.019  loss_rpn_loc: 0.051  time: 0.5679  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:51:06 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.648  loss_cls: 0.201  loss_box_reg: 0.353  loss_rpn_cls: 0.016  loss_rpn_loc: 0.056  time: 0.5675  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:51:18 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.661  loss_cls: 0.236  loss_box_reg: 0.344  loss_rpn_cls: 0.016  loss_rpn_loc: 0.047  time: 0.5680  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:51:29 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.673  loss_cls: 0.203  loss_box_reg: 0.354  loss_rpn_cls: 0.019  loss_rpn_loc: 0.068  time: 0.5678  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:51:40 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.601  loss_cls: 0.196  loss_box_reg: 0.370  loss_rpn_cls: 0.015  loss_rpn_loc: 0.047  time: 0.5675  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:51:52 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.720  loss_cls: 0.216  loss_box_reg: 0.409  loss_rpn_cls: 0.019  loss_rpn_loc: 0.067  time: 0.5679  data_time: 0.0128  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:52:04 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.458  loss_cls: 0.161  loss_box_reg: 0.247  loss_rpn_cls: 0.021  loss_rpn_loc: 0.049  time: 0.5680  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 09:52:28 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:52:28 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 09:52:28 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 09:52:28 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.681  loss_cls: 0.201  loss_box_reg: 0.348  loss_rpn_cls: 0.020  loss_rpn_loc: 0.060  time: 0.5678  data_time: 0.0110  lr: 0.005000  max_mem: 3067M
[32m[04/15 09:52:32 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:54 (0.5681 s / it)
[32m[04/15 09:52:32 d2.engine.hooks]: [0mTotal training time: 0:19:21 (0:00:27 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 09:52:37 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:52:38 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 09:52:38 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 09:52:45 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1221 s / img. ETA=0:02:38
[32m[04/15 09:52:50 d2.evaluation.evaluator]: [0mInference done 44/1257. 0.1243 s / img. ETA=0:03:01
[32m[04/15 09:52:55 d2.evaluation.evaluator]: [0mInference done 75/1257. 0.1226 s / img. ETA=0:03:04
[32m[04/15 09:53:00 d2.evaluation.evaluator]: [0mInference done 110/1257. 0.1262 s / img. ETA=0:02:54
[32m[04/15 09:53:05 d2.evaluation.evaluator]: [0mInference done 142/1257. 0.1317 s / img. ETA=0:02:50
[32m[04/15 09:53:11 d2.evaluation.evaluator]: [0mInference done 177/1257. 0.1311 s / img. ETA=0:02:43
[32m[04/15 09:53:16 d2.evaluation.evaluator]: [0mInference done 210/1257. 0.1292 s / img. ETA=0:02:39
[32m[04/15 09:53:21 d2.evaluation.evaluator]: [0mInference done 242/1257. 0.1280 s / img. ETA=0:02:35
[32m[04/15 09:53:26 d2.evaluation.evaluator]: [0mInference done 277/1257. 0.1273 s / img. ETA=0:02:28
[32m[04/15 09:53:31 d2.evaluation.evaluator]: [0mInference done 314/1257. 0.1271 s / img. ETA=0:02:21
[32m[04/15 09:53:36 d2.evaluation.evaluator]: [0mInference done 351/1257. 0.1264 s / img. ETA=0:02:14
[32m[04/15 09:53:41 d2.evaluation.evaluator]: [0mInference done 387/1257. 0.1271 s / img. ETA=0:02:08
[32m[04/15 09:53:46 d2.evaluation.evaluator]: [0mInference done 420/1257. 0.1287 s / img. ETA=0:02:03
[32m[04/15 09:53:51 d2.evaluation.evaluator]: [0mInference done 451/1257. 0.1286 s / img. ETA=0:02:00
[32m[04/15 09:53:56 d2.evaluation.evaluator]: [0mInference done 485/1257. 0.1280 s / img. ETA=0:01:55
[32m[04/15 09:54:01 d2.evaluation.evaluator]: [0mInference done 519/1257. 0.1276 s / img. ETA=0:01:50
[32m[04/15 09:54:06 d2.evaluation.evaluator]: [0mInference done 550/1257. 0.1274 s / img. ETA=0:01:46
[32m[04/15 09:54:12 d2.evaluation.evaluator]: [0mInference done 573/1257. 0.1271 s / img. ETA=0:01:45
[32m[04/15 09:54:17 d2.evaluation.evaluator]: [0mInference done 599/1257. 0.1268 s / img. ETA=0:01:43
[32m[04/15 09:54:23 d2.evaluation.evaluator]: [0mInference done 627/1257. 0.1277 s / img. ETA=0:01:39
[32m[04/15 09:54:28 d2.evaluation.evaluator]: [0mInference done 653/1257. 0.1276 s / img. ETA=0:01:36
[32m[04/15 09:54:33 d2.evaluation.evaluator]: [0mInference done 680/1257. 0.1273 s / img. ETA=0:01:32
[32m[04/15 09:54:38 d2.evaluation.evaluator]: [0mInference done 714/1257. 0.1270 s / img. ETA=0:01:26
[32m[04/15 09:54:43 d2.evaluation.evaluator]: [0mInference done 735/1257. 0.1268 s / img. ETA=0:01:24
[32m[04/15 09:54:48 d2.evaluation.evaluator]: [0mInference done 756/1257. 0.1266 s / img. ETA=0:01:22
[32m[04/15 09:54:53 d2.evaluation.evaluator]: [0mInference done 790/1257. 0.1264 s / img. ETA=0:01:16
[32m[04/15 09:54:58 d2.evaluation.evaluator]: [0mInference done 828/1257. 0.1264 s / img. ETA=0:01:09
[32m[04/15 09:55:03 d2.evaluation.evaluator]: [0mInference done 862/1257. 0.1272 s / img. ETA=0:01:04
[32m[04/15 09:55:08 d2.evaluation.evaluator]: [0mInference done 897/1257. 0.1278 s / img. ETA=0:00:58
[32m[04/15 09:55:13 d2.evaluation.evaluator]: [0mInference done 936/1257. 0.1276 s / img. ETA=0:00:51
[32m[04/15 09:55:19 d2.evaluation.evaluator]: [0mInference done 975/1257. 0.1275 s / img. ETA=0:00:44
[32m[04/15 09:55:24 d2.evaluation.evaluator]: [0mInference done 1014/1257. 0.1273 s / img. ETA=0:00:38
[32m[04/15 09:55:29 d2.evaluation.evaluator]: [0mInference done 1053/1257. 0.1273 s / img. ETA=0:00:31
[32m[04/15 09:55:34 d2.evaluation.evaluator]: [0mInference done 1093/1257. 0.1271 s / img. ETA=0:00:25
[32m[04/15 09:55:39 d2.evaluation.evaluator]: [0mInference done 1131/1257. 0.1270 s / img. ETA=0:00:19
[32m[04/15 09:55:44 d2.evaluation.evaluator]: [0mInference done 1166/1257. 0.1275 s / img. ETA=0:00:14
[32m[04/15 09:55:49 d2.evaluation.evaluator]: [0mInference done 1200/1257. 0.1280 s / img. ETA=0:00:08
[32m[04/15 09:55:54 d2.evaluation.evaluator]: [0mInference done 1238/1257. 0.1280 s / img. ETA=0:00:02
[32m[04/15 09:55:57 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:12.328696 (0.153617 s / img per device, on 1 devices)
[32m[04/15 09:55:57 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:40 (0.127928 s / img per device, on 1 devices)
[32m[04/15 09:55:57 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 09:55:57 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 09:55:57 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=6.88s).
Accumulating evaluation results...
DONE (t=0.83s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.201
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.392
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.180
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.115
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.251
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.453
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.105
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.251
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.254
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.312
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501
[32m[04/15 09:56:05 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 20.079 | 39.166 | 17.959 | 11.456 | 25.079 | 45.349 |
[32m[04/15 09:56:05 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP     | category       | AP     |
|:--------------|:-------|:--------------|:-------|:---------------|:-------|
| person        | 28.953 | bicycle       | 10.769 | car            | 40.594 |
| motorcycle    | nan    | airplane      | nan    | bus            | nan    |
| train         | nan    | truck         | nan    | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan    | stop sign      | nan    |
| parking meter | nan    | bench         | nan    | bird           | nan    |
| cat           | nan    | dog           | 0.000  | horse          | nan    |
| sheep         | nan    | cow           | nan    | elephant       | nan    |
| bear          | nan    | zebra         | nan    | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan    | handbag        | nan    |
| tie           | nan    | suitcase      | nan    | frisbee        | nan    |
| skis          | nan    | snowboard     | nan    | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan    | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan    | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan    | cup            | nan    |
| fork          | nan    | knife         | nan    | spoon          | nan    |
| bowl          | nan    | banana        | nan    | apple          | nan    |
| sandwich      | nan    | orange        | nan    | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan    | pizza          | nan    |
| donut         | nan    | cake          | nan    | chair          | nan    |
| couch         | nan    | potted plant  | nan    | bed            | nan    |
| dining table  | nan    | toilet        | nan    | tv             | nan    |
| laptop        | nan    | mouse         | nan    | remote         | nan    |
| keyboard      | nan    | cell phone    | nan    | microwave      | nan    |
| oven          | nan    | bike          | nan    | hydrant        | nan    |
| motor         | nan    | rider         | nan    | light          | nan    |
| sign          | nan    | motor vehicle | nan    | human face     | nan    |
| hair drier    | nan    | license plate | nan    |                |        |
============== The  17  *  2000  iterations ============
4 channel input
[32m[04/15 09:56:07 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 09:56:08 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.24 seconds.
[5m[31mWARNING[0m [32m[04/15 09:56:08 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 09:56:08 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 09:56:09 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 09:56:10 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 09:56:10 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 09:56:13 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 09:56:26 d2.utils.events]: [0m eta: 0:19:04  iter: 19  total_loss: 0.679  loss_cls: 0.216  loss_box_reg: 0.377  loss_rpn_cls: 0.020  loss_rpn_loc: 0.068  time: 0.5929  data_time: 0.0616  lr: 0.000100  max_mem: 3067M
[32m[04/15 09:56:38 d2.utils.events]: [0m eta: 0:18:42  iter: 39  total_loss: 0.701  loss_cls: 0.204  loss_box_reg: 0.378  loss_rpn_cls: 0.018  loss_rpn_loc: 0.051  time: 0.5872  data_time: 0.0079  lr: 0.000200  max_mem: 3067M
[32m[04/15 09:56:49 d2.utils.events]: [0m eta: 0:18:04  iter: 59  total_loss: 0.590  loss_cls: 0.191  loss_box_reg: 0.331  loss_rpn_cls: 0.017  loss_rpn_loc: 0.056  time: 0.5738  data_time: 0.0081  lr: 0.000300  max_mem: 3067M
[32m[04/15 09:57:00 d2.utils.events]: [0m eta: 0:17:49  iter: 79  total_loss: 0.587  loss_cls: 0.191  loss_box_reg: 0.316  loss_rpn_cls: 0.021  loss_rpn_loc: 0.048  time: 0.5667  data_time: 0.0112  lr: 0.000400  max_mem: 3067M
[32m[04/15 09:57:13 d2.utils.events]: [0m eta: 0:18:04  iter: 99  total_loss: 0.611  loss_cls: 0.193  loss_box_reg: 0.322  loss_rpn_cls: 0.011  loss_rpn_loc: 0.055  time: 0.5810  data_time: 0.0084  lr: 0.000500  max_mem: 3067M
[32m[04/15 09:57:23 d2.utils.events]: [0m eta: 0:17:35  iter: 119  total_loss: 0.540  loss_cls: 0.158  loss_box_reg: 0.299  loss_rpn_cls: 0.013  loss_rpn_loc: 0.042  time: 0.5740  data_time: 0.0076  lr: 0.000599  max_mem: 3067M
[32m[04/15 09:57:34 d2.utils.events]: [0m eta: 0:17:18  iter: 139  total_loss: 0.569  loss_cls: 0.165  loss_box_reg: 0.309  loss_rpn_cls: 0.015  loss_rpn_loc: 0.044  time: 0.5691  data_time: 0.0091  lr: 0.000699  max_mem: 3067M
[32m[04/15 09:57:46 d2.utils.events]: [0m eta: 0:17:02  iter: 159  total_loss: 0.558  loss_cls: 0.189  loss_box_reg: 0.312  loss_rpn_cls: 0.014  loss_rpn_loc: 0.046  time: 0.5695  data_time: 0.0066  lr: 0.000799  max_mem: 3067M
[32m[04/15 09:57:57 d2.utils.events]: [0m eta: 0:16:51  iter: 179  total_loss: 0.651  loss_cls: 0.198  loss_box_reg: 0.350  loss_rpn_cls: 0.014  loss_rpn_loc: 0.056  time: 0.5687  data_time: 0.0070  lr: 0.000899  max_mem: 3067M
[32m[04/15 09:58:08 d2.utils.events]: [0m eta: 0:16:39  iter: 199  total_loss: 0.599  loss_cls: 0.201  loss_box_reg: 0.337  loss_rpn_cls: 0.010  loss_rpn_loc: 0.048  time: 0.5669  data_time: 0.0076  lr: 0.000999  max_mem: 3067M
[32m[04/15 09:58:20 d2.utils.events]: [0m eta: 0:16:28  iter: 219  total_loss: 0.592  loss_cls: 0.193  loss_box_reg: 0.340  loss_rpn_cls: 0.014  loss_rpn_loc: 0.059  time: 0.5686  data_time: 0.0085  lr: 0.001099  max_mem: 3067M
[32m[04/15 09:58:32 d2.utils.events]: [0m eta: 0:16:22  iter: 239  total_loss: 0.589  loss_cls: 0.181  loss_box_reg: 0.336  loss_rpn_cls: 0.012  loss_rpn_loc: 0.046  time: 0.5712  data_time: 0.0070  lr: 0.001199  max_mem: 3067M
[32m[04/15 09:58:44 d2.utils.events]: [0m eta: 0:16:11  iter: 259  total_loss: 0.563  loss_cls: 0.192  loss_box_reg: 0.317  loss_rpn_cls: 0.011  loss_rpn_loc: 0.043  time: 0.5707  data_time: 0.0067  lr: 0.001299  max_mem: 3067M
[32m[04/15 09:58:55 d2.utils.events]: [0m eta: 0:15:55  iter: 279  total_loss: 0.511  loss_cls: 0.170  loss_box_reg: 0.285  loss_rpn_cls: 0.017  loss_rpn_loc: 0.051  time: 0.5693  data_time: 0.0068  lr: 0.001399  max_mem: 3067M
[32m[04/15 09:59:07 d2.utils.events]: [0m eta: 0:15:49  iter: 299  total_loss: 0.541  loss_cls: 0.174  loss_box_reg: 0.296  loss_rpn_cls: 0.010  loss_rpn_loc: 0.053  time: 0.5717  data_time: 0.0078  lr: 0.001499  max_mem: 3067M
[32m[04/15 09:59:18 d2.utils.events]: [0m eta: 0:15:36  iter: 319  total_loss: 0.661  loss_cls: 0.201  loss_box_reg: 0.355  loss_rpn_cls: 0.024  loss_rpn_loc: 0.066  time: 0.5699  data_time: 0.0107  lr: 0.001598  max_mem: 3067M
[32m[04/15 09:59:28 d2.utils.events]: [0m eta: 0:15:21  iter: 339  total_loss: 0.500  loss_cls: 0.152  loss_box_reg: 0.279  loss_rpn_cls: 0.013  loss_rpn_loc: 0.043  time: 0.5667  data_time: 0.0069  lr: 0.001698  max_mem: 3067M
[32m[04/15 09:59:41 d2.utils.events]: [0m eta: 0:15:15  iter: 359  total_loss: 0.622  loss_cls: 0.204  loss_box_reg: 0.358  loss_rpn_cls: 0.014  loss_rpn_loc: 0.045  time: 0.5706  data_time: 0.0106  lr: 0.001798  max_mem: 3067M
[32m[04/15 09:59:52 d2.utils.events]: [0m eta: 0:15:01  iter: 379  total_loss: 0.592  loss_cls: 0.180  loss_box_reg: 0.345  loss_rpn_cls: 0.012  loss_rpn_loc: 0.039  time: 0.5695  data_time: 0.0090  lr: 0.001898  max_mem: 3067M
[32m[04/15 10:00:03 d2.utils.events]: [0m eta: 0:14:48  iter: 399  total_loss: 0.502  loss_cls: 0.170  loss_box_reg: 0.260  loss_rpn_cls: 0.015  loss_rpn_loc: 0.061  time: 0.5681  data_time: 0.0067  lr: 0.001998  max_mem: 3067M
[32m[04/15 10:00:15 d2.utils.events]: [0m eta: 0:14:38  iter: 419  total_loss: 0.562  loss_cls: 0.175  loss_box_reg: 0.306  loss_rpn_cls: 0.012  loss_rpn_loc: 0.052  time: 0.5700  data_time: 0.0080  lr: 0.002098  max_mem: 3067M
[32m[04/15 10:00:27 d2.utils.events]: [0m eta: 0:14:26  iter: 439  total_loss: 0.441  loss_cls: 0.168  loss_box_reg: 0.268  loss_rpn_cls: 0.010  loss_rpn_loc: 0.022  time: 0.5690  data_time: 0.0072  lr: 0.002198  max_mem: 3067M
[32m[04/15 10:00:38 d2.utils.events]: [0m eta: 0:14:15  iter: 459  total_loss: 0.555  loss_cls: 0.172  loss_box_reg: 0.289  loss_rpn_cls: 0.013  loss_rpn_loc: 0.033  time: 0.5686  data_time: 0.0087  lr: 0.002298  max_mem: 3067M
[32m[04/15 10:00:50 d2.utils.events]: [0m eta: 0:14:05  iter: 479  total_loss: 0.574  loss_cls: 0.190  loss_box_reg: 0.310  loss_rpn_cls: 0.013  loss_rpn_loc: 0.052  time: 0.5697  data_time: 0.0077  lr: 0.002398  max_mem: 3067M
[32m[04/15 10:01:01 d2.utils.events]: [0m eta: 0:13:53  iter: 499  total_loss: 0.547  loss_cls: 0.182  loss_box_reg: 0.326  loss_rpn_cls: 0.013  loss_rpn_loc: 0.053  time: 0.5693  data_time: 0.0066  lr: 0.002498  max_mem: 3067M
[32m[04/15 10:01:12 d2.utils.events]: [0m eta: 0:13:40  iter: 519  total_loss: 0.630  loss_cls: 0.200  loss_box_reg: 0.357  loss_rpn_cls: 0.014  loss_rpn_loc: 0.047  time: 0.5680  data_time: 0.0073  lr: 0.002597  max_mem: 3067M
[32m[04/15 10:01:24 d2.utils.events]: [0m eta: 0:13:28  iter: 539  total_loss: 0.550  loss_cls: 0.182  loss_box_reg: 0.286  loss_rpn_cls: 0.011  loss_rpn_loc: 0.042  time: 0.5685  data_time: 0.0098  lr: 0.002697  max_mem: 3067M
[32m[04/15 10:01:35 d2.utils.events]: [0m eta: 0:13:17  iter: 559  total_loss: 0.669  loss_cls: 0.200  loss_box_reg: 0.369  loss_rpn_cls: 0.017  loss_rpn_loc: 0.059  time: 0.5686  data_time: 0.0074  lr: 0.002797  max_mem: 3067M
[32m[04/15 10:01:46 d2.utils.events]: [0m eta: 0:13:07  iter: 579  total_loss: 0.528  loss_cls: 0.173  loss_box_reg: 0.298  loss_rpn_cls: 0.008  loss_rpn_loc: 0.045  time: 0.5683  data_time: 0.0077  lr: 0.002897  max_mem: 3067M
[32m[04/15 10:01:57 d2.utils.events]: [0m eta: 0:12:54  iter: 599  total_loss: 0.688  loss_cls: 0.216  loss_box_reg: 0.389  loss_rpn_cls: 0.014  loss_rpn_loc: 0.063  time: 0.5676  data_time: 0.0081  lr: 0.002997  max_mem: 3067M
[32m[04/15 10:02:10 d2.utils.events]: [0m eta: 0:12:45  iter: 619  total_loss: 0.496  loss_cls: 0.194  loss_box_reg: 0.275  loss_rpn_cls: 0.011  loss_rpn_loc: 0.041  time: 0.5690  data_time: 0.0089  lr: 0.003097  max_mem: 3067M
[32m[04/15 10:02:21 d2.utils.events]: [0m eta: 0:12:35  iter: 639  total_loss: 0.565  loss_cls: 0.172  loss_box_reg: 0.302  loss_rpn_cls: 0.010  loss_rpn_loc: 0.047  time: 0.5689  data_time: 0.0073  lr: 0.003197  max_mem: 3067M
[32m[04/15 10:02:32 d2.utils.events]: [0m eta: 0:12:22  iter: 659  total_loss: 0.544  loss_cls: 0.174  loss_box_reg: 0.336  loss_rpn_cls: 0.009  loss_rpn_loc: 0.043  time: 0.5675  data_time: 0.0064  lr: 0.003297  max_mem: 3067M
[32m[04/15 10:02:44 d2.utils.events]: [0m eta: 0:12:12  iter: 679  total_loss: 0.674  loss_cls: 0.227  loss_box_reg: 0.358  loss_rpn_cls: 0.022  loss_rpn_loc: 0.075  time: 0.5687  data_time: 0.0077  lr: 0.003397  max_mem: 3067M
[32m[04/15 10:02:55 d2.utils.events]: [0m eta: 0:11:59  iter: 699  total_loss: 0.594  loss_cls: 0.195  loss_box_reg: 0.357  loss_rpn_cls: 0.014  loss_rpn_loc: 0.046  time: 0.5679  data_time: 0.0086  lr: 0.003497  max_mem: 3067M
[32m[04/15 10:03:06 d2.utils.events]: [0m eta: 0:11:48  iter: 719  total_loss: 0.535  loss_cls: 0.171  loss_box_reg: 0.292  loss_rpn_cls: 0.011  loss_rpn_loc: 0.048  time: 0.5668  data_time: 0.0066  lr: 0.003596  max_mem: 3067M
[32m[04/15 10:03:18 d2.utils.events]: [0m eta: 0:11:38  iter: 739  total_loss: 0.565  loss_cls: 0.199  loss_box_reg: 0.293  loss_rpn_cls: 0.013  loss_rpn_loc: 0.055  time: 0.5679  data_time: 0.0072  lr: 0.003696  max_mem: 3067M
[32m[04/15 10:03:31 d2.utils.events]: [0m eta: 0:11:28  iter: 759  total_loss: 0.600  loss_cls: 0.183  loss_box_reg: 0.329  loss_rpn_cls: 0.011  loss_rpn_loc: 0.038  time: 0.5694  data_time: 0.0794  lr: 0.003796  max_mem: 3067M
[32m[04/15 10:03:42 d2.utils.events]: [0m eta: 0:11:16  iter: 779  total_loss: 0.669  loss_cls: 0.214  loss_box_reg: 0.370  loss_rpn_cls: 0.014  loss_rpn_loc: 0.050  time: 0.5687  data_time: 0.0101  lr: 0.003896  max_mem: 3067M
[32m[04/15 10:03:52 d2.utils.events]: [0m eta: 0:11:04  iter: 799  total_loss: 0.613  loss_cls: 0.188  loss_box_reg: 0.335  loss_rpn_cls: 0.015  loss_rpn_loc: 0.047  time: 0.5680  data_time: 0.0098  lr: 0.003996  max_mem: 3067M
[32m[04/15 10:04:04 d2.utils.events]: [0m eta: 0:10:54  iter: 819  total_loss: 0.660  loss_cls: 0.195  loss_box_reg: 0.357  loss_rpn_cls: 0.011  loss_rpn_loc: 0.062  time: 0.5685  data_time: 0.0073  lr: 0.004096  max_mem: 3067M
[32m[04/15 10:04:16 d2.utils.events]: [0m eta: 0:10:43  iter: 839  total_loss: 0.600  loss_cls: 0.181  loss_box_reg: 0.353  loss_rpn_cls: 0.012  loss_rpn_loc: 0.047  time: 0.5691  data_time: 0.0077  lr: 0.004196  max_mem: 3067M
[32m[04/15 10:04:28 d2.utils.events]: [0m eta: 0:10:31  iter: 859  total_loss: 0.634  loss_cls: 0.181  loss_box_reg: 0.341  loss_rpn_cls: 0.017  loss_rpn_loc: 0.049  time: 0.5687  data_time: 0.0076  lr: 0.004296  max_mem: 3067M
[32m[04/15 10:04:39 d2.utils.events]: [0m eta: 0:10:20  iter: 879  total_loss: 0.575  loss_cls: 0.198  loss_box_reg: 0.318  loss_rpn_cls: 0.010  loss_rpn_loc: 0.045  time: 0.5686  data_time: 0.0070  lr: 0.004396  max_mem: 3067M
[32m[04/15 10:04:51 d2.utils.events]: [0m eta: 0:10:10  iter: 899  total_loss: 0.670  loss_cls: 0.230  loss_box_reg: 0.395  loss_rpn_cls: 0.011  loss_rpn_loc: 0.039  time: 0.5694  data_time: 0.0084  lr: 0.004496  max_mem: 3067M
[32m[04/15 10:05:02 d2.utils.events]: [0m eta: 0:09:58  iter: 919  total_loss: 0.584  loss_cls: 0.192  loss_box_reg: 0.326  loss_rpn_cls: 0.011  loss_rpn_loc: 0.044  time: 0.5687  data_time: 0.0063  lr: 0.004595  max_mem: 3067M
[32m[04/15 10:05:14 d2.utils.events]: [0m eta: 0:09:48  iter: 939  total_loss: 0.606  loss_cls: 0.202  loss_box_reg: 0.327  loss_rpn_cls: 0.017  loss_rpn_loc: 0.046  time: 0.5694  data_time: 0.0054  lr: 0.004695  max_mem: 3067M
[32m[04/15 10:05:25 d2.utils.events]: [0m eta: 0:09:36  iter: 959  total_loss: 0.645  loss_cls: 0.223  loss_box_reg: 0.346  loss_rpn_cls: 0.016  loss_rpn_loc: 0.058  time: 0.5685  data_time: 0.0067  lr: 0.004795  max_mem: 3067M
[32m[04/15 10:05:35 d2.utils.events]: [0m eta: 0:09:24  iter: 979  total_loss: 0.690  loss_cls: 0.235  loss_box_reg: 0.361  loss_rpn_cls: 0.022  loss_rpn_loc: 0.070  time: 0.5677  data_time: 0.0087  lr: 0.004895  max_mem: 3067M
[32m[04/15 10:05:46 d2.utils.events]: [0m eta: 0:09:13  iter: 999  total_loss: 0.648  loss_cls: 0.211  loss_box_reg: 0.358  loss_rpn_cls: 0.020  loss_rpn_loc: 0.057  time: 0.5670  data_time: 0.0063  lr: 0.004995  max_mem: 3067M
[32m[04/15 10:05:59 d2.utils.events]: [0m eta: 0:09:02  iter: 1019  total_loss: 0.572  loss_cls: 0.176  loss_box_reg: 0.310  loss_rpn_cls: 0.011  loss_rpn_loc: 0.037  time: 0.5683  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:06:10 d2.utils.events]: [0m eta: 0:08:50  iter: 1039  total_loss: 0.612  loss_cls: 0.195  loss_box_reg: 0.350  loss_rpn_cls: 0.013  loss_rpn_loc: 0.060  time: 0.5680  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:06:21 d2.utils.events]: [0m eta: 0:08:39  iter: 1059  total_loss: 0.619  loss_cls: 0.191  loss_box_reg: 0.350  loss_rpn_cls: 0.017  loss_rpn_loc: 0.053  time: 0.5673  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:06:32 d2.utils.events]: [0m eta: 0:08:28  iter: 1079  total_loss: 0.671  loss_cls: 0.221  loss_box_reg: 0.373  loss_rpn_cls: 0.020  loss_rpn_loc: 0.068  time: 0.5672  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:06:44 d2.utils.events]: [0m eta: 0:08:16  iter: 1099  total_loss: 0.681  loss_cls: 0.233  loss_box_reg: 0.350  loss_rpn_cls: 0.022  loss_rpn_loc: 0.049  time: 0.5676  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:06:55 d2.utils.events]: [0m eta: 0:08:05  iter: 1119  total_loss: 0.642  loss_cls: 0.183  loss_box_reg: 0.345  loss_rpn_cls: 0.019  loss_rpn_loc: 0.061  time: 0.5672  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:07:06 d2.utils.events]: [0m eta: 0:07:55  iter: 1139  total_loss: 0.685  loss_cls: 0.224  loss_box_reg: 0.378  loss_rpn_cls: 0.017  loss_rpn_loc: 0.058  time: 0.5673  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:07:18 d2.utils.events]: [0m eta: 0:07:44  iter: 1159  total_loss: 0.633  loss_cls: 0.197  loss_box_reg: 0.319  loss_rpn_cls: 0.014  loss_rpn_loc: 0.063  time: 0.5679  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:07:29 d2.utils.events]: [0m eta: 0:07:33  iter: 1179  total_loss: 0.605  loss_cls: 0.191  loss_box_reg: 0.309  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5675  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:07:41 d2.utils.events]: [0m eta: 0:07:22  iter: 1199  total_loss: 0.585  loss_cls: 0.184  loss_box_reg: 0.339  loss_rpn_cls: 0.018  loss_rpn_loc: 0.045  time: 0.5673  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:07:53 d2.utils.events]: [0m eta: 0:07:11  iter: 1219  total_loss: 0.581  loss_cls: 0.196  loss_box_reg: 0.322  loss_rpn_cls: 0.020  loss_rpn_loc: 0.036  time: 0.5683  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:08:04 d2.utils.events]: [0m eta: 0:07:00  iter: 1239  total_loss: 0.551  loss_cls: 0.184  loss_box_reg: 0.310  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5677  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:08:15 d2.utils.events]: [0m eta: 0:06:49  iter: 1259  total_loss: 0.659  loss_cls: 0.215  loss_box_reg: 0.328  loss_rpn_cls: 0.015  loss_rpn_loc: 0.058  time: 0.5673  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:08:28 d2.utils.events]: [0m eta: 0:06:38  iter: 1279  total_loss: 0.597  loss_cls: 0.211  loss_box_reg: 0.356  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5683  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:08:38 d2.utils.events]: [0m eta: 0:06:26  iter: 1299  total_loss: 0.606  loss_cls: 0.211  loss_box_reg: 0.312  loss_rpn_cls: 0.024  loss_rpn_loc: 0.056  time: 0.5678  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:08:49 d2.utils.events]: [0m eta: 0:06:15  iter: 1319  total_loss: 0.711  loss_cls: 0.228  loss_box_reg: 0.369  loss_rpn_cls: 0.024  loss_rpn_loc: 0.066  time: 0.5672  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:09:01 d2.utils.events]: [0m eta: 0:06:05  iter: 1339  total_loss: 0.803  loss_cls: 0.242  loss_box_reg: 0.404  loss_rpn_cls: 0.021  loss_rpn_loc: 0.080  time: 0.5675  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:09:13 d2.utils.events]: [0m eta: 0:05:53  iter: 1359  total_loss: 0.708  loss_cls: 0.221  loss_box_reg: 0.407  loss_rpn_cls: 0.015  loss_rpn_loc: 0.066  time: 0.5677  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:09:23 d2.utils.events]: [0m eta: 0:05:42  iter: 1379  total_loss: 0.568  loss_cls: 0.182  loss_box_reg: 0.321  loss_rpn_cls: 0.014  loss_rpn_loc: 0.064  time: 0.5673  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:09:35 d2.utils.events]: [0m eta: 0:05:31  iter: 1399  total_loss: 0.606  loss_cls: 0.176  loss_box_reg: 0.352  loss_rpn_cls: 0.014  loss_rpn_loc: 0.057  time: 0.5673  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:09:47 d2.utils.events]: [0m eta: 0:05:20  iter: 1419  total_loss: 0.682  loss_cls: 0.226  loss_box_reg: 0.374  loss_rpn_cls: 0.015  loss_rpn_loc: 0.051  time: 0.5677  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:09:58 d2.utils.events]: [0m eta: 0:05:09  iter: 1439  total_loss: 0.539  loss_cls: 0.158  loss_box_reg: 0.319  loss_rpn_cls: 0.012  loss_rpn_loc: 0.039  time: 0.5673  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:10:09 d2.utils.events]: [0m eta: 0:04:58  iter: 1459  total_loss: 0.618  loss_cls: 0.209  loss_box_reg: 0.313  loss_rpn_cls: 0.020  loss_rpn_loc: 0.061  time: 0.5671  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:10:21 d2.utils.events]: [0m eta: 0:04:47  iter: 1479  total_loss: 0.722  loss_cls: 0.225  loss_box_reg: 0.394  loss_rpn_cls: 0.022  loss_rpn_loc: 0.078  time: 0.5679  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:10:32 d2.utils.events]: [0m eta: 0:04:36  iter: 1499  total_loss: 0.670  loss_cls: 0.212  loss_box_reg: 0.360  loss_rpn_cls: 0.020  loss_rpn_loc: 0.064  time: 0.5674  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:10:43 d2.utils.events]: [0m eta: 0:04:25  iter: 1519  total_loss: 0.692  loss_cls: 0.198  loss_box_reg: 0.351  loss_rpn_cls: 0.015  loss_rpn_loc: 0.047  time: 0.5668  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:10:55 d2.utils.events]: [0m eta: 0:04:14  iter: 1539  total_loss: 0.631  loss_cls: 0.192  loss_box_reg: 0.360  loss_rpn_cls: 0.014  loss_rpn_loc: 0.056  time: 0.5675  data_time: 0.0096  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:11:07 d2.utils.events]: [0m eta: 0:04:03  iter: 1559  total_loss: 0.671  loss_cls: 0.195  loss_box_reg: 0.359  loss_rpn_cls: 0.021  loss_rpn_loc: 0.062  time: 0.5674  data_time: 0.0098  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:11:17 d2.utils.events]: [0m eta: 0:03:52  iter: 1579  total_loss: 0.537  loss_cls: 0.179  loss_box_reg: 0.330  loss_rpn_cls: 0.020  loss_rpn_loc: 0.045  time: 0.5670  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:11:30 d2.utils.events]: [0m eta: 0:03:41  iter: 1599  total_loss: 0.668  loss_cls: 0.207  loss_box_reg: 0.366  loss_rpn_cls: 0.014  loss_rpn_loc: 0.047  time: 0.5676  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:11:41 d2.utils.events]: [0m eta: 0:03:30  iter: 1619  total_loss: 0.651  loss_cls: 0.217  loss_box_reg: 0.370  loss_rpn_cls: 0.016  loss_rpn_loc: 0.069  time: 0.5677  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:11:52 d2.utils.events]: [0m eta: 0:03:19  iter: 1639  total_loss: 0.646  loss_cls: 0.193  loss_box_reg: 0.360  loss_rpn_cls: 0.018  loss_rpn_loc: 0.051  time: 0.5672  data_time: 0.0107  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:12:04 d2.utils.events]: [0m eta: 0:03:08  iter: 1659  total_loss: 0.640  loss_cls: 0.205  loss_box_reg: 0.355  loss_rpn_cls: 0.012  loss_rpn_loc: 0.062  time: 0.5674  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:12:16 d2.utils.events]: [0m eta: 0:02:57  iter: 1679  total_loss: 0.575  loss_cls: 0.177  loss_box_reg: 0.318  loss_rpn_cls: 0.011  loss_rpn_loc: 0.050  time: 0.5679  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:12:27 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.734  loss_cls: 0.222  loss_box_reg: 0.403  loss_rpn_cls: 0.015  loss_rpn_loc: 0.058  time: 0.5676  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:12:38 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.624  loss_cls: 0.190  loss_box_reg: 0.373  loss_rpn_cls: 0.013  loss_rpn_loc: 0.067  time: 0.5671  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:12:50 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.766  loss_cls: 0.229  loss_box_reg: 0.395  loss_rpn_cls: 0.022  loss_rpn_loc: 0.079  time: 0.5677  data_time: 0.0100  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:13:01 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.679  loss_cls: 0.222  loss_box_reg: 0.338  loss_rpn_cls: 0.019  loss_rpn_loc: 0.055  time: 0.5673  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:13:12 d2.utils.events]: [0m eta: 0:02:01  iter: 1779  total_loss: 0.725  loss_cls: 0.234  loss_box_reg: 0.402  loss_rpn_cls: 0.024  loss_rpn_loc: 0.071  time: 0.5668  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:13:25 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.686  loss_cls: 0.210  loss_box_reg: 0.375  loss_rpn_cls: 0.021  loss_rpn_loc: 0.067  time: 0.5677  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:13:35 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.566  loss_cls: 0.172  loss_box_reg: 0.295  loss_rpn_cls: 0.016  loss_rpn_loc: 0.061  time: 0.5673  data_time: 0.0061  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:13:46 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.696  loss_cls: 0.227  loss_box_reg: 0.349  loss_rpn_cls: 0.024  loss_rpn_loc: 0.069  time: 0.5669  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:13:58 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.677  loss_cls: 0.216  loss_box_reg: 0.337  loss_rpn_cls: 0.020  loss_rpn_loc: 0.058  time: 0.5671  data_time: 0.0106  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:14:09 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.678  loss_cls: 0.201  loss_box_reg: 0.370  loss_rpn_cls: 0.014  loss_rpn_loc: 0.056  time: 0.5671  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:14:20 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.670  loss_cls: 0.224  loss_box_reg: 0.363  loss_rpn_cls: 0.023  loss_rpn_loc: 0.058  time: 0.5669  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:14:32 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.663  loss_cls: 0.203  loss_box_reg: 0.365  loss_rpn_cls: 0.018  loss_rpn_loc: 0.054  time: 0.5672  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:14:44 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.551  loss_cls: 0.182  loss_box_reg: 0.304  loss_rpn_cls: 0.022  loss_rpn_loc: 0.049  time: 0.5672  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:14:54 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.600  loss_cls: 0.178  loss_box_reg: 0.321  loss_rpn_cls: 0.013  loss_rpn_loc: 0.061  time: 0.5668  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:15:05 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.672  loss_cls: 0.217  loss_box_reg: 0.377  loss_rpn_cls: 0.019  loss_rpn_loc: 0.067  time: 0.5667  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 10:15:40 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 10:15:40 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 10:15:40 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 10:15:40 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.609  loss_cls: 0.191  loss_box_reg: 0.349  loss_rpn_cls: 0.018  loss_rpn_loc: 0.044  time: 0.5669  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:15:43 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:52 (0.5672 s / it)
[32m[04/15 10:15:43 d2.engine.hooks]: [0mTotal training time: 0:19:28 (0:00:35 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 10:15:50 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 10:15:50 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 10:15:50 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 10:15:55 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1203 s / img. ETA=0:02:57
[32m[04/15 10:16:00 d2.evaluation.evaluator]: [0mInference done 44/1257. 0.1208 s / img. ETA=0:03:05
[32m[04/15 10:16:05 d2.evaluation.evaluator]: [0mInference done 75/1257. 0.1209 s / img. ETA=0:03:05
[32m[04/15 10:16:10 d2.evaluation.evaluator]: [0mInference done 112/1257. 0.1228 s / img. ETA=0:02:52
[32m[04/15 10:16:15 d2.evaluation.evaluator]: [0mInference done 142/1257. 0.1223 s / img. ETA=0:02:52
[32m[04/15 10:16:20 d2.evaluation.evaluator]: [0mInference done 171/1257. 0.1254 s / img. ETA=0:02:51
[32m[04/15 10:16:25 d2.evaluation.evaluator]: [0mInference done 203/1257. 0.1294 s / img. ETA=0:02:46
[32m[04/15 10:16:31 d2.evaluation.evaluator]: [0mInference done 241/1257. 0.1289 s / img. ETA=0:02:36
[32m[04/15 10:16:36 d2.evaluation.evaluator]: [0mInference done 278/1257. 0.1278 s / img. ETA=0:02:28
[32m[04/15 10:16:41 d2.evaluation.evaluator]: [0mInference done 316/1257. 0.1271 s / img. ETA=0:02:21
[32m[04/15 10:16:46 d2.evaluation.evaluator]: [0mInference done 357/1257. 0.1262 s / img. ETA=0:02:12
[32m[04/15 10:16:51 d2.evaluation.evaluator]: [0mInference done 397/1257. 0.1258 s / img. ETA=0:02:04
[32m[04/15 10:16:56 d2.evaluation.evaluator]: [0mInference done 432/1257. 0.1254 s / img. ETA=0:01:59
[32m[04/15 10:17:02 d2.evaluation.evaluator]: [0mInference done 456/1257. 0.1265 s / img. ETA=0:01:59
[32m[04/15 10:17:07 d2.evaluation.evaluator]: [0mInference done 483/1257. 0.1261 s / img. ETA=0:01:57
[32m[04/15 10:17:12 d2.evaluation.evaluator]: [0mInference done 503/1257. 0.1258 s / img. ETA=0:01:57
[32m[04/15 10:17:17 d2.evaluation.evaluator]: [0mInference done 524/1257. 0.1281 s / img. ETA=0:01:56
[32m[04/15 10:17:22 d2.evaluation.evaluator]: [0mInference done 550/1257. 0.1277 s / img. ETA=0:01:53
[32m[04/15 10:17:27 d2.evaluation.evaluator]: [0mInference done 579/1257. 0.1273 s / img. ETA=0:01:49
[32m[04/15 10:17:32 d2.evaluation.evaluator]: [0mInference done 600/1257. 0.1271 s / img. ETA=0:01:48
[32m[04/15 10:17:37 d2.evaluation.evaluator]: [0mInference done 627/1257. 0.1282 s / img. ETA=0:01:44
[32m[04/15 10:17:42 d2.evaluation.evaluator]: [0mInference done 652/1257. 0.1283 s / img. ETA=0:01:41
[32m[04/15 10:17:47 d2.evaluation.evaluator]: [0mInference done 669/1257. 0.1281 s / img. ETA=0:01:40
[32m[04/15 10:17:52 d2.evaluation.evaluator]: [0mInference done 700/1257. 0.1279 s / img. ETA=0:01:34
[32m[04/15 10:17:58 d2.evaluation.evaluator]: [0mInference done 734/1257. 0.1276 s / img. ETA=0:01:28
[32m[04/15 10:18:03 d2.evaluation.evaluator]: [0mInference done 770/1257. 0.1274 s / img. ETA=0:01:21
[32m[04/15 10:18:08 d2.evaluation.evaluator]: [0mInference done 810/1257. 0.1271 s / img. ETA=0:01:14
[32m[04/15 10:18:13 d2.evaluation.evaluator]: [0mInference done 849/1257. 0.1270 s / img. ETA=0:01:06
[32m[04/15 10:18:18 d2.evaluation.evaluator]: [0mInference done 886/1257. 0.1272 s / img. ETA=0:01:00
[32m[04/15 10:18:23 d2.evaluation.evaluator]: [0mInference done 921/1257. 0.1278 s / img. ETA=0:00:54
[32m[04/15 10:18:28 d2.evaluation.evaluator]: [0mInference done 955/1257. 0.1284 s / img. ETA=0:00:48
[32m[04/15 10:18:33 d2.evaluation.evaluator]: [0mInference done 994/1257. 0.1282 s / img. ETA=0:00:42
[32m[04/15 10:18:38 d2.evaluation.evaluator]: [0mInference done 1033/1257. 0.1280 s / img. ETA=0:00:35
[32m[04/15 10:18:43 d2.evaluation.evaluator]: [0mInference done 1072/1257. 0.1279 s / img. ETA=0:00:29
[32m[04/15 10:18:48 d2.evaluation.evaluator]: [0mInference done 1111/1257. 0.1277 s / img. ETA=0:00:22
[32m[04/15 10:18:53 d2.evaluation.evaluator]: [0mInference done 1149/1257. 0.1276 s / img. ETA=0:00:16
[32m[04/15 10:18:58 d2.evaluation.evaluator]: [0mInference done 1185/1257. 0.1278 s / img. ETA=0:00:11
[32m[04/15 10:19:03 d2.evaluation.evaluator]: [0mInference done 1219/1257. 0.1283 s / img. ETA=0:00:05
[32m[04/15 10:19:08 d2.evaluation.evaluator]: [0mInference done 1252/1257. 0.1287 s / img. ETA=0:00:00
[32m[04/15 10:19:09 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:14.806781 (0.155596 s / img per device, on 1 devices)
[32m[04/15 10:19:09 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:41 (0.128706 s / img per device, on 1 devices)
[32m[04/15 10:19:09 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 10:19:09 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 10:19:09 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.39s).
Accumulating evaluation results...
DONE (t=0.92s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.194
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.391
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.166
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.124
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.436
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.240
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.242
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.179
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.279
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.471
[32m[04/15 10:19:17 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.435 | 39.064 | 16.603 | 12.400 | 23.650 | 43.637 |
[32m[04/15 10:19:17 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 27.910 | bicycle       | 8.306 | car            | 41.524 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  18  *  2000  iterations ============
4 channel input
[32m[04/15 10:19:18 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 10:19:20 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.61 seconds.
[5m[31mWARNING[0m [32m[04/15 10:19:20 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 10:19:20 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 10:19:21 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 10:19:21 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 10:19:21 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 10:19:24 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 10:19:35 d2.utils.events]: [0m eta: 0:17:59  iter: 19  total_loss: 0.644  loss_cls: 0.204  loss_box_reg: 0.349  loss_rpn_cls: 0.015  loss_rpn_loc: 0.067  time: 0.5416  data_time: 0.0503  lr: 0.000100  max_mem: 3067M
[32m[04/15 10:19:52 d2.utils.events]: [0m eta: 0:17:55  iter: 39  total_loss: 0.618  loss_cls: 0.196  loss_box_reg: 0.345  loss_rpn_cls: 0.019  loss_rpn_loc: 0.044  time: 0.6944  data_time: 0.3252  lr: 0.000200  max_mem: 3067M
[32m[04/15 10:20:03 d2.utils.events]: [0m eta: 0:17:46  iter: 59  total_loss: 0.630  loss_cls: 0.194  loss_box_reg: 0.358  loss_rpn_cls: 0.021  loss_rpn_loc: 0.063  time: 0.6441  data_time: 0.0191  lr: 0.000300  max_mem: 3067M
[32m[04/15 10:20:16 d2.utils.events]: [0m eta: 0:17:54  iter: 79  total_loss: 0.556  loss_cls: 0.205  loss_box_reg: 0.303  loss_rpn_cls: 0.013  loss_rpn_loc: 0.040  time: 0.6349  data_time: 0.0088  lr: 0.000400  max_mem: 3067M
[32m[04/15 10:20:27 d2.utils.events]: [0m eta: 0:17:42  iter: 99  total_loss: 0.589  loss_cls: 0.205  loss_box_reg: 0.319  loss_rpn_cls: 0.016  loss_rpn_loc: 0.043  time: 0.6209  data_time: 0.0096  lr: 0.000500  max_mem: 3067M
[32m[04/15 10:20:40 d2.utils.events]: [0m eta: 0:17:45  iter: 119  total_loss: 0.647  loss_cls: 0.207  loss_box_reg: 0.382  loss_rpn_cls: 0.012  loss_rpn_loc: 0.046  time: 0.6240  data_time: 0.0082  lr: 0.000599  max_mem: 3067M
[32m[04/15 10:20:50 d2.utils.events]: [0m eta: 0:17:21  iter: 139  total_loss: 0.580  loss_cls: 0.179  loss_box_reg: 0.314  loss_rpn_cls: 0.013  loss_rpn_loc: 0.048  time: 0.6098  data_time: 0.0080  lr: 0.000699  max_mem: 3067M
[32m[04/15 10:21:02 d2.utils.events]: [0m eta: 0:17:09  iter: 159  total_loss: 0.548  loss_cls: 0.176  loss_box_reg: 0.329  loss_rpn_cls: 0.009  loss_rpn_loc: 0.036  time: 0.6058  data_time: 0.0731  lr: 0.000799  max_mem: 3067M
[32m[04/15 10:21:14 d2.utils.events]: [0m eta: 0:17:00  iter: 179  total_loss: 0.579  loss_cls: 0.192  loss_box_reg: 0.307  loss_rpn_cls: 0.010  loss_rpn_loc: 0.055  time: 0.6064  data_time: 0.0081  lr: 0.000899  max_mem: 3067M
[32m[04/15 10:21:26 d2.utils.events]: [0m eta: 0:16:47  iter: 199  total_loss: 0.584  loss_cls: 0.190  loss_box_reg: 0.326  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.6027  data_time: 0.0072  lr: 0.000999  max_mem: 3067M
[32m[04/15 10:21:37 d2.utils.events]: [0m eta: 0:16:33  iter: 219  total_loss: 0.576  loss_cls: 0.185  loss_box_reg: 0.343  loss_rpn_cls: 0.020  loss_rpn_loc: 0.042  time: 0.5965  data_time: 0.0065  lr: 0.001099  max_mem: 3067M
[32m[04/15 10:21:48 d2.utils.events]: [0m eta: 0:16:21  iter: 239  total_loss: 0.625  loss_cls: 0.188  loss_box_reg: 0.342  loss_rpn_cls: 0.014  loss_rpn_loc: 0.058  time: 0.5937  data_time: 0.0075  lr: 0.001199  max_mem: 3067M
[32m[04/15 10:22:00 d2.utils.events]: [0m eta: 0:16:09  iter: 259  total_loss: 0.563  loss_cls: 0.180  loss_box_reg: 0.287  loss_rpn_cls: 0.019  loss_rpn_loc: 0.066  time: 0.5924  data_time: 0.0070  lr: 0.001299  max_mem: 3067M
[32m[04/15 10:22:10 d2.utils.events]: [0m eta: 0:15:49  iter: 279  total_loss: 0.631  loss_cls: 0.168  loss_box_reg: 0.340  loss_rpn_cls: 0.012  loss_rpn_loc: 0.062  time: 0.5883  data_time: 0.0075  lr: 0.001399  max_mem: 3067M
[32m[04/15 10:22:21 d2.utils.events]: [0m eta: 0:15:38  iter: 299  total_loss: 0.574  loss_cls: 0.167  loss_box_reg: 0.340  loss_rpn_cls: 0.016  loss_rpn_loc: 0.041  time: 0.5856  data_time: 0.0086  lr: 0.001499  max_mem: 3067M
[32m[04/15 10:22:34 d2.utils.events]: [0m eta: 0:15:35  iter: 319  total_loss: 0.563  loss_cls: 0.175  loss_box_reg: 0.319  loss_rpn_cls: 0.011  loss_rpn_loc: 0.045  time: 0.5886  data_time: 0.0076  lr: 0.001598  max_mem: 3067M
[32m[04/15 10:22:45 d2.utils.events]: [0m eta: 0:15:14  iter: 339  total_loss: 0.657  loss_cls: 0.213  loss_box_reg: 0.380  loss_rpn_cls: 0.012  loss_rpn_loc: 0.045  time: 0.5846  data_time: 0.0073  lr: 0.001698  max_mem: 3067M
[32m[04/15 10:22:56 d2.utils.events]: [0m eta: 0:15:04  iter: 359  total_loss: 0.634  loss_cls: 0.190  loss_box_reg: 0.344  loss_rpn_cls: 0.018  loss_rpn_loc: 0.054  time: 0.5825  data_time: 0.0068  lr: 0.001798  max_mem: 3067M
[32m[04/15 10:23:08 d2.utils.events]: [0m eta: 0:14:53  iter: 379  total_loss: 0.687  loss_cls: 0.226  loss_box_reg: 0.379  loss_rpn_cls: 0.014  loss_rpn_loc: 0.067  time: 0.5832  data_time: 0.0080  lr: 0.001898  max_mem: 3067M
[32m[04/15 10:23:19 d2.utils.events]: [0m eta: 0:14:41  iter: 399  total_loss: 0.629  loss_cls: 0.195  loss_box_reg: 0.358  loss_rpn_cls: 0.019  loss_rpn_loc: 0.043  time: 0.5826  data_time: 0.0063  lr: 0.001998  max_mem: 3067M
[32m[04/15 10:23:30 d2.utils.events]: [0m eta: 0:14:31  iter: 419  total_loss: 0.682  loss_cls: 0.210  loss_box_reg: 0.391  loss_rpn_cls: 0.010  loss_rpn_loc: 0.062  time: 0.5811  data_time: 0.0078  lr: 0.002098  max_mem: 3067M
[32m[04/15 10:23:43 d2.utils.events]: [0m eta: 0:14:24  iter: 439  total_loss: 0.671  loss_cls: 0.208  loss_box_reg: 0.365  loss_rpn_cls: 0.014  loss_rpn_loc: 0.055  time: 0.5822  data_time: 0.0065  lr: 0.002198  max_mem: 3067M
[32m[04/15 10:23:55 d2.utils.events]: [0m eta: 0:14:18  iter: 459  total_loss: 0.606  loss_cls: 0.190  loss_box_reg: 0.357  loss_rpn_cls: 0.007  loss_rpn_loc: 0.042  time: 0.5834  data_time: 0.0069  lr: 0.002298  max_mem: 3067M
[32m[04/15 10:24:06 d2.utils.events]: [0m eta: 0:14:05  iter: 479  total_loss: 0.408  loss_cls: 0.141  loss_box_reg: 0.233  loss_rpn_cls: 0.006  loss_rpn_loc: 0.027  time: 0.5815  data_time: 0.0070  lr: 0.002398  max_mem: 3067M
[32m[04/15 10:24:18 d2.utils.events]: [0m eta: 0:13:56  iter: 499  total_loss: 0.563  loss_cls: 0.183  loss_box_reg: 0.307  loss_rpn_cls: 0.009  loss_rpn_loc: 0.038  time: 0.5821  data_time: 0.0061  lr: 0.002498  max_mem: 3067M
[32m[04/15 10:24:30 d2.utils.events]: [0m eta: 0:13:47  iter: 519  total_loss: 0.584  loss_cls: 0.190  loss_box_reg: 0.326  loss_rpn_cls: 0.017  loss_rpn_loc: 0.040  time: 0.5831  data_time: 0.0069  lr: 0.002597  max_mem: 3067M
[32m[04/15 10:24:40 d2.utils.events]: [0m eta: 0:13:33  iter: 539  total_loss: 0.637  loss_cls: 0.214  loss_box_reg: 0.346  loss_rpn_cls: 0.011  loss_rpn_loc: 0.061  time: 0.5808  data_time: 0.0058  lr: 0.002697  max_mem: 3067M
[32m[04/15 10:24:52 d2.utils.events]: [0m eta: 0:13:21  iter: 559  total_loss: 0.619  loss_cls: 0.204  loss_box_reg: 0.357  loss_rpn_cls: 0.013  loss_rpn_loc: 0.045  time: 0.5805  data_time: 0.0077  lr: 0.002797  max_mem: 3067M
[32m[04/15 10:25:04 d2.utils.events]: [0m eta: 0:13:11  iter: 579  total_loss: 0.483  loss_cls: 0.160  loss_box_reg: 0.291  loss_rpn_cls: 0.009  loss_rpn_loc: 0.034  time: 0.5811  data_time: 0.0090  lr: 0.002897  max_mem: 3067M
[32m[04/15 10:25:15 d2.utils.events]: [0m eta: 0:12:58  iter: 599  total_loss: 0.581  loss_cls: 0.199  loss_box_reg: 0.343  loss_rpn_cls: 0.010  loss_rpn_loc: 0.046  time: 0.5795  data_time: 0.0078  lr: 0.002997  max_mem: 3067M
[32m[04/15 10:25:26 d2.utils.events]: [0m eta: 0:12:44  iter: 619  total_loss: 0.584  loss_cls: 0.194  loss_box_reg: 0.339  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5784  data_time: 0.0071  lr: 0.003097  max_mem: 3067M
[32m[04/15 10:25:38 d2.utils.events]: [0m eta: 0:12:36  iter: 639  total_loss: 0.596  loss_cls: 0.192  loss_box_reg: 0.316  loss_rpn_cls: 0.018  loss_rpn_loc: 0.053  time: 0.5799  data_time: 0.0088  lr: 0.003197  max_mem: 3067M
[32m[04/15 10:25:49 d2.utils.events]: [0m eta: 0:12:23  iter: 659  total_loss: 0.581  loss_cls: 0.200  loss_box_reg: 0.335  loss_rpn_cls: 0.016  loss_rpn_loc: 0.044  time: 0.5788  data_time: 0.0075  lr: 0.003297  max_mem: 3067M
[32m[04/15 10:26:00 d2.utils.events]: [0m eta: 0:12:12  iter: 679  total_loss: 0.655  loss_cls: 0.223  loss_box_reg: 0.352  loss_rpn_cls: 0.013  loss_rpn_loc: 0.065  time: 0.5777  data_time: 0.0076  lr: 0.003397  max_mem: 3067M
[32m[04/15 10:26:13 d2.utils.events]: [0m eta: 0:12:02  iter: 699  total_loss: 0.594  loss_cls: 0.177  loss_box_reg: 0.308  loss_rpn_cls: 0.012  loss_rpn_loc: 0.047  time: 0.5791  data_time: 0.0078  lr: 0.003497  max_mem: 3067M
[32m[04/15 10:26:24 d2.utils.events]: [0m eta: 0:11:51  iter: 719  total_loss: 0.746  loss_cls: 0.221  loss_box_reg: 0.398  loss_rpn_cls: 0.015  loss_rpn_loc: 0.067  time: 0.5785  data_time: 0.0069  lr: 0.003596  max_mem: 3067M
[32m[04/15 10:26:35 d2.utils.events]: [0m eta: 0:11:39  iter: 739  total_loss: 0.647  loss_cls: 0.213  loss_box_reg: 0.376  loss_rpn_cls: 0.013  loss_rpn_loc: 0.052  time: 0.5775  data_time: 0.0068  lr: 0.003696  max_mem: 3067M
[32m[04/15 10:26:47 d2.utils.events]: [0m eta: 0:11:29  iter: 759  total_loss: 0.702  loss_cls: 0.232  loss_box_reg: 0.372  loss_rpn_cls: 0.020  loss_rpn_loc: 0.059  time: 0.5778  data_time: 0.0066  lr: 0.003796  max_mem: 3067M
[32m[04/15 10:26:58 d2.utils.events]: [0m eta: 0:11:18  iter: 779  total_loss: 0.517  loss_cls: 0.158  loss_box_reg: 0.316  loss_rpn_cls: 0.008  loss_rpn_loc: 0.046  time: 0.5775  data_time: 0.0069  lr: 0.003896  max_mem: 3067M
[32m[04/15 10:27:09 d2.utils.events]: [0m eta: 0:11:05  iter: 799  total_loss: 0.577  loss_cls: 0.161  loss_box_reg: 0.321  loss_rpn_cls: 0.008  loss_rpn_loc: 0.045  time: 0.5760  data_time: 0.0098  lr: 0.003996  max_mem: 3067M
[32m[04/15 10:27:20 d2.utils.events]: [0m eta: 0:10:52  iter: 819  total_loss: 0.597  loss_cls: 0.182  loss_box_reg: 0.307  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5749  data_time: 0.0083  lr: 0.004096  max_mem: 3067M
[32m[04/15 10:27:32 d2.utils.events]: [0m eta: 0:10:42  iter: 839  total_loss: 0.584  loss_cls: 0.194  loss_box_reg: 0.324  loss_rpn_cls: 0.015  loss_rpn_loc: 0.040  time: 0.5755  data_time: 0.0076  lr: 0.004196  max_mem: 3067M
[32m[04/15 10:27:42 d2.utils.events]: [0m eta: 0:10:29  iter: 859  total_loss: 0.628  loss_cls: 0.181  loss_box_reg: 0.342  loss_rpn_cls: 0.012  loss_rpn_loc: 0.054  time: 0.5743  data_time: 0.0077  lr: 0.004296  max_mem: 3067M
[32m[04/15 10:27:53 d2.utils.events]: [0m eta: 0:10:17  iter: 879  total_loss: 0.617  loss_cls: 0.210  loss_box_reg: 0.346  loss_rpn_cls: 0.015  loss_rpn_loc: 0.046  time: 0.5739  data_time: 0.0066  lr: 0.004396  max_mem: 3067M
[32m[04/15 10:28:06 d2.utils.events]: [0m eta: 0:10:08  iter: 899  total_loss: 0.571  loss_cls: 0.173  loss_box_reg: 0.308  loss_rpn_cls: 0.012  loss_rpn_loc: 0.045  time: 0.5750  data_time: 0.0082  lr: 0.004496  max_mem: 3067M
[32m[04/15 10:28:17 d2.utils.events]: [0m eta: 0:09:57  iter: 919  total_loss: 0.486  loss_cls: 0.155  loss_box_reg: 0.280  loss_rpn_cls: 0.007  loss_rpn_loc: 0.037  time: 0.5743  data_time: 0.0078  lr: 0.004595  max_mem: 3067M
[32m[04/15 10:28:28 d2.utils.events]: [0m eta: 0:09:45  iter: 939  total_loss: 0.584  loss_cls: 0.176  loss_box_reg: 0.318  loss_rpn_cls: 0.022  loss_rpn_loc: 0.063  time: 0.5736  data_time: 0.0089  lr: 0.004695  max_mem: 3067M
[32m[04/15 10:28:41 d2.utils.events]: [0m eta: 0:09:36  iter: 959  total_loss: 0.652  loss_cls: 0.186  loss_box_reg: 0.370  loss_rpn_cls: 0.014  loss_rpn_loc: 0.049  time: 0.5745  data_time: 0.0085  lr: 0.004795  max_mem: 3067M
[32m[04/15 10:28:51 d2.utils.events]: [0m eta: 0:09:23  iter: 979  total_loss: 0.673  loss_cls: 0.187  loss_box_reg: 0.390  loss_rpn_cls: 0.018  loss_rpn_loc: 0.059  time: 0.5736  data_time: 0.0076  lr: 0.004895  max_mem: 3067M
[32m[04/15 10:29:02 d2.utils.events]: [0m eta: 0:09:11  iter: 999  total_loss: 0.679  loss_cls: 0.207  loss_box_reg: 0.377  loss_rpn_cls: 0.013  loss_rpn_loc: 0.066  time: 0.5727  data_time: 0.0077  lr: 0.004995  max_mem: 3067M
[32m[04/15 10:29:13 d2.utils.events]: [0m eta: 0:09:00  iter: 1019  total_loss: 0.605  loss_cls: 0.187  loss_box_reg: 0.316  loss_rpn_cls: 0.015  loss_rpn_loc: 0.061  time: 0.5728  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:29:26 d2.utils.events]: [0m eta: 0:08:50  iter: 1039  total_loss: 0.494  loss_cls: 0.148  loss_box_reg: 0.282  loss_rpn_cls: 0.009  loss_rpn_loc: 0.040  time: 0.5732  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:29:36 d2.utils.events]: [0m eta: 0:08:39  iter: 1059  total_loss: 0.728  loss_cls: 0.253  loss_box_reg: 0.390  loss_rpn_cls: 0.016  loss_rpn_loc: 0.068  time: 0.5726  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:29:48 d2.utils.events]: [0m eta: 0:08:27  iter: 1079  total_loss: 0.686  loss_cls: 0.223  loss_box_reg: 0.361  loss_rpn_cls: 0.016  loss_rpn_loc: 0.055  time: 0.5728  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:30:00 d2.utils.events]: [0m eta: 0:08:16  iter: 1099  total_loss: 0.614  loss_cls: 0.203  loss_box_reg: 0.295  loss_rpn_cls: 0.025  loss_rpn_loc: 0.054  time: 0.5730  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:30:11 d2.utils.events]: [0m eta: 0:08:04  iter: 1119  total_loss: 0.661  loss_cls: 0.212  loss_box_reg: 0.354  loss_rpn_cls: 0.016  loss_rpn_loc: 0.066  time: 0.5724  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:30:22 d2.utils.events]: [0m eta: 0:07:53  iter: 1139  total_loss: 0.479  loss_cls: 0.162  loss_box_reg: 0.274  loss_rpn_cls: 0.011  loss_rpn_loc: 0.036  time: 0.5719  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:30:35 d2.utils.events]: [0m eta: 0:07:43  iter: 1159  total_loss: 0.551  loss_cls: 0.188  loss_box_reg: 0.312  loss_rpn_cls: 0.020  loss_rpn_loc: 0.045  time: 0.5730  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:30:46 d2.utils.events]: [0m eta: 0:07:32  iter: 1179  total_loss: 0.688  loss_cls: 0.214  loss_box_reg: 0.368  loss_rpn_cls: 0.013  loss_rpn_loc: 0.070  time: 0.5725  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:30:57 d2.utils.events]: [0m eta: 0:07:21  iter: 1199  total_loss: 0.544  loss_cls: 0.177  loss_box_reg: 0.302  loss_rpn_cls: 0.011  loss_rpn_loc: 0.046  time: 0.5720  data_time: 0.0107  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:31:09 d2.utils.events]: [0m eta: 0:07:10  iter: 1219  total_loss: 0.648  loss_cls: 0.205  loss_box_reg: 0.344  loss_rpn_cls: 0.014  loss_rpn_loc: 0.046  time: 0.5725  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:31:20 d2.utils.events]: [0m eta: 0:06:59  iter: 1239  total_loss: 0.693  loss_cls: 0.212  loss_box_reg: 0.408  loss_rpn_cls: 0.016  loss_rpn_loc: 0.056  time: 0.5725  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:31:31 d2.utils.events]: [0m eta: 0:06:48  iter: 1259  total_loss: 0.729  loss_cls: 0.256  loss_box_reg: 0.402  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5720  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:31:43 d2.utils.events]: [0m eta: 0:06:38  iter: 1279  total_loss: 0.514  loss_cls: 0.170  loss_box_reg: 0.290  loss_rpn_cls: 0.015  loss_rpn_loc: 0.040  time: 0.5720  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:31:54 d2.utils.events]: [0m eta: 0:06:27  iter: 1299  total_loss: 0.643  loss_cls: 0.219  loss_box_reg: 0.381  loss_rpn_cls: 0.017  loss_rpn_loc: 0.050  time: 0.5719  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:32:05 d2.utils.events]: [0m eta: 0:06:15  iter: 1319  total_loss: 0.644  loss_cls: 0.202  loss_box_reg: 0.340  loss_rpn_cls: 0.011  loss_rpn_loc: 0.067  time: 0.5714  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:32:16 d2.utils.events]: [0m eta: 0:06:05  iter: 1339  total_loss: 0.658  loss_cls: 0.217  loss_box_reg: 0.374  loss_rpn_cls: 0.014  loss_rpn_loc: 0.045  time: 0.5714  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:32:28 d2.utils.events]: [0m eta: 0:05:54  iter: 1359  total_loss: 0.604  loss_cls: 0.197  loss_box_reg: 0.357  loss_rpn_cls: 0.015  loss_rpn_loc: 0.043  time: 0.5718  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:32:40 d2.utils.events]: [0m eta: 0:05:43  iter: 1379  total_loss: 0.749  loss_cls: 0.229  loss_box_reg: 0.415  loss_rpn_cls: 0.015  loss_rpn_loc: 0.059  time: 0.5715  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:32:50 d2.utils.events]: [0m eta: 0:05:32  iter: 1399  total_loss: 0.679  loss_cls: 0.249  loss_box_reg: 0.350  loss_rpn_cls: 0.013  loss_rpn_loc: 0.056  time: 0.5710  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:33:02 d2.utils.events]: [0m eta: 0:05:21  iter: 1419  total_loss: 0.514  loss_cls: 0.181  loss_box_reg: 0.282  loss_rpn_cls: 0.018  loss_rpn_loc: 0.045  time: 0.5714  data_time: 0.0112  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:33:14 d2.utils.events]: [0m eta: 0:05:10  iter: 1439  total_loss: 0.624  loss_cls: 0.200  loss_box_reg: 0.349  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5711  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:33:25 d2.utils.events]: [0m eta: 0:04:58  iter: 1459  total_loss: 0.714  loss_cls: 0.225  loss_box_reg: 0.392  loss_rpn_cls: 0.016  loss_rpn_loc: 0.059  time: 0.5707  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:33:36 d2.utils.events]: [0m eta: 0:04:47  iter: 1479  total_loss: 0.586  loss_cls: 0.179  loss_box_reg: 0.330  loss_rpn_cls: 0.021  loss_rpn_loc: 0.045  time: 0.5710  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:33:48 d2.utils.events]: [0m eta: 0:04:36  iter: 1499  total_loss: 0.678  loss_cls: 0.212  loss_box_reg: 0.370  loss_rpn_cls: 0.019  loss_rpn_loc: 0.063  time: 0.5709  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:33:58 d2.utils.events]: [0m eta: 0:04:24  iter: 1519  total_loss: 0.629  loss_cls: 0.206  loss_box_reg: 0.361  loss_rpn_cls: 0.016  loss_rpn_loc: 0.051  time: 0.5702  data_time: 0.0040  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:34:09 d2.utils.events]: [0m eta: 0:04:13  iter: 1539  total_loss: 0.590  loss_cls: 0.175  loss_box_reg: 0.346  loss_rpn_cls: 0.014  loss_rpn_loc: 0.063  time: 0.5695  data_time: 0.0045  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:34:19 d2.utils.events]: [0m eta: 0:04:02  iter: 1559  total_loss: 0.662  loss_cls: 0.206  loss_box_reg: 0.355  loss_rpn_cls: 0.022  loss_rpn_loc: 0.059  time: 0.5687  data_time: 0.0040  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:34:29 d2.utils.events]: [0m eta: 0:03:50  iter: 1579  total_loss: 0.631  loss_cls: 0.200  loss_box_reg: 0.362  loss_rpn_cls: 0.019  loss_rpn_loc: 0.061  time: 0.5679  data_time: 0.0042  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:34:39 d2.utils.events]: [0m eta: 0:03:39  iter: 1599  total_loss: 0.631  loss_cls: 0.189  loss_box_reg: 0.364  loss_rpn_cls: 0.019  loss_rpn_loc: 0.060  time: 0.5671  data_time: 0.0040  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:34:49 d2.utils.events]: [0m eta: 0:03:28  iter: 1619  total_loss: 0.683  loss_cls: 0.231  loss_box_reg: 0.383  loss_rpn_cls: 0.018  loss_rpn_loc: 0.046  time: 0.5664  data_time: 0.0039  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:35:00 d2.utils.events]: [0m eta: 0:03:17  iter: 1639  total_loss: 0.659  loss_cls: 0.202  loss_box_reg: 0.384  loss_rpn_cls: 0.016  loss_rpn_loc: 0.053  time: 0.5657  data_time: 0.0041  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:35:10 d2.utils.events]: [0m eta: 0:03:05  iter: 1659  total_loss: 0.622  loss_cls: 0.193  loss_box_reg: 0.338  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5648  data_time: 0.0039  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:35:20 d2.utils.events]: [0m eta: 0:02:54  iter: 1679  total_loss: 0.752  loss_cls: 0.252  loss_box_reg: 0.399  loss_rpn_cls: 0.016  loss_rpn_loc: 0.070  time: 0.5643  data_time: 0.0040  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:35:30 d2.utils.events]: [0m eta: 0:02:43  iter: 1699  total_loss: 0.665  loss_cls: 0.209  loss_box_reg: 0.361  loss_rpn_cls: 0.023  loss_rpn_loc: 0.057  time: 0.5638  data_time: 0.0040  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:35:41 d2.utils.events]: [0m eta: 0:02:32  iter: 1719  total_loss: 0.629  loss_cls: 0.211  loss_box_reg: 0.323  loss_rpn_cls: 0.013  loss_rpn_loc: 0.060  time: 0.5632  data_time: 0.0038  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:35:51 d2.utils.events]: [0m eta: 0:02:21  iter: 1739  total_loss: 0.560  loss_cls: 0.187  loss_box_reg: 0.327  loss_rpn_cls: 0.009  loss_rpn_loc: 0.050  time: 0.5626  data_time: 0.0040  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:36:01 d2.utils.events]: [0m eta: 0:02:10  iter: 1759  total_loss: 0.601  loss_cls: 0.196  loss_box_reg: 0.332  loss_rpn_cls: 0.012  loss_rpn_loc: 0.053  time: 0.5619  data_time: 0.0040  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:36:11 d2.utils.events]: [0m eta: 0:01:58  iter: 1779  total_loss: 0.697  loss_cls: 0.220  loss_box_reg: 0.366  loss_rpn_cls: 0.025  loss_rpn_loc: 0.066  time: 0.5612  data_time: 0.0040  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:36:21 d2.utils.events]: [0m eta: 0:01:48  iter: 1799  total_loss: 0.611  loss_cls: 0.186  loss_box_reg: 0.320  loss_rpn_cls: 0.012  loss_rpn_loc: 0.056  time: 0.5605  data_time: 0.0040  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:36:32 d2.utils.events]: [0m eta: 0:01:37  iter: 1819  total_loss: 0.611  loss_cls: 0.180  loss_box_reg: 0.355  loss_rpn_cls: 0.013  loss_rpn_loc: 0.063  time: 0.5601  data_time: 0.0040  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:36:42 d2.utils.events]: [0m eta: 0:01:26  iter: 1839  total_loss: 0.510  loss_cls: 0.158  loss_box_reg: 0.291  loss_rpn_cls: 0.014  loss_rpn_loc: 0.046  time: 0.5595  data_time: 0.0040  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:36:53 d2.utils.events]: [0m eta: 0:01:15  iter: 1859  total_loss: 0.585  loss_cls: 0.196  loss_box_reg: 0.328  loss_rpn_cls: 0.012  loss_rpn_loc: 0.045  time: 0.5595  data_time: 0.0042  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:37:03 d2.utils.events]: [0m eta: 0:01:04  iter: 1879  total_loss: 0.756  loss_cls: 0.248  loss_box_reg: 0.435  loss_rpn_cls: 0.018  loss_rpn_loc: 0.057  time: 0.5589  data_time: 0.0040  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:37:13 d2.utils.events]: [0m eta: 0:00:54  iter: 1899  total_loss: 0.625  loss_cls: 0.200  loss_box_reg: 0.333  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5584  data_time: 0.0040  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:37:24 d2.utils.events]: [0m eta: 0:00:43  iter: 1919  total_loss: 0.659  loss_cls: 0.214  loss_box_reg: 0.326  loss_rpn_cls: 0.011  loss_rpn_loc: 0.065  time: 0.5579  data_time: 0.0040  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:37:34 d2.utils.events]: [0m eta: 0:00:32  iter: 1939  total_loss: 0.646  loss_cls: 0.194  loss_box_reg: 0.380  loss_rpn_cls: 0.017  loss_rpn_loc: 0.056  time: 0.5573  data_time: 0.0041  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:37:44 d2.utils.events]: [0m eta: 0:00:21  iter: 1959  total_loss: 0.607  loss_cls: 0.165  loss_box_reg: 0.358  loss_rpn_cls: 0.013  loss_rpn_loc: 0.041  time: 0.5569  data_time: 0.0041  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:37:55 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.579  loss_cls: 0.184  loss_box_reg: 0.325  loss_rpn_cls: 0.013  loss_rpn_loc: 0.051  time: 0.5565  data_time: 0.0041  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 10:38:09 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 10:38:09 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 10:38:09 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 10:38:09 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.673  loss_cls: 0.189  loss_box_reg: 0.383  loss_rpn_cls: 0.017  loss_rpn_loc: 0.056  time: 0.5560  data_time: 0.0044  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:38:09 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:30 (0.5563 s / it)
[32m[04/15 10:38:09 d2.engine.hooks]: [0mTotal training time: 0:18:43 (0:00:12 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 10:38:12 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 10:38:12 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 10:38:13 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 10:38:14 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1160 s / img. ETA=0:02:26
[32m[04/15 10:38:19 d2.evaluation.evaluator]: [0mInference done 54/1257. 0.1152 s / img. ETA=0:02:20
[32m[04/15 10:38:24 d2.evaluation.evaluator]: [0mInference done 97/1257. 0.1154 s / img. ETA=0:02:16
[32m[04/15 10:38:29 d2.evaluation.evaluator]: [0mInference done 140/1257. 0.1153 s / img. ETA=0:02:11
[32m[04/15 10:38:35 d2.evaluation.evaluator]: [0mInference done 183/1257. 0.1153 s / img. ETA=0:02:05
[32m[04/15 10:38:40 d2.evaluation.evaluator]: [0mInference done 226/1257. 0.1153 s / img. ETA=0:02:00
[32m[04/15 10:38:45 d2.evaluation.evaluator]: [0mInference done 269/1257. 0.1154 s / img. ETA=0:01:56
[32m[04/15 10:38:50 d2.evaluation.evaluator]: [0mInference done 312/1257. 0.1154 s / img. ETA=0:01:50
[32m[04/15 10:38:55 d2.evaluation.evaluator]: [0mInference done 355/1257. 0.1153 s / img. ETA=0:01:45
[32m[04/15 10:39:00 d2.evaluation.evaluator]: [0mInference done 398/1257. 0.1152 s / img. ETA=0:01:40
[32m[04/15 10:39:05 d2.evaluation.evaluator]: [0mInference done 441/1257. 0.1152 s / img. ETA=0:01:35
[32m[04/15 10:39:10 d2.evaluation.evaluator]: [0mInference done 484/1257. 0.1151 s / img. ETA=0:01:30
[32m[04/15 10:39:15 d2.evaluation.evaluator]: [0mInference done 527/1257. 0.1152 s / img. ETA=0:01:25
[32m[04/15 10:39:20 d2.evaluation.evaluator]: [0mInference done 570/1257. 0.1152 s / img. ETA=0:01:20
[32m[04/15 10:39:25 d2.evaluation.evaluator]: [0mInference done 613/1257. 0.1152 s / img. ETA=0:01:15
[32m[04/15 10:39:30 d2.evaluation.evaluator]: [0mInference done 656/1257. 0.1151 s / img. ETA=0:01:10
[32m[04/15 10:39:35 d2.evaluation.evaluator]: [0mInference done 699/1257. 0.1152 s / img. ETA=0:01:05
[32m[04/15 10:39:40 d2.evaluation.evaluator]: [0mInference done 733/1257. 0.1152 s / img. ETA=0:01:02
[32m[04/15 10:39:45 d2.evaluation.evaluator]: [0mInference done 773/1257. 0.1152 s / img. ETA=0:00:57
[32m[04/15 10:39:50 d2.evaluation.evaluator]: [0mInference done 815/1257. 0.1152 s / img. ETA=0:00:52
[32m[04/15 10:39:55 d2.evaluation.evaluator]: [0mInference done 857/1257. 0.1153 s / img. ETA=0:00:47
[32m[04/15 10:40:00 d2.evaluation.evaluator]: [0mInference done 897/1257. 0.1154 s / img. ETA=0:00:43
[32m[04/15 10:40:05 d2.evaluation.evaluator]: [0mInference done 939/1257. 0.1154 s / img. ETA=0:00:38
[32m[04/15 10:40:10 d2.evaluation.evaluator]: [0mInference done 981/1257. 0.1156 s / img. ETA=0:00:32
[32m[04/15 10:40:15 d2.evaluation.evaluator]: [0mInference done 1021/1257. 0.1159 s / img. ETA=0:00:28
[32m[04/15 10:40:20 d2.evaluation.evaluator]: [0mInference done 1061/1257. 0.1161 s / img. ETA=0:00:23
[32m[04/15 10:40:26 d2.evaluation.evaluator]: [0mInference done 1101/1257. 0.1164 s / img. ETA=0:00:18
[32m[04/15 10:40:31 d2.evaluation.evaluator]: [0mInference done 1141/1257. 0.1166 s / img. ETA=0:00:13
[32m[04/15 10:40:36 d2.evaluation.evaluator]: [0mInference done 1180/1257. 0.1168 s / img. ETA=0:00:09
[32m[04/15 10:40:41 d2.evaluation.evaluator]: [0mInference done 1220/1257. 0.1171 s / img. ETA=0:00:04
[32m[04/15 10:40:45 d2.evaluation.evaluator]: [0mTotal inference time: 0:02:31.803037 (0.121248 s / img per device, on 1 devices)
[32m[04/15 10:40:45 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:26 (0.117248 s / img per device, on 1 devices)
[32m[04/15 10:40:46 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 10:40:46 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 10:40:46 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=6.06s).
Accumulating evaluation results...
DONE (t=0.92s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.199
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.394
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.176
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.123
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.244
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.410
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.101
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.246
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.177
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.294
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.460
[32m[04/15 10:40:53 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.897 | 39.380 | 17.579 | 12.253 | 24.440 | 40.992 |
[32m[04/15 10:40:53 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 27.689 | bicycle       | 9.893 | car            | 42.004 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  19  *  2000  iterations ============
4 channel input
[32m[04/15 10:40:55 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 10:40:56 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.50 seconds.
[5m[31mWARNING[0m [32m[04/15 10:40:56 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 10:40:56 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 10:40:57 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 10:40:57 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 10:40:57 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 10:41:00 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 10:41:11 d2.utils.events]: [0m eta: 0:17:20  iter: 19  total_loss: 0.651  loss_cls: 0.198  loss_box_reg: 0.347  loss_rpn_cls: 0.013  loss_rpn_loc: 0.061  time: 0.5262  data_time: 0.0677  lr: 0.000100  max_mem: 3067M
[32m[04/15 10:41:22 d2.utils.events]: [0m eta: 0:17:14  iter: 39  total_loss: 0.594  loss_cls: 0.181  loss_box_reg: 0.321  loss_rpn_cls: 0.012  loss_rpn_loc: 0.066  time: 0.5253  data_time: 0.0084  lr: 0.000200  max_mem: 3067M
[32m[04/15 10:41:33 d2.utils.events]: [0m eta: 0:17:14  iter: 59  total_loss: 0.640  loss_cls: 0.215  loss_box_reg: 0.342  loss_rpn_cls: 0.017  loss_rpn_loc: 0.048  time: 0.5370  data_time: 0.0075  lr: 0.000300  max_mem: 3067M
[32m[04/15 10:41:46 d2.utils.events]: [0m eta: 0:17:27  iter: 79  total_loss: 0.712  loss_cls: 0.220  loss_box_reg: 0.403  loss_rpn_cls: 0.012  loss_rpn_loc: 0.068  time: 0.5583  data_time: 0.0108  lr: 0.000400  max_mem: 3067M
[32m[04/15 10:41:57 d2.utils.events]: [0m eta: 0:17:21  iter: 99  total_loss: 0.557  loss_cls: 0.182  loss_box_reg: 0.297  loss_rpn_cls: 0.013  loss_rpn_loc: 0.046  time: 0.5569  data_time: 0.0088  lr: 0.000500  max_mem: 3067M
[32m[04/15 10:42:08 d2.utils.events]: [0m eta: 0:17:10  iter: 119  total_loss: 0.655  loss_cls: 0.209  loss_box_reg: 0.379  loss_rpn_cls: 0.010  loss_rpn_loc: 0.046  time: 0.5553  data_time: 0.0075  lr: 0.000599  max_mem: 3067M
[32m[04/15 10:42:20 d2.utils.events]: [0m eta: 0:17:08  iter: 139  total_loss: 0.634  loss_cls: 0.211  loss_box_reg: 0.343  loss_rpn_cls: 0.013  loss_rpn_loc: 0.052  time: 0.5616  data_time: 0.0077  lr: 0.000699  max_mem: 3067M
[32m[04/15 10:42:32 d2.utils.events]: [0m eta: 0:17:01  iter: 159  total_loss: 0.501  loss_cls: 0.182  loss_box_reg: 0.277  loss_rpn_cls: 0.013  loss_rpn_loc: 0.038  time: 0.5633  data_time: 0.0073  lr: 0.000799  max_mem: 3067M
[32m[04/15 10:42:44 d2.utils.events]: [0m eta: 0:16:46  iter: 179  total_loss: 0.609  loss_cls: 0.188  loss_box_reg: 0.349  loss_rpn_cls: 0.016  loss_rpn_loc: 0.057  time: 0.5600  data_time: 0.0230  lr: 0.000899  max_mem: 3067M
[32m[04/15 10:42:55 d2.utils.events]: [0m eta: 0:16:35  iter: 199  total_loss: 0.539  loss_cls: 0.185  loss_box_reg: 0.298  loss_rpn_cls: 0.020  loss_rpn_loc: 0.042  time: 0.5598  data_time: 0.0081  lr: 0.000999  max_mem: 3067M
[32m[04/15 10:43:07 d2.utils.events]: [0m eta: 0:16:28  iter: 219  total_loss: 0.583  loss_cls: 0.161  loss_box_reg: 0.351  loss_rpn_cls: 0.017  loss_rpn_loc: 0.061  time: 0.5636  data_time: 0.0080  lr: 0.001099  max_mem: 3067M
[32m[04/15 10:43:18 d2.utils.events]: [0m eta: 0:16:10  iter: 239  total_loss: 0.559  loss_cls: 0.185  loss_box_reg: 0.311  loss_rpn_cls: 0.014  loss_rpn_loc: 0.041  time: 0.5615  data_time: 0.0065  lr: 0.001199  max_mem: 3067M
[32m[04/15 10:43:29 d2.utils.events]: [0m eta: 0:16:00  iter: 259  total_loss: 0.539  loss_cls: 0.173  loss_box_reg: 0.311  loss_rpn_cls: 0.018  loss_rpn_loc: 0.039  time: 0.5614  data_time: 0.0075  lr: 0.001299  max_mem: 3067M
[32m[04/15 10:43:42 d2.utils.events]: [0m eta: 0:15:54  iter: 279  total_loss: 0.626  loss_cls: 0.203  loss_box_reg: 0.322  loss_rpn_cls: 0.016  loss_rpn_loc: 0.056  time: 0.5654  data_time: 0.0092  lr: 0.001399  max_mem: 3067M
[32m[04/15 10:43:52 d2.utils.events]: [0m eta: 0:15:36  iter: 299  total_loss: 0.472  loss_cls: 0.131  loss_box_reg: 0.247  loss_rpn_cls: 0.008  loss_rpn_loc: 0.043  time: 0.5628  data_time: 0.0070  lr: 0.001499  max_mem: 3067M
[32m[04/15 10:44:03 d2.utils.events]: [0m eta: 0:15:25  iter: 319  total_loss: 0.663  loss_cls: 0.205  loss_box_reg: 0.391  loss_rpn_cls: 0.009  loss_rpn_loc: 0.068  time: 0.5612  data_time: 0.0070  lr: 0.001598  max_mem: 3067M
[32m[04/15 10:44:15 d2.utils.events]: [0m eta: 0:15:15  iter: 339  total_loss: 0.608  loss_cls: 0.198  loss_box_reg: 0.340  loss_rpn_cls: 0.025  loss_rpn_loc: 0.048  time: 0.5632  data_time: 0.0087  lr: 0.001698  max_mem: 3067M
[32m[04/15 10:44:26 d2.utils.events]: [0m eta: 0:15:03  iter: 359  total_loss: 0.598  loss_cls: 0.188  loss_box_reg: 0.334  loss_rpn_cls: 0.019  loss_rpn_loc: 0.046  time: 0.5632  data_time: 0.0134  lr: 0.001798  max_mem: 3067M
[32m[04/15 10:44:37 d2.utils.events]: [0m eta: 0:14:52  iter: 379  total_loss: 0.738  loss_cls: 0.235  loss_box_reg: 0.412  loss_rpn_cls: 0.017  loss_rpn_loc: 0.066  time: 0.5621  data_time: 0.0066  lr: 0.001898  max_mem: 3067M
[32m[04/15 10:44:49 d2.utils.events]: [0m eta: 0:14:42  iter: 399  total_loss: 0.614  loss_cls: 0.187  loss_box_reg: 0.352  loss_rpn_cls: 0.013  loss_rpn_loc: 0.045  time: 0.5638  data_time: 0.0088  lr: 0.001998  max_mem: 3067M
[32m[04/15 10:45:01 d2.utils.events]: [0m eta: 0:14:35  iter: 419  total_loss: 0.510  loss_cls: 0.153  loss_box_reg: 0.290  loss_rpn_cls: 0.011  loss_rpn_loc: 0.035  time: 0.5647  data_time: 0.0107  lr: 0.002098  max_mem: 3067M
[32m[04/15 10:45:12 d2.utils.events]: [0m eta: 0:14:22  iter: 439  total_loss: 0.521  loss_cls: 0.170  loss_box_reg: 0.287  loss_rpn_cls: 0.006  loss_rpn_loc: 0.049  time: 0.5636  data_time: 0.0078  lr: 0.002198  max_mem: 3067M
[32m[04/15 10:45:24 d2.utils.events]: [0m eta: 0:14:11  iter: 459  total_loss: 0.583  loss_cls: 0.186  loss_box_reg: 0.322  loss_rpn_cls: 0.011  loss_rpn_loc: 0.054  time: 0.5640  data_time: 0.0074  lr: 0.002298  max_mem: 3067M
[32m[04/15 10:45:36 d2.utils.events]: [0m eta: 0:14:01  iter: 479  total_loss: 0.570  loss_cls: 0.180  loss_box_reg: 0.338  loss_rpn_cls: 0.008  loss_rpn_loc: 0.041  time: 0.5652  data_time: 0.0071  lr: 0.002398  max_mem: 3067M
[32m[04/15 10:45:46 d2.utils.events]: [0m eta: 0:13:49  iter: 499  total_loss: 0.662  loss_cls: 0.200  loss_box_reg: 0.379  loss_rpn_cls: 0.011  loss_rpn_loc: 0.052  time: 0.5643  data_time: 0.0073  lr: 0.002498  max_mem: 3067M
[32m[04/15 10:45:58 d2.utils.events]: [0m eta: 0:13:39  iter: 519  total_loss: 0.555  loss_cls: 0.169  loss_box_reg: 0.311  loss_rpn_cls: 0.008  loss_rpn_loc: 0.038  time: 0.5641  data_time: 0.0080  lr: 0.002597  max_mem: 3067M
[32m[04/15 10:46:10 d2.utils.events]: [0m eta: 0:13:30  iter: 539  total_loss: 0.539  loss_cls: 0.151  loss_box_reg: 0.317  loss_rpn_cls: 0.009  loss_rpn_loc: 0.031  time: 0.5663  data_time: 0.0072  lr: 0.002697  max_mem: 3067M
[32m[04/15 10:46:21 d2.utils.events]: [0m eta: 0:13:19  iter: 559  total_loss: 0.663  loss_cls: 0.233  loss_box_reg: 0.369  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5652  data_time: 0.0080  lr: 0.002797  max_mem: 3067M
[32m[04/15 10:46:32 d2.utils.events]: [0m eta: 0:13:05  iter: 579  total_loss: 0.511  loss_cls: 0.161  loss_box_reg: 0.297  loss_rpn_cls: 0.009  loss_rpn_loc: 0.043  time: 0.5642  data_time: 0.0106  lr: 0.002897  max_mem: 3067M
[32m[04/15 10:46:45 d2.utils.events]: [0m eta: 0:12:56  iter: 599  total_loss: 0.584  loss_cls: 0.195  loss_box_reg: 0.332  loss_rpn_cls: 0.012  loss_rpn_loc: 0.050  time: 0.5665  data_time: 0.0065  lr: 0.002997  max_mem: 3067M
[32m[04/15 10:46:56 d2.utils.events]: [0m eta: 0:12:44  iter: 619  total_loss: 0.605  loss_cls: 0.187  loss_box_reg: 0.353  loss_rpn_cls: 0.009  loss_rpn_loc: 0.041  time: 0.5660  data_time: 0.0069  lr: 0.003097  max_mem: 3067M
[32m[04/15 10:47:07 d2.utils.events]: [0m eta: 0:12:33  iter: 639  total_loss: 0.553  loss_cls: 0.172  loss_box_reg: 0.326  loss_rpn_cls: 0.010  loss_rpn_loc: 0.051  time: 0.5655  data_time: 0.0084  lr: 0.003197  max_mem: 3067M
[32m[04/15 10:47:19 d2.utils.events]: [0m eta: 0:12:23  iter: 659  total_loss: 0.577  loss_cls: 0.178  loss_box_reg: 0.304  loss_rpn_cls: 0.013  loss_rpn_loc: 0.044  time: 0.5666  data_time: 0.0066  lr: 0.003297  max_mem: 3067M
[32m[04/15 10:47:31 d2.utils.events]: [0m eta: 0:12:12  iter: 679  total_loss: 0.658  loss_cls: 0.198  loss_box_reg: 0.371  loss_rpn_cls: 0.015  loss_rpn_loc: 0.064  time: 0.5667  data_time: 0.0104  lr: 0.003397  max_mem: 3067M
[32m[04/15 10:47:41 d2.utils.events]: [0m eta: 0:12:01  iter: 699  total_loss: 0.635  loss_cls: 0.192  loss_box_reg: 0.348  loss_rpn_cls: 0.013  loss_rpn_loc: 0.063  time: 0.5659  data_time: 0.0074  lr: 0.003497  max_mem: 3067M
[32m[04/15 10:47:53 d2.utils.events]: [0m eta: 0:11:50  iter: 719  total_loss: 0.653  loss_cls: 0.204  loss_box_reg: 0.363  loss_rpn_cls: 0.016  loss_rpn_loc: 0.049  time: 0.5664  data_time: 0.0070  lr: 0.003596  max_mem: 3067M
[32m[04/15 10:48:05 d2.utils.events]: [0m eta: 0:11:39  iter: 739  total_loss: 0.521  loss_cls: 0.164  loss_box_reg: 0.312  loss_rpn_cls: 0.009  loss_rpn_loc: 0.053  time: 0.5668  data_time: 0.0078  lr: 0.003696  max_mem: 3067M
[32m[04/15 10:48:16 d2.utils.events]: [0m eta: 0:11:28  iter: 759  total_loss: 0.565  loss_cls: 0.179  loss_box_reg: 0.268  loss_rpn_cls: 0.012  loss_rpn_loc: 0.055  time: 0.5660  data_time: 0.0080  lr: 0.003796  max_mem: 3067M
[32m[04/15 10:48:27 d2.utils.events]: [0m eta: 0:11:17  iter: 779  total_loss: 0.511  loss_cls: 0.169  loss_box_reg: 0.291  loss_rpn_cls: 0.011  loss_rpn_loc: 0.030  time: 0.5656  data_time: 0.0074  lr: 0.003896  max_mem: 3067M
[32m[04/15 10:48:39 d2.utils.events]: [0m eta: 0:11:06  iter: 799  total_loss: 0.610  loss_cls: 0.187  loss_box_reg: 0.351  loss_rpn_cls: 0.018  loss_rpn_loc: 0.057  time: 0.5666  data_time: 0.0075  lr: 0.003996  max_mem: 3067M
[32m[04/15 10:48:50 d2.utils.events]: [0m eta: 0:10:54  iter: 819  total_loss: 0.528  loss_cls: 0.151  loss_box_reg: 0.298  loss_rpn_cls: 0.018  loss_rpn_loc: 0.047  time: 0.5652  data_time: 0.0078  lr: 0.004096  max_mem: 3067M
[32m[04/15 10:49:00 d2.utils.events]: [0m eta: 0:10:41  iter: 839  total_loss: 0.603  loss_cls: 0.207  loss_box_reg: 0.324  loss_rpn_cls: 0.017  loss_rpn_loc: 0.053  time: 0.5642  data_time: 0.0069  lr: 0.004196  max_mem: 3067M
[32m[04/15 10:49:13 d2.utils.events]: [0m eta: 0:10:32  iter: 859  total_loss: 0.614  loss_cls: 0.180  loss_box_reg: 0.335  loss_rpn_cls: 0.014  loss_rpn_loc: 0.051  time: 0.5656  data_time: 0.0090  lr: 0.004296  max_mem: 3067M
[32m[04/15 10:49:24 d2.utils.events]: [0m eta: 0:10:21  iter: 879  total_loss: 0.708  loss_cls: 0.237  loss_box_reg: 0.385  loss_rpn_cls: 0.014  loss_rpn_loc: 0.058  time: 0.5653  data_time: 0.0082  lr: 0.004396  max_mem: 3067M
[32m[04/15 10:49:34 d2.utils.events]: [0m eta: 0:10:09  iter: 899  total_loss: 0.539  loss_cls: 0.173  loss_box_reg: 0.323  loss_rpn_cls: 0.014  loss_rpn_loc: 0.060  time: 0.5645  data_time: 0.0068  lr: 0.004496  max_mem: 3067M
[32m[04/15 10:49:47 d2.utils.events]: [0m eta: 0:09:58  iter: 919  total_loss: 0.642  loss_cls: 0.190  loss_box_reg: 0.380  loss_rpn_cls: 0.018  loss_rpn_loc: 0.070  time: 0.5654  data_time: 0.0082  lr: 0.004595  max_mem: 3067M
[32m[04/15 10:49:58 d2.utils.events]: [0m eta: 0:09:47  iter: 939  total_loss: 0.609  loss_cls: 0.205  loss_box_reg: 0.362  loss_rpn_cls: 0.017  loss_rpn_loc: 0.052  time: 0.5653  data_time: 0.0106  lr: 0.004695  max_mem: 3067M
[32m[04/15 10:50:09 d2.utils.events]: [0m eta: 0:09:35  iter: 959  total_loss: 0.617  loss_cls: 0.197  loss_box_reg: 0.350  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5646  data_time: 0.0069  lr: 0.004795  max_mem: 3067M
[32m[04/15 10:50:20 d2.utils.events]: [0m eta: 0:09:24  iter: 979  total_loss: 0.580  loss_cls: 0.217  loss_box_reg: 0.333  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5647  data_time: 0.0068  lr: 0.004895  max_mem: 3067M
[32m[04/15 10:50:32 d2.utils.events]: [0m eta: 0:09:13  iter: 999  total_loss: 0.589  loss_cls: 0.184  loss_box_reg: 0.317  loss_rpn_cls: 0.015  loss_rpn_loc: 0.073  time: 0.5648  data_time: 0.0082  lr: 0.004995  max_mem: 3067M
[32m[04/15 10:50:43 d2.utils.events]: [0m eta: 0:09:03  iter: 1019  total_loss: 0.691  loss_cls: 0.237  loss_box_reg: 0.381  loss_rpn_cls: 0.018  loss_rpn_loc: 0.056  time: 0.5647  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:50:54 d2.utils.events]: [0m eta: 0:08:52  iter: 1039  total_loss: 0.682  loss_cls: 0.225  loss_box_reg: 0.378  loss_rpn_cls: 0.019  loss_rpn_loc: 0.061  time: 0.5647  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:51:07 d2.utils.events]: [0m eta: 0:08:41  iter: 1059  total_loss: 0.560  loss_cls: 0.180  loss_box_reg: 0.307  loss_rpn_cls: 0.015  loss_rpn_loc: 0.060  time: 0.5654  data_time: 0.0118  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:51:17 d2.utils.events]: [0m eta: 0:08:30  iter: 1079  total_loss: 0.599  loss_cls: 0.194  loss_box_reg: 0.356  loss_rpn_cls: 0.016  loss_rpn_loc: 0.046  time: 0.5650  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:51:28 d2.utils.events]: [0m eta: 0:08:18  iter: 1099  total_loss: 0.603  loss_cls: 0.192  loss_box_reg: 0.347  loss_rpn_cls: 0.011  loss_rpn_loc: 0.044  time: 0.5647  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:51:41 d2.utils.events]: [0m eta: 0:08:08  iter: 1119  total_loss: 0.580  loss_cls: 0.197  loss_box_reg: 0.352  loss_rpn_cls: 0.015  loss_rpn_loc: 0.042  time: 0.5659  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:51:52 d2.utils.events]: [0m eta: 0:07:57  iter: 1139  total_loss: 0.609  loss_cls: 0.189  loss_box_reg: 0.359  loss_rpn_cls: 0.013  loss_rpn_loc: 0.055  time: 0.5657  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:52:03 d2.utils.events]: [0m eta: 0:07:45  iter: 1159  total_loss: 0.640  loss_cls: 0.197  loss_box_reg: 0.387  loss_rpn_cls: 0.014  loss_rpn_loc: 0.057  time: 0.5653  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:52:16 d2.utils.events]: [0m eta: 0:07:35  iter: 1179  total_loss: 0.657  loss_cls: 0.207  loss_box_reg: 0.372  loss_rpn_cls: 0.014  loss_rpn_loc: 0.063  time: 0.5662  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:52:27 d2.utils.events]: [0m eta: 0:07:23  iter: 1199  total_loss: 0.628  loss_cls: 0.210  loss_box_reg: 0.369  loss_rpn_cls: 0.016  loss_rpn_loc: 0.057  time: 0.5660  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:52:38 d2.utils.events]: [0m eta: 0:07:12  iter: 1219  total_loss: 0.595  loss_cls: 0.182  loss_box_reg: 0.338  loss_rpn_cls: 0.021  loss_rpn_loc: 0.048  time: 0.5656  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:52:50 d2.utils.events]: [0m eta: 0:07:01  iter: 1239  total_loss: 0.681  loss_cls: 0.203  loss_box_reg: 0.366  loss_rpn_cls: 0.025  loss_rpn_loc: 0.072  time: 0.5664  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:53:02 d2.utils.events]: [0m eta: 0:06:50  iter: 1259  total_loss: 0.660  loss_cls: 0.207  loss_box_reg: 0.370  loss_rpn_cls: 0.014  loss_rpn_loc: 0.055  time: 0.5663  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:53:12 d2.utils.events]: [0m eta: 0:06:39  iter: 1279  total_loss: 0.665  loss_cls: 0.211  loss_box_reg: 0.354  loss_rpn_cls: 0.014  loss_rpn_loc: 0.063  time: 0.5657  data_time: 0.0061  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:53:24 d2.utils.events]: [0m eta: 0:06:28  iter: 1299  total_loss: 0.487  loss_cls: 0.160  loss_box_reg: 0.275  loss_rpn_cls: 0.013  loss_rpn_loc: 0.027  time: 0.5660  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:53:36 d2.utils.events]: [0m eta: 0:06:18  iter: 1319  total_loss: 0.550  loss_cls: 0.190  loss_box_reg: 0.325  loss_rpn_cls: 0.012  loss_rpn_loc: 0.043  time: 0.5662  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:53:47 d2.utils.events]: [0m eta: 0:06:07  iter: 1339  total_loss: 0.562  loss_cls: 0.201  loss_box_reg: 0.324  loss_rpn_cls: 0.018  loss_rpn_loc: 0.042  time: 0.5662  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:53:59 d2.utils.events]: [0m eta: 0:05:56  iter: 1359  total_loss: 0.645  loss_cls: 0.190  loss_box_reg: 0.376  loss_rpn_cls: 0.014  loss_rpn_loc: 0.056  time: 0.5665  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:54:11 d2.utils.events]: [0m eta: 0:05:45  iter: 1379  total_loss: 0.604  loss_cls: 0.211  loss_box_reg: 0.360  loss_rpn_cls: 0.023  loss_rpn_loc: 0.055  time: 0.5667  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:54:21 d2.utils.events]: [0m eta: 0:05:33  iter: 1399  total_loss: 0.717  loss_cls: 0.207  loss_box_reg: 0.366  loss_rpn_cls: 0.014  loss_rpn_loc: 0.086  time: 0.5661  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:54:32 d2.utils.events]: [0m eta: 0:05:21  iter: 1419  total_loss: 0.578  loss_cls: 0.189  loss_box_reg: 0.313  loss_rpn_cls: 0.015  loss_rpn_loc: 0.056  time: 0.5659  data_time: 0.0114  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:54:45 d2.utils.events]: [0m eta: 0:05:11  iter: 1439  total_loss: 0.551  loss_cls: 0.178  loss_box_reg: 0.308  loss_rpn_cls: 0.012  loss_rpn_loc: 0.060  time: 0.5665  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:54:56 d2.utils.events]: [0m eta: 0:04:59  iter: 1459  total_loss: 0.697  loss_cls: 0.231  loss_box_reg: 0.403  loss_rpn_cls: 0.014  loss_rpn_loc: 0.054  time: 0.5662  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:55:07 d2.utils.events]: [0m eta: 0:04:48  iter: 1479  total_loss: 0.745  loss_cls: 0.213  loss_box_reg: 0.350  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5660  data_time: 0.0122  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:55:18 d2.utils.events]: [0m eta: 0:04:37  iter: 1499  total_loss: 0.590  loss_cls: 0.183  loss_box_reg: 0.322  loss_rpn_cls: 0.017  loss_rpn_loc: 0.049  time: 0.5661  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:55:29 d2.utils.events]: [0m eta: 0:04:26  iter: 1519  total_loss: 0.670  loss_cls: 0.216  loss_box_reg: 0.387  loss_rpn_cls: 0.013  loss_rpn_loc: 0.056  time: 0.5658  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:55:40 d2.utils.events]: [0m eta: 0:04:14  iter: 1539  total_loss: 0.618  loss_cls: 0.197  loss_box_reg: 0.362  loss_rpn_cls: 0.016  loss_rpn_loc: 0.048  time: 0.5655  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:55:52 d2.utils.events]: [0m eta: 0:04:03  iter: 1559  total_loss: 0.683  loss_cls: 0.220  loss_box_reg: 0.369  loss_rpn_cls: 0.014  loss_rpn_loc: 0.067  time: 0.5660  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:56:03 d2.utils.events]: [0m eta: 0:03:53  iter: 1579  total_loss: 0.681  loss_cls: 0.236  loss_box_reg: 0.369  loss_rpn_cls: 0.024  loss_rpn_loc: 0.053  time: 0.5659  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:56:14 d2.utils.events]: [0m eta: 0:03:41  iter: 1599  total_loss: 0.714  loss_cls: 0.204  loss_box_reg: 0.386  loss_rpn_cls: 0.020  loss_rpn_loc: 0.058  time: 0.5656  data_time: 0.0140  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:56:27 d2.utils.events]: [0m eta: 0:03:31  iter: 1619  total_loss: 0.551  loss_cls: 0.182  loss_box_reg: 0.303  loss_rpn_cls: 0.012  loss_rpn_loc: 0.038  time: 0.5663  data_time: 0.0898  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:56:40 d2.utils.events]: [0m eta: 0:03:20  iter: 1639  total_loss: 0.584  loss_cls: 0.178  loss_box_reg: 0.323  loss_rpn_cls: 0.017  loss_rpn_loc: 0.035  time: 0.5670  data_time: 0.0699  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:56:51 d2.utils.events]: [0m eta: 0:03:09  iter: 1659  total_loss: 0.599  loss_cls: 0.214  loss_box_reg: 0.328  loss_rpn_cls: 0.012  loss_rpn_loc: 0.054  time: 0.5668  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:57:03 d2.utils.events]: [0m eta: 0:02:57  iter: 1679  total_loss: 0.641  loss_cls: 0.179  loss_box_reg: 0.345  loss_rpn_cls: 0.016  loss_rpn_loc: 0.065  time: 0.5673  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:57:14 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.722  loss_cls: 0.237  loss_box_reg: 0.370  loss_rpn_cls: 0.023  loss_rpn_loc: 0.067  time: 0.5673  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:57:26 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.600  loss_cls: 0.172  loss_box_reg: 0.354  loss_rpn_cls: 0.016  loss_rpn_loc: 0.047  time: 0.5676  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:57:38 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.640  loss_cls: 0.203  loss_box_reg: 0.342  loss_rpn_cls: 0.017  loss_rpn_loc: 0.040  time: 0.5672  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:57:49 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.567  loss_cls: 0.187  loss_box_reg: 0.314  loss_rpn_cls: 0.016  loss_rpn_loc: 0.050  time: 0.5667  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:58:01 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.622  loss_cls: 0.185  loss_box_reg: 0.340  loss_rpn_cls: 0.016  loss_rpn_loc: 0.070  time: 0.5672  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:58:12 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.662  loss_cls: 0.203  loss_box_reg: 0.351  loss_rpn_cls: 0.024  loss_rpn_loc: 0.065  time: 0.5669  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:58:23 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.667  loss_cls: 0.222  loss_box_reg: 0.375  loss_rpn_cls: 0.020  loss_rpn_loc: 0.056  time: 0.5664  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:58:35 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.641  loss_cls: 0.226  loss_box_reg: 0.353  loss_rpn_cls: 0.013  loss_rpn_loc: 0.043  time: 0.5669  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:58:47 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.770  loss_cls: 0.240  loss_box_reg: 0.421  loss_rpn_cls: 0.021  loss_rpn_loc: 0.065  time: 0.5670  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:58:57 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.706  loss_cls: 0.213  loss_box_reg: 0.412  loss_rpn_cls: 0.021  loss_rpn_loc: 0.065  time: 0.5666  data_time: 0.0124  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:59:09 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.675  loss_cls: 0.206  loss_box_reg: 0.365  loss_rpn_cls: 0.013  loss_rpn_loc: 0.046  time: 0.5666  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:59:21 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.545  loss_cls: 0.190  loss_box_reg: 0.318  loss_rpn_cls: 0.008  loss_rpn_loc: 0.047  time: 0.5669  data_time: 0.0092  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:59:32 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.656  loss_cls: 0.184  loss_box_reg: 0.376  loss_rpn_cls: 0.021  loss_rpn_loc: 0.057  time: 0.5667  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:59:43 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.648  loss_cls: 0.202  loss_box_reg: 0.335  loss_rpn_cls: 0.019  loss_rpn_loc: 0.054  time: 0.5665  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 10:59:56 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.679  loss_cls: 0.219  loss_box_reg: 0.359  loss_rpn_cls: 0.014  loss_rpn_loc: 0.057  time: 0.5672  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 11:00:17 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:00:17 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 11:00:17 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 11:00:17 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.713  loss_cls: 0.224  loss_box_reg: 0.399  loss_rpn_cls: 0.022  loss_rpn_loc: 0.054  time: 0.5670  data_time: 0.0115  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:00:21 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:52 (0.5673 s / it)
[32m[04/15 11:00:21 d2.engine.hooks]: [0mTotal training time: 0:19:19 (0:00:26 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 11:00:26 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:00:26 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 11:00:26 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 11:00:29 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1213 s / img. ETA=0:02:36
[32m[04/15 11:00:34 d2.evaluation.evaluator]: [0mInference done 46/1257. 0.1217 s / img. ETA=0:02:50
[32m[04/15 11:00:39 d2.evaluation.evaluator]: [0mInference done 82/1257. 0.1214 s / img. ETA=0:02:44
[32m[04/15 11:00:44 d2.evaluation.evaluator]: [0mInference done 119/1257. 0.1213 s / img. ETA=0:02:39
[32m[04/15 11:00:49 d2.evaluation.evaluator]: [0mInference done 153/1257. 0.1211 s / img. ETA=0:02:36
[32m[04/15 11:00:54 d2.evaluation.evaluator]: [0mInference done 193/1257. 0.1213 s / img. ETA=0:02:27
[32m[04/15 11:00:59 d2.evaluation.evaluator]: [0mInference done 226/1257. 0.1232 s / img. ETA=0:02:25
[32m[04/15 11:01:04 d2.evaluation.evaluator]: [0mInference done 257/1257. 0.1258 s / img. ETA=0:02:23
[32m[04/15 11:01:09 d2.evaluation.evaluator]: [0mInference done 293/1257. 0.1263 s / img. ETA=0:02:17
[32m[04/15 11:01:14 d2.evaluation.evaluator]: [0mInference done 332/1257. 0.1257 s / img. ETA=0:02:10
[32m[04/15 11:01:19 d2.evaluation.evaluator]: [0mInference done 372/1257. 0.1253 s / img. ETA=0:02:03
[32m[04/15 11:01:24 d2.evaluation.evaluator]: [0mInference done 411/1257. 0.1250 s / img. ETA=0:01:57
[32m[04/15 11:01:29 d2.evaluation.evaluator]: [0mInference done 441/1257. 0.1246 s / img. ETA=0:01:54
[32m[04/15 11:01:34 d2.evaluation.evaluator]: [0mInference done 473/1257. 0.1241 s / img. ETA=0:01:51
[32m[04/15 11:01:39 d2.evaluation.evaluator]: [0mInference done 504/1257. 0.1246 s / img. ETA=0:01:47
[32m[04/15 11:01:44 d2.evaluation.evaluator]: [0mInference done 534/1257. 0.1258 s / img. ETA=0:01:44
[32m[04/15 11:01:50 d2.evaluation.evaluator]: [0mInference done 562/1257. 0.1255 s / img. ETA=0:01:42
[32m[04/15 11:01:55 d2.evaluation.evaluator]: [0mInference done 589/1257. 0.1252 s / img. ETA=0:01:39
[32m[04/15 11:02:00 d2.evaluation.evaluator]: [0mInference done 616/1257. 0.1250 s / img. ETA=0:01:36
[32m[04/15 11:02:05 d2.evaluation.evaluator]: [0mInference done 642/1257. 0.1247 s / img. ETA=0:01:33
[32m[04/15 11:02:10 d2.evaluation.evaluator]: [0mInference done 665/1257. 0.1247 s / img. ETA=0:01:31
[32m[04/15 11:02:15 d2.evaluation.evaluator]: [0mInference done 695/1257. 0.1249 s / img. ETA=0:01:27
[32m[04/15 11:02:20 d2.evaluation.evaluator]: [0mInference done 725/1257. 0.1258 s / img. ETA=0:01:23
[32m[04/15 11:02:25 d2.evaluation.evaluator]: [0mInference done 750/1257. 0.1255 s / img. ETA=0:01:20
[32m[04/15 11:02:30 d2.evaluation.evaluator]: [0mInference done 781/1257. 0.1253 s / img. ETA=0:01:15
[32m[04/15 11:02:36 d2.evaluation.evaluator]: [0mInference done 814/1257. 0.1251 s / img. ETA=0:01:09
[32m[04/15 11:02:41 d2.evaluation.evaluator]: [0mInference done 854/1257. 0.1251 s / img. ETA=0:01:03
[32m[04/15 11:02:46 d2.evaluation.evaluator]: [0mInference done 894/1257. 0.1249 s / img. ETA=0:00:56
[32m[04/15 11:02:51 d2.evaluation.evaluator]: [0mInference done 933/1257. 0.1250 s / img. ETA=0:00:49
[32m[04/15 11:02:56 d2.evaluation.evaluator]: [0mInference done 972/1257. 0.1251 s / img. ETA=0:00:43
[32m[04/15 11:03:01 d2.evaluation.evaluator]: [0mInference done 1007/1257. 0.1257 s / img. ETA=0:00:38
[32m[04/15 11:03:06 d2.evaluation.evaluator]: [0mInference done 1042/1257. 0.1262 s / img. ETA=0:00:32
[32m[04/15 11:03:11 d2.evaluation.evaluator]: [0mInference done 1081/1257. 0.1262 s / img. ETA=0:00:26
[32m[04/15 11:03:16 d2.evaluation.evaluator]: [0mInference done 1120/1257. 0.1261 s / img. ETA=0:00:20
[32m[04/15 11:03:21 d2.evaluation.evaluator]: [0mInference done 1157/1257. 0.1260 s / img. ETA=0:00:15
[32m[04/15 11:03:26 d2.evaluation.evaluator]: [0mInference done 1197/1257. 0.1258 s / img. ETA=0:00:08
[32m[04/15 11:03:31 d2.evaluation.evaluator]: [0mInference done 1236/1257. 0.1258 s / img. ETA=0:00:03
[32m[04/15 11:03:34 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:06.636956 (0.149071 s / img per device, on 1 devices)
[32m[04/15 11:03:34 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:37 (0.125846 s / img per device, on 1 devices)
[32m[04/15 11:03:35 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 11:03:35 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 11:03:35 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.05s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=6.47s).
Accumulating evaluation results...
DONE (t=0.93s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.191
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.373
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.168
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.115
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.243
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.437
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.240
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.173
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.285
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.474
[32m[04/15 11:03:43 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.121 | 37.339 | 16.845 | 11.529 | 24.251 | 43.718 |
[32m[04/15 11:03:43 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 27.164 | bicycle       | 6.393 | car            | 42.927 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  20  *  2000  iterations ============
4 channel input
[32m[04/15 11:03:44 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 11:03:46 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.21 seconds.
[5m[31mWARNING[0m [32m[04/15 11:03:46 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:03:46 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 11:03:47 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 11:03:47 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 11:03:47 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 11:03:49 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 11:04:01 d2.utils.events]: [0m eta: 0:18:03  iter: 19  total_loss: 0.634  loss_cls: 0.200  loss_box_reg: 0.355  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5523  data_time: 0.0547  lr: 0.000100  max_mem: 3067M
[32m[04/15 11:04:12 d2.utils.events]: [0m eta: 0:17:57  iter: 39  total_loss: 0.687  loss_cls: 0.205  loss_box_reg: 0.367  loss_rpn_cls: 0.014  loss_rpn_loc: 0.060  time: 0.5650  data_time: 0.0077  lr: 0.000200  max_mem: 3067M
[32m[04/15 11:04:24 d2.utils.events]: [0m eta: 0:17:51  iter: 59  total_loss: 0.635  loss_cls: 0.198  loss_box_reg: 0.370  loss_rpn_cls: 0.012  loss_rpn_loc: 0.052  time: 0.5707  data_time: 0.0087  lr: 0.000300  max_mem: 3067M
[32m[04/15 11:04:35 d2.utils.events]: [0m eta: 0:17:40  iter: 79  total_loss: 0.681  loss_cls: 0.219  loss_box_reg: 0.348  loss_rpn_cls: 0.018  loss_rpn_loc: 0.062  time: 0.5675  data_time: 0.0068  lr: 0.000400  max_mem: 3067M
[32m[04/15 11:04:46 d2.utils.events]: [0m eta: 0:17:43  iter: 99  total_loss: 0.662  loss_cls: 0.211  loss_box_reg: 0.366  loss_rpn_cls: 0.017  loss_rpn_loc: 0.063  time: 0.5654  data_time: 0.0083  lr: 0.000500  max_mem: 3067M
[32m[04/15 11:04:57 d2.utils.events]: [0m eta: 0:17:18  iter: 119  total_loss: 0.640  loss_cls: 0.185  loss_box_reg: 0.343  loss_rpn_cls: 0.012  loss_rpn_loc: 0.049  time: 0.5601  data_time: 0.0062  lr: 0.000599  max_mem: 3067M
[32m[04/15 11:05:10 d2.utils.events]: [0m eta: 0:17:32  iter: 139  total_loss: 0.641  loss_cls: 0.194  loss_box_reg: 0.376  loss_rpn_cls: 0.016  loss_rpn_loc: 0.055  time: 0.5700  data_time: 0.0078  lr: 0.000699  max_mem: 3067M
[32m[04/15 11:05:21 d2.utils.events]: [0m eta: 0:17:20  iter: 159  total_loss: 0.665  loss_cls: 0.199  loss_box_reg: 0.360  loss_rpn_cls: 0.013  loss_rpn_loc: 0.067  time: 0.5683  data_time: 0.0078  lr: 0.000799  max_mem: 3067M
[32m[04/15 11:05:32 d2.utils.events]: [0m eta: 0:17:03  iter: 179  total_loss: 0.527  loss_cls: 0.215  loss_box_reg: 0.292  loss_rpn_cls: 0.014  loss_rpn_loc: 0.044  time: 0.5656  data_time: 0.0072  lr: 0.000899  max_mem: 3067M
[32m[04/15 11:05:44 d2.utils.events]: [0m eta: 0:16:57  iter: 199  total_loss: 0.573  loss_cls: 0.186  loss_box_reg: 0.315  loss_rpn_cls: 0.012  loss_rpn_loc: 0.059  time: 0.5690  data_time: 0.0077  lr: 0.000999  max_mem: 3067M
[32m[04/15 11:05:55 d2.utils.events]: [0m eta: 0:16:40  iter: 219  total_loss: 0.551  loss_cls: 0.183  loss_box_reg: 0.305  loss_rpn_cls: 0.010  loss_rpn_loc: 0.038  time: 0.5672  data_time: 0.0071  lr: 0.001099  max_mem: 3067M
[32m[04/15 11:06:06 d2.utils.events]: [0m eta: 0:16:24  iter: 239  total_loss: 0.641  loss_cls: 0.208  loss_box_reg: 0.359  loss_rpn_cls: 0.012  loss_rpn_loc: 0.061  time: 0.5653  data_time: 0.0067  lr: 0.001199  max_mem: 3067M
[32m[04/15 11:06:18 d2.utils.events]: [0m eta: 0:16:17  iter: 259  total_loss: 0.552  loss_cls: 0.176  loss_box_reg: 0.312  loss_rpn_cls: 0.014  loss_rpn_loc: 0.043  time: 0.5667  data_time: 0.0090  lr: 0.001299  max_mem: 3067M
[32m[04/15 11:06:29 d2.utils.events]: [0m eta: 0:16:05  iter: 279  total_loss: 0.646  loss_cls: 0.198  loss_box_reg: 0.370  loss_rpn_cls: 0.013  loss_rpn_loc: 0.055  time: 0.5664  data_time: 0.0088  lr: 0.001399  max_mem: 3067M
[32m[04/15 11:06:41 d2.utils.events]: [0m eta: 0:15:54  iter: 299  total_loss: 0.473  loss_cls: 0.152  loss_box_reg: 0.292  loss_rpn_cls: 0.009  loss_rpn_loc: 0.038  time: 0.5656  data_time: 0.0090  lr: 0.001499  max_mem: 3067M
[32m[04/15 11:06:52 d2.utils.events]: [0m eta: 0:15:43  iter: 319  total_loss: 0.534  loss_cls: 0.171  loss_box_reg: 0.313  loss_rpn_cls: 0.017  loss_rpn_loc: 0.041  time: 0.5673  data_time: 0.0072  lr: 0.001598  max_mem: 3067M
[32m[04/15 11:07:05 d2.utils.events]: [0m eta: 0:15:35  iter: 339  total_loss: 0.621  loss_cls: 0.197  loss_box_reg: 0.377  loss_rpn_cls: 0.011  loss_rpn_loc: 0.051  time: 0.5697  data_time: 0.0076  lr: 0.001698  max_mem: 3067M
[32m[04/15 11:07:16 d2.utils.events]: [0m eta: 0:15:22  iter: 359  total_loss: 0.528  loss_cls: 0.173  loss_box_reg: 0.303  loss_rpn_cls: 0.009  loss_rpn_loc: 0.039  time: 0.5681  data_time: 0.0081  lr: 0.001798  max_mem: 3067M
[32m[04/15 11:07:27 d2.utils.events]: [0m eta: 0:15:10  iter: 379  total_loss: 0.624  loss_cls: 0.203  loss_box_reg: 0.347  loss_rpn_cls: 0.011  loss_rpn_loc: 0.049  time: 0.5684  data_time: 0.0078  lr: 0.001898  max_mem: 3067M
[32m[04/15 11:07:39 d2.utils.events]: [0m eta: 0:14:59  iter: 399  total_loss: 0.562  loss_cls: 0.184  loss_box_reg: 0.315  loss_rpn_cls: 0.012  loss_rpn_loc: 0.046  time: 0.5694  data_time: 0.0081  lr: 0.001998  max_mem: 3067M
[32m[04/15 11:07:49 d2.utils.events]: [0m eta: 0:14:43  iter: 419  total_loss: 0.550  loss_cls: 0.173  loss_box_reg: 0.305  loss_rpn_cls: 0.011  loss_rpn_loc: 0.042  time: 0.5668  data_time: 0.0076  lr: 0.002098  max_mem: 3067M
[32m[04/15 11:08:00 d2.utils.events]: [0m eta: 0:14:31  iter: 439  total_loss: 0.664  loss_cls: 0.203  loss_box_reg: 0.354  loss_rpn_cls: 0.011  loss_rpn_loc: 0.050  time: 0.5660  data_time: 0.0079  lr: 0.002198  max_mem: 3067M
[32m[04/15 11:08:13 d2.utils.events]: [0m eta: 0:14:23  iter: 459  total_loss: 0.501  loss_cls: 0.156  loss_box_reg: 0.295  loss_rpn_cls: 0.011  loss_rpn_loc: 0.044  time: 0.5686  data_time: 0.0066  lr: 0.002298  max_mem: 3067M
[32m[04/15 11:08:23 d2.utils.events]: [0m eta: 0:14:07  iter: 479  total_loss: 0.543  loss_cls: 0.167  loss_box_reg: 0.310  loss_rpn_cls: 0.013  loss_rpn_loc: 0.047  time: 0.5663  data_time: 0.0074  lr: 0.002398  max_mem: 3067M
[32m[04/15 11:08:34 d2.utils.events]: [0m eta: 0:13:54  iter: 499  total_loss: 0.538  loss_cls: 0.180  loss_box_reg: 0.330  loss_rpn_cls: 0.012  loss_rpn_loc: 0.042  time: 0.5657  data_time: 0.0064  lr: 0.002498  max_mem: 3067M
[32m[04/15 11:08:46 d2.utils.events]: [0m eta: 0:13:46  iter: 519  total_loss: 0.610  loss_cls: 0.185  loss_box_reg: 0.319  loss_rpn_cls: 0.014  loss_rpn_loc: 0.064  time: 0.5669  data_time: 0.0069  lr: 0.002597  max_mem: 3067M
[32m[04/15 11:08:58 d2.utils.events]: [0m eta: 0:13:34  iter: 539  total_loss: 0.525  loss_cls: 0.169  loss_box_reg: 0.260  loss_rpn_cls: 0.013  loss_rpn_loc: 0.046  time: 0.5671  data_time: 0.0107  lr: 0.002697  max_mem: 3067M
[32m[04/15 11:09:09 d2.utils.events]: [0m eta: 0:13:22  iter: 559  total_loss: 0.564  loss_cls: 0.172  loss_box_reg: 0.325  loss_rpn_cls: 0.009  loss_rpn_loc: 0.056  time: 0.5667  data_time: 0.0068  lr: 0.002797  max_mem: 3067M
[32m[04/15 11:09:20 d2.utils.events]: [0m eta: 0:13:09  iter: 579  total_loss: 0.595  loss_cls: 0.195  loss_box_reg: 0.338  loss_rpn_cls: 0.014  loss_rpn_loc: 0.061  time: 0.5657  data_time: 0.0065  lr: 0.002897  max_mem: 3067M
[32m[04/15 11:09:32 d2.utils.events]: [0m eta: 0:12:59  iter: 599  total_loss: 0.589  loss_cls: 0.171  loss_box_reg: 0.278  loss_rpn_cls: 0.009  loss_rpn_loc: 0.048  time: 0.5670  data_time: 0.0095  lr: 0.002997  max_mem: 3067M
[32m[04/15 11:09:43 d2.utils.events]: [0m eta: 0:12:46  iter: 619  total_loss: 0.649  loss_cls: 0.187  loss_box_reg: 0.367  loss_rpn_cls: 0.011  loss_rpn_loc: 0.060  time: 0.5655  data_time: 0.0099  lr: 0.003097  max_mem: 3067M
[32m[04/15 11:09:54 d2.utils.events]: [0m eta: 0:12:34  iter: 639  total_loss: 0.464  loss_cls: 0.153  loss_box_reg: 0.285  loss_rpn_cls: 0.010  loss_rpn_loc: 0.053  time: 0.5651  data_time: 0.0067  lr: 0.003197  max_mem: 3067M
[32m[04/15 11:10:07 d2.utils.events]: [0m eta: 0:12:25  iter: 659  total_loss: 0.571  loss_cls: 0.196  loss_box_reg: 0.343  loss_rpn_cls: 0.007  loss_rpn_loc: 0.036  time: 0.5672  data_time: 0.0076  lr: 0.003297  max_mem: 3067M
[32m[04/15 11:10:17 d2.utils.events]: [0m eta: 0:12:13  iter: 679  total_loss: 0.674  loss_cls: 0.208  loss_box_reg: 0.400  loss_rpn_cls: 0.013  loss_rpn_loc: 0.062  time: 0.5663  data_time: 0.0067  lr: 0.003397  max_mem: 3067M
[32m[04/15 11:10:28 d2.utils.events]: [0m eta: 0:12:00  iter: 699  total_loss: 0.559  loss_cls: 0.170  loss_box_reg: 0.330  loss_rpn_cls: 0.008  loss_rpn_loc: 0.049  time: 0.5654  data_time: 0.0079  lr: 0.003497  max_mem: 3067M
[32m[04/15 11:10:40 d2.utils.events]: [0m eta: 0:11:50  iter: 719  total_loss: 0.653  loss_cls: 0.202  loss_box_reg: 0.345  loss_rpn_cls: 0.017  loss_rpn_loc: 0.055  time: 0.5664  data_time: 0.0084  lr: 0.003596  max_mem: 3067M
[32m[04/15 11:10:52 d2.utils.events]: [0m eta: 0:11:40  iter: 739  total_loss: 0.622  loss_cls: 0.184  loss_box_reg: 0.354  loss_rpn_cls: 0.014  loss_rpn_loc: 0.050  time: 0.5667  data_time: 0.0081  lr: 0.003696  max_mem: 3067M
[32m[04/15 11:11:03 d2.utils.events]: [0m eta: 0:11:27  iter: 759  total_loss: 0.569  loss_cls: 0.171  loss_box_reg: 0.314  loss_rpn_cls: 0.008  loss_rpn_loc: 0.045  time: 0.5660  data_time: 0.0066  lr: 0.003796  max_mem: 3067M
[32m[04/15 11:11:15 d2.utils.events]: [0m eta: 0:11:17  iter: 779  total_loss: 0.664  loss_cls: 0.207  loss_box_reg: 0.376  loss_rpn_cls: 0.018  loss_rpn_loc: 0.060  time: 0.5665  data_time: 0.0095  lr: 0.003896  max_mem: 3067M
[32m[04/15 11:11:26 d2.utils.events]: [0m eta: 0:11:05  iter: 799  total_loss: 0.621  loss_cls: 0.220  loss_box_reg: 0.341  loss_rpn_cls: 0.010  loss_rpn_loc: 0.056  time: 0.5661  data_time: 0.0071  lr: 0.003996  max_mem: 3067M
[32m[04/15 11:11:36 d2.utils.events]: [0m eta: 0:10:53  iter: 819  total_loss: 0.578  loss_cls: 0.176  loss_box_reg: 0.347  loss_rpn_cls: 0.015  loss_rpn_loc: 0.053  time: 0.5652  data_time: 0.0080  lr: 0.004096  max_mem: 3067M
[32m[04/15 11:11:48 d2.utils.events]: [0m eta: 0:10:42  iter: 839  total_loss: 0.602  loss_cls: 0.188  loss_box_reg: 0.333  loss_rpn_cls: 0.011  loss_rpn_loc: 0.045  time: 0.5655  data_time: 0.0071  lr: 0.004196  max_mem: 3067M
[32m[04/15 11:12:00 d2.utils.events]: [0m eta: 0:10:31  iter: 859  total_loss: 0.684  loss_cls: 0.217  loss_box_reg: 0.406  loss_rpn_cls: 0.023  loss_rpn_loc: 0.054  time: 0.5660  data_time: 0.0120  lr: 0.004296  max_mem: 3067M
[32m[04/15 11:12:11 d2.utils.events]: [0m eta: 0:10:18  iter: 879  total_loss: 0.669  loss_cls: 0.205  loss_box_reg: 0.388  loss_rpn_cls: 0.011  loss_rpn_loc: 0.053  time: 0.5653  data_time: 0.0082  lr: 0.004396  max_mem: 3067M
[32m[04/15 11:12:23 d2.utils.events]: [0m eta: 0:10:07  iter: 899  total_loss: 0.595  loss_cls: 0.212  loss_box_reg: 0.319  loss_rpn_cls: 0.011  loss_rpn_loc: 0.053  time: 0.5669  data_time: 0.1656  lr: 0.004496  max_mem: 3067M
[32m[04/15 11:12:35 d2.utils.events]: [0m eta: 0:09:56  iter: 919  total_loss: 0.627  loss_cls: 0.200  loss_box_reg: 0.342  loss_rpn_cls: 0.012  loss_rpn_loc: 0.069  time: 0.5667  data_time: 0.0160  lr: 0.004595  max_mem: 3067M
[32m[04/15 11:12:46 d2.utils.events]: [0m eta: 0:09:46  iter: 939  total_loss: 0.641  loss_cls: 0.213  loss_box_reg: 0.369  loss_rpn_cls: 0.012  loss_rpn_loc: 0.051  time: 0.5668  data_time: 0.0121  lr: 0.004695  max_mem: 3067M
[32m[04/15 11:12:58 d2.utils.events]: [0m eta: 0:09:36  iter: 959  total_loss: 0.626  loss_cls: 0.184  loss_box_reg: 0.344  loss_rpn_cls: 0.013  loss_rpn_loc: 0.055  time: 0.5672  data_time: 0.0072  lr: 0.004795  max_mem: 3067M
[32m[04/15 11:13:09 d2.utils.events]: [0m eta: 0:09:24  iter: 979  total_loss: 0.658  loss_cls: 0.220  loss_box_reg: 0.365  loss_rpn_cls: 0.022  loss_rpn_loc: 0.059  time: 0.5665  data_time: 0.0073  lr: 0.004895  max_mem: 3067M
[32m[04/15 11:13:21 d2.utils.events]: [0m eta: 0:09:14  iter: 999  total_loss: 0.655  loss_cls: 0.214  loss_box_reg: 0.374  loss_rpn_cls: 0.015  loss_rpn_loc: 0.055  time: 0.5672  data_time: 0.0067  lr: 0.004995  max_mem: 3067M
[32m[04/15 11:13:32 d2.utils.events]: [0m eta: 0:09:03  iter: 1019  total_loss: 0.491  loss_cls: 0.164  loss_box_reg: 0.276  loss_rpn_cls: 0.013  loss_rpn_loc: 0.049  time: 0.5667  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:13:43 d2.utils.events]: [0m eta: 0:08:51  iter: 1039  total_loss: 0.725  loss_cls: 0.222  loss_box_reg: 0.408  loss_rpn_cls: 0.024  loss_rpn_loc: 0.063  time: 0.5663  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:13:55 d2.utils.events]: [0m eta: 0:08:39  iter: 1059  total_loss: 0.541  loss_cls: 0.179  loss_box_reg: 0.297  loss_rpn_cls: 0.012  loss_rpn_loc: 0.059  time: 0.5663  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:14:06 d2.utils.events]: [0m eta: 0:08:28  iter: 1079  total_loss: 0.611  loss_cls: 0.201  loss_box_reg: 0.300  loss_rpn_cls: 0.020  loss_rpn_loc: 0.053  time: 0.5664  data_time: 0.0092  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:14:17 d2.utils.events]: [0m eta: 0:08:16  iter: 1099  total_loss: 0.638  loss_cls: 0.212  loss_box_reg: 0.362  loss_rpn_cls: 0.016  loss_rpn_loc: 0.056  time: 0.5657  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:14:28 d2.utils.events]: [0m eta: 0:08:05  iter: 1119  total_loss: 0.674  loss_cls: 0.224  loss_box_reg: 0.374  loss_rpn_cls: 0.010  loss_rpn_loc: 0.053  time: 0.5656  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:14:40 d2.utils.events]: [0m eta: 0:07:54  iter: 1139  total_loss: 0.654  loss_cls: 0.201  loss_box_reg: 0.356  loss_rpn_cls: 0.017  loss_rpn_loc: 0.051  time: 0.5662  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:14:51 d2.utils.events]: [0m eta: 0:07:41  iter: 1159  total_loss: 0.548  loss_cls: 0.172  loss_box_reg: 0.296  loss_rpn_cls: 0.011  loss_rpn_loc: 0.046  time: 0.5652  data_time: 0.0100  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:15:02 d2.utils.events]: [0m eta: 0:07:30  iter: 1179  total_loss: 0.620  loss_cls: 0.210  loss_box_reg: 0.350  loss_rpn_cls: 0.013  loss_rpn_loc: 0.055  time: 0.5648  data_time: 0.0102  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:15:14 d2.utils.events]: [0m eta: 0:07:19  iter: 1199  total_loss: 0.648  loss_cls: 0.198  loss_box_reg: 0.366  loss_rpn_cls: 0.012  loss_rpn_loc: 0.058  time: 0.5656  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:15:25 d2.utils.events]: [0m eta: 0:07:09  iter: 1219  total_loss: 0.588  loss_cls: 0.172  loss_box_reg: 0.340  loss_rpn_cls: 0.016  loss_rpn_loc: 0.046  time: 0.5653  data_time: 0.0098  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:15:36 d2.utils.events]: [0m eta: 0:06:58  iter: 1239  total_loss: 0.614  loss_cls: 0.189  loss_box_reg: 0.337  loss_rpn_cls: 0.018  loss_rpn_loc: 0.050  time: 0.5652  data_time: 0.0102  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:15:49 d2.utils.events]: [0m eta: 0:06:47  iter: 1259  total_loss: 0.542  loss_cls: 0.169  loss_box_reg: 0.316  loss_rpn_cls: 0.014  loss_rpn_loc: 0.051  time: 0.5661  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:16:00 d2.utils.events]: [0m eta: 0:06:36  iter: 1279  total_loss: 0.672  loss_cls: 0.220  loss_box_reg: 0.369  loss_rpn_cls: 0.018  loss_rpn_loc: 0.050  time: 0.5662  data_time: 0.0118  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:16:11 d2.utils.events]: [0m eta: 0:06:25  iter: 1299  total_loss: 0.604  loss_cls: 0.174  loss_box_reg: 0.346  loss_rpn_cls: 0.012  loss_rpn_loc: 0.048  time: 0.5658  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:16:23 d2.utils.events]: [0m eta: 0:06:14  iter: 1319  total_loss: 0.727  loss_cls: 0.259  loss_box_reg: 0.391  loss_rpn_cls: 0.020  loss_rpn_loc: 0.057  time: 0.5664  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:16:35 d2.utils.events]: [0m eta: 0:06:02  iter: 1339  total_loss: 0.643  loss_cls: 0.187  loss_box_reg: 0.348  loss_rpn_cls: 0.021  loss_rpn_loc: 0.062  time: 0.5663  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:16:45 d2.utils.events]: [0m eta: 0:05:51  iter: 1359  total_loss: 0.554  loss_cls: 0.159  loss_box_reg: 0.282  loss_rpn_cls: 0.015  loss_rpn_loc: 0.060  time: 0.5659  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:16:57 d2.utils.events]: [0m eta: 0:05:40  iter: 1379  total_loss: 0.657  loss_cls: 0.192  loss_box_reg: 0.355  loss_rpn_cls: 0.016  loss_rpn_loc: 0.062  time: 0.5661  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:17:08 d2.utils.events]: [0m eta: 0:05:29  iter: 1399  total_loss: 0.506  loss_cls: 0.169  loss_box_reg: 0.263  loss_rpn_cls: 0.012  loss_rpn_loc: 0.040  time: 0.5659  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:17:19 d2.utils.events]: [0m eta: 0:05:19  iter: 1419  total_loss: 0.610  loss_cls: 0.206  loss_box_reg: 0.333  loss_rpn_cls: 0.022  loss_rpn_loc: 0.050  time: 0.5656  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:17:31 d2.utils.events]: [0m eta: 0:05:08  iter: 1439  total_loss: 0.719  loss_cls: 0.235  loss_box_reg: 0.384  loss_rpn_cls: 0.024  loss_rpn_loc: 0.049  time: 0.5657  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:17:43 d2.utils.events]: [0m eta: 0:04:56  iter: 1459  total_loss: 0.578  loss_cls: 0.180  loss_box_reg: 0.310  loss_rpn_cls: 0.013  loss_rpn_loc: 0.044  time: 0.5663  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:17:54 d2.utils.events]: [0m eta: 0:04:45  iter: 1479  total_loss: 0.707  loss_cls: 0.214  loss_box_reg: 0.372  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5657  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:18:05 d2.utils.events]: [0m eta: 0:04:34  iter: 1499  total_loss: 0.705  loss_cls: 0.230  loss_box_reg: 0.403  loss_rpn_cls: 0.019  loss_rpn_loc: 0.063  time: 0.5656  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:18:18 d2.utils.events]: [0m eta: 0:04:24  iter: 1519  total_loss: 0.642  loss_cls: 0.198  loss_box_reg: 0.390  loss_rpn_cls: 0.012  loss_rpn_loc: 0.047  time: 0.5664  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:18:29 d2.utils.events]: [0m eta: 0:04:12  iter: 1539  total_loss: 0.502  loss_cls: 0.163  loss_box_reg: 0.310  loss_rpn_cls: 0.013  loss_rpn_loc: 0.047  time: 0.5661  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:18:39 d2.utils.events]: [0m eta: 0:04:01  iter: 1559  total_loss: 0.565  loss_cls: 0.195  loss_box_reg: 0.318  loss_rpn_cls: 0.017  loss_rpn_loc: 0.042  time: 0.5658  data_time: 0.0102  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:18:52 d2.utils.events]: [0m eta: 0:03:51  iter: 1579  total_loss: 0.595  loss_cls: 0.195  loss_box_reg: 0.317  loss_rpn_cls: 0.023  loss_rpn_loc: 0.064  time: 0.5662  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:19:03 d2.utils.events]: [0m eta: 0:03:40  iter: 1599  total_loss: 0.621  loss_cls: 0.199  loss_box_reg: 0.317  loss_rpn_cls: 0.022  loss_rpn_loc: 0.049  time: 0.5663  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:19:14 d2.utils.events]: [0m eta: 0:03:29  iter: 1619  total_loss: 0.567  loss_cls: 0.178  loss_box_reg: 0.309  loss_rpn_cls: 0.013  loss_rpn_loc: 0.056  time: 0.5660  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:19:25 d2.utils.events]: [0m eta: 0:03:18  iter: 1639  total_loss: 0.587  loss_cls: 0.181  loss_box_reg: 0.324  loss_rpn_cls: 0.014  loss_rpn_loc: 0.051  time: 0.5659  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:19:37 d2.utils.events]: [0m eta: 0:03:07  iter: 1659  total_loss: 0.507  loss_cls: 0.165  loss_box_reg: 0.283  loss_rpn_cls: 0.009  loss_rpn_loc: 0.041  time: 0.5660  data_time: 0.0096  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:19:48 d2.utils.events]: [0m eta: 0:02:56  iter: 1679  total_loss: 0.567  loss_cls: 0.182  loss_box_reg: 0.333  loss_rpn_cls: 0.015  loss_rpn_loc: 0.063  time: 0.5659  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:20:00 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.628  loss_cls: 0.190  loss_box_reg: 0.350  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5661  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:20:12 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.579  loss_cls: 0.187  loss_box_reg: 0.332  loss_rpn_cls: 0.015  loss_rpn_loc: 0.046  time: 0.5665  data_time: 0.0103  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:20:23 d2.utils.events]: [0m eta: 0:02:23  iter: 1739  total_loss: 0.671  loss_cls: 0.225  loss_box_reg: 0.368  loss_rpn_cls: 0.016  loss_rpn_loc: 0.066  time: 0.5662  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:20:34 d2.utils.events]: [0m eta: 0:02:12  iter: 1759  total_loss: 0.618  loss_cls: 0.188  loss_box_reg: 0.364  loss_rpn_cls: 0.016  loss_rpn_loc: 0.058  time: 0.5661  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:20:46 d2.utils.events]: [0m eta: 0:02:01  iter: 1779  total_loss: 0.528  loss_cls: 0.182  loss_box_reg: 0.282  loss_rpn_cls: 0.014  loss_rpn_loc: 0.038  time: 0.5664  data_time: 0.0106  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:20:58 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.668  loss_cls: 0.216  loss_box_reg: 0.368  loss_rpn_cls: 0.021  loss_rpn_loc: 0.055  time: 0.5664  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:21:09 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.667  loss_cls: 0.219  loss_box_reg: 0.338  loss_rpn_cls: 0.016  loss_rpn_loc: 0.061  time: 0.5663  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:21:21 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.592  loss_cls: 0.193  loss_box_reg: 0.332  loss_rpn_cls: 0.018  loss_rpn_loc: 0.058  time: 0.5668  data_time: 0.0136  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:21:32 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.638  loss_cls: 0.208  loss_box_reg: 0.356  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5667  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:21:43 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.576  loss_cls: 0.164  loss_box_reg: 0.305  loss_rpn_cls: 0.014  loss_rpn_loc: 0.046  time: 0.5663  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:21:55 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.665  loss_cls: 0.208  loss_box_reg: 0.375  loss_rpn_cls: 0.023  loss_rpn_loc: 0.062  time: 0.5665  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:22:06 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.705  loss_cls: 0.220  loss_box_reg: 0.361  loss_rpn_cls: 0.017  loss_rpn_loc: 0.083  time: 0.5665  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:22:18 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.671  loss_cls: 0.209  loss_box_reg: 0.365  loss_rpn_cls: 0.015  loss_rpn_loc: 0.054  time: 0.5665  data_time: 0.0092  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:22:29 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.596  loss_cls: 0.192  loss_box_reg: 0.347  loss_rpn_cls: 0.016  loss_rpn_loc: 0.059  time: 0.5666  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:22:41 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.574  loss_cls: 0.191  loss_box_reg: 0.319  loss_rpn_cls: 0.016  loss_rpn_loc: 0.049  time: 0.5666  data_time: 0.0095  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 11:23:03 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:23:03 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 11:23:04 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 11:23:04 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.721  loss_cls: 0.211  loss_box_reg: 0.404  loss_rpn_cls: 0.015  loss_rpn_loc: 0.061  time: 0.5664  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:23:06 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:51 (0.5667 s / it)
[32m[04/15 11:23:06 d2.engine.hooks]: [0mTotal training time: 0:19:15 (0:00:23 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 11:23:13 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:23:13 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 11:23:13 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 11:23:18 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1281 s / img. ETA=0:02:45
[32m[04/15 11:23:23 d2.evaluation.evaluator]: [0mInference done 43/1257. 0.1228 s / img. ETA=0:03:09
[32m[04/15 11:23:28 d2.evaluation.evaluator]: [0mInference done 78/1257. 0.1225 s / img. ETA=0:02:56
[32m[04/15 11:23:33 d2.evaluation.evaluator]: [0mInference done 111/1257. 0.1226 s / img. ETA=0:02:53
[32m[04/15 11:23:38 d2.evaluation.evaluator]: [0mInference done 143/1257. 0.1220 s / img. ETA=0:02:49
[32m[04/15 11:23:43 d2.evaluation.evaluator]: [0mInference done 179/1257. 0.1236 s / img. ETA=0:02:42
[32m[04/15 11:23:48 d2.evaluation.evaluator]: [0mInference done 212/1257. 0.1274 s / img. ETA=0:02:37
[32m[04/15 11:23:53 d2.evaluation.evaluator]: [0mInference done 248/1257. 0.1279 s / img. ETA=0:02:30
[32m[04/15 11:23:58 d2.evaluation.evaluator]: [0mInference done 286/1257. 0.1270 s / img. ETA=0:02:23
[32m[04/15 11:24:03 d2.evaluation.evaluator]: [0mInference done 324/1257. 0.1263 s / img. ETA=0:02:15
[32m[04/15 11:24:08 d2.evaluation.evaluator]: [0mInference done 363/1257. 0.1257 s / img. ETA=0:02:08
[32m[04/15 11:24:13 d2.evaluation.evaluator]: [0mInference done 400/1257. 0.1257 s / img. ETA=0:02:02
[32m[04/15 11:24:18 d2.evaluation.evaluator]: [0mInference done 433/1257. 0.1254 s / img. ETA=0:01:58
[32m[04/15 11:24:23 d2.evaluation.evaluator]: [0mInference done 459/1257. 0.1263 s / img. ETA=0:01:57
[32m[04/15 11:24:28 d2.evaluation.evaluator]: [0mInference done 483/1257. 0.1269 s / img. ETA=0:01:56
[32m[04/15 11:24:34 d2.evaluation.evaluator]: [0mInference done 508/1257. 0.1265 s / img. ETA=0:01:54
[32m[04/15 11:24:39 d2.evaluation.evaluator]: [0mInference done 539/1257. 0.1261 s / img. ETA=0:01:50
[32m[04/15 11:24:44 d2.evaluation.evaluator]: [0mInference done 567/1257. 0.1259 s / img. ETA=0:01:46
[32m[04/15 11:24:49 d2.evaluation.evaluator]: [0mInference done 600/1257. 0.1258 s / img. ETA=0:01:41
[32m[04/15 11:24:54 d2.evaluation.evaluator]: [0mInference done 626/1257. 0.1256 s / img. ETA=0:01:38
[32m[04/15 11:24:59 d2.evaluation.evaluator]: [0mInference done 648/1257. 0.1258 s / img. ETA=0:01:37
[32m[04/15 11:25:04 d2.evaluation.evaluator]: [0mInference done 675/1257. 0.1261 s / img. ETA=0:01:33
[32m[04/15 11:25:09 d2.evaluation.evaluator]: [0mInference done 698/1257. 0.1259 s / img. ETA=0:01:30
[32m[04/15 11:25:15 d2.evaluation.evaluator]: [0mInference done 718/1257. 0.1257 s / img. ETA=0:01:29
[32m[04/15 11:25:20 d2.evaluation.evaluator]: [0mInference done 745/1257. 0.1255 s / img. ETA=0:01:24
[32m[04/15 11:25:25 d2.evaluation.evaluator]: [0mInference done 783/1257. 0.1254 s / img. ETA=0:01:17
[32m[04/15 11:25:30 d2.evaluation.evaluator]: [0mInference done 821/1257. 0.1256 s / img. ETA=0:01:11
[32m[04/15 11:25:35 d2.evaluation.evaluator]: [0mInference done 858/1257. 0.1258 s / img. ETA=0:01:04
[32m[04/15 11:25:40 d2.evaluation.evaluator]: [0mInference done 892/1257. 0.1265 s / img. ETA=0:00:58
[32m[04/15 11:25:45 d2.evaluation.evaluator]: [0mInference done 926/1257. 0.1272 s / img. ETA=0:00:53
[32m[04/15 11:25:50 d2.evaluation.evaluator]: [0mInference done 966/1257. 0.1270 s / img. ETA=0:00:46
[32m[04/15 11:25:55 d2.evaluation.evaluator]: [0mInference done 1006/1257. 0.1267 s / img. ETA=0:00:39
[32m[04/15 11:26:00 d2.evaluation.evaluator]: [0mInference done 1045/1257. 0.1266 s / img. ETA=0:00:33
[32m[04/15 11:26:05 d2.evaluation.evaluator]: [0mInference done 1085/1257. 0.1264 s / img. ETA=0:00:26
[32m[04/15 11:26:10 d2.evaluation.evaluator]: [0mInference done 1124/1257. 0.1264 s / img. ETA=0:00:20
[32m[04/15 11:26:15 d2.evaluation.evaluator]: [0mInference done 1162/1257. 0.1264 s / img. ETA=0:00:14
[32m[04/15 11:26:20 d2.evaluation.evaluator]: [0mInference done 1197/1257. 0.1267 s / img. ETA=0:00:09
[32m[04/15 11:26:25 d2.evaluation.evaluator]: [0mInference done 1232/1257. 0.1272 s / img. ETA=0:00:03
[32m[04/15 11:26:29 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:12.324944 (0.153614 s / img per device, on 1 devices)
[32m[04/15 11:26:29 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:39 (0.127355 s / img per device, on 1 devices)
[32m[04/15 11:26:29 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 11:26:29 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 11:26:29 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.05s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.74s).
Accumulating evaluation results...
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.204
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.423
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.167
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.251
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.445
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.102
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.256
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.259
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.189
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.304
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.484
[32m[04/15 11:26:37 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 20.364 | 42.274 | 16.734 | 12.979 | 25.079 | 44.486 |
[32m[04/15 11:26:37 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP     | category       | AP     |
|:--------------|:-------|:--------------|:-------|:---------------|:-------|
| person        | 28.683 | bicycle       | 10.702 | car            | 42.069 |
| motorcycle    | nan    | airplane      | nan    | bus            | nan    |
| train         | nan    | truck         | nan    | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan    | stop sign      | nan    |
| parking meter | nan    | bench         | nan    | bird           | nan    |
| cat           | nan    | dog           | 0.000  | horse          | nan    |
| sheep         | nan    | cow           | nan    | elephant       | nan    |
| bear          | nan    | zebra         | nan    | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan    | handbag        | nan    |
| tie           | nan    | suitcase      | nan    | frisbee        | nan    |
| skis          | nan    | snowboard     | nan    | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan    | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan    | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan    | cup            | nan    |
| fork          | nan    | knife         | nan    | spoon          | nan    |
| bowl          | nan    | banana        | nan    | apple          | nan    |
| sandwich      | nan    | orange        | nan    | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan    | pizza          | nan    |
| donut         | nan    | cake          | nan    | chair          | nan    |
| couch         | nan    | potted plant  | nan    | bed            | nan    |
| dining table  | nan    | toilet        | nan    | tv             | nan    |
| laptop        | nan    | mouse         | nan    | remote         | nan    |
| keyboard      | nan    | cell phone    | nan    | microwave      | nan    |
| oven          | nan    | bike          | nan    | hydrant        | nan    |
| motor         | nan    | rider         | nan    | light          | nan    |
| sign          | nan    | motor vehicle | nan    | human face     | nan    |
| hair drier    | nan    | license plate | nan    |                |        |
============== The  21  *  2000  iterations ============
4 channel input
[32m[04/15 11:26:38 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 11:26:40 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.66 seconds.
[5m[31mWARNING[0m [32m[04/15 11:26:40 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:26:40 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 11:26:40 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 11:26:41 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 11:26:41 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 11:26:44 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 11:26:56 d2.utils.events]: [0m eta: 0:17:39  iter: 19  total_loss: 0.682  loss_cls: 0.201  loss_box_reg: 0.390  loss_rpn_cls: 0.021  loss_rpn_loc: 0.058  time: 0.5335  data_time: 0.0573  lr: 0.000100  max_mem: 3067M
[32m[04/15 11:27:08 d2.utils.events]: [0m eta: 0:18:17  iter: 39  total_loss: 0.678  loss_cls: 0.203  loss_box_reg: 0.379  loss_rpn_cls: 0.016  loss_rpn_loc: 0.062  time: 0.5742  data_time: 0.0081  lr: 0.000200  max_mem: 3067M
[32m[04/15 11:27:19 d2.utils.events]: [0m eta: 0:17:33  iter: 59  total_loss: 0.711  loss_cls: 0.219  loss_box_reg: 0.393  loss_rpn_cls: 0.018  loss_rpn_loc: 0.064  time: 0.5619  data_time: 0.0092  lr: 0.000300  max_mem: 3067M
[32m[04/15 11:27:31 d2.utils.events]: [0m eta: 0:17:35  iter: 79  total_loss: 0.575  loss_cls: 0.175  loss_box_reg: 0.363  loss_rpn_cls: 0.009  loss_rpn_loc: 0.043  time: 0.5648  data_time: 0.0081  lr: 0.000400  max_mem: 3067M
[32m[04/15 11:27:42 d2.utils.events]: [0m eta: 0:17:35  iter: 99  total_loss: 0.549  loss_cls: 0.179  loss_box_reg: 0.304  loss_rpn_cls: 0.013  loss_rpn_loc: 0.042  time: 0.5685  data_time: 0.0075  lr: 0.000500  max_mem: 3067M
[32m[04/15 11:27:54 d2.utils.events]: [0m eta: 0:17:26  iter: 119  total_loss: 0.675  loss_cls: 0.210  loss_box_reg: 0.370  loss_rpn_cls: 0.019  loss_rpn_loc: 0.064  time: 0.5715  data_time: 0.0082  lr: 0.000599  max_mem: 3067M
[32m[04/15 11:28:05 d2.utils.events]: [0m eta: 0:17:10  iter: 139  total_loss: 0.582  loss_cls: 0.173  loss_box_reg: 0.335  loss_rpn_cls: 0.010  loss_rpn_loc: 0.043  time: 0.5680  data_time: 0.0070  lr: 0.000699  max_mem: 3067M
[32m[04/15 11:28:21 d2.utils.events]: [0m eta: 0:17:04  iter: 159  total_loss: 0.573  loss_cls: 0.185  loss_box_reg: 0.333  loss_rpn_cls: 0.010  loss_rpn_loc: 0.047  time: 0.5920  data_time: 0.2594  lr: 0.000799  max_mem: 3067M
[32m[04/15 11:28:32 d2.utils.events]: [0m eta: 0:16:54  iter: 179  total_loss: 0.556  loss_cls: 0.168  loss_box_reg: 0.322  loss_rpn_cls: 0.013  loss_rpn_loc: 0.052  time: 0.5890  data_time: 0.0377  lr: 0.000899  max_mem: 3067M
[32m[04/15 11:28:44 d2.utils.events]: [0m eta: 0:16:42  iter: 199  total_loss: 0.549  loss_cls: 0.170  loss_box_reg: 0.324  loss_rpn_cls: 0.012  loss_rpn_loc: 0.048  time: 0.5866  data_time: 0.0071  lr: 0.000999  max_mem: 3067M
[32m[04/15 11:28:56 d2.utils.events]: [0m eta: 0:16:40  iter: 219  total_loss: 0.649  loss_cls: 0.201  loss_box_reg: 0.371  loss_rpn_cls: 0.012  loss_rpn_loc: 0.053  time: 0.5882  data_time: 0.0074  lr: 0.001099  max_mem: 3067M
[32m[04/15 11:29:07 d2.utils.events]: [0m eta: 0:16:29  iter: 239  total_loss: 0.604  loss_cls: 0.187  loss_box_reg: 0.330  loss_rpn_cls: 0.013  loss_rpn_loc: 0.050  time: 0.5860  data_time: 0.0075  lr: 0.001199  max_mem: 3067M
[32m[04/15 11:29:19 d2.utils.events]: [0m eta: 0:16:17  iter: 259  total_loss: 0.555  loss_cls: 0.170  loss_box_reg: 0.319  loss_rpn_cls: 0.012  loss_rpn_loc: 0.044  time: 0.5861  data_time: 0.0087  lr: 0.001299  max_mem: 3067M
[32m[04/15 11:29:30 d2.utils.events]: [0m eta: 0:16:01  iter: 279  total_loss: 0.572  loss_cls: 0.196  loss_box_reg: 0.324  loss_rpn_cls: 0.010  loss_rpn_loc: 0.043  time: 0.5829  data_time: 0.0076  lr: 0.001399  max_mem: 3067M
[32m[04/15 11:29:42 d2.utils.events]: [0m eta: 0:15:52  iter: 299  total_loss: 0.516  loss_cls: 0.165  loss_box_reg: 0.296  loss_rpn_cls: 0.008  loss_rpn_loc: 0.047  time: 0.5820  data_time: 0.0080  lr: 0.001499  max_mem: 3067M
[32m[04/15 11:29:54 d2.utils.events]: [0m eta: 0:15:45  iter: 319  total_loss: 0.701  loss_cls: 0.211  loss_box_reg: 0.382  loss_rpn_cls: 0.016  loss_rpn_loc: 0.055  time: 0.5834  data_time: 0.0077  lr: 0.001598  max_mem: 3067M
[32m[04/15 11:30:05 d2.utils.events]: [0m eta: 0:15:27  iter: 339  total_loss: 0.656  loss_cls: 0.197  loss_box_reg: 0.364  loss_rpn_cls: 0.017  loss_rpn_loc: 0.052  time: 0.5804  data_time: 0.0071  lr: 0.001698  max_mem: 3067M
[32m[04/15 11:30:16 d2.utils.events]: [0m eta: 0:15:15  iter: 359  total_loss: 0.587  loss_cls: 0.188  loss_box_reg: 0.345  loss_rpn_cls: 0.016  loss_rpn_loc: 0.045  time: 0.5781  data_time: 0.0082  lr: 0.001798  max_mem: 3067M
[32m[04/15 11:30:28 d2.utils.events]: [0m eta: 0:15:06  iter: 379  total_loss: 0.533  loss_cls: 0.186  loss_box_reg: 0.339  loss_rpn_cls: 0.011  loss_rpn_loc: 0.045  time: 0.5791  data_time: 0.0082  lr: 0.001898  max_mem: 3067M
[32m[04/15 11:30:39 d2.utils.events]: [0m eta: 0:14:55  iter: 399  total_loss: 0.503  loss_cls: 0.154  loss_box_reg: 0.280  loss_rpn_cls: 0.009  loss_rpn_loc: 0.045  time: 0.5789  data_time: 0.0093  lr: 0.001998  max_mem: 3067M
[32m[04/15 11:30:50 d2.utils.events]: [0m eta: 0:14:42  iter: 419  total_loss: 0.561  loss_cls: 0.178  loss_box_reg: 0.308  loss_rpn_cls: 0.013  loss_rpn_loc: 0.044  time: 0.5770  data_time: 0.0065  lr: 0.002098  max_mem: 3067M
[32m[04/15 11:31:02 d2.utils.events]: [0m eta: 0:14:32  iter: 439  total_loss: 0.641  loss_cls: 0.199  loss_box_reg: 0.363  loss_rpn_cls: 0.014  loss_rpn_loc: 0.043  time: 0.5771  data_time: 0.0064  lr: 0.002198  max_mem: 3067M
[32m[04/15 11:31:13 d2.utils.events]: [0m eta: 0:14:20  iter: 459  total_loss: 0.627  loss_cls: 0.198  loss_box_reg: 0.335  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5766  data_time: 0.0078  lr: 0.002298  max_mem: 3067M
[32m[04/15 11:31:24 d2.utils.events]: [0m eta: 0:14:08  iter: 479  total_loss: 0.522  loss_cls: 0.161  loss_box_reg: 0.301  loss_rpn_cls: 0.012  loss_rpn_loc: 0.053  time: 0.5755  data_time: 0.0072  lr: 0.002398  max_mem: 3067M
[32m[04/15 11:31:36 d2.utils.events]: [0m eta: 0:13:58  iter: 499  total_loss: 0.572  loss_cls: 0.175  loss_box_reg: 0.330  loss_rpn_cls: 0.011  loss_rpn_loc: 0.057  time: 0.5757  data_time: 0.0068  lr: 0.002498  max_mem: 3067M
[32m[04/15 11:31:48 d2.utils.events]: [0m eta: 0:13:48  iter: 519  total_loss: 0.503  loss_cls: 0.165  loss_box_reg: 0.306  loss_rpn_cls: 0.011  loss_rpn_loc: 0.040  time: 0.5767  data_time: 0.0080  lr: 0.002597  max_mem: 3067M
[32m[04/15 11:31:59 d2.utils.events]: [0m eta: 0:13:35  iter: 539  total_loss: 0.592  loss_cls: 0.189  loss_box_reg: 0.361  loss_rpn_cls: 0.014  loss_rpn_loc: 0.045  time: 0.5747  data_time: 0.0114  lr: 0.002697  max_mem: 3067M
[32m[04/15 11:32:10 d2.utils.events]: [0m eta: 0:13:23  iter: 559  total_loss: 0.632  loss_cls: 0.220  loss_box_reg: 0.340  loss_rpn_cls: 0.012  loss_rpn_loc: 0.047  time: 0.5739  data_time: 0.0068  lr: 0.002797  max_mem: 3067M
[32m[04/15 11:32:22 d2.utils.events]: [0m eta: 0:13:13  iter: 579  total_loss: 0.644  loss_cls: 0.176  loss_box_reg: 0.392  loss_rpn_cls: 0.010  loss_rpn_loc: 0.052  time: 0.5750  data_time: 0.0086  lr: 0.002897  max_mem: 3067M
[32m[04/15 11:32:33 d2.utils.events]: [0m eta: 0:12:58  iter: 599  total_loss: 0.649  loss_cls: 0.196  loss_box_reg: 0.362  loss_rpn_cls: 0.010  loss_rpn_loc: 0.052  time: 0.5731  data_time: 0.0057  lr: 0.002997  max_mem: 3067M
[32m[04/15 11:32:44 d2.utils.events]: [0m eta: 0:12:48  iter: 619  total_loss: 0.662  loss_cls: 0.207  loss_box_reg: 0.373  loss_rpn_cls: 0.011  loss_rpn_loc: 0.058  time: 0.5737  data_time: 0.0048  lr: 0.003097  max_mem: 3067M
[32m[04/15 11:32:55 d2.utils.events]: [0m eta: 0:12:36  iter: 639  total_loss: 0.527  loss_cls: 0.168  loss_box_reg: 0.276  loss_rpn_cls: 0.010  loss_rpn_loc: 0.051  time: 0.5725  data_time: 0.0061  lr: 0.003197  max_mem: 3067M
[32m[04/15 11:33:06 d2.utils.events]: [0m eta: 0:12:25  iter: 659  total_loss: 0.583  loss_cls: 0.191  loss_box_reg: 0.328  loss_rpn_cls: 0.013  loss_rpn_loc: 0.052  time: 0.5716  data_time: 0.0070  lr: 0.003297  max_mem: 3067M
[32m[04/15 11:33:17 d2.utils.events]: [0m eta: 0:12:13  iter: 679  total_loss: 0.605  loss_cls: 0.189  loss_box_reg: 0.338  loss_rpn_cls: 0.009  loss_rpn_loc: 0.062  time: 0.5705  data_time: 0.0066  lr: 0.003397  max_mem: 3067M
[32m[04/15 11:33:29 d2.utils.events]: [0m eta: 0:12:03  iter: 699  total_loss: 0.527  loss_cls: 0.170  loss_box_reg: 0.300  loss_rpn_cls: 0.013  loss_rpn_loc: 0.042  time: 0.5716  data_time: 0.0067  lr: 0.003497  max_mem: 3067M
[32m[04/15 11:33:41 d2.utils.events]: [0m eta: 0:11:52  iter: 719  total_loss: 0.670  loss_cls: 0.205  loss_box_reg: 0.362  loss_rpn_cls: 0.012  loss_rpn_loc: 0.048  time: 0.5713  data_time: 0.0064  lr: 0.003596  max_mem: 3067M
[32m[04/15 11:33:52 d2.utils.events]: [0m eta: 0:11:41  iter: 739  total_loss: 0.565  loss_cls: 0.181  loss_box_reg: 0.309  loss_rpn_cls: 0.010  loss_rpn_loc: 0.047  time: 0.5708  data_time: 0.0066  lr: 0.003696  max_mem: 3067M
[32m[04/15 11:34:04 d2.utils.events]: [0m eta: 0:11:31  iter: 759  total_loss: 0.571  loss_cls: 0.181  loss_box_reg: 0.330  loss_rpn_cls: 0.011  loss_rpn_loc: 0.063  time: 0.5712  data_time: 0.0086  lr: 0.003796  max_mem: 3067M
[32m[04/15 11:34:16 d2.utils.events]: [0m eta: 0:11:21  iter: 779  total_loss: 0.648  loss_cls: 0.210  loss_box_reg: 0.364  loss_rpn_cls: 0.013  loss_rpn_loc: 0.049  time: 0.5721  data_time: 0.0079  lr: 0.003896  max_mem: 3067M
[32m[04/15 11:34:27 d2.utils.events]: [0m eta: 0:11:10  iter: 799  total_loss: 0.653  loss_cls: 0.215  loss_box_reg: 0.348  loss_rpn_cls: 0.014  loss_rpn_loc: 0.077  time: 0.5713  data_time: 0.0099  lr: 0.003996  max_mem: 3067M
[32m[04/15 11:34:38 d2.utils.events]: [0m eta: 0:10:58  iter: 819  total_loss: 0.580  loss_cls: 0.187  loss_box_reg: 0.291  loss_rpn_cls: 0.013  loss_rpn_loc: 0.061  time: 0.5707  data_time: 0.0063  lr: 0.004096  max_mem: 3067M
[32m[04/15 11:34:50 d2.utils.events]: [0m eta: 0:10:48  iter: 839  total_loss: 0.665  loss_cls: 0.225  loss_box_reg: 0.346  loss_rpn_cls: 0.013  loss_rpn_loc: 0.063  time: 0.5718  data_time: 0.0075  lr: 0.004196  max_mem: 3067M
[32m[04/15 11:35:02 d2.utils.events]: [0m eta: 0:10:37  iter: 859  total_loss: 0.575  loss_cls: 0.188  loss_box_reg: 0.339  loss_rpn_cls: 0.016  loss_rpn_loc: 0.050  time: 0.5713  data_time: 0.0072  lr: 0.004296  max_mem: 3067M
[32m[04/15 11:35:13 d2.utils.events]: [0m eta: 0:10:24  iter: 879  total_loss: 0.545  loss_cls: 0.178  loss_box_reg: 0.327  loss_rpn_cls: 0.014  loss_rpn_loc: 0.060  time: 0.5707  data_time: 0.0062  lr: 0.004396  max_mem: 3067M
[32m[04/15 11:35:25 d2.utils.events]: [0m eta: 0:10:14  iter: 899  total_loss: 0.630  loss_cls: 0.218  loss_box_reg: 0.344  loss_rpn_cls: 0.015  loss_rpn_loc: 0.043  time: 0.5720  data_time: 0.0071  lr: 0.004496  max_mem: 3067M
[32m[04/15 11:35:36 d2.utils.events]: [0m eta: 0:10:03  iter: 919  total_loss: 0.663  loss_cls: 0.201  loss_box_reg: 0.378  loss_rpn_cls: 0.011  loss_rpn_loc: 0.053  time: 0.5712  data_time: 0.0076  lr: 0.004595  max_mem: 3067M
[32m[04/15 11:35:47 d2.utils.events]: [0m eta: 0:09:51  iter: 939  total_loss: 0.661  loss_cls: 0.201  loss_box_reg: 0.359  loss_rpn_cls: 0.016  loss_rpn_loc: 0.071  time: 0.5706  data_time: 0.0066  lr: 0.004695  max_mem: 3067M
[32m[04/15 11:35:59 d2.utils.events]: [0m eta: 0:09:41  iter: 959  total_loss: 0.546  loss_cls: 0.191  loss_box_reg: 0.294  loss_rpn_cls: 0.015  loss_rpn_loc: 0.047  time: 0.5716  data_time: 0.0077  lr: 0.004795  max_mem: 3067M
[32m[04/15 11:36:11 d2.utils.events]: [0m eta: 0:09:29  iter: 979  total_loss: 0.637  loss_cls: 0.192  loss_box_reg: 0.372  loss_rpn_cls: 0.015  loss_rpn_loc: 0.058  time: 0.5712  data_time: 0.0089  lr: 0.004895  max_mem: 3067M
[32m[04/15 11:36:21 d2.utils.events]: [0m eta: 0:09:17  iter: 999  total_loss: 0.634  loss_cls: 0.196  loss_box_reg: 0.349  loss_rpn_cls: 0.021  loss_rpn_loc: 0.062  time: 0.5704  data_time: 0.0112  lr: 0.004995  max_mem: 3067M
[32m[04/15 11:36:34 d2.utils.events]: [0m eta: 0:09:09  iter: 1019  total_loss: 0.649  loss_cls: 0.198  loss_box_reg: 0.358  loss_rpn_cls: 0.014  loss_rpn_loc: 0.053  time: 0.5714  data_time: 0.0104  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:36:45 d2.utils.events]: [0m eta: 0:08:56  iter: 1039  total_loss: 0.683  loss_cls: 0.205  loss_box_reg: 0.363  loss_rpn_cls: 0.019  loss_rpn_loc: 0.046  time: 0.5711  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:36:56 d2.utils.events]: [0m eta: 0:08:46  iter: 1059  total_loss: 0.585  loss_cls: 0.194  loss_box_reg: 0.349  loss_rpn_cls: 0.010  loss_rpn_loc: 0.045  time: 0.5707  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:37:08 d2.utils.events]: [0m eta: 0:08:35  iter: 1079  total_loss: 0.552  loss_cls: 0.165  loss_box_reg: 0.313  loss_rpn_cls: 0.018  loss_rpn_loc: 0.050  time: 0.5714  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:37:20 d2.utils.events]: [0m eta: 0:08:24  iter: 1099  total_loss: 0.545  loss_cls: 0.177  loss_box_reg: 0.284  loss_rpn_cls: 0.010  loss_rpn_loc: 0.053  time: 0.5717  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:37:31 d2.utils.events]: [0m eta: 0:08:11  iter: 1119  total_loss: 0.641  loss_cls: 0.232  loss_box_reg: 0.357  loss_rpn_cls: 0.020  loss_rpn_loc: 0.040  time: 0.5710  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:37:43 d2.utils.events]: [0m eta: 0:08:01  iter: 1139  total_loss: 0.596  loss_cls: 0.168  loss_box_reg: 0.332  loss_rpn_cls: 0.011  loss_rpn_loc: 0.034  time: 0.5712  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:37:55 d2.utils.events]: [0m eta: 0:07:50  iter: 1159  total_loss: 0.576  loss_cls: 0.189  loss_box_reg: 0.331  loss_rpn_cls: 0.011  loss_rpn_loc: 0.050  time: 0.5721  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:38:06 d2.utils.events]: [0m eta: 0:07:38  iter: 1179  total_loss: 0.614  loss_cls: 0.202  loss_box_reg: 0.338  loss_rpn_cls: 0.016  loss_rpn_loc: 0.046  time: 0.5714  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:38:17 d2.utils.events]: [0m eta: 0:07:27  iter: 1199  total_loss: 0.645  loss_cls: 0.200  loss_box_reg: 0.365  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5712  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:38:29 d2.utils.events]: [0m eta: 0:07:16  iter: 1219  total_loss: 0.612  loss_cls: 0.202  loss_box_reg: 0.363  loss_rpn_cls: 0.011  loss_rpn_loc: 0.037  time: 0.5718  data_time: 0.0105  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:38:40 d2.utils.events]: [0m eta: 0:07:04  iter: 1239  total_loss: 0.613  loss_cls: 0.192  loss_box_reg: 0.359  loss_rpn_cls: 0.009  loss_rpn_loc: 0.062  time: 0.5711  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:38:51 d2.utils.events]: [0m eta: 0:06:53  iter: 1259  total_loss: 0.665  loss_cls: 0.212  loss_box_reg: 0.365  loss_rpn_cls: 0.015  loss_rpn_loc: 0.064  time: 0.5708  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:39:04 d2.utils.events]: [0m eta: 0:06:42  iter: 1279  total_loss: 0.591  loss_cls: 0.191  loss_box_reg: 0.336  loss_rpn_cls: 0.011  loss_rpn_loc: 0.050  time: 0.5714  data_time: 0.0112  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:39:15 d2.utils.events]: [0m eta: 0:06:30  iter: 1299  total_loss: 0.511  loss_cls: 0.169  loss_box_reg: 0.301  loss_rpn_cls: 0.010  loss_rpn_loc: 0.036  time: 0.5712  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:39:26 d2.utils.events]: [0m eta: 0:06:19  iter: 1319  total_loss: 0.647  loss_cls: 0.204  loss_box_reg: 0.374  loss_rpn_cls: 0.019  loss_rpn_loc: 0.059  time: 0.5707  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:39:37 d2.utils.events]: [0m eta: 0:06:08  iter: 1339  total_loss: 0.587  loss_cls: 0.202  loss_box_reg: 0.297  loss_rpn_cls: 0.010  loss_rpn_loc: 0.043  time: 0.5703  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:39:49 d2.utils.events]: [0m eta: 0:05:57  iter: 1359  total_loss: 0.553  loss_cls: 0.183  loss_box_reg: 0.326  loss_rpn_cls: 0.013  loss_rpn_loc: 0.048  time: 0.5707  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:39:59 d2.utils.events]: [0m eta: 0:05:45  iter: 1379  total_loss: 0.653  loss_cls: 0.205  loss_box_reg: 0.377  loss_rpn_cls: 0.013  loss_rpn_loc: 0.056  time: 0.5700  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:40:10 d2.utils.events]: [0m eta: 0:05:33  iter: 1399  total_loss: 0.586  loss_cls: 0.181  loss_box_reg: 0.321  loss_rpn_cls: 0.016  loss_rpn_loc: 0.044  time: 0.5699  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:40:23 d2.utils.events]: [0m eta: 0:05:23  iter: 1419  total_loss: 0.617  loss_cls: 0.200  loss_box_reg: 0.307  loss_rpn_cls: 0.021  loss_rpn_loc: 0.046  time: 0.5705  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:40:34 d2.utils.events]: [0m eta: 0:05:11  iter: 1439  total_loss: 0.576  loss_cls: 0.167  loss_box_reg: 0.337  loss_rpn_cls: 0.010  loss_rpn_loc: 0.038  time: 0.5700  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:40:44 d2.utils.events]: [0m eta: 0:05:00  iter: 1459  total_loss: 0.613  loss_cls: 0.207  loss_box_reg: 0.332  loss_rpn_cls: 0.028  loss_rpn_loc: 0.055  time: 0.5694  data_time: 0.0062  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:40:57 d2.utils.events]: [0m eta: 0:04:50  iter: 1479  total_loss: 0.682  loss_cls: 0.206  loss_box_reg: 0.351  loss_rpn_cls: 0.016  loss_rpn_loc: 0.064  time: 0.5702  data_time: 0.0104  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:41:08 d2.utils.events]: [0m eta: 0:04:38  iter: 1499  total_loss: 0.779  loss_cls: 0.250  loss_box_reg: 0.415  loss_rpn_cls: 0.019  loss_rpn_loc: 0.087  time: 0.5698  data_time: 0.0106  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:41:19 d2.utils.events]: [0m eta: 0:04:27  iter: 1519  total_loss: 0.639  loss_cls: 0.216  loss_box_reg: 0.340  loss_rpn_cls: 0.016  loss_rpn_loc: 0.065  time: 0.5696  data_time: 0.0104  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:41:32 d2.utils.events]: [0m eta: 0:04:16  iter: 1539  total_loss: 0.718  loss_cls: 0.219  loss_box_reg: 0.390  loss_rpn_cls: 0.015  loss_rpn_loc: 0.081  time: 0.5704  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:41:42 d2.utils.events]: [0m eta: 0:04:05  iter: 1559  total_loss: 0.591  loss_cls: 0.203  loss_box_reg: 0.334  loss_rpn_cls: 0.014  loss_rpn_loc: 0.047  time: 0.5699  data_time: 0.0117  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:41:53 d2.utils.events]: [0m eta: 0:03:53  iter: 1579  total_loss: 0.582  loss_cls: 0.182  loss_box_reg: 0.310  loss_rpn_cls: 0.018  loss_rpn_loc: 0.051  time: 0.5693  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:42:05 d2.utils.events]: [0m eta: 0:03:43  iter: 1599  total_loss: 0.680  loss_cls: 0.209  loss_box_reg: 0.357  loss_rpn_cls: 0.020  loss_rpn_loc: 0.075  time: 0.5696  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:42:17 d2.utils.events]: [0m eta: 0:03:32  iter: 1619  total_loss: 0.698  loss_cls: 0.225  loss_box_reg: 0.376  loss_rpn_cls: 0.018  loss_rpn_loc: 0.050  time: 0.5700  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:42:28 d2.utils.events]: [0m eta: 0:03:20  iter: 1639  total_loss: 0.729  loss_cls: 0.203  loss_box_reg: 0.386  loss_rpn_cls: 0.014  loss_rpn_loc: 0.062  time: 0.5695  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:42:39 d2.utils.events]: [0m eta: 0:03:10  iter: 1659  total_loss: 0.693  loss_cls: 0.207  loss_box_reg: 0.367  loss_rpn_cls: 0.020  loss_rpn_loc: 0.071  time: 0.5696  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:42:51 d2.utils.events]: [0m eta: 0:02:59  iter: 1679  total_loss: 0.657  loss_cls: 0.214  loss_box_reg: 0.369  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5699  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:43:02 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.593  loss_cls: 0.200  loss_box_reg: 0.324  loss_rpn_cls: 0.012  loss_rpn_loc: 0.053  time: 0.5693  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:43:13 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.606  loss_cls: 0.187  loss_box_reg: 0.363  loss_rpn_cls: 0.018  loss_rpn_loc: 0.054  time: 0.5690  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:43:25 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.624  loss_cls: 0.196  loss_box_reg: 0.351  loss_rpn_cls: 0.014  loss_rpn_loc: 0.050  time: 0.5696  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:43:36 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.635  loss_cls: 0.200  loss_box_reg: 0.358  loss_rpn_cls: 0.011  loss_rpn_loc: 0.050  time: 0.5692  data_time: 0.0114  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:43:47 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.677  loss_cls: 0.188  loss_box_reg: 0.329  loss_rpn_cls: 0.015  loss_rpn_loc: 0.065  time: 0.5690  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:44:01 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.638  loss_cls: 0.213  loss_box_reg: 0.338  loss_rpn_cls: 0.014  loss_rpn_loc: 0.048  time: 0.5701  data_time: 0.1042  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:44:13 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.667  loss_cls: 0.184  loss_box_reg: 0.377  loss_rpn_cls: 0.012  loss_rpn_loc: 0.054  time: 0.5707  data_time: 0.0787  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:44:25 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.627  loss_cls: 0.215  loss_box_reg: 0.367  loss_rpn_cls: 0.017  loss_rpn_loc: 0.062  time: 0.5706  data_time: 0.0133  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:44:38 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.629  loss_cls: 0.222  loss_box_reg: 0.348  loss_rpn_cls: 0.016  loss_rpn_loc: 0.047  time: 0.5707  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:44:51 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.573  loss_cls: 0.188  loss_box_reg: 0.323  loss_rpn_cls: 0.017  loss_rpn_loc: 0.056  time: 0.5712  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:45:01 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.741  loss_cls: 0.225  loss_box_reg: 0.383  loss_rpn_cls: 0.026  loss_rpn_loc: 0.071  time: 0.5708  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:45:12 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.674  loss_cls: 0.199  loss_box_reg: 0.359  loss_rpn_cls: 0.015  loss_rpn_loc: 0.063  time: 0.5706  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:45:25 d2.utils.events]: [0m eta: 0:00:34  iter: 1939  total_loss: 0.658  loss_cls: 0.214  loss_box_reg: 0.361  loss_rpn_cls: 0.014  loss_rpn_loc: 0.053  time: 0.5710  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:45:36 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.557  loss_cls: 0.173  loss_box_reg: 0.326  loss_rpn_cls: 0.011  loss_rpn_loc: 0.055  time: 0.5708  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:45:46 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.650  loss_cls: 0.207  loss_box_reg: 0.370  loss_rpn_cls: 0.012  loss_rpn_loc: 0.054  time: 0.5703  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:46:14 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json takes 1.45 seconds.
[5m[31mWARNING[0m [32m[04/15 11:46:14 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:46:14 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 11:46:14 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 11:46:14 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.590  loss_cls: 0.189  loss_box_reg: 0.325  loss_rpn_cls: 0.010  loss_rpn_loc: 0.044  time: 0.5706  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:46:15 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:19:00 (0.5709 s / it)
[32m[04/15 11:46:15 d2.engine.hooks]: [0mTotal training time: 0:19:28 (0:00:28 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 11:46:21 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:46:21 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 11:46:21 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 11:46:24 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1227 s / img. ETA=0:02:40
[32m[04/15 11:46:29 d2.evaluation.evaluator]: [0mInference done 41/1257. 0.1290 s / img. ETA=0:03:17
[32m[04/15 11:46:34 d2.evaluation.evaluator]: [0mInference done 73/1257. 0.1358 s / img. ETA=0:03:10
[32m[04/15 11:46:39 d2.evaluation.evaluator]: [0mInference done 105/1257. 0.1411 s / img. ETA=0:03:03
[32m[04/15 11:46:45 d2.evaluation.evaluator]: [0mInference done 140/1257. 0.1359 s / img. ETA=0:02:54
[32m[04/15 11:46:50 d2.evaluation.evaluator]: [0mInference done 170/1257. 0.1336 s / img. ETA=0:02:51
[32m[04/15 11:46:55 d2.evaluation.evaluator]: [0mInference done 205/1257. 0.1315 s / img. ETA=0:02:44
[32m[04/15 11:47:00 d2.evaluation.evaluator]: [0mInference done 239/1257. 0.1300 s / img. ETA=0:02:37
[32m[04/15 11:47:05 d2.evaluation.evaluator]: [0mInference done 273/1257. 0.1288 s / img. ETA=0:02:31
[32m[04/15 11:47:10 d2.evaluation.evaluator]: [0mInference done 305/1257. 0.1307 s / img. ETA=0:02:27
[32m[04/15 11:47:15 d2.evaluation.evaluator]: [0mInference done 339/1257. 0.1325 s / img. ETA=0:02:21
[32m[04/15 11:47:20 d2.evaluation.evaluator]: [0mInference done 379/1257. 0.1313 s / img. ETA=0:02:13
[32m[04/15 11:47:25 d2.evaluation.evaluator]: [0mInference done 416/1257. 0.1303 s / img. ETA=0:02:06
[32m[04/15 11:47:31 d2.evaluation.evaluator]: [0mInference done 439/1257. 0.1297 s / img. ETA=0:02:06
[32m[04/15 11:47:36 d2.evaluation.evaluator]: [0mInference done 471/1257. 0.1290 s / img. ETA=0:02:01
[32m[04/15 11:47:41 d2.evaluation.evaluator]: [0mInference done 503/1257. 0.1283 s / img. ETA=0:01:57
[32m[04/15 11:47:46 d2.evaluation.evaluator]: [0mInference done 533/1257. 0.1289 s / img. ETA=0:01:53
[32m[04/15 11:47:51 d2.evaluation.evaluator]: [0mInference done 562/1257. 0.1297 s / img. ETA=0:01:49
[32m[04/15 11:47:56 d2.evaluation.evaluator]: [0mInference done 594/1257. 0.1294 s / img. ETA=0:01:44
[32m[04/15 11:48:01 d2.evaluation.evaluator]: [0mInference done 624/1257. 0.1289 s / img. ETA=0:01:40
[32m[04/15 11:48:06 d2.evaluation.evaluator]: [0mInference done 653/1257. 0.1285 s / img. ETA=0:01:35
[32m[04/15 11:48:12 d2.evaluation.evaluator]: [0mInference done 679/1257. 0.1291 s / img. ETA=0:01:32
[32m[04/15 11:48:17 d2.evaluation.evaluator]: [0mInference done 699/1257. 0.1288 s / img. ETA=0:01:30
[32m[04/15 11:48:22 d2.evaluation.evaluator]: [0mInference done 721/1257. 0.1287 s / img. ETA=0:01:28
[32m[04/15 11:48:27 d2.evaluation.evaluator]: [0mInference done 746/1257. 0.1288 s / img. ETA=0:01:25
[32m[04/15 11:48:32 d2.evaluation.evaluator]: [0mInference done 782/1257. 0.1290 s / img. ETA=0:01:18
[32m[04/15 11:48:37 d2.evaluation.evaluator]: [0mInference done 821/1257. 0.1288 s / img. ETA=0:01:11
[32m[04/15 11:48:42 d2.evaluation.evaluator]: [0mInference done 860/1257. 0.1286 s / img. ETA=0:01:04
[32m[04/15 11:48:47 d2.evaluation.evaluator]: [0mInference done 900/1257. 0.1283 s / img. ETA=0:00:57
[32m[04/15 11:48:53 d2.evaluation.evaluator]: [0mInference done 939/1257. 0.1281 s / img. ETA=0:00:50
[32m[04/15 11:48:58 d2.evaluation.evaluator]: [0mInference done 978/1257. 0.1280 s / img. ETA=0:00:44
[32m[04/15 11:49:03 d2.evaluation.evaluator]: [0mInference done 1018/1257. 0.1278 s / img. ETA=0:00:37
[32m[04/15 11:49:08 d2.evaluation.evaluator]: [0mInference done 1051/1257. 0.1284 s / img. ETA=0:00:32
[32m[04/15 11:49:13 d2.evaluation.evaluator]: [0mInference done 1084/1257. 0.1290 s / img. ETA=0:00:27
[32m[04/15 11:49:18 d2.evaluation.evaluator]: [0mInference done 1121/1257. 0.1288 s / img. ETA=0:00:21
[32m[04/15 11:49:23 d2.evaluation.evaluator]: [0mInference done 1159/1257. 0.1287 s / img. ETA=0:00:15
[32m[04/15 11:49:28 d2.evaluation.evaluator]: [0mInference done 1197/1257. 0.1285 s / img. ETA=0:00:09
[32m[04/15 11:49:33 d2.evaluation.evaluator]: [0mInference done 1237/1257. 0.1283 s / img. ETA=0:00:03
[32m[04/15 11:49:36 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:12.108387 (0.153441 s / img per device, on 1 devices)
[32m[04/15 11:49:36 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:40 (0.128234 s / img per device, on 1 devices)
[32m[04/15 11:49:36 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 11:49:36 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 11:49:36 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.76s).
Accumulating evaluation results...
DONE (t=1.13s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.433
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.190
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.263
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.449
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.108
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.269
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.273
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.203
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.512
[32m[04/15 11:49:44 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 21.730 | 43.347 | 19.008 | 14.220 | 26.310 | 44.950 |
[32m[04/15 11:49:44 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP     | category       | AP     |
|:--------------|:-------|:--------------|:-------|:---------------|:-------|
| person        | 30.570 | bicycle       | 11.805 | car            | 44.544 |
| motorcycle    | nan    | airplane      | nan    | bus            | nan    |
| train         | nan    | truck         | nan    | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan    | stop sign      | nan    |
| parking meter | nan    | bench         | nan    | bird           | nan    |
| cat           | nan    | dog           | 0.000  | horse          | nan    |
| sheep         | nan    | cow           | nan    | elephant       | nan    |
| bear          | nan    | zebra         | nan    | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan    | handbag        | nan    |
| tie           | nan    | suitcase      | nan    | frisbee        | nan    |
| skis          | nan    | snowboard     | nan    | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan    | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan    | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan    | cup            | nan    |
| fork          | nan    | knife         | nan    | spoon          | nan    |
| bowl          | nan    | banana        | nan    | apple          | nan    |
| sandwich      | nan    | orange        | nan    | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan    | pizza          | nan    |
| donut         | nan    | cake          | nan    | chair          | nan    |
| couch         | nan    | potted plant  | nan    | bed            | nan    |
| dining table  | nan    | toilet        | nan    | tv             | nan    |
| laptop        | nan    | mouse         | nan    | remote         | nan    |
| keyboard      | nan    | cell phone    | nan    | microwave      | nan    |
| oven          | nan    | bike          | nan    | hydrant        | nan    |
| motor         | nan    | rider         | nan    | light          | nan    |
| sign          | nan    | motor vehicle | nan    | human face     | nan    |
| hair drier    | nan    | license plate | nan    |                |        |
============== The  22  *  2000  iterations ============
4 channel input
[32m[04/15 11:49:45 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 11:49:47 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.57 seconds.
[5m[31mWARNING[0m [32m[04/15 11:49:47 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 11:49:47 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 11:49:48 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 11:49:48 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 11:49:48 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 11:49:50 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 11:50:02 d2.utils.events]: [0m eta: 0:18:25  iter: 19  total_loss: 0.667  loss_cls: 0.208  loss_box_reg: 0.398  loss_rpn_cls: 0.018  loss_rpn_loc: 0.061  time: 0.5663  data_time: 0.0400  lr: 0.000100  max_mem: 3067M
[32m[04/15 11:50:13 d2.utils.events]: [0m eta: 0:17:40  iter: 39  total_loss: 0.608  loss_cls: 0.185  loss_box_reg: 0.324  loss_rpn_cls: 0.011  loss_rpn_loc: 0.054  time: 0.5420  data_time: 0.0063  lr: 0.000200  max_mem: 3067M
[32m[04/15 11:50:24 d2.utils.events]: [0m eta: 0:17:34  iter: 59  total_loss: 0.609  loss_cls: 0.184  loss_box_reg: 0.349  loss_rpn_cls: 0.017  loss_rpn_loc: 0.072  time: 0.5413  data_time: 0.0065  lr: 0.000300  max_mem: 3067M
[32m[04/15 11:50:36 d2.utils.events]: [0m eta: 0:17:45  iter: 79  total_loss: 0.600  loss_cls: 0.197  loss_box_reg: 0.351  loss_rpn_cls: 0.012  loss_rpn_loc: 0.060  time: 0.5560  data_time: 0.0068  lr: 0.000400  max_mem: 3067M
[32m[04/15 11:50:47 d2.utils.events]: [0m eta: 0:17:23  iter: 99  total_loss: 0.694  loss_cls: 0.207  loss_box_reg: 0.371  loss_rpn_cls: 0.014  loss_rpn_loc: 0.061  time: 0.5558  data_time: 0.0096  lr: 0.000500  max_mem: 3067M
[32m[04/15 11:50:58 d2.utils.events]: [0m eta: 0:17:07  iter: 119  total_loss: 0.636  loss_cls: 0.213  loss_box_reg: 0.341  loss_rpn_cls: 0.010  loss_rpn_loc: 0.041  time: 0.5537  data_time: 0.0077  lr: 0.000599  max_mem: 3067M
[32m[04/15 11:51:09 d2.utils.events]: [0m eta: 0:16:58  iter: 139  total_loss: 0.610  loss_cls: 0.184  loss_box_reg: 0.348  loss_rpn_cls: 0.013  loss_rpn_loc: 0.049  time: 0.5556  data_time: 0.0073  lr: 0.000699  max_mem: 3067M
[32m[04/15 11:51:21 d2.utils.events]: [0m eta: 0:16:52  iter: 159  total_loss: 0.629  loss_cls: 0.193  loss_box_reg: 0.371  loss_rpn_cls: 0.012  loss_rpn_loc: 0.044  time: 0.5612  data_time: 0.0077  lr: 0.000799  max_mem: 3067M
[32m[04/15 11:51:32 d2.utils.events]: [0m eta: 0:16:48  iter: 179  total_loss: 0.584  loss_cls: 0.177  loss_box_reg: 0.342  loss_rpn_cls: 0.008  loss_rpn_loc: 0.039  time: 0.5600  data_time: 0.0071  lr: 0.000899  max_mem: 3067M
[32m[04/15 11:51:44 d2.utils.events]: [0m eta: 0:16:33  iter: 199  total_loss: 0.578  loss_cls: 0.200  loss_box_reg: 0.318  loss_rpn_cls: 0.011  loss_rpn_loc: 0.041  time: 0.5596  data_time: 0.0057  lr: 0.000999  max_mem: 3067M
[32m[04/15 11:51:56 d2.utils.events]: [0m eta: 0:16:29  iter: 219  total_loss: 0.606  loss_cls: 0.188  loss_box_reg: 0.346  loss_rpn_cls: 0.015  loss_rpn_loc: 0.042  time: 0.5640  data_time: 0.0071  lr: 0.001099  max_mem: 3067M
[32m[04/15 11:52:07 d2.utils.events]: [0m eta: 0:16:11  iter: 239  total_loss: 0.555  loss_cls: 0.169  loss_box_reg: 0.276  loss_rpn_cls: 0.012  loss_rpn_loc: 0.051  time: 0.5617  data_time: 0.0070  lr: 0.001199  max_mem: 3067M
[32m[04/15 11:52:18 d2.utils.events]: [0m eta: 0:16:00  iter: 259  total_loss: 0.571  loss_cls: 0.189  loss_box_reg: 0.338  loss_rpn_cls: 0.008  loss_rpn_loc: 0.046  time: 0.5607  data_time: 0.0070  lr: 0.001299  max_mem: 3067M
[32m[04/15 11:52:30 d2.utils.events]: [0m eta: 0:15:52  iter: 279  total_loss: 0.597  loss_cls: 0.192  loss_box_reg: 0.345  loss_rpn_cls: 0.010  loss_rpn_loc: 0.053  time: 0.5629  data_time: 0.0070  lr: 0.001399  max_mem: 3067M
[32m[04/15 11:52:41 d2.utils.events]: [0m eta: 0:15:41  iter: 299  total_loss: 0.572  loss_cls: 0.179  loss_box_reg: 0.328  loss_rpn_cls: 0.011  loss_rpn_loc: 0.035  time: 0.5621  data_time: 0.0076  lr: 0.001499  max_mem: 3067M
[32m[04/15 11:52:52 d2.utils.events]: [0m eta: 0:15:25  iter: 319  total_loss: 0.510  loss_cls: 0.157  loss_box_reg: 0.302  loss_rpn_cls: 0.011  loss_rpn_loc: 0.052  time: 0.5599  data_time: 0.0073  lr: 0.001598  max_mem: 3067M
[32m[04/15 11:53:03 d2.utils.events]: [0m eta: 0:15:17  iter: 339  total_loss: 0.680  loss_cls: 0.218  loss_box_reg: 0.365  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5605  data_time: 0.0077  lr: 0.001698  max_mem: 3067M
[32m[04/15 11:53:15 d2.utils.events]: [0m eta: 0:15:07  iter: 359  total_loss: 0.580  loss_cls: 0.181  loss_box_reg: 0.292  loss_rpn_cls: 0.013  loss_rpn_loc: 0.043  time: 0.5616  data_time: 0.0071  lr: 0.001798  max_mem: 3067M
[32m[04/15 11:53:26 d2.utils.events]: [0m eta: 0:14:55  iter: 379  total_loss: 0.565  loss_cls: 0.165  loss_box_reg: 0.318  loss_rpn_cls: 0.012  loss_rpn_loc: 0.050  time: 0.5604  data_time: 0.0084  lr: 0.001898  max_mem: 3067M
[32m[04/15 11:53:37 d2.utils.events]: [0m eta: 0:14:44  iter: 399  total_loss: 0.593  loss_cls: 0.178  loss_box_reg: 0.356  loss_rpn_cls: 0.007  loss_rpn_loc: 0.038  time: 0.5604  data_time: 0.0069  lr: 0.001998  max_mem: 3067M
[32m[04/15 11:53:49 d2.utils.events]: [0m eta: 0:14:37  iter: 419  total_loss: 0.626  loss_cls: 0.197  loss_box_reg: 0.341  loss_rpn_cls: 0.021  loss_rpn_loc: 0.051  time: 0.5632  data_time: 0.0072  lr: 0.002098  max_mem: 3067M
[32m[04/15 11:54:00 d2.utils.events]: [0m eta: 0:14:22  iter: 439  total_loss: 0.531  loss_cls: 0.162  loss_box_reg: 0.307  loss_rpn_cls: 0.011  loss_rpn_loc: 0.053  time: 0.5615  data_time: 0.0071  lr: 0.002198  max_mem: 3067M
[32m[04/15 11:54:11 d2.utils.events]: [0m eta: 0:14:07  iter: 459  total_loss: 0.572  loss_cls: 0.193  loss_box_reg: 0.342  loss_rpn_cls: 0.011  loss_rpn_loc: 0.047  time: 0.5607  data_time: 0.0064  lr: 0.002298  max_mem: 3067M
[32m[04/15 11:54:23 d2.utils.events]: [0m eta: 0:13:58  iter: 479  total_loss: 0.629  loss_cls: 0.202  loss_box_reg: 0.356  loss_rpn_cls: 0.015  loss_rpn_loc: 0.049  time: 0.5621  data_time: 0.0105  lr: 0.002398  max_mem: 3067M
[32m[04/15 11:54:34 d2.utils.events]: [0m eta: 0:13:46  iter: 499  total_loss: 0.493  loss_cls: 0.151  loss_box_reg: 0.279  loss_rpn_cls: 0.011  loss_rpn_loc: 0.043  time: 0.5617  data_time: 0.0069  lr: 0.002498  max_mem: 3067M
[32m[04/15 11:54:45 d2.utils.events]: [0m eta: 0:13:35  iter: 519  total_loss: 0.523  loss_cls: 0.164  loss_box_reg: 0.311  loss_rpn_cls: 0.014  loss_rpn_loc: 0.040  time: 0.5614  data_time: 0.0070  lr: 0.002597  max_mem: 3067M
[32m[04/15 11:54:58 d2.utils.events]: [0m eta: 0:13:29  iter: 539  total_loss: 0.610  loss_cls: 0.167  loss_box_reg: 0.334  loss_rpn_cls: 0.012  loss_rpn_loc: 0.049  time: 0.5634  data_time: 0.0084  lr: 0.002697  max_mem: 3067M
[32m[04/15 11:55:09 d2.utils.events]: [0m eta: 0:13:17  iter: 559  total_loss: 0.540  loss_cls: 0.176  loss_box_reg: 0.297  loss_rpn_cls: 0.010  loss_rpn_loc: 0.051  time: 0.5632  data_time: 0.0075  lr: 0.002797  max_mem: 3067M
[32m[04/15 11:55:20 d2.utils.events]: [0m eta: 0:13:05  iter: 579  total_loss: 0.556  loss_cls: 0.187  loss_box_reg: 0.320  loss_rpn_cls: 0.009  loss_rpn_loc: 0.044  time: 0.5623  data_time: 0.0065  lr: 0.002897  max_mem: 3067M
[32m[04/15 11:55:31 d2.utils.events]: [0m eta: 0:12:54  iter: 599  total_loss: 0.586  loss_cls: 0.204  loss_box_reg: 0.320  loss_rpn_cls: 0.010  loss_rpn_loc: 0.051  time: 0.5623  data_time: 0.0075  lr: 0.002997  max_mem: 3067M
[32m[04/15 11:55:42 d2.utils.events]: [0m eta: 0:12:43  iter: 619  total_loss: 0.733  loss_cls: 0.225  loss_box_reg: 0.390  loss_rpn_cls: 0.015  loss_rpn_loc: 0.064  time: 0.5621  data_time: 0.0077  lr: 0.003097  max_mem: 3067M
[32m[04/15 11:55:53 d2.utils.events]: [0m eta: 0:12:30  iter: 639  total_loss: 0.652  loss_cls: 0.238  loss_box_reg: 0.356  loss_rpn_cls: 0.012  loss_rpn_loc: 0.051  time: 0.5611  data_time: 0.0095  lr: 0.003197  max_mem: 3067M
[32m[04/15 11:56:04 d2.utils.events]: [0m eta: 0:12:19  iter: 659  total_loss: 0.558  loss_cls: 0.178  loss_box_reg: 0.303  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5608  data_time: 0.0066  lr: 0.003297  max_mem: 3067M
[32m[04/15 11:56:17 d2.utils.events]: [0m eta: 0:12:10  iter: 679  total_loss: 0.669  loss_cls: 0.217  loss_box_reg: 0.373  loss_rpn_cls: 0.012  loss_rpn_loc: 0.063  time: 0.5622  data_time: 0.0069  lr: 0.003397  max_mem: 3067M
[32m[04/15 11:56:27 d2.utils.events]: [0m eta: 0:11:57  iter: 699  total_loss: 0.600  loss_cls: 0.176  loss_box_reg: 0.317  loss_rpn_cls: 0.011  loss_rpn_loc: 0.055  time: 0.5615  data_time: 0.0104  lr: 0.003497  max_mem: 3067M
[32m[04/15 11:56:39 d2.utils.events]: [0m eta: 0:11:47  iter: 719  total_loss: 0.698  loss_cls: 0.194  loss_box_reg: 0.344  loss_rpn_cls: 0.017  loss_rpn_loc: 0.063  time: 0.5613  data_time: 0.0058  lr: 0.003596  max_mem: 3067M
[32m[04/15 11:56:50 d2.utils.events]: [0m eta: 0:11:37  iter: 739  total_loss: 0.574  loss_cls: 0.189  loss_box_reg: 0.320  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5617  data_time: 0.0069  lr: 0.003696  max_mem: 3067M
[32m[04/15 11:57:01 d2.utils.events]: [0m eta: 0:11:24  iter: 759  total_loss: 0.589  loss_cls: 0.179  loss_box_reg: 0.327  loss_rpn_cls: 0.014  loss_rpn_loc: 0.048  time: 0.5613  data_time: 0.0102  lr: 0.003796  max_mem: 3067M
[32m[04/15 11:57:12 d2.utils.events]: [0m eta: 0:11:12  iter: 779  total_loss: 0.559  loss_cls: 0.182  loss_box_reg: 0.314  loss_rpn_cls: 0.016  loss_rpn_loc: 0.057  time: 0.5609  data_time: 0.0074  lr: 0.003896  max_mem: 3067M
[32m[04/15 11:57:24 d2.utils.events]: [0m eta: 0:11:03  iter: 799  total_loss: 0.476  loss_cls: 0.154  loss_box_reg: 0.250  loss_rpn_cls: 0.007  loss_rpn_loc: 0.043  time: 0.5616  data_time: 0.0069  lr: 0.003996  max_mem: 3067M
[32m[04/15 11:57:36 d2.utils.events]: [0m eta: 0:10:51  iter: 819  total_loss: 0.627  loss_cls: 0.188  loss_box_reg: 0.342  loss_rpn_cls: 0.016  loss_rpn_loc: 0.045  time: 0.5616  data_time: 0.0075  lr: 0.004096  max_mem: 3067M
[32m[04/15 11:57:47 d2.utils.events]: [0m eta: 0:10:40  iter: 839  total_loss: 0.517  loss_cls: 0.169  loss_box_reg: 0.281  loss_rpn_cls: 0.019  loss_rpn_loc: 0.055  time: 0.5614  data_time: 0.0070  lr: 0.004196  max_mem: 3067M
[32m[04/15 11:57:58 d2.utils.events]: [0m eta: 0:10:30  iter: 859  total_loss: 0.654  loss_cls: 0.185  loss_box_reg: 0.345  loss_rpn_cls: 0.014  loss_rpn_loc: 0.072  time: 0.5616  data_time: 0.0069  lr: 0.004296  max_mem: 3067M
[32m[04/15 11:58:10 d2.utils.events]: [0m eta: 0:10:19  iter: 879  total_loss: 0.592  loss_cls: 0.168  loss_box_reg: 0.318  loss_rpn_cls: 0.016  loss_rpn_loc: 0.063  time: 0.5622  data_time: 0.0074  lr: 0.004396  max_mem: 3067M
[32m[04/15 11:58:21 d2.utils.events]: [0m eta: 0:10:08  iter: 899  total_loss: 0.543  loss_cls: 0.181  loss_box_reg: 0.301  loss_rpn_cls: 0.013  loss_rpn_loc: 0.043  time: 0.5619  data_time: 0.0067  lr: 0.004496  max_mem: 3067M
[32m[04/15 11:58:33 d2.utils.events]: [0m eta: 0:09:57  iter: 919  total_loss: 0.611  loss_cls: 0.179  loss_box_reg: 0.342  loss_rpn_cls: 0.014  loss_rpn_loc: 0.049  time: 0.5619  data_time: 0.0069  lr: 0.004595  max_mem: 3067M
[32m[04/15 11:58:45 d2.utils.events]: [0m eta: 0:09:48  iter: 939  total_loss: 0.583  loss_cls: 0.194  loss_box_reg: 0.325  loss_rpn_cls: 0.015  loss_rpn_loc: 0.052  time: 0.5632  data_time: 0.0092  lr: 0.004695  max_mem: 3067M
[32m[04/15 11:58:56 d2.utils.events]: [0m eta: 0:09:35  iter: 959  total_loss: 0.627  loss_cls: 0.202  loss_box_reg: 0.381  loss_rpn_cls: 0.012  loss_rpn_loc: 0.058  time: 0.5626  data_time: 0.0073  lr: 0.004795  max_mem: 3067M
[32m[04/15 11:59:07 d2.utils.events]: [0m eta: 0:09:24  iter: 979  total_loss: 0.580  loss_cls: 0.194  loss_box_reg: 0.323  loss_rpn_cls: 0.020  loss_rpn_loc: 0.049  time: 0.5623  data_time: 0.0075  lr: 0.004895  max_mem: 3067M
[32m[04/15 11:59:19 d2.utils.events]: [0m eta: 0:09:14  iter: 999  total_loss: 0.534  loss_cls: 0.154  loss_box_reg: 0.295  loss_rpn_cls: 0.011  loss_rpn_loc: 0.042  time: 0.5634  data_time: 0.0091  lr: 0.004995  max_mem: 3067M
[32m[04/15 11:59:31 d2.utils.events]: [0m eta: 0:09:03  iter: 1019  total_loss: 0.571  loss_cls: 0.187  loss_box_reg: 0.337  loss_rpn_cls: 0.016  loss_rpn_loc: 0.047  time: 0.5632  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:59:42 d2.utils.events]: [0m eta: 0:08:53  iter: 1039  total_loss: 0.653  loss_cls: 0.220  loss_box_reg: 0.336  loss_rpn_cls: 0.013  loss_rpn_loc: 0.059  time: 0.5633  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 11:59:53 d2.utils.events]: [0m eta: 0:08:42  iter: 1059  total_loss: 0.625  loss_cls: 0.202  loss_box_reg: 0.335  loss_rpn_cls: 0.011  loss_rpn_loc: 0.055  time: 0.5629  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:00:06 d2.utils.events]: [0m eta: 0:08:30  iter: 1079  total_loss: 0.598  loss_cls: 0.185  loss_box_reg: 0.340  loss_rpn_cls: 0.022  loss_rpn_loc: 0.056  time: 0.5646  data_time: 0.1510  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:00:17 d2.utils.events]: [0m eta: 0:08:20  iter: 1099  total_loss: 0.611  loss_cls: 0.188  loss_box_reg: 0.345  loss_rpn_cls: 0.016  loss_rpn_loc: 0.062  time: 0.5642  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:00:29 d2.utils.events]: [0m eta: 0:08:09  iter: 1119  total_loss: 0.661  loss_cls: 0.202  loss_box_reg: 0.388  loss_rpn_cls: 0.015  loss_rpn_loc: 0.054  time: 0.5648  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:00:42 d2.utils.events]: [0m eta: 0:07:58  iter: 1139  total_loss: 0.703  loss_cls: 0.222  loss_box_reg: 0.376  loss_rpn_cls: 0.018  loss_rpn_loc: 0.054  time: 0.5657  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:00:53 d2.utils.events]: [0m eta: 0:07:47  iter: 1159  total_loss: 0.596  loss_cls: 0.184  loss_box_reg: 0.326  loss_rpn_cls: 0.015  loss_rpn_loc: 0.056  time: 0.5657  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:01:04 d2.utils.events]: [0m eta: 0:07:36  iter: 1179  total_loss: 0.628  loss_cls: 0.196  loss_box_reg: 0.380  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5656  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:01:16 d2.utils.events]: [0m eta: 0:07:25  iter: 1199  total_loss: 0.553  loss_cls: 0.176  loss_box_reg: 0.296  loss_rpn_cls: 0.013  loss_rpn_loc: 0.045  time: 0.5657  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:01:27 d2.utils.events]: [0m eta: 0:07:14  iter: 1219  total_loss: 0.568  loss_cls: 0.205  loss_box_reg: 0.317  loss_rpn_cls: 0.013  loss_rpn_loc: 0.051  time: 0.5657  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:01:39 d2.utils.events]: [0m eta: 0:07:03  iter: 1239  total_loss: 0.611  loss_cls: 0.206  loss_box_reg: 0.339  loss_rpn_cls: 0.022  loss_rpn_loc: 0.050  time: 0.5656  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:01:49 d2.utils.events]: [0m eta: 0:06:51  iter: 1259  total_loss: 0.666  loss_cls: 0.179  loss_box_reg: 0.335  loss_rpn_cls: 0.018  loss_rpn_loc: 0.075  time: 0.5648  data_time: 0.0058  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:02:01 d2.utils.events]: [0m eta: 0:06:40  iter: 1279  total_loss: 0.589  loss_cls: 0.180  loss_box_reg: 0.347  loss_rpn_cls: 0.020  loss_rpn_loc: 0.055  time: 0.5655  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:02:12 d2.utils.events]: [0m eta: 0:06:29  iter: 1299  total_loss: 0.558  loss_cls: 0.171  loss_box_reg: 0.302  loss_rpn_cls: 0.011  loss_rpn_loc: 0.043  time: 0.5653  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:02:23 d2.utils.events]: [0m eta: 0:06:18  iter: 1319  total_loss: 0.681  loss_cls: 0.206  loss_box_reg: 0.389  loss_rpn_cls: 0.010  loss_rpn_loc: 0.050  time: 0.5649  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:02:35 d2.utils.events]: [0m eta: 0:06:07  iter: 1339  total_loss: 0.540  loss_cls: 0.167  loss_box_reg: 0.296  loss_rpn_cls: 0.011  loss_rpn_loc: 0.048  time: 0.5653  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:02:47 d2.utils.events]: [0m eta: 0:05:56  iter: 1359  total_loss: 0.732  loss_cls: 0.245  loss_box_reg: 0.395  loss_rpn_cls: 0.024  loss_rpn_loc: 0.070  time: 0.5653  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:02:57 d2.utils.events]: [0m eta: 0:05:44  iter: 1379  total_loss: 0.601  loss_cls: 0.189  loss_box_reg: 0.327  loss_rpn_cls: 0.012  loss_rpn_loc: 0.052  time: 0.5649  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:03:09 d2.utils.events]: [0m eta: 0:05:33  iter: 1399  total_loss: 0.609  loss_cls: 0.201  loss_box_reg: 0.331  loss_rpn_cls: 0.016  loss_rpn_loc: 0.059  time: 0.5647  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:03:21 d2.utils.events]: [0m eta: 0:05:22  iter: 1419  total_loss: 0.627  loss_cls: 0.218  loss_box_reg: 0.331  loss_rpn_cls: 0.013  loss_rpn_loc: 0.050  time: 0.5651  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:03:32 d2.utils.events]: [0m eta: 0:05:11  iter: 1439  total_loss: 0.631  loss_cls: 0.198  loss_box_reg: 0.345  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5650  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:03:42 d2.utils.events]: [0m eta: 0:05:00  iter: 1459  total_loss: 0.722  loss_cls: 0.220  loss_box_reg: 0.390  loss_rpn_cls: 0.016  loss_rpn_loc: 0.058  time: 0.5645  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:03:55 d2.utils.events]: [0m eta: 0:04:49  iter: 1479  total_loss: 0.665  loss_cls: 0.194  loss_box_reg: 0.394  loss_rpn_cls: 0.017  loss_rpn_loc: 0.070  time: 0.5656  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:04:07 d2.utils.events]: [0m eta: 0:04:38  iter: 1499  total_loss: 0.580  loss_cls: 0.182  loss_box_reg: 0.326  loss_rpn_cls: 0.013  loss_rpn_loc: 0.046  time: 0.5656  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:04:18 d2.utils.events]: [0m eta: 0:04:27  iter: 1519  total_loss: 0.587  loss_cls: 0.181  loss_box_reg: 0.340  loss_rpn_cls: 0.015  loss_rpn_loc: 0.053  time: 0.5653  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:04:30 d2.utils.events]: [0m eta: 0:04:16  iter: 1539  total_loss: 0.713  loss_cls: 0.220  loss_box_reg: 0.378  loss_rpn_cls: 0.024  loss_rpn_loc: 0.074  time: 0.5661  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:04:41 d2.utils.events]: [0m eta: 0:04:04  iter: 1559  total_loss: 0.561  loss_cls: 0.178  loss_box_reg: 0.312  loss_rpn_cls: 0.012  loss_rpn_loc: 0.049  time: 0.5658  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:04:52 d2.utils.events]: [0m eta: 0:03:53  iter: 1579  total_loss: 0.606  loss_cls: 0.200  loss_box_reg: 0.337  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5654  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:05:03 d2.utils.events]: [0m eta: 0:03:42  iter: 1599  total_loss: 0.657  loss_cls: 0.195  loss_box_reg: 0.372  loss_rpn_cls: 0.025  loss_rpn_loc: 0.067  time: 0.5654  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:05:15 d2.utils.events]: [0m eta: 0:03:31  iter: 1619  total_loss: 0.505  loss_cls: 0.175  loss_box_reg: 0.282  loss_rpn_cls: 0.013  loss_rpn_loc: 0.041  time: 0.5655  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:05:27 d2.utils.events]: [0m eta: 0:03:20  iter: 1639  total_loss: 0.624  loss_cls: 0.208  loss_box_reg: 0.333  loss_rpn_cls: 0.011  loss_rpn_loc: 0.062  time: 0.5656  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:05:38 d2.utils.events]: [0m eta: 0:03:09  iter: 1659  total_loss: 0.519  loss_cls: 0.172  loss_box_reg: 0.295  loss_rpn_cls: 0.010  loss_rpn_loc: 0.051  time: 0.5656  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:05:50 d2.utils.events]: [0m eta: 0:02:58  iter: 1679  total_loss: 0.712  loss_cls: 0.223  loss_box_reg: 0.383  loss_rpn_cls: 0.015  loss_rpn_loc: 0.072  time: 0.5660  data_time: 0.0110  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:06:02 d2.utils.events]: [0m eta: 0:02:47  iter: 1699  total_loss: 0.604  loss_cls: 0.206  loss_box_reg: 0.354  loss_rpn_cls: 0.012  loss_rpn_loc: 0.055  time: 0.5660  data_time: 0.0097  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:06:12 d2.utils.events]: [0m eta: 0:02:36  iter: 1719  total_loss: 0.581  loss_cls: 0.199  loss_box_reg: 0.337  loss_rpn_cls: 0.014  loss_rpn_loc: 0.054  time: 0.5656  data_time: 0.0060  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:06:25 d2.utils.events]: [0m eta: 0:02:25  iter: 1739  total_loss: 0.575  loss_cls: 0.175  loss_box_reg: 0.329  loss_rpn_cls: 0.012  loss_rpn_loc: 0.042  time: 0.5662  data_time: 0.0092  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:06:36 d2.utils.events]: [0m eta: 0:02:14  iter: 1759  total_loss: 0.605  loss_cls: 0.203  loss_box_reg: 0.338  loss_rpn_cls: 0.013  loss_rpn_loc: 0.041  time: 0.5659  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:06:46 d2.utils.events]: [0m eta: 0:02:03  iter: 1779  total_loss: 0.645  loss_cls: 0.187  loss_box_reg: 0.352  loss_rpn_cls: 0.013  loss_rpn_loc: 0.059  time: 0.5655  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:06:58 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.633  loss_cls: 0.182  loss_box_reg: 0.342  loss_rpn_cls: 0.018  loss_rpn_loc: 0.057  time: 0.5656  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:07:10 d2.utils.events]: [0m eta: 0:01:41  iter: 1819  total_loss: 0.535  loss_cls: 0.165  loss_box_reg: 0.331  loss_rpn_cls: 0.009  loss_rpn_loc: 0.055  time: 0.5660  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:07:21 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.551  loss_cls: 0.180  loss_box_reg: 0.295  loss_rpn_cls: 0.012  loss_rpn_loc: 0.046  time: 0.5658  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:07:33 d2.utils.events]: [0m eta: 0:01:18  iter: 1859  total_loss: 0.623  loss_cls: 0.185  loss_box_reg: 0.357  loss_rpn_cls: 0.019  loss_rpn_loc: 0.061  time: 0.5660  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:07:45 d2.utils.events]: [0m eta: 0:01:07  iter: 1879  total_loss: 0.575  loss_cls: 0.201  loss_box_reg: 0.307  loss_rpn_cls: 0.013  loss_rpn_loc: 0.055  time: 0.5662  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:07:55 d2.utils.events]: [0m eta: 0:00:56  iter: 1899  total_loss: 0.671  loss_cls: 0.207  loss_box_reg: 0.376  loss_rpn_cls: 0.012  loss_rpn_loc: 0.057  time: 0.5658  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:08:06 d2.utils.events]: [0m eta: 0:00:45  iter: 1919  total_loss: 0.734  loss_cls: 0.238  loss_box_reg: 0.399  loss_rpn_cls: 0.015  loss_rpn_loc: 0.073  time: 0.5656  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:08:18 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.537  loss_cls: 0.180  loss_box_reg: 0.294  loss_rpn_cls: 0.014  loss_rpn_loc: 0.051  time: 0.5658  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:08:29 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.635  loss_cls: 0.214  loss_box_reg: 0.344  loss_rpn_cls: 0.016  loss_rpn_loc: 0.054  time: 0.5657  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:08:40 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.634  loss_cls: 0.204  loss_box_reg: 0.362  loss_rpn_cls: 0.016  loss_rpn_loc: 0.051  time: 0.5653  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:09:06 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json takes 1.06 seconds.
[5m[31mWARNING[0m [32m[04/15 12:09:06 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 12:09:06 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 12:09:07 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 12:09:07 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.674  loss_cls: 0.201  loss_box_reg: 0.362  loss_rpn_cls: 0.019  loss_rpn_loc: 0.058  time: 0.5658  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:09:08 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:50 (0.5661 s / it)
[32m[04/15 12:09:08 d2.engine.hooks]: [0mTotal training time: 0:19:15 (0:00:25 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 12:09:14 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 12:09:14 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 12:09:14 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 12:09:20 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1224 s / img. ETA=0:04:15
[32m[04/15 12:09:25 d2.evaluation.evaluator]: [0mInference done 44/1257. 0.1406 s / img. ETA=0:03:16
[32m[04/15 12:09:30 d2.evaluation.evaluator]: [0mInference done 77/1257. 0.1423 s / img. ETA=0:03:06
[32m[04/15 12:09:35 d2.evaluation.evaluator]: [0mInference done 108/1257. 0.1409 s / img. ETA=0:03:04
[32m[04/15 12:09:41 d2.evaluation.evaluator]: [0mInference done 138/1257. 0.1364 s / img. ETA=0:03:03
[32m[04/15 12:09:46 d2.evaluation.evaluator]: [0mInference done 170/1257. 0.1333 s / img. ETA=0:02:56
[32m[04/15 12:09:51 d2.evaluation.evaluator]: [0mInference done 202/1257. 0.1313 s / img. ETA=0:02:50
[32m[04/15 12:09:56 d2.evaluation.evaluator]: [0mInference done 224/1257. 0.1301 s / img. ETA=0:02:54
[32m[04/15 12:10:01 d2.evaluation.evaluator]: [0mInference done 254/1257. 0.1302 s / img. ETA=0:02:49
[32m[04/15 12:10:06 d2.evaluation.evaluator]: [0mInference done 287/1257. 0.1323 s / img. ETA=0:02:42
[32m[04/15 12:10:11 d2.evaluation.evaluator]: [0mInference done 317/1257. 0.1322 s / img. ETA=0:02:37
[32m[04/15 12:10:16 d2.evaluation.evaluator]: [0mInference done 348/1257. 0.1316 s / img. ETA=0:02:32
[32m[04/15 12:10:21 d2.evaluation.evaluator]: [0mInference done 371/1257. 0.1308 s / img. ETA=0:02:31
[32m[04/15 12:10:27 d2.evaluation.evaluator]: [0mInference done 390/1257. 0.1302 s / img. ETA=0:02:32
[32m[04/15 12:10:32 d2.evaluation.evaluator]: [0mInference done 419/1257. 0.1294 s / img. ETA=0:02:27
[32m[04/15 12:10:37 d2.evaluation.evaluator]: [0mInference done 437/1257. 0.1289 s / img. ETA=0:02:27
[32m[04/15 12:10:42 d2.evaluation.evaluator]: [0mInference done 464/1257. 0.1283 s / img. ETA=0:02:23
[32m[04/15 12:10:47 d2.evaluation.evaluator]: [0mInference done 493/1257. 0.1293 s / img. ETA=0:02:17
[32m[04/15 12:10:52 d2.evaluation.evaluator]: [0mInference done 519/1257. 0.1294 s / img. ETA=0:02:13
[32m[04/15 12:10:57 d2.evaluation.evaluator]: [0mInference done 548/1257. 0.1292 s / img. ETA=0:02:08
[32m[04/15 12:11:02 d2.evaluation.evaluator]: [0mInference done 572/1257. 0.1287 s / img. ETA=0:02:04
[32m[04/15 12:11:07 d2.evaluation.evaluator]: [0mInference done 604/1257. 0.1282 s / img. ETA=0:01:58
[32m[04/15 12:11:13 d2.evaluation.evaluator]: [0mInference done 631/1257. 0.1278 s / img. ETA=0:01:53
[32m[04/15 12:11:18 d2.evaluation.evaluator]: [0mInference done 655/1257. 0.1274 s / img. ETA=0:01:50
[32m[04/15 12:11:23 d2.evaluation.evaluator]: [0mInference done 679/1257. 0.1279 s / img. ETA=0:01:46
[32m[04/15 12:11:28 d2.evaluation.evaluator]: [0mInference done 712/1257. 0.1279 s / img. ETA=0:01:39
[32m[04/15 12:11:33 d2.evaluation.evaluator]: [0mInference done 743/1257. 0.1278 s / img. ETA=0:01:33
[32m[04/15 12:11:38 d2.evaluation.evaluator]: [0mInference done 774/1257. 0.1275 s / img. ETA=0:01:27
[32m[04/15 12:11:43 d2.evaluation.evaluator]: [0mInference done 808/1257. 0.1272 s / img. ETA=0:01:20
[32m[04/15 12:11:48 d2.evaluation.evaluator]: [0mInference done 835/1257. 0.1269 s / img. ETA=0:01:15
[32m[04/15 12:11:53 d2.evaluation.evaluator]: [0mInference done 858/1257. 0.1267 s / img. ETA=0:01:12
[32m[04/15 12:11:58 d2.evaluation.evaluator]: [0mInference done 879/1257. 0.1273 s / img. ETA=0:01:09
[32m[04/15 12:12:04 d2.evaluation.evaluator]: [0mInference done 904/1257. 0.1273 s / img. ETA=0:01:04
[32m[04/15 12:12:09 d2.evaluation.evaluator]: [0mInference done 925/1257. 0.1272 s / img. ETA=0:01:01
[32m[04/15 12:12:14 d2.evaluation.evaluator]: [0mInference done 948/1257. 0.1270 s / img. ETA=0:00:57
[32m[04/15 12:12:19 d2.evaluation.evaluator]: [0mInference done 974/1257. 0.1268 s / img. ETA=0:00:52
[32m[04/15 12:12:24 d2.evaluation.evaluator]: [0mInference done 997/1257. 0.1266 s / img. ETA=0:00:48
[32m[04/15 12:12:29 d2.evaluation.evaluator]: [0mInference done 1018/1257. 0.1265 s / img. ETA=0:00:44
[32m[04/15 12:12:34 d2.evaluation.evaluator]: [0mInference done 1043/1257. 0.1269 s / img. ETA=0:00:40
[32m[04/15 12:12:39 d2.evaluation.evaluator]: [0mInference done 1067/1257. 0.1269 s / img. ETA=0:00:35
[32m[04/15 12:12:44 d2.evaluation.evaluator]: [0mInference done 1090/1257. 0.1268 s / img. ETA=0:00:31
[32m[04/15 12:12:50 d2.evaluation.evaluator]: [0mInference done 1116/1257. 0.1266 s / img. ETA=0:00:26
[32m[04/15 12:12:55 d2.evaluation.evaluator]: [0mInference done 1130/1257. 0.1266 s / img. ETA=0:00:24
[32m[04/15 12:13:00 d2.evaluation.evaluator]: [0mInference done 1159/1257. 0.1265 s / img. ETA=0:00:18
[32m[04/15 12:13:05 d2.evaluation.evaluator]: [0mInference done 1189/1257. 0.1263 s / img. ETA=0:00:12
[32m[04/15 12:13:10 d2.evaluation.evaluator]: [0mInference done 1215/1257. 0.1262 s / img. ETA=0:00:08
[32m[04/15 12:13:15 d2.evaluation.evaluator]: [0mInference done 1242/1257. 0.1263 s / img. ETA=0:00:02
[32m[04/15 12:13:18 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:58.788637 (0.190726 s / img per device, on 1 devices)
[32m[04/15 12:13:18 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:38 (0.126495 s / img per device, on 1 devices)
[32m[04/15 12:13:18 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 12:13:18 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 12:13:18 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.05s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=6.10s).
Accumulating evaluation results...
DONE (t=1.53s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.205
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.416
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.170
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.128
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.250
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.471
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.105
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.256
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.259
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.189
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.299
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.537
[32m[04/15 12:13:29 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 20.484 | 41.632 | 17.009 | 12.838 | 24.975 | 47.086 |
[32m[04/15 12:13:29 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP     | category       | AP     |
|:--------------|:-------|:--------------|:-------|:---------------|:-------|
| person        | 27.984 | bicycle       | 10.690 | car            | 43.261 |
| motorcycle    | nan    | airplane      | nan    | bus            | nan    |
| train         | nan    | truck         | nan    | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan    | stop sign      | nan    |
| parking meter | nan    | bench         | nan    | bird           | nan    |
| cat           | nan    | dog           | 0.000  | horse          | nan    |
| sheep         | nan    | cow           | nan    | elephant       | nan    |
| bear          | nan    | zebra         | nan    | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan    | handbag        | nan    |
| tie           | nan    | suitcase      | nan    | frisbee        | nan    |
| skis          | nan    | snowboard     | nan    | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan    | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan    | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan    | cup            | nan    |
| fork          | nan    | knife         | nan    | spoon          | nan    |
| bowl          | nan    | banana        | nan    | apple          | nan    |
| sandwich      | nan    | orange        | nan    | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan    | pizza          | nan    |
| donut         | nan    | cake          | nan    | chair          | nan    |
| couch         | nan    | potted plant  | nan    | bed            | nan    |
| dining table  | nan    | toilet        | nan    | tv             | nan    |
| laptop        | nan    | mouse         | nan    | remote         | nan    |
| keyboard      | nan    | cell phone    | nan    | microwave      | nan    |
| oven          | nan    | bike          | nan    | hydrant        | nan    |
| motor         | nan    | rider         | nan    | light          | nan    |
| sign          | nan    | motor vehicle | nan    | human face     | nan    |
| hair drier    | nan    | license plate | nan    |                |        |
============== The  23  *  2000  iterations ============
4 channel input
[32m[04/15 12:13:30 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 12:13:31 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.17 seconds.
[5m[31mWARNING[0m [32m[04/15 12:13:31 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 12:13:31 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 12:13:32 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 12:13:32 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 12:13:32 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 12:13:37 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 12:13:51 d2.utils.events]: [0m eta: 0:18:01  iter: 19  total_loss: 0.628  loss_cls: 0.190  loss_box_reg: 0.362  loss_rpn_cls: 0.014  loss_rpn_loc: 0.045  time: 0.5388  data_time: 0.1352  lr: 0.000100  max_mem: 3067M
[32m[04/15 12:14:02 d2.utils.events]: [0m eta: 0:18:12  iter: 39  total_loss: 0.616  loss_cls: 0.219  loss_box_reg: 0.346  loss_rpn_cls: 0.015  loss_rpn_loc: 0.048  time: 0.5631  data_time: 0.0075  lr: 0.000200  max_mem: 3067M
[32m[04/15 12:14:14 d2.utils.events]: [0m eta: 0:17:55  iter: 59  total_loss: 0.645  loss_cls: 0.208  loss_box_reg: 0.380  loss_rpn_cls: 0.009  loss_rpn_loc: 0.049  time: 0.5628  data_time: 0.0060  lr: 0.000300  max_mem: 3067M
[32m[04/15 12:14:26 d2.utils.events]: [0m eta: 0:17:22  iter: 79  total_loss: 0.561  loss_cls: 0.190  loss_box_reg: 0.323  loss_rpn_cls: 0.011  loss_rpn_loc: 0.064  time: 0.5517  data_time: 0.0120  lr: 0.000400  max_mem: 3067M
[32m[04/15 12:14:42 d2.utils.events]: [0m eta: 0:16:59  iter: 99  total_loss: 0.593  loss_cls: 0.189  loss_box_reg: 0.331  loss_rpn_cls: 0.015  loss_rpn_loc: 0.055  time: 0.5456  data_time: 0.0069  lr: 0.000500  max_mem: 3067M
[32m[04/15 12:14:54 d2.utils.events]: [0m eta: 0:17:08  iter: 119  total_loss: 0.581  loss_cls: 0.192  loss_box_reg: 0.334  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5552  data_time: 0.0143  lr: 0.000599  max_mem: 3067M
[32m[04/15 12:15:08 d2.utils.events]: [0m eta: 0:16:46  iter: 139  total_loss: 0.498  loss_cls: 0.154  loss_box_reg: 0.293  loss_rpn_cls: 0.012  loss_rpn_loc: 0.044  time: 0.5509  data_time: 0.0075  lr: 0.000699  max_mem: 3067M
[32m[04/15 12:15:20 d2.utils.events]: [0m eta: 0:16:43  iter: 159  total_loss: 0.580  loss_cls: 0.181  loss_box_reg: 0.343  loss_rpn_cls: 0.012  loss_rpn_loc: 0.052  time: 0.5520  data_time: 0.0062  lr: 0.000799  max_mem: 3067M
[32m[04/15 12:15:32 d2.utils.events]: [0m eta: 0:16:37  iter: 179  total_loss: 0.546  loss_cls: 0.178  loss_box_reg: 0.292  loss_rpn_cls: 0.012  loss_rpn_loc: 0.042  time: 0.5550  data_time: 0.0076  lr: 0.000899  max_mem: 3067M
[32m[04/15 12:15:43 d2.utils.events]: [0m eta: 0:16:35  iter: 199  total_loss: 0.558  loss_cls: 0.181  loss_box_reg: 0.333  loss_rpn_cls: 0.013  loss_rpn_loc: 0.047  time: 0.5588  data_time: 0.0072  lr: 0.000999  max_mem: 3067M
[32m[04/15 12:15:54 d2.utils.events]: [0m eta: 0:16:16  iter: 219  total_loss: 0.539  loss_cls: 0.164  loss_box_reg: 0.303  loss_rpn_cls: 0.013  loss_rpn_loc: 0.042  time: 0.5564  data_time: 0.0063  lr: 0.001099  max_mem: 3067M
[32m[04/15 12:16:05 d2.utils.events]: [0m eta: 0:16:04  iter: 239  total_loss: 0.586  loss_cls: 0.177  loss_box_reg: 0.319  loss_rpn_cls: 0.012  loss_rpn_loc: 0.042  time: 0.5550  data_time: 0.0078  lr: 0.001199  max_mem: 3067M
[32m[04/15 12:16:17 d2.utils.events]: [0m eta: 0:15:51  iter: 259  total_loss: 0.500  loss_cls: 0.165  loss_box_reg: 0.297  loss_rpn_cls: 0.009  loss_rpn_loc: 0.039  time: 0.5588  data_time: 0.0729  lr: 0.001299  max_mem: 3067M
[32m[04/15 12:16:33 d2.utils.events]: [0m eta: 0:15:43  iter: 279  total_loss: 0.609  loss_cls: 0.200  loss_box_reg: 0.341  loss_rpn_cls: 0.011  loss_rpn_loc: 0.039  time: 0.5725  data_time: 0.2509  lr: 0.001399  max_mem: 3067M
[32m[04/15 12:16:45 d2.utils.events]: [0m eta: 0:15:34  iter: 299  total_loss: 0.412  loss_cls: 0.131  loss_box_reg: 0.245  loss_rpn_cls: 0.011  loss_rpn_loc: 0.034  time: 0.5736  data_time: 0.0060  lr: 0.001499  max_mem: 3067M
[32m[04/15 12:17:01 d2.utils.events]: [0m eta: 0:15:22  iter: 319  total_loss: 0.617  loss_cls: 0.180  loss_box_reg: 0.345  loss_rpn_cls: 0.011  loss_rpn_loc: 0.059  time: 0.5730  data_time: 0.0171  lr: 0.001598  max_mem: 3067M
[32m[04/15 12:17:13 d2.utils.events]: [0m eta: 0:15:13  iter: 339  total_loss: 0.640  loss_cls: 0.225  loss_box_reg: 0.336  loss_rpn_cls: 0.014  loss_rpn_loc: 0.050  time: 0.5731  data_time: 0.0072  lr: 0.001698  max_mem: 3067M
[32m[04/15 12:17:24 d2.utils.events]: [0m eta: 0:15:00  iter: 359  total_loss: 0.591  loss_cls: 0.184  loss_box_reg: 0.348  loss_rpn_cls: 0.013  loss_rpn_loc: 0.049  time: 0.5706  data_time: 0.0068  lr: 0.001798  max_mem: 3067M
[32m[04/15 12:17:35 d2.utils.events]: [0m eta: 0:14:49  iter: 379  total_loss: 0.539  loss_cls: 0.162  loss_box_reg: 0.295  loss_rpn_cls: 0.008  loss_rpn_loc: 0.053  time: 0.5688  data_time: 0.0085  lr: 0.001898  max_mem: 3067M
[32m[04/15 12:17:48 d2.utils.events]: [0m eta: 0:14:40  iter: 399  total_loss: 0.666  loss_cls: 0.205  loss_box_reg: 0.355  loss_rpn_cls: 0.012  loss_rpn_loc: 0.059  time: 0.5711  data_time: 0.0080  lr: 0.001998  max_mem: 3067M
[32m[04/15 12:17:59 d2.utils.events]: [0m eta: 0:14:29  iter: 419  total_loss: 0.646  loss_cls: 0.194  loss_box_reg: 0.352  loss_rpn_cls: 0.012  loss_rpn_loc: 0.054  time: 0.5693  data_time: 0.0091  lr: 0.002098  max_mem: 3067M
[32m[04/15 12:18:10 d2.utils.events]: [0m eta: 0:14:16  iter: 439  total_loss: 0.521  loss_cls: 0.166  loss_box_reg: 0.298  loss_rpn_cls: 0.015  loss_rpn_loc: 0.041  time: 0.5677  data_time: 0.0082  lr: 0.002198  max_mem: 3067M
[32m[04/15 12:18:21 d2.utils.events]: [0m eta: 0:14:06  iter: 459  total_loss: 0.619  loss_cls: 0.213  loss_box_reg: 0.335  loss_rpn_cls: 0.012  loss_rpn_loc: 0.059  time: 0.5675  data_time: 0.0108  lr: 0.002298  max_mem: 3067M
[32m[04/15 12:18:33 d2.utils.events]: [0m eta: 0:13:56  iter: 479  total_loss: 0.644  loss_cls: 0.205  loss_box_reg: 0.364  loss_rpn_cls: 0.013  loss_rpn_loc: 0.068  time: 0.5680  data_time: 0.0083  lr: 0.002398  max_mem: 3067M
[32m[04/15 12:18:44 d2.utils.events]: [0m eta: 0:13:44  iter: 499  total_loss: 0.571  loss_cls: 0.182  loss_box_reg: 0.331  loss_rpn_cls: 0.013  loss_rpn_loc: 0.039  time: 0.5662  data_time: 0.0070  lr: 0.002498  max_mem: 3067M
[32m[04/15 12:18:55 d2.utils.events]: [0m eta: 0:13:33  iter: 519  total_loss: 0.626  loss_cls: 0.191  loss_box_reg: 0.367  loss_rpn_cls: 0.013  loss_rpn_loc: 0.056  time: 0.5649  data_time: 0.0067  lr: 0.002597  max_mem: 3067M
[32m[04/15 12:19:07 d2.utils.events]: [0m eta: 0:13:24  iter: 539  total_loss: 0.673  loss_cls: 0.209  loss_box_reg: 0.380  loss_rpn_cls: 0.009  loss_rpn_loc: 0.052  time: 0.5673  data_time: 0.0071  lr: 0.002697  max_mem: 3067M
[32m[04/15 12:19:24 d2.utils.events]: [0m eta: 0:13:12  iter: 559  total_loss: 0.630  loss_cls: 0.189  loss_box_reg: 0.376  loss_rpn_cls: 0.010  loss_rpn_loc: 0.049  time: 0.5662  data_time: 0.0097  lr: 0.002797  max_mem: 3067M
[32m[04/15 12:19:35 d2.utils.events]: [0m eta: 0:13:01  iter: 579  total_loss: 0.459  loss_cls: 0.149  loss_box_reg: 0.270  loss_rpn_cls: 0.009  loss_rpn_loc: 0.040  time: 0.5648  data_time: 0.0072  lr: 0.002897  max_mem: 3067M
[32m[04/15 12:19:48 d2.utils.events]: [0m eta: 0:12:53  iter: 599  total_loss: 0.600  loss_cls: 0.183  loss_box_reg: 0.336  loss_rpn_cls: 0.011  loss_rpn_loc: 0.046  time: 0.5679  data_time: 0.0090  lr: 0.002997  max_mem: 3067M
[32m[04/15 12:19:59 d2.utils.events]: [0m eta: 0:12:41  iter: 619  total_loss: 0.630  loss_cls: 0.198  loss_box_reg: 0.350  loss_rpn_cls: 0.008  loss_rpn_loc: 0.043  time: 0.5670  data_time: 0.0085  lr: 0.003097  max_mem: 3067M
[32m[04/15 12:20:11 d2.utils.events]: [0m eta: 0:12:29  iter: 639  total_loss: 0.526  loss_cls: 0.161  loss_box_reg: 0.303  loss_rpn_cls: 0.011  loss_rpn_loc: 0.044  time: 0.5658  data_time: 0.0074  lr: 0.003197  max_mem: 3067M
[32m[04/15 12:20:23 d2.utils.events]: [0m eta: 0:12:19  iter: 659  total_loss: 0.595  loss_cls: 0.200  loss_box_reg: 0.334  loss_rpn_cls: 0.017  loss_rpn_loc: 0.055  time: 0.5668  data_time: 0.0069  lr: 0.003297  max_mem: 3067M
[32m[04/15 12:20:34 d2.utils.events]: [0m eta: 0:12:09  iter: 679  total_loss: 0.487  loss_cls: 0.151  loss_box_reg: 0.291  loss_rpn_cls: 0.013  loss_rpn_loc: 0.033  time: 0.5668  data_time: 0.0071  lr: 0.003397  max_mem: 3067M
[32m[04/15 12:20:45 d2.utils.events]: [0m eta: 0:11:56  iter: 699  total_loss: 0.496  loss_cls: 0.148  loss_box_reg: 0.299  loss_rpn_cls: 0.009  loss_rpn_loc: 0.034  time: 0.5659  data_time: 0.0093  lr: 0.003497  max_mem: 3067M
[32m[04/15 12:20:56 d2.utils.events]: [0m eta: 0:11:45  iter: 719  total_loss: 0.513  loss_cls: 0.167  loss_box_reg: 0.289  loss_rpn_cls: 0.011  loss_rpn_loc: 0.051  time: 0.5652  data_time: 0.0068  lr: 0.003596  max_mem: 3067M
[32m[04/15 12:21:08 d2.utils.events]: [0m eta: 0:11:35  iter: 739  total_loss: 0.556  loss_cls: 0.171  loss_box_reg: 0.307  loss_rpn_cls: 0.007  loss_rpn_loc: 0.051  time: 0.5663  data_time: 0.0064  lr: 0.003696  max_mem: 3067M
[32m[04/15 12:21:21 d2.utils.events]: [0m eta: 0:11:23  iter: 759  total_loss: 0.672  loss_cls: 0.206  loss_box_reg: 0.359  loss_rpn_cls: 0.019  loss_rpn_loc: 0.047  time: 0.5655  data_time: 0.0061  lr: 0.003796  max_mem: 3067M
[32m[04/15 12:21:32 d2.utils.events]: [0m eta: 0:11:11  iter: 779  total_loss: 0.526  loss_cls: 0.178  loss_box_reg: 0.307  loss_rpn_cls: 0.011  loss_rpn_loc: 0.038  time: 0.5649  data_time: 0.0064  lr: 0.003896  max_mem: 3067M
[32m[04/15 12:21:45 d2.utils.events]: [0m eta: 0:11:01  iter: 799  total_loss: 0.596  loss_cls: 0.171  loss_box_reg: 0.362  loss_rpn_cls: 0.013  loss_rpn_loc: 0.044  time: 0.5660  data_time: 0.0071  lr: 0.003996  max_mem: 3067M
[32m[04/15 12:21:55 d2.utils.events]: [0m eta: 0:10:50  iter: 819  total_loss: 0.518  loss_cls: 0.154  loss_box_reg: 0.282  loss_rpn_cls: 0.011  loss_rpn_loc: 0.049  time: 0.5652  data_time: 0.0162  lr: 0.004096  max_mem: 3067M
[32m[04/15 12:22:06 d2.utils.events]: [0m eta: 0:10:38  iter: 839  total_loss: 0.697  loss_cls: 0.217  loss_box_reg: 0.392  loss_rpn_cls: 0.016  loss_rpn_loc: 0.060  time: 0.5639  data_time: 0.0066  lr: 0.004196  max_mem: 3067M
[32m[04/15 12:22:19 d2.utils.events]: [0m eta: 0:10:29  iter: 859  total_loss: 0.571  loss_cls: 0.177  loss_box_reg: 0.318  loss_rpn_cls: 0.016  loss_rpn_loc: 0.044  time: 0.5655  data_time: 0.0073  lr: 0.004296  max_mem: 3067M
[32m[04/15 12:22:36 d2.utils.events]: [0m eta: 0:10:17  iter: 879  total_loss: 0.575  loss_cls: 0.194  loss_box_reg: 0.334  loss_rpn_cls: 0.016  loss_rpn_loc: 0.052  time: 0.5650  data_time: 0.0071  lr: 0.004396  max_mem: 3067M
[32m[04/15 12:22:47 d2.utils.events]: [0m eta: 0:10:06  iter: 899  total_loss: 0.543  loss_cls: 0.177  loss_box_reg: 0.298  loss_rpn_cls: 0.018  loss_rpn_loc: 0.053  time: 0.5649  data_time: 0.0323  lr: 0.004496  max_mem: 3067M
[32m[04/15 12:22:59 d2.utils.events]: [0m eta: 0:09:55  iter: 919  total_loss: 0.541  loss_cls: 0.168  loss_box_reg: 0.293  loss_rpn_cls: 0.011  loss_rpn_loc: 0.043  time: 0.5659  data_time: 0.0074  lr: 0.004595  max_mem: 3067M
[32m[04/15 12:23:10 d2.utils.events]: [0m eta: 0:09:43  iter: 939  total_loss: 0.592  loss_cls: 0.190  loss_box_reg: 0.338  loss_rpn_cls: 0.013  loss_rpn_loc: 0.050  time: 0.5650  data_time: 0.0077  lr: 0.004695  max_mem: 3067M
[32m[04/15 12:23:21 d2.utils.events]: [0m eta: 0:09:32  iter: 959  total_loss: 0.522  loss_cls: 0.184  loss_box_reg: 0.283  loss_rpn_cls: 0.010  loss_rpn_loc: 0.036  time: 0.5645  data_time: 0.0059  lr: 0.004795  max_mem: 3067M
[32m[04/15 12:23:33 d2.utils.events]: [0m eta: 0:09:22  iter: 979  total_loss: 0.590  loss_cls: 0.209  loss_box_reg: 0.342  loss_rpn_cls: 0.016  loss_rpn_loc: 0.048  time: 0.5652  data_time: 0.0095  lr: 0.004895  max_mem: 3067M
[32m[04/15 12:23:45 d2.utils.events]: [0m eta: 0:09:10  iter: 999  total_loss: 0.585  loss_cls: 0.187  loss_box_reg: 0.303  loss_rpn_cls: 0.012  loss_rpn_loc: 0.055  time: 0.5650  data_time: 0.0302  lr: 0.004995  max_mem: 3067M
[32m[04/15 12:23:56 d2.utils.events]: [0m eta: 0:09:00  iter: 1019  total_loss: 0.480  loss_cls: 0.156  loss_box_reg: 0.275  loss_rpn_cls: 0.017  loss_rpn_loc: 0.049  time: 0.5646  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:24:07 d2.utils.events]: [0m eta: 0:08:48  iter: 1039  total_loss: 0.601  loss_cls: 0.193  loss_box_reg: 0.320  loss_rpn_cls: 0.012  loss_rpn_loc: 0.072  time: 0.5647  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:24:19 d2.utils.events]: [0m eta: 0:08:38  iter: 1059  total_loss: 0.612  loss_cls: 0.196  loss_box_reg: 0.347  loss_rpn_cls: 0.015  loss_rpn_loc: 0.058  time: 0.5654  data_time: 0.0380  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:24:32 d2.utils.events]: [0m eta: 0:08:27  iter: 1079  total_loss: 0.572  loss_cls: 0.190  loss_box_reg: 0.330  loss_rpn_cls: 0.016  loss_rpn_loc: 0.048  time: 0.5645  data_time: 0.0097  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:24:44 d2.utils.events]: [0m eta: 0:08:17  iter: 1099  total_loss: 0.505  loss_cls: 0.182  loss_box_reg: 0.280  loss_rpn_cls: 0.011  loss_rpn_loc: 0.054  time: 0.5644  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:24:56 d2.utils.events]: [0m eta: 0:08:06  iter: 1119  total_loss: 0.669  loss_cls: 0.209  loss_box_reg: 0.343  loss_rpn_cls: 0.018  loss_rpn_loc: 0.048  time: 0.5653  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:25:06 d2.utils.events]: [0m eta: 0:07:55  iter: 1139  total_loss: 0.636  loss_cls: 0.200  loss_box_reg: 0.374  loss_rpn_cls: 0.016  loss_rpn_loc: 0.054  time: 0.5643  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:25:17 d2.utils.events]: [0m eta: 0:07:42  iter: 1159  total_loss: 0.577  loss_cls: 0.177  loss_box_reg: 0.308  loss_rpn_cls: 0.009  loss_rpn_loc: 0.051  time: 0.5641  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:25:30 d2.utils.events]: [0m eta: 0:07:32  iter: 1179  total_loss: 0.704  loss_cls: 0.199  loss_box_reg: 0.394  loss_rpn_cls: 0.012  loss_rpn_loc: 0.057  time: 0.5647  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:25:40 d2.utils.events]: [0m eta: 0:07:19  iter: 1199  total_loss: 0.582  loss_cls: 0.187  loss_box_reg: 0.341  loss_rpn_cls: 0.015  loss_rpn_loc: 0.042  time: 0.5638  data_time: 0.0040  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:25:52 d2.utils.events]: [0m eta: 0:07:09  iter: 1219  total_loss: 0.669  loss_cls: 0.206  loss_box_reg: 0.381  loss_rpn_cls: 0.016  loss_rpn_loc: 0.059  time: 0.5645  data_time: 0.0049  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:26:03 d2.utils.events]: [0m eta: 0:06:59  iter: 1239  total_loss: 0.423  loss_cls: 0.131  loss_box_reg: 0.230  loss_rpn_cls: 0.014  loss_rpn_loc: 0.031  time: 0.5641  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:26:14 d2.utils.events]: [0m eta: 0:06:47  iter: 1259  total_loss: 0.548  loss_cls: 0.187  loss_box_reg: 0.314  loss_rpn_cls: 0.014  loss_rpn_loc: 0.053  time: 0.5634  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:26:24 d2.utils.events]: [0m eta: 0:06:36  iter: 1279  total_loss: 0.712  loss_cls: 0.213  loss_box_reg: 0.417  loss_rpn_cls: 0.016  loss_rpn_loc: 0.059  time: 0.5628  data_time: 0.0096  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:26:37 d2.utils.events]: [0m eta: 0:06:25  iter: 1299  total_loss: 0.534  loss_cls: 0.159  loss_box_reg: 0.333  loss_rpn_cls: 0.014  loss_rpn_loc: 0.028  time: 0.5636  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:26:48 d2.utils.events]: [0m eta: 0:06:14  iter: 1319  total_loss: 0.562  loss_cls: 0.173  loss_box_reg: 0.307  loss_rpn_cls: 0.014  loss_rpn_loc: 0.047  time: 0.5635  data_time: 0.0261  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:26:59 d2.utils.events]: [0m eta: 0:06:03  iter: 1339  total_loss: 0.604  loss_cls: 0.204  loss_box_reg: 0.332  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5629  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:27:14 d2.utils.events]: [0m eta: 0:05:52  iter: 1359  total_loss: 0.633  loss_cls: 0.191  loss_box_reg: 0.368  loss_rpn_cls: 0.012  loss_rpn_loc: 0.053  time: 0.5636  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:27:25 d2.utils.events]: [0m eta: 0:05:41  iter: 1379  total_loss: 0.576  loss_cls: 0.177  loss_box_reg: 0.313  loss_rpn_cls: 0.013  loss_rpn_loc: 0.035  time: 0.5633  data_time: 0.0102  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:27:36 d2.utils.events]: [0m eta: 0:05:30  iter: 1399  total_loss: 0.623  loss_cls: 0.201  loss_box_reg: 0.335  loss_rpn_cls: 0.012  loss_rpn_loc: 0.049  time: 0.5630  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:28:17 d2.utils.events]: [0m eta: 0:05:19  iter: 1419  total_loss: 0.615  loss_cls: 0.182  loss_box_reg: 0.339  loss_rpn_cls: 0.016  loss_rpn_loc: 0.054  time: 0.5631  data_time: 0.0096  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:28:29 d2.utils.events]: [0m eta: 0:05:08  iter: 1439  total_loss: 0.687  loss_cls: 0.208  loss_box_reg: 0.376  loss_rpn_cls: 0.013  loss_rpn_loc: 0.064  time: 0.5634  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:28:40 d2.utils.events]: [0m eta: 0:04:57  iter: 1459  total_loss: 0.758  loss_cls: 0.219  loss_box_reg: 0.419  loss_rpn_cls: 0.017  loss_rpn_loc: 0.068  time: 0.5629  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:28:52 d2.utils.events]: [0m eta: 0:04:46  iter: 1479  total_loss: 0.683  loss_cls: 0.220  loss_box_reg: 0.391  loss_rpn_cls: 0.013  loss_rpn_loc: 0.056  time: 0.5626  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:29:04 d2.utils.events]: [0m eta: 0:04:35  iter: 1499  total_loss: 0.598  loss_cls: 0.198  loss_box_reg: 0.319  loss_rpn_cls: 0.014  loss_rpn_loc: 0.051  time: 0.5630  data_time: 0.0094  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:29:15 d2.utils.events]: [0m eta: 0:04:24  iter: 1519  total_loss: 0.574  loss_cls: 0.177  loss_box_reg: 0.315  loss_rpn_cls: 0.013  loss_rpn_loc: 0.046  time: 0.5627  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:29:27 d2.utils.events]: [0m eta: 0:04:12  iter: 1539  total_loss: 0.632  loss_cls: 0.208  loss_box_reg: 0.359  loss_rpn_cls: 0.012  loss_rpn_loc: 0.060  time: 0.5624  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:29:39 d2.utils.events]: [0m eta: 0:04:02  iter: 1559  total_loss: 0.721  loss_cls: 0.235  loss_box_reg: 0.361  loss_rpn_cls: 0.020  loss_rpn_loc: 0.074  time: 0.5631  data_time: 0.0108  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:29:49 d2.utils.events]: [0m eta: 0:03:51  iter: 1579  total_loss: 0.699  loss_cls: 0.212  loss_box_reg: 0.405  loss_rpn_cls: 0.021  loss_rpn_loc: 0.061  time: 0.5626  data_time: 0.0101  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:30:01 d2.utils.events]: [0m eta: 0:03:39  iter: 1599  total_loss: 0.552  loss_cls: 0.158  loss_box_reg: 0.324  loss_rpn_cls: 0.012  loss_rpn_loc: 0.049  time: 0.5624  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:30:13 d2.utils.events]: [0m eta: 0:03:29  iter: 1619  total_loss: 0.576  loss_cls: 0.180  loss_box_reg: 0.305  loss_rpn_cls: 0.012  loss_rpn_loc: 0.055  time: 0.5629  data_time: 0.0095  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:30:24 d2.utils.events]: [0m eta: 0:03:18  iter: 1639  total_loss: 0.697  loss_cls: 0.215  loss_box_reg: 0.395  loss_rpn_cls: 0.014  loss_rpn_loc: 0.062  time: 0.5628  data_time: 0.0099  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:30:35 d2.utils.events]: [0m eta: 0:03:07  iter: 1659  total_loss: 0.683  loss_cls: 0.224  loss_box_reg: 0.395  loss_rpn_cls: 0.018  loss_rpn_loc: 0.065  time: 0.5624  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:30:47 d2.utils.events]: [0m eta: 0:02:56  iter: 1679  total_loss: 0.579  loss_cls: 0.193  loss_box_reg: 0.338  loss_rpn_cls: 0.015  loss_rpn_loc: 0.046  time: 0.5628  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:30:58 d2.utils.events]: [0m eta: 0:02:45  iter: 1699  total_loss: 0.559  loss_cls: 0.190  loss_box_reg: 0.312  loss_rpn_cls: 0.011  loss_rpn_loc: 0.054  time: 0.5628  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:31:09 d2.utils.events]: [0m eta: 0:02:34  iter: 1719  total_loss: 0.551  loss_cls: 0.187  loss_box_reg: 0.314  loss_rpn_cls: 0.016  loss_rpn_loc: 0.058  time: 0.5626  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:31:21 d2.utils.events]: [0m eta: 0:02:23  iter: 1739  total_loss: 0.580  loss_cls: 0.188  loss_box_reg: 0.315  loss_rpn_cls: 0.014  loss_rpn_loc: 0.061  time: 0.5628  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:31:33 d2.utils.events]: [0m eta: 0:02:12  iter: 1759  total_loss: 0.580  loss_cls: 0.188  loss_box_reg: 0.289  loss_rpn_cls: 0.011  loss_rpn_loc: 0.059  time: 0.5631  data_time: 0.0135  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:31:44 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.754  loss_cls: 0.226  loss_box_reg: 0.405  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5629  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:31:55 d2.utils.events]: [0m eta: 0:01:50  iter: 1799  total_loss: 0.601  loss_cls: 0.185  loss_box_reg: 0.372  loss_rpn_cls: 0.013  loss_rpn_loc: 0.052  time: 0.5625  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:32:06 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.608  loss_cls: 0.175  loss_box_reg: 0.334  loss_rpn_cls: 0.021  loss_rpn_loc: 0.053  time: 0.5627  data_time: 0.0118  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:32:18 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.634  loss_cls: 0.204  loss_box_reg: 0.359  loss_rpn_cls: 0.011  loss_rpn_loc: 0.053  time: 0.5626  data_time: 0.0181  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:32:29 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.673  loss_cls: 0.220  loss_box_reg: 0.370  loss_rpn_cls: 0.018  loss_rpn_loc: 0.065  time: 0.5626  data_time: 0.0128  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:32:41 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.514  loss_cls: 0.151  loss_box_reg: 0.316  loss_rpn_cls: 0.008  loss_rpn_loc: 0.042  time: 0.5631  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:32:53 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.604  loss_cls: 0.200  loss_box_reg: 0.355  loss_rpn_cls: 0.020  loss_rpn_loc: 0.055  time: 0.5631  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:33:04 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.676  loss_cls: 0.220  loss_box_reg: 0.356  loss_rpn_cls: 0.014  loss_rpn_loc: 0.067  time: 0.5633  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:33:15 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.566  loss_cls: 0.188  loss_box_reg: 0.334  loss_rpn_cls: 0.016  loss_rpn_loc: 0.049  time: 0.5630  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:33:26 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.700  loss_cls: 0.215  loss_box_reg: 0.384  loss_rpn_cls: 0.016  loss_rpn_loc: 0.054  time: 0.5627  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:33:39 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.644  loss_cls: 0.211  loss_box_reg: 0.351  loss_rpn_cls: 0.013  loss_rpn_loc: 0.053  time: 0.5633  data_time: 0.0100  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 12:34:21 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 12:34:21 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 12:34:21 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 12:34:21 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.643  loss_cls: 0.215  loss_box_reg: 0.388  loss_rpn_cls: 0.015  loss_rpn_loc: 0.057  time: 0.5631  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:34:21 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:45 (0.5634 s / it)
[32m[04/15 12:34:21 d2.engine.hooks]: [0mTotal training time: 0:20:40 (0:01:54 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 12:34:32 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 12:34:32 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 12:34:32 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 12:34:36 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1191 s / img. ETA=0:04:57
[32m[04/15 12:34:41 d2.evaluation.evaluator]: [0mInference done 32/1257. 0.1307 s / img. ETA=0:04:52
[32m[04/15 12:34:46 d2.evaluation.evaluator]: [0mInference done 49/1257. 0.1416 s / img. ETA=0:05:14
[32m[04/15 12:34:51 d2.evaluation.evaluator]: [0mInference done 72/1257. 0.1342 s / img. ETA=0:04:52
[32m[04/15 12:34:56 d2.evaluation.evaluator]: [0mInference done 95/1257. 0.1308 s / img. ETA=0:04:39
[32m[04/15 12:35:01 d2.evaluation.evaluator]: [0mInference done 123/1257. 0.1311 s / img. ETA=0:04:16
[32m[04/15 12:35:06 d2.evaluation.evaluator]: [0mInference done 152/1257. 0.1289 s / img. ETA=0:03:59
[32m[04/15 12:35:11 d2.evaluation.evaluator]: [0mInference done 186/1257. 0.1273 s / img. ETA=0:03:38
[32m[04/15 12:35:16 d2.evaluation.evaluator]: [0mInference done 213/1257. 0.1266 s / img. ETA=0:03:31
[32m[04/15 12:35:22 d2.evaluation.evaluator]: [0mInference done 237/1257. 0.1278 s / img. ETA=0:03:29
[32m[04/15 12:35:27 d2.evaluation.evaluator]: [0mInference done 257/1257. 0.1284 s / img. ETA=0:03:30
[32m[04/15 12:35:32 d2.evaluation.evaluator]: [0mInference done 273/1257. 0.1279 s / img. ETA=0:03:32
[32m[04/15 12:35:37 d2.evaluation.evaluator]: [0mInference done 291/1257. 0.1274 s / img. ETA=0:03:32
[32m[04/15 12:35:42 d2.evaluation.evaluator]: [0mInference done 306/1257. 0.1269 s / img. ETA=0:03:35
[32m[04/15 12:35:48 d2.evaluation.evaluator]: [0mInference done 334/1257. 0.1262 s / img. ETA=0:03:25
[32m[04/15 12:35:53 d2.evaluation.evaluator]: [0mInference done 370/1257. 0.1266 s / img. ETA=0:03:10
[32m[04/15 12:35:58 d2.evaluation.evaluator]: [0mInference done 400/1257. 0.1277 s / img. ETA=0:03:00
[32m[04/15 12:36:03 d2.evaluation.evaluator]: [0mInference done 431/1257. 0.1284 s / img. ETA=0:02:52
[32m[04/15 12:36:08 d2.evaluation.evaluator]: [0mInference done 447/1257. 0.1281 s / img. ETA=0:02:52
[32m[04/15 12:36:14 d2.evaluation.evaluator]: [0mInference done 472/1257. 0.1276 s / img. ETA=0:02:47
[32m[04/15 12:36:19 d2.evaluation.evaluator]: [0mInference done 492/1257. 0.1273 s / img. ETA=0:02:44
[32m[04/15 12:36:24 d2.evaluation.evaluator]: [0mInference done 512/1257. 0.1270 s / img. ETA=0:02:41
[32m[04/15 12:36:29 d2.evaluation.evaluator]: [0mInference done 531/1257. 0.1268 s / img. ETA=0:02:38
[32m[04/15 12:36:34 d2.evaluation.evaluator]: [0mInference done 551/1257. 0.1272 s / img. ETA=0:02:35
[32m[04/15 12:36:40 d2.evaluation.evaluator]: [0mInference done 571/1257. 0.1293 s / img. ETA=0:02:31
[32m[04/15 12:36:45 d2.evaluation.evaluator]: [0mInference done 602/1257. 0.1288 s / img. ETA=0:02:23
[32m[04/15 12:36:50 d2.evaluation.evaluator]: [0mInference done 629/1257. 0.1284 s / img. ETA=0:02:16
[32m[04/15 12:36:55 d2.evaluation.evaluator]: [0mInference done 649/1257. 0.1281 s / img. ETA=0:02:13
[32m[04/15 12:37:00 d2.evaluation.evaluator]: [0mInference done 674/1257. 0.1279 s / img. ETA=0:02:07
[32m[04/15 12:37:05 d2.evaluation.evaluator]: [0mInference done 693/1257. 0.1276 s / img. ETA=0:02:03
[32m[04/15 12:37:10 d2.evaluation.evaluator]: [0mInference done 708/1257. 0.1277 s / img. ETA=0:02:01
[32m[04/15 12:37:16 d2.evaluation.evaluator]: [0mInference done 740/1257. 0.1286 s / img. ETA=0:01:53
[32m[04/15 12:37:21 d2.evaluation.evaluator]: [0mInference done 765/1257. 0.1283 s / img. ETA=0:01:47
[32m[04/15 12:37:26 d2.evaluation.evaluator]: [0mInference done 793/1257. 0.1280 s / img. ETA=0:01:40
[32m[04/15 12:37:31 d2.evaluation.evaluator]: [0mInference done 811/1257. 0.1278 s / img. ETA=0:01:37
[32m[04/15 12:37:36 d2.evaluation.evaluator]: [0mInference done 835/1257. 0.1276 s / img. ETA=0:01:32
[32m[04/15 12:37:41 d2.evaluation.evaluator]: [0mInference done 857/1257. 0.1274 s / img. ETA=0:01:27
[32m[04/15 12:37:47 d2.evaluation.evaluator]: [0mInference done 883/1257. 0.1273 s / img. ETA=0:01:21
[32m[04/15 12:37:52 d2.evaluation.evaluator]: [0mInference done 903/1257. 0.1277 s / img. ETA=0:01:17
[32m[04/15 12:37:57 d2.evaluation.evaluator]: [0mInference done 915/1257. 0.1276 s / img. ETA=0:01:16
[32m[04/15 12:38:02 d2.evaluation.evaluator]: [0mInference done 936/1257. 0.1274 s / img. ETA=0:01:11
[32m[04/15 12:38:07 d2.evaluation.evaluator]: [0mInference done 968/1257. 0.1271 s / img. ETA=0:01:03
[32m[04/15 12:38:12 d2.evaluation.evaluator]: [0mInference done 1002/1257. 0.1269 s / img. ETA=0:00:55
[32m[04/15 12:38:17 d2.evaluation.evaluator]: [0mInference done 1038/1257. 0.1266 s / img. ETA=0:00:47
[32m[04/15 12:38:22 d2.evaluation.evaluator]: [0mInference done 1072/1257. 0.1266 s / img. ETA=0:00:39
[32m[04/15 12:38:27 d2.evaluation.evaluator]: [0mInference done 1105/1257. 0.1273 s / img. ETA=0:00:32
[32m[04/15 12:38:32 d2.evaluation.evaluator]: [0mInference done 1135/1257. 0.1272 s / img. ETA=0:00:25
[32m[04/15 12:38:37 d2.evaluation.evaluator]: [0mInference done 1160/1257. 0.1270 s / img. ETA=0:00:20
[32m[04/15 12:38:42 d2.evaluation.evaluator]: [0mInference done 1188/1257. 0.1268 s / img. ETA=0:00:14
[32m[04/15 12:38:47 d2.evaluation.evaluator]: [0mInference done 1209/1257. 0.1267 s / img. ETA=0:00:10
[32m[04/15 12:38:53 d2.evaluation.evaluator]: [0mInference done 1219/1257. 0.1267 s / img. ETA=0:00:08
[32m[04/15 12:38:58 d2.evaluation.evaluator]: [0mInference done 1243/1257. 0.1265 s / img. ETA=0:00:02
[32m[04/15 12:39:00 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:25.782073 (0.212286 s / img per device, on 1 devices)
[32m[04/15 12:39:00 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:38 (0.126481 s / img per device, on 1 devices)
[32m[04/15 12:39:00 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 12:39:00 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 12:39:01 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.67s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=6.41s).
Accumulating evaluation results...
DONE (t=2.21s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.195
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.404
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.122
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.241
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.416
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.113
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.265
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.267
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.169
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.454
[32m[04/15 12:39:12 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.513 | 40.409 | 15.894 | 12.237 | 24.055 | 41.570 |
[32m[04/15 12:39:12 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 26.487 | bicycle       | 8.505 | car            | 42.385 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.676 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  24  *  2000  iterations ============
4 channel input
[32m[04/15 12:39:13 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 12:39:14 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.19 seconds.
[5m[31mWARNING[0m [32m[04/15 12:39:14 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 12:39:15 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 12:39:15 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 12:39:16 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 12:39:16 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 12:39:22 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 12:39:35 d2.utils.events]: [0m eta: 0:17:20  iter: 19  total_loss: 0.578  loss_cls: 0.176  loss_box_reg: 0.306  loss_rpn_cls: 0.010  loss_rpn_loc: 0.051  time: 0.5228  data_time: 0.0944  lr: 0.000100  max_mem: 3067M
[32m[04/15 12:39:47 d2.utils.events]: [0m eta: 0:18:07  iter: 39  total_loss: 0.491  loss_cls: 0.157  loss_box_reg: 0.289  loss_rpn_cls: 0.013  loss_rpn_loc: 0.042  time: 0.5635  data_time: 0.0121  lr: 0.000200  max_mem: 3067M
[32m[04/15 12:39:58 d2.utils.events]: [0m eta: 0:17:53  iter: 59  total_loss: 0.564  loss_cls: 0.179  loss_box_reg: 0.317  loss_rpn_cls: 0.010  loss_rpn_loc: 0.045  time: 0.5678  data_time: 0.0297  lr: 0.000300  max_mem: 3067M
[32m[04/15 12:40:09 d2.utils.events]: [0m eta: 0:17:29  iter: 79  total_loss: 0.626  loss_cls: 0.202  loss_box_reg: 0.305  loss_rpn_cls: 0.028  loss_rpn_loc: 0.073  time: 0.5602  data_time: 0.0270  lr: 0.000400  max_mem: 3067M
[32m[04/15 12:40:20 d2.utils.events]: [0m eta: 0:17:15  iter: 99  total_loss: 0.581  loss_cls: 0.197  loss_box_reg: 0.324  loss_rpn_cls: 0.013  loss_rpn_loc: 0.055  time: 0.5549  data_time: 0.0065  lr: 0.000500  max_mem: 3067M
[32m[04/15 12:40:32 d2.utils.events]: [0m eta: 0:17:14  iter: 119  total_loss: 0.569  loss_cls: 0.161  loss_box_reg: 0.322  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5647  data_time: 0.0353  lr: 0.000599  max_mem: 3067M
[32m[04/15 12:40:44 d2.utils.events]: [0m eta: 0:17:00  iter: 139  total_loss: 0.647  loss_cls: 0.189  loss_box_reg: 0.388  loss_rpn_cls: 0.014  loss_rpn_loc: 0.049  time: 0.5624  data_time: 0.0626  lr: 0.000699  max_mem: 3067M
[32m[04/15 12:40:58 d2.utils.events]: [0m eta: 0:16:52  iter: 159  total_loss: 0.556  loss_cls: 0.181  loss_box_reg: 0.348  loss_rpn_cls: 0.009  loss_rpn_loc: 0.051  time: 0.5598  data_time: 0.0063  lr: 0.000799  max_mem: 3067M
[32m[04/15 12:41:10 d2.utils.events]: [0m eta: 0:16:45  iter: 179  total_loss: 0.672  loss_cls: 0.213  loss_box_reg: 0.375  loss_rpn_cls: 0.013  loss_rpn_loc: 0.052  time: 0.5671  data_time: 0.0368  lr: 0.000899  max_mem: 3067M
[32m[04/15 12:41:29 d2.utils.events]: [0m eta: 0:16:26  iter: 199  total_loss: 0.544  loss_cls: 0.164  loss_box_reg: 0.291  loss_rpn_cls: 0.014  loss_rpn_loc: 0.048  time: 0.5617  data_time: 0.0069  lr: 0.000999  max_mem: 3067M
[32m[04/15 12:41:41 d2.utils.events]: [0m eta: 0:16:16  iter: 219  total_loss: 0.520  loss_cls: 0.161  loss_box_reg: 0.307  loss_rpn_cls: 0.014  loss_rpn_loc: 0.037  time: 0.5627  data_time: 0.0098  lr: 0.001099  max_mem: 3067M
[32m[04/15 12:41:53 d2.utils.events]: [0m eta: 0:16:16  iter: 239  total_loss: 0.608  loss_cls: 0.189  loss_box_reg: 0.360  loss_rpn_cls: 0.008  loss_rpn_loc: 0.043  time: 0.5654  data_time: 0.0077  lr: 0.001199  max_mem: 3067M
[32m[04/15 12:42:04 d2.utils.events]: [0m eta: 0:16:05  iter: 259  total_loss: 0.513  loss_cls: 0.165  loss_box_reg: 0.291  loss_rpn_cls: 0.007  loss_rpn_loc: 0.037  time: 0.5635  data_time: 0.0102  lr: 0.001299  max_mem: 3067M
[32m[04/15 12:42:15 d2.utils.events]: [0m eta: 0:15:50  iter: 279  total_loss: 0.597  loss_cls: 0.192  loss_box_reg: 0.358  loss_rpn_cls: 0.010  loss_rpn_loc: 0.048  time: 0.5612  data_time: 0.0104  lr: 0.001399  max_mem: 3067M
[32m[04/15 12:42:29 d2.utils.events]: [0m eta: 0:15:47  iter: 299  total_loss: 0.548  loss_cls: 0.182  loss_box_reg: 0.330  loss_rpn_cls: 0.012  loss_rpn_loc: 0.052  time: 0.5662  data_time: 0.0075  lr: 0.001499  max_mem: 3067M
[32m[04/15 12:42:39 d2.utils.events]: [0m eta: 0:15:32  iter: 319  total_loss: 0.496  loss_cls: 0.173  loss_box_reg: 0.319  loss_rpn_cls: 0.012  loss_rpn_loc: 0.038  time: 0.5635  data_time: 0.0063  lr: 0.001598  max_mem: 3067M
[32m[04/15 12:42:50 d2.utils.events]: [0m eta: 0:15:15  iter: 339  total_loss: 0.458  loss_cls: 0.149  loss_box_reg: 0.256  loss_rpn_cls: 0.008  loss_rpn_loc: 0.043  time: 0.5607  data_time: 0.0070  lr: 0.001698  max_mem: 3067M
[32m[04/15 12:43:04 d2.utils.events]: [0m eta: 0:15:07  iter: 359  total_loss: 0.548  loss_cls: 0.157  loss_box_reg: 0.302  loss_rpn_cls: 0.009  loss_rpn_loc: 0.040  time: 0.5628  data_time: 0.0072  lr: 0.001798  max_mem: 3067M
[32m[04/15 12:43:15 d2.utils.events]: [0m eta: 0:14:58  iter: 379  total_loss: 0.589  loss_cls: 0.167  loss_box_reg: 0.337  loss_rpn_cls: 0.008  loss_rpn_loc: 0.055  time: 0.5622  data_time: 0.0075  lr: 0.001898  max_mem: 3067M
[32m[04/15 12:43:28 d2.utils.events]: [0m eta: 0:14:48  iter: 399  total_loss: 0.574  loss_cls: 0.171  loss_box_reg: 0.294  loss_rpn_cls: 0.008  loss_rpn_loc: 0.040  time: 0.5626  data_time: 0.0762  lr: 0.001998  max_mem: 3067M
[32m[04/15 12:43:46 d2.utils.events]: [0m eta: 0:14:37  iter: 419  total_loss: 0.529  loss_cls: 0.163  loss_box_reg: 0.299  loss_rpn_cls: 0.014  loss_rpn_loc: 0.038  time: 0.5638  data_time: 0.0077  lr: 0.002098  max_mem: 3067M
[32m[04/15 12:43:58 d2.utils.events]: [0m eta: 0:14:24  iter: 439  total_loss: 0.467  loss_cls: 0.148  loss_box_reg: 0.271  loss_rpn_cls: 0.009  loss_rpn_loc: 0.052  time: 0.5623  data_time: 0.0076  lr: 0.002198  max_mem: 3067M
[32m[04/15 12:44:09 d2.utils.events]: [0m eta: 0:14:11  iter: 459  total_loss: 0.534  loss_cls: 0.161  loss_box_reg: 0.285  loss_rpn_cls: 0.012  loss_rpn_loc: 0.046  time: 0.5610  data_time: 0.0171  lr: 0.002298  max_mem: 3067M
[32m[04/15 12:44:22 d2.utils.events]: [0m eta: 0:14:03  iter: 479  total_loss: 0.564  loss_cls: 0.168  loss_box_reg: 0.312  loss_rpn_cls: 0.008  loss_rpn_loc: 0.044  time: 0.5630  data_time: 0.0085  lr: 0.002398  max_mem: 3067M
[32m[04/15 12:44:34 d2.utils.events]: [0m eta: 0:13:52  iter: 499  total_loss: 0.489  loss_cls: 0.135  loss_box_reg: 0.279  loss_rpn_cls: 0.011  loss_rpn_loc: 0.041  time: 0.5626  data_time: 0.0629  lr: 0.002498  max_mem: 3067M
[32m[04/15 12:44:46 d2.utils.events]: [0m eta: 0:13:37  iter: 519  total_loss: 0.510  loss_cls: 0.171  loss_box_reg: 0.285  loss_rpn_cls: 0.009  loss_rpn_loc: 0.039  time: 0.5617  data_time: 0.0073  lr: 0.002597  max_mem: 3067M
[32m[04/15 12:44:58 d2.utils.events]: [0m eta: 0:13:30  iter: 539  total_loss: 0.570  loss_cls: 0.170  loss_box_reg: 0.347  loss_rpn_cls: 0.013  loss_rpn_loc: 0.051  time: 0.5635  data_time: 0.0093  lr: 0.002697  max_mem: 3067M
[32m[04/15 12:45:09 d2.utils.events]: [0m eta: 0:13:18  iter: 559  total_loss: 0.609  loss_cls: 0.209  loss_box_reg: 0.331  loss_rpn_cls: 0.011  loss_rpn_loc: 0.047  time: 0.5627  data_time: 0.0093  lr: 0.002797  max_mem: 3067M
[32m[04/15 12:45:20 d2.utils.events]: [0m eta: 0:13:07  iter: 579  total_loss: 0.505  loss_cls: 0.177  loss_box_reg: 0.275  loss_rpn_cls: 0.010  loss_rpn_loc: 0.058  time: 0.5618  data_time: 0.0156  lr: 0.002897  max_mem: 3067M
[32m[04/15 12:45:33 d2.utils.events]: [0m eta: 0:12:56  iter: 599  total_loss: 0.596  loss_cls: 0.164  loss_box_reg: 0.352  loss_rpn_cls: 0.012  loss_rpn_loc: 0.064  time: 0.5629  data_time: 0.0081  lr: 0.002997  max_mem: 3067M
[32m[04/15 12:45:45 d2.utils.events]: [0m eta: 0:12:45  iter: 619  total_loss: 0.648  loss_cls: 0.182  loss_box_reg: 0.341  loss_rpn_cls: 0.013  loss_rpn_loc: 0.078  time: 0.5629  data_time: 0.0090  lr: 0.003097  max_mem: 3067M
[32m[04/15 12:45:55 d2.utils.events]: [0m eta: 0:12:30  iter: 639  total_loss: 0.597  loss_cls: 0.192  loss_box_reg: 0.355  loss_rpn_cls: 0.011  loss_rpn_loc: 0.043  time: 0.5615  data_time: 0.0073  lr: 0.003197  max_mem: 3067M
[32m[04/15 12:46:10 d2.utils.events]: [0m eta: 0:12:19  iter: 659  total_loss: 0.701  loss_cls: 0.223  loss_box_reg: 0.380  loss_rpn_cls: 0.012  loss_rpn_loc: 0.064  time: 0.5617  data_time: 0.0097  lr: 0.003297  max_mem: 3067M
[32m[04/15 12:46:22 d2.utils.events]: [0m eta: 0:12:09  iter: 679  total_loss: 0.641  loss_cls: 0.193  loss_box_reg: 0.344  loss_rpn_cls: 0.013  loss_rpn_loc: 0.066  time: 0.5620  data_time: 0.0084  lr: 0.003397  max_mem: 3067M
[32m[04/15 12:46:34 d2.utils.events]: [0m eta: 0:11:57  iter: 699  total_loss: 0.561  loss_cls: 0.158  loss_box_reg: 0.334  loss_rpn_cls: 0.012  loss_rpn_loc: 0.042  time: 0.5613  data_time: 0.0087  lr: 0.003497  max_mem: 3067M
[32m[04/15 12:46:45 d2.utils.events]: [0m eta: 0:11:46  iter: 719  total_loss: 0.634  loss_cls: 0.186  loss_box_reg: 0.363  loss_rpn_cls: 0.009  loss_rpn_loc: 0.052  time: 0.5613  data_time: 0.0093  lr: 0.003596  max_mem: 3067M
[32m[04/15 12:46:57 d2.utils.events]: [0m eta: 0:11:36  iter: 739  total_loss: 0.606  loss_cls: 0.175  loss_box_reg: 0.336  loss_rpn_cls: 0.015  loss_rpn_loc: 0.051  time: 0.5622  data_time: 0.0209  lr: 0.003696  max_mem: 3067M
[32m[04/15 12:47:08 d2.utils.events]: [0m eta: 0:11:24  iter: 759  total_loss: 0.571  loss_cls: 0.190  loss_box_reg: 0.340  loss_rpn_cls: 0.010  loss_rpn_loc: 0.050  time: 0.5616  data_time: 0.0079  lr: 0.003796  max_mem: 3067M
[32m[04/15 12:47:22 d2.utils.events]: [0m eta: 0:11:13  iter: 779  total_loss: 0.568  loss_cls: 0.170  loss_box_reg: 0.329  loss_rpn_cls: 0.011  loss_rpn_loc: 0.052  time: 0.5614  data_time: 0.0266  lr: 0.003896  max_mem: 3067M
[32m[04/15 12:47:35 d2.utils.events]: [0m eta: 0:11:03  iter: 799  total_loss: 0.706  loss_cls: 0.230  loss_box_reg: 0.394  loss_rpn_cls: 0.015  loss_rpn_loc: 0.076  time: 0.5626  data_time: 0.0111  lr: 0.003996  max_mem: 3067M
[32m[04/15 12:47:47 d2.utils.events]: [0m eta: 0:10:52  iter: 819  total_loss: 0.592  loss_cls: 0.185  loss_box_reg: 0.312  loss_rpn_cls: 0.014  loss_rpn_loc: 0.054  time: 0.5631  data_time: 0.0551  lr: 0.004096  max_mem: 3067M
[32m[04/15 12:47:59 d2.utils.events]: [0m eta: 0:10:43  iter: 839  total_loss: 0.561  loss_cls: 0.174  loss_box_reg: 0.319  loss_rpn_cls: 0.009  loss_rpn_loc: 0.039  time: 0.5633  data_time: 0.0071  lr: 0.004196  max_mem: 3067M
[32m[04/15 12:48:10 d2.utils.events]: [0m eta: 0:10:30  iter: 859  total_loss: 0.649  loss_cls: 0.220  loss_box_reg: 0.379  loss_rpn_cls: 0.014  loss_rpn_loc: 0.059  time: 0.5630  data_time: 0.0282  lr: 0.004296  max_mem: 3067M
[32m[04/15 12:48:26 d2.utils.events]: [0m eta: 0:10:20  iter: 879  total_loss: 0.608  loss_cls: 0.190  loss_box_reg: 0.354  loss_rpn_cls: 0.015  loss_rpn_loc: 0.047  time: 0.5648  data_time: 0.1324  lr: 0.004396  max_mem: 3067M
[32m[04/15 12:48:38 d2.utils.events]: [0m eta: 0:10:09  iter: 899  total_loss: 0.682  loss_cls: 0.218  loss_box_reg: 0.373  loss_rpn_cls: 0.016  loss_rpn_loc: 0.059  time: 0.5645  data_time: 0.0104  lr: 0.004496  max_mem: 3067M
[32m[04/15 12:48:50 d2.utils.events]: [0m eta: 0:09:59  iter: 919  total_loss: 0.534  loss_cls: 0.165  loss_box_reg: 0.282  loss_rpn_cls: 0.009  loss_rpn_loc: 0.058  time: 0.5653  data_time: 0.0085  lr: 0.004595  max_mem: 3067M
[32m[04/15 12:49:04 d2.utils.events]: [0m eta: 0:09:49  iter: 939  total_loss: 0.595  loss_cls: 0.183  loss_box_reg: 0.314  loss_rpn_cls: 0.014  loss_rpn_loc: 0.049  time: 0.5667  data_time: 0.0919  lr: 0.004695  max_mem: 3067M
[32m[04/15 12:49:19 d2.utils.events]: [0m eta: 0:09:37  iter: 959  total_loss: 0.569  loss_cls: 0.167  loss_box_reg: 0.291  loss_rpn_cls: 0.012  loss_rpn_loc: 0.060  time: 0.5663  data_time: 0.0491  lr: 0.004795  max_mem: 3067M
[32m[04/15 12:49:35 d2.utils.events]: [0m eta: 0:09:28  iter: 979  total_loss: 0.524  loss_cls: 0.165  loss_box_reg: 0.281  loss_rpn_cls: 0.012  loss_rpn_loc: 0.047  time: 0.5678  data_time: 0.0085  lr: 0.004895  max_mem: 3067M
[32m[04/15 12:49:50 d2.utils.events]: [0m eta: 0:09:17  iter: 999  total_loss: 0.586  loss_cls: 0.194  loss_box_reg: 0.334  loss_rpn_cls: 0.010  loss_rpn_loc: 0.035  time: 0.5683  data_time: 0.0813  lr: 0.004995  max_mem: 3067M
[32m[04/15 12:50:03 d2.utils.events]: [0m eta: 0:09:06  iter: 1019  total_loss: 0.710  loss_cls: 0.197  loss_box_reg: 0.366  loss_rpn_cls: 0.018  loss_rpn_loc: 0.076  time: 0.5678  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:50:15 d2.utils.events]: [0m eta: 0:08:55  iter: 1039  total_loss: 0.545  loss_cls: 0.171  loss_box_reg: 0.316  loss_rpn_cls: 0.011  loss_rpn_loc: 0.046  time: 0.5684  data_time: 0.0371  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:50:30 d2.utils.events]: [0m eta: 0:08:43  iter: 1059  total_loss: 0.525  loss_cls: 0.165  loss_box_reg: 0.314  loss_rpn_cls: 0.013  loss_rpn_loc: 0.052  time: 0.5676  data_time: 0.0376  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:50:41 d2.utils.events]: [0m eta: 0:08:32  iter: 1079  total_loss: 0.687  loss_cls: 0.214  loss_box_reg: 0.361  loss_rpn_cls: 0.014  loss_rpn_loc: 0.055  time: 0.5671  data_time: 0.0351  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:50:54 d2.utils.events]: [0m eta: 0:08:22  iter: 1099  total_loss: 0.666  loss_cls: 0.194  loss_box_reg: 0.367  loss_rpn_cls: 0.011  loss_rpn_loc: 0.054  time: 0.5683  data_time: 0.0092  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:51:05 d2.utils.events]: [0m eta: 0:08:09  iter: 1119  total_loss: 0.513  loss_cls: 0.166  loss_box_reg: 0.306  loss_rpn_cls: 0.018  loss_rpn_loc: 0.035  time: 0.5674  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:51:17 d2.utils.events]: [0m eta: 0:07:57  iter: 1139  total_loss: 0.586  loss_cls: 0.167  loss_box_reg: 0.329  loss_rpn_cls: 0.010  loss_rpn_loc: 0.053  time: 0.5666  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:51:30 d2.utils.events]: [0m eta: 0:07:46  iter: 1159  total_loss: 0.645  loss_cls: 0.213  loss_box_reg: 0.367  loss_rpn_cls: 0.012  loss_rpn_loc: 0.053  time: 0.5674  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:51:44 d2.utils.events]: [0m eta: 0:07:34  iter: 1179  total_loss: 0.534  loss_cls: 0.170  loss_box_reg: 0.289  loss_rpn_cls: 0.014  loss_rpn_loc: 0.043  time: 0.5669  data_time: 0.0238  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:51:55 d2.utils.events]: [0m eta: 0:07:23  iter: 1199  total_loss: 0.637  loss_cls: 0.219  loss_box_reg: 0.364  loss_rpn_cls: 0.022  loss_rpn_loc: 0.052  time: 0.5659  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:52:08 d2.utils.events]: [0m eta: 0:07:12  iter: 1219  total_loss: 0.713  loss_cls: 0.241  loss_box_reg: 0.369  loss_rpn_cls: 0.017  loss_rpn_loc: 0.062  time: 0.5669  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:52:18 d2.utils.events]: [0m eta: 0:07:00  iter: 1239  total_loss: 0.605  loss_cls: 0.184  loss_box_reg: 0.337  loss_rpn_cls: 0.009  loss_rpn_loc: 0.045  time: 0.5663  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:52:29 d2.utils.events]: [0m eta: 0:06:49  iter: 1259  total_loss: 0.683  loss_cls: 0.209  loss_box_reg: 0.378  loss_rpn_cls: 0.015  loss_rpn_loc: 0.079  time: 0.5657  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:52:42 d2.utils.events]: [0m eta: 0:06:38  iter: 1279  total_loss: 0.516  loss_cls: 0.153  loss_box_reg: 0.292  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5662  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:52:53 d2.utils.events]: [0m eta: 0:06:26  iter: 1299  total_loss: 0.623  loss_cls: 0.196  loss_box_reg: 0.350  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5662  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:53:04 d2.utils.events]: [0m eta: 0:06:15  iter: 1319  total_loss: 0.631  loss_cls: 0.193  loss_box_reg: 0.361  loss_rpn_cls: 0.011  loss_rpn_loc: 0.049  time: 0.5655  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:53:17 d2.utils.events]: [0m eta: 0:06:05  iter: 1339  total_loss: 0.630  loss_cls: 0.206  loss_box_reg: 0.360  loss_rpn_cls: 0.011  loss_rpn_loc: 0.043  time: 0.5655  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:53:30 d2.utils.events]: [0m eta: 0:05:54  iter: 1359  total_loss: 0.649  loss_cls: 0.218  loss_box_reg: 0.355  loss_rpn_cls: 0.020  loss_rpn_loc: 0.054  time: 0.5661  data_time: 0.0261  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:53:46 d2.utils.events]: [0m eta: 0:05:42  iter: 1379  total_loss: 0.597  loss_cls: 0.192  loss_box_reg: 0.332  loss_rpn_cls: 0.015  loss_rpn_loc: 0.074  time: 0.5656  data_time: 0.0062  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:54:01 d2.utils.events]: [0m eta: 0:05:31  iter: 1399  total_loss: 0.602  loss_cls: 0.188  loss_box_reg: 0.316  loss_rpn_cls: 0.015  loss_rpn_loc: 0.053  time: 0.5665  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:54:12 d2.utils.events]: [0m eta: 0:05:20  iter: 1419  total_loss: 0.638  loss_cls: 0.201  loss_box_reg: 0.336  loss_rpn_cls: 0.013  loss_rpn_loc: 0.047  time: 0.5662  data_time: 0.0168  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:54:24 d2.utils.events]: [0m eta: 0:05:09  iter: 1439  total_loss: 0.574  loss_cls: 0.173  loss_box_reg: 0.337  loss_rpn_cls: 0.014  loss_rpn_loc: 0.048  time: 0.5659  data_time: 0.0139  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:54:37 d2.utils.events]: [0m eta: 0:04:59  iter: 1459  total_loss: 0.618  loss_cls: 0.185  loss_box_reg: 0.351  loss_rpn_cls: 0.018  loss_rpn_loc: 0.062  time: 0.5666  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:54:48 d2.utils.events]: [0m eta: 0:04:47  iter: 1479  total_loss: 0.671  loss_cls: 0.217  loss_box_reg: 0.386  loss_rpn_cls: 0.014  loss_rpn_loc: 0.054  time: 0.5664  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:55:02 d2.utils.events]: [0m eta: 0:04:36  iter: 1499  total_loss: 0.622  loss_cls: 0.190  loss_box_reg: 0.335  loss_rpn_cls: 0.012  loss_rpn_loc: 0.042  time: 0.5663  data_time: 0.0348  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:55:15 d2.utils.events]: [0m eta: 0:04:26  iter: 1519  total_loss: 0.712  loss_cls: 0.238  loss_box_reg: 0.370  loss_rpn_cls: 0.016  loss_rpn_loc: 0.056  time: 0.5669  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:55:26 d2.utils.events]: [0m eta: 0:04:14  iter: 1539  total_loss: 0.703  loss_cls: 0.220  loss_box_reg: 0.362  loss_rpn_cls: 0.019  loss_rpn_loc: 0.074  time: 0.5664  data_time: 0.0224  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:55:38 d2.utils.events]: [0m eta: 0:04:03  iter: 1559  total_loss: 0.510  loss_cls: 0.158  loss_box_reg: 0.325  loss_rpn_cls: 0.010  loss_rpn_loc: 0.038  time: 0.5662  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:55:50 d2.utils.events]: [0m eta: 0:03:53  iter: 1579  total_loss: 0.533  loss_cls: 0.178  loss_box_reg: 0.301  loss_rpn_cls: 0.011  loss_rpn_loc: 0.048  time: 0.5667  data_time: 0.0133  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:56:02 d2.utils.events]: [0m eta: 0:03:41  iter: 1599  total_loss: 0.590  loss_cls: 0.197  loss_box_reg: 0.305  loss_rpn_cls: 0.013  loss_rpn_loc: 0.053  time: 0.5665  data_time: 0.0282  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:56:14 d2.utils.events]: [0m eta: 0:03:30  iter: 1619  total_loss: 0.526  loss_cls: 0.191  loss_box_reg: 0.299  loss_rpn_cls: 0.017  loss_rpn_loc: 0.051  time: 0.5661  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:56:27 d2.utils.events]: [0m eta: 0:03:20  iter: 1639  total_loss: 0.668  loss_cls: 0.181  loss_box_reg: 0.347  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5665  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:56:38 d2.utils.events]: [0m eta: 0:03:08  iter: 1659  total_loss: 0.696  loss_cls: 0.217  loss_box_reg: 0.393  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5663  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:56:48 d2.utils.events]: [0m eta: 0:02:57  iter: 1679  total_loss: 0.615  loss_cls: 0.194  loss_box_reg: 0.353  loss_rpn_cls: 0.015  loss_rpn_loc: 0.053  time: 0.5655  data_time: 0.0128  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:57:00 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.613  loss_cls: 0.174  loss_box_reg: 0.328  loss_rpn_cls: 0.012  loss_rpn_loc: 0.050  time: 0.5655  data_time: 0.0216  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:57:12 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.650  loss_cls: 0.212  loss_box_reg: 0.367  loss_rpn_cls: 0.016  loss_rpn_loc: 0.059  time: 0.5659  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:57:27 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.676  loss_cls: 0.224  loss_box_reg: 0.341  loss_rpn_cls: 0.013  loss_rpn_loc: 0.053  time: 0.5653  data_time: 0.0174  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:57:39 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.643  loss_cls: 0.198  loss_box_reg: 0.367  loss_rpn_cls: 0.014  loss_rpn_loc: 0.061  time: 0.5654  data_time: 0.0174  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:57:51 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.595  loss_cls: 0.185  loss_box_reg: 0.309  loss_rpn_cls: 0.015  loss_rpn_loc: 0.060  time: 0.5658  data_time: 0.0151  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:58:03 d2.utils.events]: [0m eta: 0:01:51  iter: 1799  total_loss: 0.626  loss_cls: 0.190  loss_box_reg: 0.325  loss_rpn_cls: 0.019  loss_rpn_loc: 0.069  time: 0.5655  data_time: 0.0143  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:58:16 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.634  loss_cls: 0.191  loss_box_reg: 0.363  loss_rpn_cls: 0.014  loss_rpn_loc: 0.056  time: 0.5655  data_time: 0.0100  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:58:28 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.588  loss_cls: 0.168  loss_box_reg: 0.331  loss_rpn_cls: 0.013  loss_rpn_loc: 0.049  time: 0.5657  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:58:39 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.604  loss_cls: 0.185  loss_box_reg: 0.310  loss_rpn_cls: 0.014  loss_rpn_loc: 0.058  time: 0.5654  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:58:54 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.486  loss_cls: 0.165  loss_box_reg: 0.259  loss_rpn_cls: 0.013  loss_rpn_loc: 0.037  time: 0.5655  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:59:06 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.589  loss_cls: 0.185  loss_box_reg: 0.315  loss_rpn_cls: 0.012  loss_rpn_loc: 0.041  time: 0.5653  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:59:17 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.667  loss_cls: 0.207  loss_box_reg: 0.329  loss_rpn_cls: 0.019  loss_rpn_loc: 0.063  time: 0.5648  data_time: 0.0171  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:59:31 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.607  loss_cls: 0.184  loss_box_reg: 0.345  loss_rpn_cls: 0.011  loss_rpn_loc: 0.063  time: 0.5646  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:59:44 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.545  loss_cls: 0.193  loss_box_reg: 0.310  loss_rpn_cls: 0.020  loss_rpn_loc: 0.043  time: 0.5648  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 12:59:54 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.588  loss_cls: 0.194  loss_box_reg: 0.317  loss_rpn_cls: 0.016  loss_rpn_loc: 0.058  time: 0.5643  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:00:27 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json takes 1.25 seconds.
[5m[31mWARNING[0m [32m[04/15 13:00:27 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:00:27 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 13:00:27 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 13:00:27 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.592  loss_cls: 0.198  loss_box_reg: 0.313  loss_rpn_cls: 0.011  loss_rpn_loc: 0.054  time: 0.5641  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:00:36 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:47 (0.5644 s / it)
[32m[04/15 13:00:36 d2.engine.hooks]: [0mTotal training time: 0:21:11 (0:02:24 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 13:00:45 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:00:45 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 13:00:45 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 13:00:50 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1178 s / img. ETA=0:04:07
[32m[04/15 13:00:56 d2.evaluation.evaluator]: [0mInference done 31/1257. 0.1188 s / img. ETA=0:05:14
[32m[04/15 13:01:01 d2.evaluation.evaluator]: [0mInference done 51/1257. 0.1203 s / img. ETA=0:05:15
[32m[04/15 13:01:06 d2.evaluation.evaluator]: [0mInference done 79/1257. 0.1209 s / img. ETA=0:04:32
[32m[04/15 13:01:11 d2.evaluation.evaluator]: [0mInference done 106/1257. 0.1234 s / img. ETA=0:04:12
[32m[04/15 13:01:16 d2.evaluation.evaluator]: [0mInference done 134/1257. 0.1228 s / img. ETA=0:03:57
[32m[04/15 13:01:21 d2.evaluation.evaluator]: [0mInference done 159/1257. 0.1247 s / img. ETA=0:03:50
[32m[04/15 13:01:27 d2.evaluation.evaluator]: [0mInference done 189/1257. 0.1308 s / img. ETA=0:03:39
[32m[04/15 13:01:32 d2.evaluation.evaluator]: [0mInference done 216/1257. 0.1298 s / img. ETA=0:03:32
[32m[04/15 13:01:37 d2.evaluation.evaluator]: [0mInference done 234/1257. 0.1290 s / img. ETA=0:03:35
[32m[04/15 13:01:42 d2.evaluation.evaluator]: [0mInference done 252/1257. 0.1282 s / img. ETA=0:03:36
[32m[04/15 13:01:48 d2.evaluation.evaluator]: [0mInference done 276/1257. 0.1274 s / img. ETA=0:03:31
[32m[04/15 13:01:53 d2.evaluation.evaluator]: [0mInference done 307/1257. 0.1266 s / img. ETA=0:03:20
[32m[04/15 13:01:58 d2.evaluation.evaluator]: [0mInference done 332/1257. 0.1270 s / img. ETA=0:03:14
[32m[04/15 13:02:03 d2.evaluation.evaluator]: [0mInference done 358/1257. 0.1282 s / img. ETA=0:03:08
[32m[04/15 13:02:08 d2.evaluation.evaluator]: [0mInference done 382/1257. 0.1281 s / img. ETA=0:03:03
[32m[04/15 13:02:13 d2.evaluation.evaluator]: [0mInference done 411/1257. 0.1274 s / img. ETA=0:02:55
[32m[04/15 13:02:18 d2.evaluation.evaluator]: [0mInference done 429/1257. 0.1271 s / img. ETA=0:02:54
[32m[04/15 13:02:23 d2.evaluation.evaluator]: [0mInference done 452/1257. 0.1266 s / img. ETA=0:02:49
[32m[04/15 13:02:28 d2.evaluation.evaluator]: [0mInference done 483/1257. 0.1261 s / img. ETA=0:02:40
[32m[04/15 13:02:33 d2.evaluation.evaluator]: [0mInference done 507/1257. 0.1258 s / img. ETA=0:02:35
[32m[04/15 13:02:39 d2.evaluation.evaluator]: [0mInference done 529/1257. 0.1265 s / img. ETA=0:02:32
[32m[04/15 13:02:44 d2.evaluation.evaluator]: [0mInference done 552/1257. 0.1268 s / img. ETA=0:02:28
[32m[04/15 13:02:49 d2.evaluation.evaluator]: [0mInference done 582/1257. 0.1266 s / img. ETA=0:02:20
[32m[04/15 13:02:55 d2.evaluation.evaluator]: [0mInference done 602/1257. 0.1263 s / img. ETA=0:02:18
[32m[04/15 13:03:00 d2.evaluation.evaluator]: [0mInference done 622/1257. 0.1261 s / img. ETA=0:02:14
[32m[04/15 13:03:05 d2.evaluation.evaluator]: [0mInference done 647/1257. 0.1259 s / img. ETA=0:02:09
[32m[04/15 13:03:11 d2.evaluation.evaluator]: [0mInference done 668/1257. 0.1257 s / img. ETA=0:02:05
[32m[04/15 13:03:16 d2.evaluation.evaluator]: [0mInference done 691/1257. 0.1262 s / img. ETA=0:02:01
[32m[04/15 13:03:21 d2.evaluation.evaluator]: [0mInference done 712/1257. 0.1264 s / img. ETA=0:01:57
[32m[04/15 13:03:26 d2.evaluation.evaluator]: [0mInference done 741/1257. 0.1261 s / img. ETA=0:01:50
[32m[04/15 13:03:31 d2.evaluation.evaluator]: [0mInference done 748/1257. 0.1260 s / img. ETA=0:01:51
[32m[04/15 13:03:37 d2.evaluation.evaluator]: [0mInference done 759/1257. 0.1259 s / img. ETA=0:01:51
[32m[04/15 13:03:42 d2.evaluation.evaluator]: [0mInference done 787/1257. 0.1257 s / img. ETA=0:01:44
[32m[04/15 13:03:47 d2.evaluation.evaluator]: [0mInference done 821/1257. 0.1255 s / img. ETA=0:01:35
[32m[04/15 13:03:52 d2.evaluation.evaluator]: [0mInference done 847/1257. 0.1256 s / img. ETA=0:01:29
[32m[04/15 13:03:57 d2.evaluation.evaluator]: [0mInference done 871/1257. 0.1262 s / img. ETA=0:01:23
[32m[04/15 13:04:03 d2.evaluation.evaluator]: [0mInference done 890/1257. 0.1260 s / img. ETA=0:01:20
[32m[04/15 13:04:08 d2.evaluation.evaluator]: [0mInference done 900/1257. 0.1259 s / img. ETA=0:01:19
[32m[04/15 13:04:13 d2.evaluation.evaluator]: [0mInference done 913/1257. 0.1258 s / img. ETA=0:01:17
[32m[04/15 13:04:18 d2.evaluation.evaluator]: [0mInference done 933/1257. 0.1258 s / img. ETA=0:01:12
[32m[04/15 13:04:23 d2.evaluation.evaluator]: [0mInference done 956/1257. 0.1259 s / img. ETA=0:01:07
[32m[04/15 13:04:29 d2.evaluation.evaluator]: [0mInference done 988/1257. 0.1259 s / img. ETA=0:01:00
[32m[04/15 13:04:34 d2.evaluation.evaluator]: [0mInference done 1022/1257. 0.1256 s / img. ETA=0:00:51
[32m[04/15 13:04:39 d2.evaluation.evaluator]: [0mInference done 1062/1257. 0.1252 s / img. ETA=0:00:42
[32m[04/15 13:04:44 d2.evaluation.evaluator]: [0mInference done 1104/1257. 0.1249 s / img. ETA=0:00:32
[32m[04/15 13:04:49 d2.evaluation.evaluator]: [0mInference done 1138/1257. 0.1254 s / img. ETA=0:00:25
[32m[04/15 13:04:54 d2.evaluation.evaluator]: [0mInference done 1168/1257. 0.1260 s / img. ETA=0:00:18
[32m[04/15 13:05:00 d2.evaluation.evaluator]: [0mInference done 1197/1257. 0.1260 s / img. ETA=0:00:12
[32m[04/15 13:05:05 d2.evaluation.evaluator]: [0mInference done 1227/1257. 0.1258 s / img. ETA=0:00:06
[32m[04/15 13:05:10 d2.evaluation.evaluator]: [0mInference done 1255/1257. 0.1257 s / img. ETA=0:00:00
[32m[04/15 13:05:10 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:20.918188 (0.208401 s / img per device, on 1 devices)
[32m[04/15 13:05:10 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:37 (0.125660 s / img per device, on 1 devices)
[32m[04/15 13:05:13 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 13:05:13 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 13:05:15 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.71s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=6.16s).
Accumulating evaluation results...
DONE (t=0.77s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.419
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.189
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.127
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.442
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.109
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.183
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.309
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.478
[32m[04/15 13:05:23 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 21.316 | 41.911 | 18.877 | 12.703 | 26.399 | 44.214 |
[32m[04/15 13:05:23 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP     | category       | AP     |
|:--------------|:-------|:--------------|:-------|:---------------|:-------|
| person        | 30.806 | bicycle       | 10.327 | car            | 43.865 |
| motorcycle    | nan    | airplane      | nan    | bus            | nan    |
| train         | nan    | truck         | nan    | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan    | stop sign      | nan    |
| parking meter | nan    | bench         | nan    | bird           | nan    |
| cat           | nan    | dog           | 0.264  | horse          | nan    |
| sheep         | nan    | cow           | nan    | elephant       | nan    |
| bear          | nan    | zebra         | nan    | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan    | handbag        | nan    |
| tie           | nan    | suitcase      | nan    | frisbee        | nan    |
| skis          | nan    | snowboard     | nan    | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan    | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan    | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan    | cup            | nan    |
| fork          | nan    | knife         | nan    | spoon          | nan    |
| bowl          | nan    | banana        | nan    | apple          | nan    |
| sandwich      | nan    | orange        | nan    | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan    | pizza          | nan    |
| donut         | nan    | cake          | nan    | chair          | nan    |
| couch         | nan    | potted plant  | nan    | bed            | nan    |
| dining table  | nan    | toilet        | nan    | tv             | nan    |
| laptop        | nan    | mouse         | nan    | remote         | nan    |
| keyboard      | nan    | cell phone    | nan    | microwave      | nan    |
| oven          | nan    | bike          | nan    | hydrant        | nan    |
| motor         | nan    | rider         | nan    | light          | nan    |
| sign          | nan    | motor vehicle | nan    | human face     | nan    |
| hair drier    | nan    | license plate | nan    |                |        |
============== The  25  *  2000  iterations ============
4 channel input
[32m[04/15 13:05:25 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 13:05:26 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.27 seconds.
[5m[31mWARNING[0m [32m[04/15 13:05:26 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:05:26 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 13:05:27 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 13:05:27 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 13:05:27 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 13:05:41 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 13:05:54 d2.utils.events]: [0m eta: 0:18:02  iter: 19  total_loss: 0.651  loss_cls: 0.210  loss_box_reg: 0.353  loss_rpn_cls: 0.011  loss_rpn_loc: 0.055  time: 0.5660  data_time: 0.1483  lr: 0.000100  max_mem: 3067M
[32m[04/15 13:06:06 d2.utils.events]: [0m eta: 0:18:05  iter: 39  total_loss: 0.617  loss_cls: 0.192  loss_box_reg: 0.344  loss_rpn_cls: 0.018  loss_rpn_loc: 0.040  time: 0.5804  data_time: 0.0083  lr: 0.000200  max_mem: 3067M
[32m[04/15 13:06:17 d2.utils.events]: [0m eta: 0:17:40  iter: 59  total_loss: 0.521  loss_cls: 0.149  loss_box_reg: 0.315  loss_rpn_cls: 0.011  loss_rpn_loc: 0.044  time: 0.5640  data_time: 0.0278  lr: 0.000300  max_mem: 3067M
[32m[04/15 13:06:28 d2.utils.events]: [0m eta: 0:17:19  iter: 79  total_loss: 0.514  loss_cls: 0.155  loss_box_reg: 0.295  loss_rpn_cls: 0.011  loss_rpn_loc: 0.045  time: 0.5550  data_time: 0.0067  lr: 0.000400  max_mem: 3067M
[32m[04/15 13:06:46 d2.utils.events]: [0m eta: 0:17:26  iter: 99  total_loss: 0.654  loss_cls: 0.208  loss_box_reg: 0.369  loss_rpn_cls: 0.014  loss_rpn_loc: 0.047  time: 0.5688  data_time: 0.0080  lr: 0.000500  max_mem: 3067M
[32m[04/15 13:07:10 d2.utils.events]: [0m eta: 0:17:01  iter: 119  total_loss: 0.559  loss_cls: 0.182  loss_box_reg: 0.325  loss_rpn_cls: 0.017  loss_rpn_loc: 0.041  time: 0.5682  data_time: 0.1012  lr: 0.000599  max_mem: 3067M
[32m[04/15 13:07:22 d2.utils.events]: [0m eta: 0:16:58  iter: 139  total_loss: 0.549  loss_cls: 0.164  loss_box_reg: 0.331  loss_rpn_cls: 0.011  loss_rpn_loc: 0.043  time: 0.5748  data_time: 0.0084  lr: 0.000699  max_mem: 3067M
[32m[04/15 13:07:35 d2.utils.events]: [0m eta: 0:16:53  iter: 159  total_loss: 0.534  loss_cls: 0.155  loss_box_reg: 0.300  loss_rpn_cls: 0.011  loss_rpn_loc: 0.040  time: 0.5812  data_time: 0.1201  lr: 0.000799  max_mem: 3067M
[32m[04/15 13:07:53 d2.utils.events]: [0m eta: 0:16:39  iter: 179  total_loss: 0.620  loss_cls: 0.190  loss_box_reg: 0.352  loss_rpn_cls: 0.009  loss_rpn_loc: 0.050  time: 0.5777  data_time: 0.0103  lr: 0.000899  max_mem: 3067M
[32m[04/15 13:08:06 d2.utils.events]: [0m eta: 0:16:31  iter: 199  total_loss: 0.586  loss_cls: 0.199  loss_box_reg: 0.314  loss_rpn_cls: 0.013  loss_rpn_loc: 0.056  time: 0.5812  data_time: 0.0568  lr: 0.000999  max_mem: 3067M
[32m[04/15 13:08:21 d2.utils.events]: [0m eta: 0:16:17  iter: 219  total_loss: 0.533  loss_cls: 0.164  loss_box_reg: 0.293  loss_rpn_cls: 0.013  loss_rpn_loc: 0.044  time: 0.5849  data_time: 0.1375  lr: 0.001099  max_mem: 3067M
[32m[04/15 13:08:36 d2.utils.events]: [0m eta: 0:16:13  iter: 239  total_loss: 0.592  loss_cls: 0.190  loss_box_reg: 0.332  loss_rpn_cls: 0.010  loss_rpn_loc: 0.041  time: 0.5852  data_time: 0.0437  lr: 0.001199  max_mem: 3067M
[32m[04/15 13:08:59 d2.utils.events]: [0m eta: 0:15:54  iter: 259  total_loss: 0.635  loss_cls: 0.186  loss_box_reg: 0.353  loss_rpn_cls: 0.011  loss_rpn_loc: 0.065  time: 0.5802  data_time: 0.0078  lr: 0.001299  max_mem: 3067M
[32m[04/15 13:09:10 d2.utils.events]: [0m eta: 0:15:39  iter: 279  total_loss: 0.580  loss_cls: 0.186  loss_box_reg: 0.324  loss_rpn_cls: 0.008  loss_rpn_loc: 0.051  time: 0.5767  data_time: 0.0259  lr: 0.001399  max_mem: 3067M
[32m[04/15 13:09:22 d2.utils.events]: [0m eta: 0:15:30  iter: 299  total_loss: 0.468  loss_cls: 0.152  loss_box_reg: 0.271  loss_rpn_cls: 0.008  loss_rpn_loc: 0.027  time: 0.5780  data_time: 0.0100  lr: 0.001499  max_mem: 3067M
[32m[04/15 13:09:34 d2.utils.events]: [0m eta: 0:15:21  iter: 319  total_loss: 0.614  loss_cls: 0.176  loss_box_reg: 0.358  loss_rpn_cls: 0.009  loss_rpn_loc: 0.051  time: 0.5759  data_time: 0.0088  lr: 0.001598  max_mem: 3067M
[32m[04/15 13:09:45 d2.utils.events]: [0m eta: 0:15:09  iter: 339  total_loss: 0.629  loss_cls: 0.197  loss_box_reg: 0.364  loss_rpn_cls: 0.014  loss_rpn_loc: 0.057  time: 0.5730  data_time: 0.0260  lr: 0.001698  max_mem: 3067M
[32m[04/15 13:09:58 d2.utils.events]: [0m eta: 0:14:59  iter: 359  total_loss: 0.564  loss_cls: 0.185  loss_box_reg: 0.318  loss_rpn_cls: 0.011  loss_rpn_loc: 0.039  time: 0.5743  data_time: 0.0072  lr: 0.001798  max_mem: 3067M
[32m[04/15 13:10:10 d2.utils.events]: [0m eta: 0:14:50  iter: 379  total_loss: 0.665  loss_cls: 0.216  loss_box_reg: 0.373  loss_rpn_cls: 0.013  loss_rpn_loc: 0.051  time: 0.5736  data_time: 0.0187  lr: 0.001898  max_mem: 3067M
[32m[04/15 13:10:22 d2.utils.events]: [0m eta: 0:14:37  iter: 399  total_loss: 0.549  loss_cls: 0.171  loss_box_reg: 0.299  loss_rpn_cls: 0.008  loss_rpn_loc: 0.044  time: 0.5711  data_time: 0.0079  lr: 0.001998  max_mem: 3067M
[32m[04/15 13:10:44 d2.utils.events]: [0m eta: 0:14:26  iter: 419  total_loss: 0.523  loss_cls: 0.159  loss_box_reg: 0.320  loss_rpn_cls: 0.015  loss_rpn_loc: 0.039  time: 0.5709  data_time: 0.0075  lr: 0.002098  max_mem: 3067M
[32m[04/15 13:10:56 d2.utils.events]: [0m eta: 0:14:15  iter: 439  total_loss: 0.571  loss_cls: 0.181  loss_box_reg: 0.331  loss_rpn_cls: 0.011  loss_rpn_loc: 0.050  time: 0.5699  data_time: 0.0249  lr: 0.002198  max_mem: 3067M
[32m[04/15 13:11:09 d2.utils.events]: [0m eta: 0:14:05  iter: 459  total_loss: 0.547  loss_cls: 0.187  loss_box_reg: 0.296  loss_rpn_cls: 0.015  loss_rpn_loc: 0.058  time: 0.5709  data_time: 0.0331  lr: 0.002298  max_mem: 3067M
[32m[04/15 13:11:21 d2.utils.events]: [0m eta: 0:13:52  iter: 479  total_loss: 0.610  loss_cls: 0.187  loss_box_reg: 0.297  loss_rpn_cls: 0.013  loss_rpn_loc: 0.048  time: 0.5697  data_time: 0.0088  lr: 0.002398  max_mem: 3067M
[32m[04/15 13:11:32 d2.utils.events]: [0m eta: 0:13:42  iter: 499  total_loss: 0.620  loss_cls: 0.186  loss_box_reg: 0.325  loss_rpn_cls: 0.012  loss_rpn_loc: 0.052  time: 0.5687  data_time: 0.0091  lr: 0.002498  max_mem: 3067M
[32m[04/15 13:11:44 d2.utils.events]: [0m eta: 0:13:31  iter: 519  total_loss: 0.595  loss_cls: 0.177  loss_box_reg: 0.338  loss_rpn_cls: 0.008  loss_rpn_loc: 0.046  time: 0.5686  data_time: 0.0372  lr: 0.002597  max_mem: 3067M
[32m[04/15 13:11:57 d2.utils.events]: [0m eta: 0:13:20  iter: 539  total_loss: 0.583  loss_cls: 0.206  loss_box_reg: 0.322  loss_rpn_cls: 0.014  loss_rpn_loc: 0.053  time: 0.5695  data_time: 0.0104  lr: 0.002697  max_mem: 3067M
[32m[04/15 13:12:07 d2.utils.events]: [0m eta: 0:13:07  iter: 559  total_loss: 0.566  loss_cls: 0.187  loss_box_reg: 0.312  loss_rpn_cls: 0.012  loss_rpn_loc: 0.050  time: 0.5679  data_time: 0.0250  lr: 0.002797  max_mem: 3067M
[32m[04/15 13:12:19 d2.utils.events]: [0m eta: 0:12:57  iter: 579  total_loss: 0.522  loss_cls: 0.150  loss_box_reg: 0.282  loss_rpn_cls: 0.008  loss_rpn_loc: 0.047  time: 0.5672  data_time: 0.0060  lr: 0.002897  max_mem: 3067M
[32m[04/15 13:12:31 d2.utils.events]: [0m eta: 0:12:46  iter: 599  total_loss: 0.657  loss_cls: 0.197  loss_box_reg: 0.371  loss_rpn_cls: 0.011  loss_rpn_loc: 0.066  time: 0.5684  data_time: 0.0310  lr: 0.002997  max_mem: 3067M
[32m[04/15 13:12:55 d2.utils.events]: [0m eta: 0:12:36  iter: 619  total_loss: 0.597  loss_cls: 0.183  loss_box_reg: 0.341  loss_rpn_cls: 0.009  loss_rpn_loc: 0.051  time: 0.5726  data_time: 0.2025  lr: 0.003097  max_mem: 3067M
[32m[04/15 13:13:11 d2.utils.events]: [0m eta: 0:12:24  iter: 639  total_loss: 0.639  loss_cls: 0.196  loss_box_reg: 0.320  loss_rpn_cls: 0.014  loss_rpn_loc: 0.047  time: 0.5721  data_time: 0.0074  lr: 0.003197  max_mem: 3067M
[32m[04/15 13:13:25 d2.utils.events]: [0m eta: 0:12:12  iter: 659  total_loss: 0.590  loss_cls: 0.187  loss_box_reg: 0.355  loss_rpn_cls: 0.011  loss_rpn_loc: 0.044  time: 0.5706  data_time: 0.0081  lr: 0.003297  max_mem: 3067M
[32m[04/15 13:13:36 d2.utils.events]: [0m eta: 0:12:01  iter: 679  total_loss: 0.571  loss_cls: 0.175  loss_box_reg: 0.308  loss_rpn_cls: 0.008  loss_rpn_loc: 0.058  time: 0.5704  data_time: 0.0066  lr: 0.003397  max_mem: 3067M
[32m[04/15 13:13:48 d2.utils.events]: [0m eta: 0:11:51  iter: 699  total_loss: 0.586  loss_cls: 0.207  loss_box_reg: 0.332  loss_rpn_cls: 0.011  loss_rpn_loc: 0.063  time: 0.5707  data_time: 0.0099  lr: 0.003497  max_mem: 3067M
[32m[04/15 13:13:59 d2.utils.events]: [0m eta: 0:11:39  iter: 719  total_loss: 0.588  loss_cls: 0.163  loss_box_reg: 0.349  loss_rpn_cls: 0.014  loss_rpn_loc: 0.059  time: 0.5695  data_time: 0.0084  lr: 0.003596  max_mem: 3067M
[32m[04/15 13:14:18 d2.utils.events]: [0m eta: 0:11:29  iter: 739  total_loss: 0.561  loss_cls: 0.161  loss_box_reg: 0.320  loss_rpn_cls: 0.008  loss_rpn_loc: 0.042  time: 0.5702  data_time: 0.0069  lr: 0.003696  max_mem: 3067M
[32m[04/15 13:14:29 d2.utils.events]: [0m eta: 0:11:18  iter: 759  total_loss: 0.575  loss_cls: 0.185  loss_box_reg: 0.331  loss_rpn_cls: 0.009  loss_rpn_loc: 0.057  time: 0.5699  data_time: 0.0080  lr: 0.003796  max_mem: 3067M
[32m[04/15 13:14:41 d2.utils.events]: [0m eta: 0:11:07  iter: 779  total_loss: 0.489  loss_cls: 0.157  loss_box_reg: 0.273  loss_rpn_cls: 0.013  loss_rpn_loc: 0.046  time: 0.5687  data_time: 0.0087  lr: 0.003896  max_mem: 3067M
[32m[04/15 13:14:55 d2.utils.events]: [0m eta: 0:10:56  iter: 799  total_loss: 0.662  loss_cls: 0.195  loss_box_reg: 0.351  loss_rpn_cls: 0.011  loss_rpn_loc: 0.069  time: 0.5695  data_time: 0.0073  lr: 0.003996  max_mem: 3067M
[32m[04/15 13:15:08 d2.utils.events]: [0m eta: 0:10:45  iter: 819  total_loss: 0.548  loss_cls: 0.156  loss_box_reg: 0.320  loss_rpn_cls: 0.011  loss_rpn_loc: 0.048  time: 0.5691  data_time: 0.0103  lr: 0.004096  max_mem: 3067M
[32m[04/15 13:15:19 d2.utils.events]: [0m eta: 0:10:34  iter: 839  total_loss: 0.687  loss_cls: 0.192  loss_box_reg: 0.398  loss_rpn_cls: 0.016  loss_rpn_loc: 0.073  time: 0.5682  data_time: 0.0062  lr: 0.004196  max_mem: 3067M
[32m[04/15 13:15:33 d2.utils.events]: [0m eta: 0:10:23  iter: 859  total_loss: 0.572  loss_cls: 0.163  loss_box_reg: 0.328  loss_rpn_cls: 0.008  loss_rpn_loc: 0.049  time: 0.5697  data_time: 0.0625  lr: 0.004296  max_mem: 3067M
[32m[04/15 13:15:48 d2.utils.events]: [0m eta: 0:10:12  iter: 879  total_loss: 0.612  loss_cls: 0.194  loss_box_reg: 0.328  loss_rpn_cls: 0.013  loss_rpn_loc: 0.066  time: 0.5712  data_time: 0.1560  lr: 0.004396  max_mem: 3067M
[32m[04/15 13:16:00 d2.utils.events]: [0m eta: 0:10:01  iter: 899  total_loss: 0.636  loss_cls: 0.210  loss_box_reg: 0.365  loss_rpn_cls: 0.019  loss_rpn_loc: 0.059  time: 0.5701  data_time: 0.0071  lr: 0.004496  max_mem: 3067M
[32m[04/15 13:16:12 d2.utils.events]: [0m eta: 0:09:50  iter: 919  total_loss: 0.644  loss_cls: 0.195  loss_box_reg: 0.330  loss_rpn_cls: 0.012  loss_rpn_loc: 0.064  time: 0.5701  data_time: 0.0235  lr: 0.004595  max_mem: 3067M
[32m[04/15 13:16:25 d2.utils.events]: [0m eta: 0:09:40  iter: 939  total_loss: 0.618  loss_cls: 0.184  loss_box_reg: 0.322  loss_rpn_cls: 0.011  loss_rpn_loc: 0.051  time: 0.5700  data_time: 0.0375  lr: 0.004695  max_mem: 3067M
[32m[04/15 13:16:35 d2.utils.events]: [0m eta: 0:09:29  iter: 959  total_loss: 0.684  loss_cls: 0.210  loss_box_reg: 0.354  loss_rpn_cls: 0.014  loss_rpn_loc: 0.060  time: 0.5690  data_time: 0.0071  lr: 0.004795  max_mem: 3067M
[32m[04/15 13:16:47 d2.utils.events]: [0m eta: 0:09:18  iter: 979  total_loss: 0.612  loss_cls: 0.184  loss_box_reg: 0.344  loss_rpn_cls: 0.011  loss_rpn_loc: 0.060  time: 0.5694  data_time: 0.0092  lr: 0.004895  max_mem: 3067M
[32m[04/15 13:16:59 d2.utils.events]: [0m eta: 0:09:07  iter: 999  total_loss: 0.730  loss_cls: 0.228  loss_box_reg: 0.342  loss_rpn_cls: 0.019  loss_rpn_loc: 0.065  time: 0.5695  data_time: 0.0081  lr: 0.004995  max_mem: 3067M
[32m[04/15 13:17:11 d2.utils.events]: [0m eta: 0:08:56  iter: 1019  total_loss: 0.667  loss_cls: 0.197  loss_box_reg: 0.389  loss_rpn_cls: 0.015  loss_rpn_loc: 0.061  time: 0.5690  data_time: 0.0689  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:17:29 d2.utils.events]: [0m eta: 0:08:45  iter: 1039  total_loss: 0.541  loss_cls: 0.182  loss_box_reg: 0.313  loss_rpn_cls: 0.013  loss_rpn_loc: 0.053  time: 0.5702  data_time: 0.0385  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:17:41 d2.utils.events]: [0m eta: 0:08:34  iter: 1059  total_loss: 0.697  loss_cls: 0.210  loss_box_reg: 0.384  loss_rpn_cls: 0.013  loss_rpn_loc: 0.057  time: 0.5695  data_time: 0.0163  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:18:00 d2.utils.events]: [0m eta: 0:08:24  iter: 1079  total_loss: 0.673  loss_cls: 0.216  loss_box_reg: 0.367  loss_rpn_cls: 0.015  loss_rpn_loc: 0.071  time: 0.5696  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:18:11 d2.utils.events]: [0m eta: 0:08:12  iter: 1099  total_loss: 0.578  loss_cls: 0.186  loss_box_reg: 0.310  loss_rpn_cls: 0.017  loss_rpn_loc: 0.055  time: 0.5694  data_time: 0.0139  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:18:22 d2.utils.events]: [0m eta: 0:08:01  iter: 1119  total_loss: 0.497  loss_cls: 0.160  loss_box_reg: 0.302  loss_rpn_cls: 0.009  loss_rpn_loc: 0.040  time: 0.5690  data_time: 0.0047  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:18:34 d2.utils.events]: [0m eta: 0:07:50  iter: 1139  total_loss: 0.558  loss_cls: 0.174  loss_box_reg: 0.311  loss_rpn_cls: 0.012  loss_rpn_loc: 0.039  time: 0.5694  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:18:45 d2.utils.events]: [0m eta: 0:07:39  iter: 1159  total_loss: 0.570  loss_cls: 0.178  loss_box_reg: 0.333  loss_rpn_cls: 0.009  loss_rpn_loc: 0.046  time: 0.5687  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:19:00 d2.utils.events]: [0m eta: 0:07:28  iter: 1179  total_loss: 0.671  loss_cls: 0.220  loss_box_reg: 0.378  loss_rpn_cls: 0.016  loss_rpn_loc: 0.060  time: 0.5683  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:19:11 d2.utils.events]: [0m eta: 0:07:17  iter: 1199  total_loss: 0.608  loss_cls: 0.188  loss_box_reg: 0.325  loss_rpn_cls: 0.012  loss_rpn_loc: 0.052  time: 0.5681  data_time: 0.0106  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:19:23 d2.utils.events]: [0m eta: 0:07:06  iter: 1219  total_loss: 0.615  loss_cls: 0.199  loss_box_reg: 0.349  loss_rpn_cls: 0.013  loss_rpn_loc: 0.055  time: 0.5688  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:19:34 d2.utils.events]: [0m eta: 0:06:55  iter: 1239  total_loss: 0.673  loss_cls: 0.208  loss_box_reg: 0.380  loss_rpn_cls: 0.017  loss_rpn_loc: 0.051  time: 0.5684  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:19:46 d2.utils.events]: [0m eta: 0:06:45  iter: 1259  total_loss: 0.536  loss_cls: 0.178  loss_box_reg: 0.294  loss_rpn_cls: 0.011  loss_rpn_loc: 0.035  time: 0.5680  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:19:58 d2.utils.events]: [0m eta: 0:06:35  iter: 1279  total_loss: 0.626  loss_cls: 0.217  loss_box_reg: 0.346  loss_rpn_cls: 0.011  loss_rpn_loc: 0.052  time: 0.5686  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:20:12 d2.utils.events]: [0m eta: 0:06:24  iter: 1299  total_loss: 0.532  loss_cls: 0.176  loss_box_reg: 0.297  loss_rpn_cls: 0.014  loss_rpn_loc: 0.043  time: 0.5700  data_time: 0.1681  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:20:23 d2.utils.events]: [0m eta: 0:06:13  iter: 1319  total_loss: 0.511  loss_cls: 0.183  loss_box_reg: 0.286  loss_rpn_cls: 0.012  loss_rpn_loc: 0.047  time: 0.5700  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:20:35 d2.utils.events]: [0m eta: 0:06:02  iter: 1339  total_loss: 0.673  loss_cls: 0.191  loss_box_reg: 0.370  loss_rpn_cls: 0.014  loss_rpn_loc: 0.074  time: 0.5701  data_time: 0.0122  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:20:48 d2.utils.events]: [0m eta: 0:05:51  iter: 1359  total_loss: 0.559  loss_cls: 0.167  loss_box_reg: 0.338  loss_rpn_cls: 0.014  loss_rpn_loc: 0.051  time: 0.5706  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:20:59 d2.utils.events]: [0m eta: 0:05:40  iter: 1379  total_loss: 0.564  loss_cls: 0.181  loss_box_reg: 0.315  loss_rpn_cls: 0.011  loss_rpn_loc: 0.057  time: 0.5702  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:21:10 d2.utils.events]: [0m eta: 0:05:29  iter: 1399  total_loss: 0.595  loss_cls: 0.192  loss_box_reg: 0.353  loss_rpn_cls: 0.012  loss_rpn_loc: 0.044  time: 0.5695  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:21:21 d2.utils.events]: [0m eta: 0:05:18  iter: 1419  total_loss: 0.511  loss_cls: 0.179  loss_box_reg: 0.277  loss_rpn_cls: 0.013  loss_rpn_loc: 0.051  time: 0.5696  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:21:33 d2.utils.events]: [0m eta: 0:05:07  iter: 1439  total_loss: 0.686  loss_cls: 0.199  loss_box_reg: 0.392  loss_rpn_cls: 0.013  loss_rpn_loc: 0.057  time: 0.5696  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:21:43 d2.utils.events]: [0m eta: 0:04:56  iter: 1459  total_loss: 0.607  loss_cls: 0.194  loss_box_reg: 0.324  loss_rpn_cls: 0.008  loss_rpn_loc: 0.038  time: 0.5690  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:21:54 d2.utils.events]: [0m eta: 0:04:45  iter: 1479  total_loss: 0.547  loss_cls: 0.189  loss_box_reg: 0.306  loss_rpn_cls: 0.015  loss_rpn_loc: 0.042  time: 0.5685  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:22:07 d2.utils.events]: [0m eta: 0:04:34  iter: 1499  total_loss: 0.613  loss_cls: 0.186  loss_box_reg: 0.368  loss_rpn_cls: 0.016  loss_rpn_loc: 0.063  time: 0.5691  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:22:18 d2.utils.events]: [0m eta: 0:04:23  iter: 1519  total_loss: 0.548  loss_cls: 0.173  loss_box_reg: 0.295  loss_rpn_cls: 0.013  loss_rpn_loc: 0.041  time: 0.5689  data_time: 0.0098  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:22:29 d2.utils.events]: [0m eta: 0:04:12  iter: 1539  total_loss: 0.561  loss_cls: 0.174  loss_box_reg: 0.320  loss_rpn_cls: 0.010  loss_rpn_loc: 0.042  time: 0.5686  data_time: 0.0444  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:22:42 d2.utils.events]: [0m eta: 0:04:03  iter: 1559  total_loss: 0.700  loss_cls: 0.223  loss_box_reg: 0.393  loss_rpn_cls: 0.018  loss_rpn_loc: 0.073  time: 0.5693  data_time: 0.0092  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:22:53 d2.utils.events]: [0m eta: 0:03:52  iter: 1579  total_loss: 0.543  loss_cls: 0.167  loss_box_reg: 0.331  loss_rpn_cls: 0.015  loss_rpn_loc: 0.046  time: 0.5691  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:23:04 d2.utils.events]: [0m eta: 0:03:41  iter: 1599  total_loss: 0.547  loss_cls: 0.174  loss_box_reg: 0.334  loss_rpn_cls: 0.012  loss_rpn_loc: 0.037  time: 0.5687  data_time: 0.0113  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:23:16 d2.utils.events]: [0m eta: 0:03:30  iter: 1619  total_loss: 0.554  loss_cls: 0.176  loss_box_reg: 0.317  loss_rpn_cls: 0.015  loss_rpn_loc: 0.032  time: 0.5693  data_time: 0.0120  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:23:28 d2.utils.events]: [0m eta: 0:03:19  iter: 1639  total_loss: 0.592  loss_cls: 0.210  loss_box_reg: 0.321  loss_rpn_cls: 0.017  loss_rpn_loc: 0.045  time: 0.5693  data_time: 0.0289  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:23:38 d2.utils.events]: [0m eta: 0:03:08  iter: 1659  total_loss: 0.544  loss_cls: 0.185  loss_box_reg: 0.304  loss_rpn_cls: 0.015  loss_rpn_loc: 0.052  time: 0.5685  data_time: 0.0161  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:23:50 d2.utils.events]: [0m eta: 0:02:57  iter: 1679  total_loss: 0.616  loss_cls: 0.210  loss_box_reg: 0.340  loss_rpn_cls: 0.012  loss_rpn_loc: 0.049  time: 0.5689  data_time: 0.0114  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:24:02 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.495  loss_cls: 0.158  loss_box_reg: 0.295  loss_rpn_cls: 0.017  loss_rpn_loc: 0.035  time: 0.5688  data_time: 0.0252  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:24:13 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.682  loss_cls: 0.209  loss_box_reg: 0.365  loss_rpn_cls: 0.016  loss_rpn_loc: 0.060  time: 0.5683  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:24:24 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.591  loss_cls: 0.197  loss_box_reg: 0.328  loss_rpn_cls: 0.017  loss_rpn_loc: 0.060  time: 0.5682  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:24:36 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.751  loss_cls: 0.236  loss_box_reg: 0.431  loss_rpn_cls: 0.019  loss_rpn_loc: 0.065  time: 0.5687  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:24:47 d2.utils.events]: [0m eta: 0:02:02  iter: 1779  total_loss: 0.613  loss_cls: 0.170  loss_box_reg: 0.332  loss_rpn_cls: 0.010  loss_rpn_loc: 0.045  time: 0.5683  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:24:58 d2.utils.events]: [0m eta: 0:01:50  iter: 1799  total_loss: 0.579  loss_cls: 0.197  loss_box_reg: 0.308  loss_rpn_cls: 0.018  loss_rpn_loc: 0.052  time: 0.5678  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:25:10 d2.utils.events]: [0m eta: 0:01:40  iter: 1819  total_loss: 0.627  loss_cls: 0.186  loss_box_reg: 0.334  loss_rpn_cls: 0.013  loss_rpn_loc: 0.063  time: 0.5685  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:25:21 d2.utils.events]: [0m eta: 0:01:29  iter: 1839  total_loss: 0.579  loss_cls: 0.194  loss_box_reg: 0.332  loss_rpn_cls: 0.016  loss_rpn_loc: 0.057  time: 0.5682  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:25:32 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.563  loss_cls: 0.183  loss_box_reg: 0.311  loss_rpn_cls: 0.017  loss_rpn_loc: 0.048  time: 0.5679  data_time: 0.0145  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:25:45 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.522  loss_cls: 0.172  loss_box_reg: 0.320  loss_rpn_cls: 0.020  loss_rpn_loc: 0.056  time: 0.5682  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:25:56 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.660  loss_cls: 0.191  loss_box_reg: 0.351  loss_rpn_cls: 0.020  loss_rpn_loc: 0.056  time: 0.5680  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:26:07 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.658  loss_cls: 0.208  loss_box_reg: 0.349  loss_rpn_cls: 0.024  loss_rpn_loc: 0.067  time: 0.5677  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:26:18 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.630  loss_cls: 0.192  loss_box_reg: 0.325  loss_rpn_cls: 0.019  loss_rpn_loc: 0.068  time: 0.5675  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:26:30 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.555  loss_cls: 0.164  loss_box_reg: 0.325  loss_rpn_cls: 0.013  loss_rpn_loc: 0.050  time: 0.5678  data_time: 0.0097  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:26:42 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.714  loss_cls: 0.226  loss_box_reg: 0.389  loss_rpn_cls: 0.021  loss_rpn_loc: 0.061  time: 0.5675  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:27:59 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json takes 1.60 seconds.
[5m[31mWARNING[0m [32m[04/15 13:28:11 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:28:11 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 13:28:11 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 13:28:11 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.624  loss_cls: 0.206  loss_box_reg: 0.346  loss_rpn_cls: 0.018  loss_rpn_loc: 0.042  time: 0.5674  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:28:11 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:53 (0.5677 s / it)
[32m[04/15 13:28:11 d2.engine.hooks]: [0mTotal training time: 0:22:28 (0:03:34 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 13:28:25 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:28:25 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 13:28:26 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 13:28:32 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1208 s / img. ETA=0:06:56
[32m[04/15 13:28:37 d2.evaluation.evaluator]: [0mInference done 31/1257. 0.1310 s / img. ETA=0:05:39
[32m[04/15 13:28:43 d2.evaluation.evaluator]: [0mInference done 53/1257. 0.1263 s / img. ETA=0:05:09
[32m[04/15 13:28:48 d2.evaluation.evaluator]: [0mInference done 72/1257. 0.1245 s / img. ETA=0:05:07
[32m[04/15 13:28:53 d2.evaluation.evaluator]: [0mInference done 87/1257. 0.1236 s / img. ETA=0:05:24
[32m[04/15 13:28:58 d2.evaluation.evaluator]: [0mInference done 103/1257. 0.1255 s / img. ETA=0:05:29
[32m[04/15 13:29:04 d2.evaluation.evaluator]: [0mInference done 123/1257. 0.1245 s / img. ETA=0:05:22
[32m[04/15 13:29:09 d2.evaluation.evaluator]: [0mInference done 144/1257. 0.1255 s / img. ETA=0:05:09
[32m[04/15 13:29:14 d2.evaluation.evaluator]: [0mInference done 168/1257. 0.1282 s / img. ETA=0:04:51
[32m[04/15 13:29:19 d2.evaluation.evaluator]: [0mInference done 184/1257. 0.1277 s / img. ETA=0:04:52
[32m[04/15 13:29:24 d2.evaluation.evaluator]: [0mInference done 203/1257. 0.1270 s / img. ETA=0:04:48
[32m[04/15 13:29:29 d2.evaluation.evaluator]: [0mInference done 228/1257. 0.1262 s / img. ETA=0:04:33
[32m[04/15 13:29:34 d2.evaluation.evaluator]: [0mInference done 253/1257. 0.1256 s / img. ETA=0:04:19
[32m[04/15 13:29:40 d2.evaluation.evaluator]: [0mInference done 283/1257. 0.1250 s / img. ETA=0:04:02
[32m[04/15 13:29:45 d2.evaluation.evaluator]: [0mInference done 314/1257. 0.1256 s / img. ETA=0:03:46
[32m[04/15 13:29:50 d2.evaluation.evaluator]: [0mInference done 348/1257. 0.1274 s / img. ETA=0:03:30
[32m[04/15 13:29:55 d2.evaluation.evaluator]: [0mInference done 382/1257. 0.1274 s / img. ETA=0:03:16
[32m[04/15 13:30:00 d2.evaluation.evaluator]: [0mInference done 418/1257. 0.1268 s / img. ETA=0:03:02
[32m[04/15 13:30:05 d2.evaluation.evaluator]: [0mInference done 445/1257. 0.1263 s / img. ETA=0:02:54
[32m[04/15 13:30:10 d2.evaluation.evaluator]: [0mInference done 476/1257. 0.1259 s / img. ETA=0:02:45
[32m[04/15 13:30:15 d2.evaluation.evaluator]: [0mInference done 508/1257. 0.1255 s / img. ETA=0:02:36
[32m[04/15 13:30:20 d2.evaluation.evaluator]: [0mInference done 543/1257. 0.1252 s / img. ETA=0:02:26
[32m[04/15 13:30:25 d2.evaluation.evaluator]: [0mInference done 573/1257. 0.1255 s / img. ETA=0:02:18
[32m[04/15 13:30:31 d2.evaluation.evaluator]: [0mInference done 606/1257. 0.1267 s / img. ETA=0:02:10
[32m[04/15 13:30:36 d2.evaluation.evaluator]: [0mInference done 637/1257. 0.1264 s / img. ETA=0:02:03
[32m[04/15 13:30:41 d2.evaluation.evaluator]: [0mInference done 664/1257. 0.1261 s / img. ETA=0:01:57
[32m[04/15 13:30:46 d2.evaluation.evaluator]: [0mInference done 690/1257. 0.1260 s / img. ETA=0:01:52
[32m[04/15 13:30:51 d2.evaluation.evaluator]: [0mInference done 724/1257. 0.1257 s / img. ETA=0:01:44
[32m[04/15 13:30:56 d2.evaluation.evaluator]: [0mInference done 760/1257. 0.1255 s / img. ETA=0:01:36
[32m[04/15 13:31:02 d2.evaluation.evaluator]: [0mInference done 800/1257. 0.1253 s / img. ETA=0:01:26
[32m[04/15 13:31:07 d2.evaluation.evaluator]: [0mInference done 833/1257. 0.1259 s / img. ETA=0:01:20
[32m[04/15 13:31:12 d2.evaluation.evaluator]: [0mInference done 865/1257. 0.1267 s / img. ETA=0:01:13
[32m[04/15 13:31:17 d2.evaluation.evaluator]: [0mInference done 900/1257. 0.1267 s / img. ETA=0:01:06
[32m[04/15 13:31:22 d2.evaluation.evaluator]: [0mInference done 934/1257. 0.1265 s / img. ETA=0:00:59
[32m[04/15 13:31:27 d2.evaluation.evaluator]: [0mInference done 963/1257. 0.1262 s / img. ETA=0:00:54
[32m[04/15 13:31:32 d2.evaluation.evaluator]: [0mInference done 988/1257. 0.1261 s / img. ETA=0:00:49
[32m[04/15 13:31:37 d2.evaluation.evaluator]: [0mInference done 1021/1257. 0.1259 s / img. ETA=0:00:43
[32m[04/15 13:31:42 d2.evaluation.evaluator]: [0mInference done 1050/1257. 0.1258 s / img. ETA=0:00:38
[32m[04/15 13:31:47 d2.evaluation.evaluator]: [0mInference done 1080/1257. 0.1261 s / img. ETA=0:00:32
[32m[04/15 13:31:52 d2.evaluation.evaluator]: [0mInference done 1112/1257. 0.1267 s / img. ETA=0:00:26
[32m[04/15 13:31:57 d2.evaluation.evaluator]: [0mInference done 1144/1257. 0.1267 s / img. ETA=0:00:20
[32m[04/15 13:32:03 d2.evaluation.evaluator]: [0mInference done 1180/1257. 0.1265 s / img. ETA=0:00:13
[32m[04/15 13:32:08 d2.evaluation.evaluator]: [0mInference done 1210/1257. 0.1264 s / img. ETA=0:00:08
[32m[04/15 13:32:13 d2.evaluation.evaluator]: [0mInference done 1246/1257. 0.1262 s / img. ETA=0:00:01
[32m[04/15 13:32:15 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:44.277768 (0.179136 s / img per device, on 1 devices)
[32m[04/15 13:32:15 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:37 (0.126158 s / img per device, on 1 devices)
[32m[04/15 13:32:20 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 13:32:20 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 13:32:20 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.59s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=6.03s).
Accumulating evaluation results...
DONE (t=1.36s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.416
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.190
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.265
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.473
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.109
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.263
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.183
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.315
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.523
[32m[04/15 13:32:34 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 21.309 | 41.619 | 18.965 | 12.852 | 26.451 | 47.307 |
[32m[04/15 13:32:34 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP     | category       | AP     |
|:--------------|:-------|:--------------|:-------|:---------------|:-------|
| person        | 31.216 | bicycle       | 11.367 | car            | 42.652 |
| motorcycle    | nan    | airplane      | nan    | bus            | nan    |
| train         | nan    | truck         | nan    | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan    | stop sign      | nan    |
| parking meter | nan    | bench         | nan    | bird           | nan    |
| cat           | nan    | dog           | 0.000  | horse          | nan    |
| sheep         | nan    | cow           | nan    | elephant       | nan    |
| bear          | nan    | zebra         | nan    | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan    | handbag        | nan    |
| tie           | nan    | suitcase      | nan    | frisbee        | nan    |
| skis          | nan    | snowboard     | nan    | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan    | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan    | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan    | cup            | nan    |
| fork          | nan    | knife         | nan    | spoon          | nan    |
| bowl          | nan    | banana        | nan    | apple          | nan    |
| sandwich      | nan    | orange        | nan    | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan    | pizza          | nan    |
| donut         | nan    | cake          | nan    | chair          | nan    |
| couch         | nan    | potted plant  | nan    | bed            | nan    |
| dining table  | nan    | toilet        | nan    | tv             | nan    |
| laptop        | nan    | mouse         | nan    | remote         | nan    |
| keyboard      | nan    | cell phone    | nan    | microwave      | nan    |
| oven          | nan    | bike          | nan    | hydrant        | nan    |
| motor         | nan    | rider         | nan    | light          | nan    |
| sign          | nan    | motor vehicle | nan    | human face     | nan    |
| hair drier    | nan    | license plate | nan    |                |        |
============== The  26  *  2000  iterations ============
4 channel input
[32m[04/15 13:32:35 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 13:32:36 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.30 seconds.
[5m[31mWARNING[0m [32m[04/15 13:32:36 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:32:36 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 13:32:37 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 13:32:37 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 13:32:37 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 13:32:47 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 13:33:05 d2.utils.events]: [0m eta: 0:20:42  iter: 19  total_loss: 0.662  loss_cls: 0.206  loss_box_reg: 0.358  loss_rpn_cls: 0.012  loss_rpn_loc: 0.056  time: 0.6465  data_time: 0.3434  lr: 0.000100  max_mem: 3067M
[32m[04/15 13:33:20 d2.utils.events]: [0m eta: 0:17:52  iter: 39  total_loss: 0.644  loss_cls: 0.205  loss_box_reg: 0.368  loss_rpn_cls: 0.018  loss_rpn_loc: 0.055  time: 0.5826  data_time: 0.0070  lr: 0.000200  max_mem: 3067M
[32m[04/15 13:33:45 d2.utils.events]: [0m eta: 0:18:01  iter: 59  total_loss: 0.480  loss_cls: 0.157  loss_box_reg: 0.277  loss_rpn_cls: 0.014  loss_rpn_loc: 0.037  time: 0.5910  data_time: 0.0063  lr: 0.000300  max_mem: 3067M
[32m[04/15 13:33:57 d2.utils.events]: [0m eta: 0:17:43  iter: 79  total_loss: 0.650  loss_cls: 0.207  loss_box_reg: 0.373  loss_rpn_cls: 0.017  loss_rpn_loc: 0.062  time: 0.5785  data_time: 0.0093  lr: 0.000400  max_mem: 3067M
[32m[04/15 13:34:08 d2.utils.events]: [0m eta: 0:17:24  iter: 99  total_loss: 0.688  loss_cls: 0.225  loss_box_reg: 0.387  loss_rpn_cls: 0.015  loss_rpn_loc: 0.057  time: 0.5685  data_time: 0.0065  lr: 0.000500  max_mem: 3067M
[32m[04/15 13:34:20 d2.utils.events]: [0m eta: 0:17:13  iter: 119  total_loss: 0.453  loss_cls: 0.156  loss_box_reg: 0.252  loss_rpn_cls: 0.012  loss_rpn_loc: 0.036  time: 0.5670  data_time: 0.0087  lr: 0.000599  max_mem: 3067M
[32m[04/15 13:34:31 d2.utils.events]: [0m eta: 0:17:03  iter: 139  total_loss: 0.513  loss_cls: 0.166  loss_box_reg: 0.282  loss_rpn_cls: 0.014  loss_rpn_loc: 0.052  time: 0.5686  data_time: 0.0081  lr: 0.000699  max_mem: 3067M
[32m[04/15 13:34:46 d2.utils.events]: [0m eta: 0:16:50  iter: 159  total_loss: 0.525  loss_cls: 0.163  loss_box_reg: 0.318  loss_rpn_cls: 0.008  loss_rpn_loc: 0.046  time: 0.5644  data_time: 0.0071  lr: 0.000799  max_mem: 3067M
[32m[04/15 13:34:56 d2.utils.events]: [0m eta: 0:16:34  iter: 179  total_loss: 0.570  loss_cls: 0.178  loss_box_reg: 0.318  loss_rpn_cls: 0.014  loss_rpn_loc: 0.043  time: 0.5613  data_time: 0.0074  lr: 0.000899  max_mem: 3067M
[32m[04/15 13:35:09 d2.utils.events]: [0m eta: 0:16:25  iter: 199  total_loss: 0.548  loss_cls: 0.162  loss_box_reg: 0.299  loss_rpn_cls: 0.013  loss_rpn_loc: 0.052  time: 0.5649  data_time: 0.0078  lr: 0.000999  max_mem: 3067M
[32m[04/15 13:35:21 d2.utils.events]: [0m eta: 0:16:10  iter: 219  total_loss: 0.559  loss_cls: 0.180  loss_box_reg: 0.295  loss_rpn_cls: 0.009  loss_rpn_loc: 0.047  time: 0.5598  data_time: 0.0063  lr: 0.001099  max_mem: 3067M
[32m[04/15 13:35:32 d2.utils.events]: [0m eta: 0:15:58  iter: 239  total_loss: 0.493  loss_cls: 0.161  loss_box_reg: 0.303  loss_rpn_cls: 0.008  loss_rpn_loc: 0.037  time: 0.5576  data_time: 0.0063  lr: 0.001199  max_mem: 3067M
[32m[04/15 13:35:45 d2.utils.events]: [0m eta: 0:15:51  iter: 259  total_loss: 0.515  loss_cls: 0.140  loss_box_reg: 0.289  loss_rpn_cls: 0.011  loss_rpn_loc: 0.025  time: 0.5608  data_time: 0.0082  lr: 0.001299  max_mem: 3067M
[32m[04/15 13:36:00 d2.utils.events]: [0m eta: 0:15:43  iter: 279  total_loss: 0.530  loss_cls: 0.175  loss_box_reg: 0.286  loss_rpn_cls: 0.010  loss_rpn_loc: 0.044  time: 0.5766  data_time: 0.2589  lr: 0.001399  max_mem: 3067M
[32m[04/15 13:36:12 d2.utils.events]: [0m eta: 0:15:35  iter: 299  total_loss: 0.543  loss_cls: 0.153  loss_box_reg: 0.298  loss_rpn_cls: 0.012  loss_rpn_loc: 0.046  time: 0.5755  data_time: 0.0115  lr: 0.001499  max_mem: 3067M
[32m[04/15 13:36:24 d2.utils.events]: [0m eta: 0:15:23  iter: 319  total_loss: 0.554  loss_cls: 0.164  loss_box_reg: 0.296  loss_rpn_cls: 0.014  loss_rpn_loc: 0.045  time: 0.5749  data_time: 0.0078  lr: 0.001598  max_mem: 3067M
[32m[04/15 13:36:45 d2.utils.events]: [0m eta: 0:15:11  iter: 339  total_loss: 0.566  loss_cls: 0.145  loss_box_reg: 0.318  loss_rpn_cls: 0.010  loss_rpn_loc: 0.061  time: 0.5721  data_time: 0.0068  lr: 0.001698  max_mem: 3067M
[32m[04/15 13:37:00 d2.utils.events]: [0m eta: 0:14:59  iter: 359  total_loss: 0.611  loss_cls: 0.186  loss_box_reg: 0.309  loss_rpn_cls: 0.014  loss_rpn_loc: 0.050  time: 0.5701  data_time: 0.0097  lr: 0.001798  max_mem: 3067M
[32m[04/15 13:37:11 d2.utils.events]: [0m eta: 0:14:50  iter: 379  total_loss: 0.559  loss_cls: 0.164  loss_box_reg: 0.318  loss_rpn_cls: 0.009  loss_rpn_loc: 0.034  time: 0.5711  data_time: 0.0065  lr: 0.001898  max_mem: 3067M
[32m[04/15 13:37:23 d2.utils.events]: [0m eta: 0:14:38  iter: 399  total_loss: 0.611  loss_cls: 0.192  loss_box_reg: 0.351  loss_rpn_cls: 0.011  loss_rpn_loc: 0.054  time: 0.5698  data_time: 0.0081  lr: 0.001998  max_mem: 3067M
[32m[04/15 13:37:33 d2.utils.events]: [0m eta: 0:14:25  iter: 419  total_loss: 0.582  loss_cls: 0.171  loss_box_reg: 0.327  loss_rpn_cls: 0.012  loss_rpn_loc: 0.045  time: 0.5673  data_time: 0.0074  lr: 0.002098  max_mem: 3067M
[32m[04/15 13:37:45 d2.utils.events]: [0m eta: 0:14:14  iter: 439  total_loss: 0.643  loss_cls: 0.196  loss_box_reg: 0.365  loss_rpn_cls: 0.007  loss_rpn_loc: 0.048  time: 0.5682  data_time: 0.0865  lr: 0.002198  max_mem: 3067M
[32m[04/15 13:37:58 d2.utils.events]: [0m eta: 0:14:06  iter: 459  total_loss: 0.449  loss_cls: 0.140  loss_box_reg: 0.269  loss_rpn_cls: 0.007  loss_rpn_loc: 0.035  time: 0.5707  data_time: 0.0067  lr: 0.002298  max_mem: 3067M
[32m[04/15 13:38:09 d2.utils.events]: [0m eta: 0:13:54  iter: 479  total_loss: 0.619  loss_cls: 0.181  loss_box_reg: 0.345  loss_rpn_cls: 0.010  loss_rpn_loc: 0.054  time: 0.5688  data_time: 0.0061  lr: 0.002398  max_mem: 3067M
[32m[04/15 13:38:19 d2.utils.events]: [0m eta: 0:13:42  iter: 499  total_loss: 0.620  loss_cls: 0.185  loss_box_reg: 0.372  loss_rpn_cls: 0.008  loss_rpn_loc: 0.041  time: 0.5675  data_time: 0.0068  lr: 0.002498  max_mem: 3067M
[32m[04/15 13:38:32 d2.utils.events]: [0m eta: 0:13:34  iter: 519  total_loss: 0.522  loss_cls: 0.169  loss_box_reg: 0.313  loss_rpn_cls: 0.010  loss_rpn_loc: 0.039  time: 0.5694  data_time: 0.0069  lr: 0.002597  max_mem: 3067M
[32m[04/15 13:38:43 d2.utils.events]: [0m eta: 0:13:22  iter: 539  total_loss: 0.558  loss_cls: 0.174  loss_box_reg: 0.317  loss_rpn_cls: 0.009  loss_rpn_loc: 0.054  time: 0.5684  data_time: 0.0068  lr: 0.002697  max_mem: 3067M
[32m[04/15 13:38:54 d2.utils.events]: [0m eta: 0:13:10  iter: 559  total_loss: 0.615  loss_cls: 0.201  loss_box_reg: 0.343  loss_rpn_cls: 0.012  loss_rpn_loc: 0.058  time: 0.5668  data_time: 0.0082  lr: 0.002797  max_mem: 3067M
[32m[04/15 13:39:06 d2.utils.events]: [0m eta: 0:13:00  iter: 579  total_loss: 0.617  loss_cls: 0.197  loss_box_reg: 0.349  loss_rpn_cls: 0.012  loss_rpn_loc: 0.046  time: 0.5679  data_time: 0.0098  lr: 0.002897  max_mem: 3067M
[32m[04/15 13:39:17 d2.utils.events]: [0m eta: 0:12:49  iter: 599  total_loss: 0.502  loss_cls: 0.150  loss_box_reg: 0.296  loss_rpn_cls: 0.007  loss_rpn_loc: 0.045  time: 0.5681  data_time: 0.0140  lr: 0.002997  max_mem: 3067M
[32m[04/15 13:39:29 d2.utils.events]: [0m eta: 0:12:37  iter: 619  total_loss: 0.602  loss_cls: 0.189  loss_box_reg: 0.346  loss_rpn_cls: 0.009  loss_rpn_loc: 0.042  time: 0.5668  data_time: 0.0081  lr: 0.003097  max_mem: 3067M
[32m[04/15 13:39:40 d2.utils.events]: [0m eta: 0:12:26  iter: 639  total_loss: 0.577  loss_cls: 0.167  loss_box_reg: 0.335  loss_rpn_cls: 0.010  loss_rpn_loc: 0.048  time: 0.5666  data_time: 0.0062  lr: 0.003197  max_mem: 3067M
[32m[04/15 13:39:52 d2.utils.events]: [0m eta: 0:12:16  iter: 659  total_loss: 0.526  loss_cls: 0.146  loss_box_reg: 0.297  loss_rpn_cls: 0.007  loss_rpn_loc: 0.043  time: 0.5675  data_time: 0.0067  lr: 0.003297  max_mem: 3067M
[32m[04/15 13:40:04 d2.utils.events]: [0m eta: 0:12:03  iter: 679  total_loss: 0.472  loss_cls: 0.132  loss_box_reg: 0.297  loss_rpn_cls: 0.006  loss_rpn_loc: 0.035  time: 0.5666  data_time: 0.0490  lr: 0.003397  max_mem: 3067M
[32m[04/15 13:40:16 d2.utils.events]: [0m eta: 0:11:52  iter: 699  total_loss: 0.525  loss_cls: 0.161  loss_box_reg: 0.300  loss_rpn_cls: 0.011  loss_rpn_loc: 0.052  time: 0.5656  data_time: 0.0066  lr: 0.003497  max_mem: 3067M
[32m[04/15 13:40:29 d2.utils.events]: [0m eta: 0:11:42  iter: 719  total_loss: 0.612  loss_cls: 0.191  loss_box_reg: 0.340  loss_rpn_cls: 0.014  loss_rpn_loc: 0.063  time: 0.5673  data_time: 0.0077  lr: 0.003596  max_mem: 3067M
[32m[04/15 13:40:40 d2.utils.events]: [0m eta: 0:11:30  iter: 739  total_loss: 0.579  loss_cls: 0.185  loss_box_reg: 0.349  loss_rpn_cls: 0.010  loss_rpn_loc: 0.049  time: 0.5660  data_time: 0.0068  lr: 0.003696  max_mem: 3067M
[32m[04/15 13:40:52 d2.utils.events]: [0m eta: 0:11:21  iter: 759  total_loss: 0.547  loss_cls: 0.155  loss_box_reg: 0.336  loss_rpn_cls: 0.011  loss_rpn_loc: 0.042  time: 0.5658  data_time: 0.0067  lr: 0.003796  max_mem: 3067M
[32m[04/15 13:41:04 d2.utils.events]: [0m eta: 0:11:10  iter: 779  total_loss: 0.594  loss_cls: 0.168  loss_box_reg: 0.328  loss_rpn_cls: 0.013  loss_rpn_loc: 0.051  time: 0.5663  data_time: 0.0100  lr: 0.003896  max_mem: 3067M
[32m[04/15 13:41:15 d2.utils.events]: [0m eta: 0:10:59  iter: 799  total_loss: 0.631  loss_cls: 0.192  loss_box_reg: 0.353  loss_rpn_cls: 0.012  loss_rpn_loc: 0.059  time: 0.5662  data_time: 0.0074  lr: 0.003996  max_mem: 3067M
[32m[04/15 13:41:25 d2.utils.events]: [0m eta: 0:10:47  iter: 819  total_loss: 0.527  loss_cls: 0.150  loss_box_reg: 0.302  loss_rpn_cls: 0.011  loss_rpn_loc: 0.052  time: 0.5649  data_time: 0.0102  lr: 0.004096  max_mem: 3067M
[32m[04/15 13:41:37 d2.utils.events]: [0m eta: 0:10:36  iter: 839  total_loss: 0.502  loss_cls: 0.166  loss_box_reg: 0.296  loss_rpn_cls: 0.007  loss_rpn_loc: 0.039  time: 0.5648  data_time: 0.0062  lr: 0.004196  max_mem: 3067M
[32m[04/15 13:41:49 d2.utils.events]: [0m eta: 0:10:26  iter: 859  total_loss: 0.691  loss_cls: 0.203  loss_box_reg: 0.395  loss_rpn_cls: 0.010  loss_rpn_loc: 0.068  time: 0.5657  data_time: 0.0066  lr: 0.004296  max_mem: 3067M
[32m[04/15 13:42:00 d2.utils.events]: [0m eta: 0:10:15  iter: 879  total_loss: 0.552  loss_cls: 0.166  loss_box_reg: 0.292  loss_rpn_cls: 0.011  loss_rpn_loc: 0.055  time: 0.5650  data_time: 0.0077  lr: 0.004396  max_mem: 3067M
[32m[04/15 13:42:11 d2.utils.events]: [0m eta: 0:10:04  iter: 899  total_loss: 0.529  loss_cls: 0.179  loss_box_reg: 0.288  loss_rpn_cls: 0.015  loss_rpn_loc: 0.050  time: 0.5645  data_time: 0.0093  lr: 0.004496  max_mem: 3067M
[32m[04/15 13:42:23 d2.utils.events]: [0m eta: 0:09:54  iter: 919  total_loss: 0.564  loss_cls: 0.190  loss_box_reg: 0.335  loss_rpn_cls: 0.013  loss_rpn_loc: 0.037  time: 0.5654  data_time: 0.0083  lr: 0.004595  max_mem: 3067M
[32m[04/15 13:42:34 d2.utils.events]: [0m eta: 0:09:43  iter: 939  total_loss: 0.632  loss_cls: 0.190  loss_box_reg: 0.352  loss_rpn_cls: 0.013  loss_rpn_loc: 0.051  time: 0.5649  data_time: 0.0090  lr: 0.004695  max_mem: 3067M
[32m[04/15 13:42:45 d2.utils.events]: [0m eta: 0:09:31  iter: 959  total_loss: 0.523  loss_cls: 0.179  loss_box_reg: 0.316  loss_rpn_cls: 0.009  loss_rpn_loc: 0.042  time: 0.5641  data_time: 0.0090  lr: 0.004795  max_mem: 3067M
[32m[04/15 13:42:56 d2.utils.events]: [0m eta: 0:09:20  iter: 979  total_loss: 0.626  loss_cls: 0.189  loss_box_reg: 0.334  loss_rpn_cls: 0.014  loss_rpn_loc: 0.055  time: 0.5640  data_time: 0.0075  lr: 0.004895  max_mem: 3067M
[32m[04/15 13:43:08 d2.utils.events]: [0m eta: 0:09:10  iter: 999  total_loss: 0.577  loss_cls: 0.178  loss_box_reg: 0.327  loss_rpn_cls: 0.014  loss_rpn_loc: 0.049  time: 0.5646  data_time: 0.0073  lr: 0.004995  max_mem: 3067M
[32m[04/15 13:43:19 d2.utils.events]: [0m eta: 0:08:59  iter: 1019  total_loss: 0.679  loss_cls: 0.203  loss_box_reg: 0.375  loss_rpn_cls: 0.013  loss_rpn_loc: 0.060  time: 0.5642  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:43:30 d2.utils.events]: [0m eta: 0:08:48  iter: 1039  total_loss: 0.622  loss_cls: 0.198  loss_box_reg: 0.360  loss_rpn_cls: 0.018  loss_rpn_loc: 0.063  time: 0.5637  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:43:43 d2.utils.events]: [0m eta: 0:08:37  iter: 1059  total_loss: 0.641  loss_cls: 0.177  loss_box_reg: 0.366  loss_rpn_cls: 0.016  loss_rpn_loc: 0.048  time: 0.5646  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:43:53 d2.utils.events]: [0m eta: 0:08:26  iter: 1079  total_loss: 0.579  loss_cls: 0.174  loss_box_reg: 0.322  loss_rpn_cls: 0.009  loss_rpn_loc: 0.042  time: 0.5640  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:44:04 d2.utils.events]: [0m eta: 0:08:15  iter: 1099  total_loss: 0.644  loss_cls: 0.210  loss_box_reg: 0.361  loss_rpn_cls: 0.011  loss_rpn_loc: 0.060  time: 0.5634  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:44:15 d2.utils.events]: [0m eta: 0:08:04  iter: 1119  total_loss: 0.734  loss_cls: 0.234  loss_box_reg: 0.383  loss_rpn_cls: 0.015  loss_rpn_loc: 0.055  time: 0.5630  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:44:27 d2.utils.events]: [0m eta: 0:07:53  iter: 1139  total_loss: 0.637  loss_cls: 0.190  loss_box_reg: 0.379  loss_rpn_cls: 0.016  loss_rpn_loc: 0.054  time: 0.5636  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:44:38 d2.utils.events]: [0m eta: 0:07:43  iter: 1159  total_loss: 0.546  loss_cls: 0.176  loss_box_reg: 0.342  loss_rpn_cls: 0.014  loss_rpn_loc: 0.052  time: 0.5631  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:44:50 d2.utils.events]: [0m eta: 0:07:32  iter: 1179  total_loss: 0.632  loss_cls: 0.205  loss_box_reg: 0.372  loss_rpn_cls: 0.013  loss_rpn_loc: 0.052  time: 0.5633  data_time: 0.0208  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:45:02 d2.utils.events]: [0m eta: 0:07:21  iter: 1199  total_loss: 0.586  loss_cls: 0.173  loss_box_reg: 0.332  loss_rpn_cls: 0.014  loss_rpn_loc: 0.057  time: 0.5640  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:45:13 d2.utils.events]: [0m eta: 0:07:11  iter: 1219  total_loss: 0.493  loss_cls: 0.176  loss_box_reg: 0.258  loss_rpn_cls: 0.010  loss_rpn_loc: 0.043  time: 0.5636  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:45:24 d2.utils.events]: [0m eta: 0:06:59  iter: 1239  total_loss: 0.641  loss_cls: 0.167  loss_box_reg: 0.338  loss_rpn_cls: 0.020  loss_rpn_loc: 0.072  time: 0.5629  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:45:37 d2.utils.events]: [0m eta: 0:06:48  iter: 1259  total_loss: 0.603  loss_cls: 0.199  loss_box_reg: 0.329  loss_rpn_cls: 0.016  loss_rpn_loc: 0.051  time: 0.5638  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:45:48 d2.utils.events]: [0m eta: 0:06:37  iter: 1279  total_loss: 0.712  loss_cls: 0.221  loss_box_reg: 0.381  loss_rpn_cls: 0.016  loss_rpn_loc: 0.064  time: 0.5635  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:45:59 d2.utils.events]: [0m eta: 0:06:26  iter: 1299  total_loss: 0.626  loss_cls: 0.189  loss_box_reg: 0.334  loss_rpn_cls: 0.021  loss_rpn_loc: 0.054  time: 0.5633  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:46:10 d2.utils.events]: [0m eta: 0:06:15  iter: 1319  total_loss: 0.688  loss_cls: 0.226  loss_box_reg: 0.371  loss_rpn_cls: 0.015  loss_rpn_loc: 0.062  time: 0.5632  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:46:22 d2.utils.events]: [0m eta: 0:06:04  iter: 1339  total_loss: 0.646  loss_cls: 0.193  loss_box_reg: 0.328  loss_rpn_cls: 0.020  loss_rpn_loc: 0.067  time: 0.5632  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:46:33 d2.utils.events]: [0m eta: 0:05:53  iter: 1359  total_loss: 0.555  loss_cls: 0.170  loss_box_reg: 0.332  loss_rpn_cls: 0.016  loss_rpn_loc: 0.045  time: 0.5628  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:46:45 d2.utils.events]: [0m eta: 0:05:42  iter: 1379  total_loss: 0.608  loss_cls: 0.197  loss_box_reg: 0.339  loss_rpn_cls: 0.021  loss_rpn_loc: 0.061  time: 0.5628  data_time: 0.0125  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:46:57 d2.utils.events]: [0m eta: 0:05:31  iter: 1399  total_loss: 0.507  loss_cls: 0.143  loss_box_reg: 0.297  loss_rpn_cls: 0.011  loss_rpn_loc: 0.041  time: 0.5635  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:47:08 d2.utils.events]: [0m eta: 0:05:21  iter: 1419  total_loss: 0.639  loss_cls: 0.212  loss_box_reg: 0.348  loss_rpn_cls: 0.008  loss_rpn_loc: 0.053  time: 0.5631  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:47:19 d2.utils.events]: [0m eta: 0:05:09  iter: 1439  total_loss: 0.634  loss_cls: 0.194  loss_box_reg: 0.353  loss_rpn_cls: 0.012  loss_rpn_loc: 0.055  time: 0.5627  data_time: 0.0184  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:47:31 d2.utils.events]: [0m eta: 0:04:58  iter: 1459  total_loss: 0.630  loss_cls: 0.206  loss_box_reg: 0.325  loss_rpn_cls: 0.016  loss_rpn_loc: 0.061  time: 0.5636  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:47:42 d2.utils.events]: [0m eta: 0:04:47  iter: 1479  total_loss: 0.564  loss_cls: 0.179  loss_box_reg: 0.351  loss_rpn_cls: 0.011  loss_rpn_loc: 0.052  time: 0.5632  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:47:54 d2.utils.events]: [0m eta: 0:04:36  iter: 1499  total_loss: 0.437  loss_cls: 0.141  loss_box_reg: 0.255  loss_rpn_cls: 0.009  loss_rpn_loc: 0.037  time: 0.5627  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:48:06 d2.utils.events]: [0m eta: 0:04:25  iter: 1519  total_loss: 0.625  loss_cls: 0.186  loss_box_reg: 0.361  loss_rpn_cls: 0.016  loss_rpn_loc: 0.061  time: 0.5633  data_time: 0.0103  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:48:18 d2.utils.events]: [0m eta: 0:04:14  iter: 1539  total_loss: 0.651  loss_cls: 0.202  loss_box_reg: 0.355  loss_rpn_cls: 0.014  loss_rpn_loc: 0.058  time: 0.5632  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:48:29 d2.utils.events]: [0m eta: 0:04:03  iter: 1559  total_loss: 0.820  loss_cls: 0.265  loss_box_reg: 0.423  loss_rpn_cls: 0.021  loss_rpn_loc: 0.065  time: 0.5631  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:48:40 d2.utils.events]: [0m eta: 0:03:52  iter: 1579  total_loss: 0.632  loss_cls: 0.211  loss_box_reg: 0.328  loss_rpn_cls: 0.018  loss_rpn_loc: 0.041  time: 0.5629  data_time: 0.0130  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:48:52 d2.utils.events]: [0m eta: 0:03:41  iter: 1599  total_loss: 0.751  loss_cls: 0.212  loss_box_reg: 0.420  loss_rpn_cls: 0.014  loss_rpn_loc: 0.075  time: 0.5631  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:49:03 d2.utils.events]: [0m eta: 0:03:30  iter: 1619  total_loss: 0.578  loss_cls: 0.183  loss_box_reg: 0.341  loss_rpn_cls: 0.014  loss_rpn_loc: 0.051  time: 0.5628  data_time: 0.0179  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:49:14 d2.utils.events]: [0m eta: 0:03:19  iter: 1639  total_loss: 0.654  loss_cls: 0.219  loss_box_reg: 0.355  loss_rpn_cls: 0.017  loss_rpn_loc: 0.065  time: 0.5625  data_time: 0.0062  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:49:26 d2.utils.events]: [0m eta: 0:03:08  iter: 1659  total_loss: 0.545  loss_cls: 0.187  loss_box_reg: 0.286  loss_rpn_cls: 0.021  loss_rpn_loc: 0.063  time: 0.5630  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:49:37 d2.utils.events]: [0m eta: 0:02:57  iter: 1679  total_loss: 0.637  loss_cls: 0.220  loss_box_reg: 0.340  loss_rpn_cls: 0.015  loss_rpn_loc: 0.054  time: 0.5628  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:49:48 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.671  loss_cls: 0.207  loss_box_reg: 0.388  loss_rpn_cls: 0.023  loss_rpn_loc: 0.052  time: 0.5624  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:50:00 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.701  loss_cls: 0.212  loss_box_reg: 0.384  loss_rpn_cls: 0.016  loss_rpn_loc: 0.076  time: 0.5628  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:50:11 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.634  loss_cls: 0.187  loss_box_reg: 0.341  loss_rpn_cls: 0.012  loss_rpn_loc: 0.057  time: 0.5627  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:50:22 d2.utils.events]: [0m eta: 0:02:12  iter: 1759  total_loss: 0.598  loss_cls: 0.178  loss_box_reg: 0.332  loss_rpn_cls: 0.015  loss_rpn_loc: 0.047  time: 0.5624  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:50:33 d2.utils.events]: [0m eta: 0:02:01  iter: 1779  total_loss: 0.575  loss_cls: 0.188  loss_box_reg: 0.311  loss_rpn_cls: 0.012  loss_rpn_loc: 0.051  time: 0.5620  data_time: 0.0206  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:50:46 d2.utils.events]: [0m eta: 0:01:50  iter: 1799  total_loss: 0.695  loss_cls: 0.232  loss_box_reg: 0.405  loss_rpn_cls: 0.014  loss_rpn_loc: 0.058  time: 0.5626  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:50:57 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.521  loss_cls: 0.167  loss_box_reg: 0.294  loss_rpn_cls: 0.011  loss_rpn_loc: 0.046  time: 0.5623  data_time: 0.0094  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:51:08 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.638  loss_cls: 0.197  loss_box_reg: 0.375  loss_rpn_cls: 0.016  loss_rpn_loc: 0.051  time: 0.5623  data_time: 0.0229  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:51:21 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.509  loss_cls: 0.152  loss_box_reg: 0.282  loss_rpn_cls: 0.008  loss_rpn_loc: 0.038  time: 0.5629  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:51:32 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.600  loss_cls: 0.184  loss_box_reg: 0.332  loss_rpn_cls: 0.009  loss_rpn_loc: 0.054  time: 0.5630  data_time: 0.0462  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:51:44 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.653  loss_cls: 0.207  loss_box_reg: 0.374  loss_rpn_cls: 0.016  loss_rpn_loc: 0.056  time: 0.5629  data_time: 0.0060  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:51:55 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.530  loss_cls: 0.156  loss_box_reg: 0.312  loss_rpn_cls: 0.013  loss_rpn_loc: 0.053  time: 0.5628  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:52:08 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.615  loss_cls: 0.195  loss_box_reg: 0.311  loss_rpn_cls: 0.015  loss_rpn_loc: 0.051  time: 0.5638  data_time: 0.1318  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:52:20 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.651  loss_cls: 0.218  loss_box_reg: 0.368  loss_rpn_cls: 0.014  loss_rpn_loc: 0.059  time: 0.5637  data_time: 0.0116  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:52:32 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.572  loss_cls: 0.195  loss_box_reg: 0.296  loss_rpn_cls: 0.013  loss_rpn_loc: 0.049  time: 0.5637  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:53:40 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json takes 2.21 seconds.
[5m[31mWARNING[0m [32m[04/15 13:53:40 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:53:40 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 13:53:41 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 13:53:41 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.626  loss_cls: 0.190  loss_box_reg: 0.350  loss_rpn_cls: 0.016  loss_rpn_loc: 0.050  time: 0.5645  data_time: 0.0437  lr: 0.005000  max_mem: 3067M
[32m[04/15 13:53:43 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:47 (0.5648 s / it)
[32m[04/15 13:53:43 d2.engine.hooks]: [0mTotal training time: 0:20:49 (0:02:01 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 13:53:55 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:53:55 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 13:53:55 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 13:54:12 d2.evaluation.evaluator]: [0mInference done 1/1257. 5.2919 s / img. ETA=6:03:37
[32m[04/15 13:54:18 d2.evaluation.evaluator]: [0mInference done 24/1257. 0.1212 s / img. ETA=0:05:01
[32m[04/15 13:54:23 d2.evaluation.evaluator]: [0mInference done 51/1257. 0.1206 s / img. ETA=0:04:15
[32m[04/15 13:54:28 d2.evaluation.evaluator]: [0mInference done 79/1257. 0.1236 s / img. ETA=0:04:05
[32m[04/15 13:54:34 d2.evaluation.evaluator]: [0mInference done 100/1257. 0.1423 s / img. ETA=0:04:20
[32m[04/15 13:54:39 d2.evaluation.evaluator]: [0mInference done 127/1257. 0.1372 s / img. ETA=0:04:05
[32m[04/15 13:54:45 d2.evaluation.evaluator]: [0mInference done 148/1257. 0.1346 s / img. ETA=0:04:04
[32m[04/15 13:54:50 d2.evaluation.evaluator]: [0mInference done 174/1257. 0.1322 s / img. ETA=0:03:55
[32m[04/15 13:54:55 d2.evaluation.evaluator]: [0mInference done 197/1257. 0.1311 s / img. ETA=0:03:53
[32m[04/15 13:55:01 d2.evaluation.evaluator]: [0mInference done 211/1257. 0.1302 s / img. ETA=0:04:01
[32m[04/15 13:55:06 d2.evaluation.evaluator]: [0mInference done 228/1257. 0.1306 s / img. ETA=0:04:04
[32m[04/15 13:55:11 d2.evaluation.evaluator]: [0mInference done 242/1257. 0.1300 s / img. ETA=0:04:09
[32m[04/15 13:55:17 d2.evaluation.evaluator]: [0mInference done 262/1257. 0.1292 s / img. ETA=0:04:07
[32m[04/15 13:55:22 d2.evaluation.evaluator]: [0mInference done 292/1257. 0.1283 s / img. ETA=0:03:51
[32m[04/15 13:55:27 d2.evaluation.evaluator]: [0mInference done 321/1257. 0.1275 s / img. ETA=0:03:39
[32m[04/15 13:55:32 d2.evaluation.evaluator]: [0mInference done 354/1257. 0.1274 s / img. ETA=0:03:25
[32m[04/15 13:55:38 d2.evaluation.evaluator]: [0mInference done 379/1257. 0.1269 s / img. ETA=0:03:18
[32m[04/15 13:55:43 d2.evaluation.evaluator]: [0mInference done 402/1257. 0.1282 s / img. ETA=0:03:13
[32m[04/15 13:55:48 d2.evaluation.evaluator]: [0mInference done 427/1257. 0.1280 s / img. ETA=0:03:06
[32m[04/15 13:55:54 d2.evaluation.evaluator]: [0mInference done 449/1257. 0.1275 s / img. ETA=0:03:02
[32m[04/15 13:55:59 d2.evaluation.evaluator]: [0mInference done 470/1257. 0.1271 s / img. ETA=0:02:58
[32m[04/15 13:56:04 d2.evaluation.evaluator]: [0mInference done 496/1257. 0.1267 s / img. ETA=0:02:51
[32m[04/15 13:56:09 d2.evaluation.evaluator]: [0mInference done 519/1257. 0.1264 s / img. ETA=0:02:46
[32m[04/15 13:56:14 d2.evaluation.evaluator]: [0mInference done 545/1257. 0.1263 s / img. ETA=0:02:39
[32m[04/15 13:56:19 d2.evaluation.evaluator]: [0mInference done 568/1257. 0.1261 s / img. ETA=0:02:34
[32m[04/15 13:56:24 d2.evaluation.evaluator]: [0mInference done 592/1257. 0.1265 s / img. ETA=0:02:28
[32m[04/15 13:56:29 d2.evaluation.evaluator]: [0mInference done 610/1257. 0.1263 s / img. ETA=0:02:25
[32m[04/15 13:56:34 d2.evaluation.evaluator]: [0mInference done 634/1257. 0.1261 s / img. ETA=0:02:20
[32m[04/15 13:56:40 d2.evaluation.evaluator]: [0mInference done 653/1257. 0.1259 s / img. ETA=0:02:16
[32m[04/15 13:56:45 d2.evaluation.evaluator]: [0mInference done 671/1257. 0.1257 s / img. ETA=0:02:13
[32m[04/15 13:56:50 d2.evaluation.evaluator]: [0mInference done 691/1257. 0.1256 s / img. ETA=0:02:09
[32m[04/15 13:56:55 d2.evaluation.evaluator]: [0mInference done 718/1257. 0.1254 s / img. ETA=0:02:02
[32m[04/15 13:57:00 d2.evaluation.evaluator]: [0mInference done 740/1257. 0.1260 s / img. ETA=0:01:57
[32m[04/15 13:57:05 d2.evaluation.evaluator]: [0mInference done 762/1257. 0.1263 s / img. ETA=0:01:52
[32m[04/15 13:57:10 d2.evaluation.evaluator]: [0mInference done 799/1257. 0.1261 s / img. ETA=0:01:42
[32m[04/15 13:57:16 d2.evaluation.evaluator]: [0mInference done 824/1257. 0.1259 s / img. ETA=0:01:36
[32m[04/15 13:57:21 d2.evaluation.evaluator]: [0mInference done 851/1257. 0.1258 s / img. ETA=0:01:30
[32m[04/15 13:57:26 d2.evaluation.evaluator]: [0mInference done 884/1257. 0.1257 s / img. ETA=0:01:21
[32m[04/15 13:57:31 d2.evaluation.evaluator]: [0mInference done 917/1257. 0.1255 s / img. ETA=0:01:13
[32m[04/15 13:57:36 d2.evaluation.evaluator]: [0mInference done 940/1257. 0.1258 s / img. ETA=0:01:08
[32m[04/15 13:57:41 d2.evaluation.evaluator]: [0mInference done 965/1257. 0.1258 s / img. ETA=0:01:03
[32m[04/15 13:57:46 d2.evaluation.evaluator]: [0mInference done 993/1257. 0.1257 s / img. ETA=0:00:57
[32m[04/15 13:57:52 d2.evaluation.evaluator]: [0mInference done 1021/1257. 0.1256 s / img. ETA=0:00:50
[32m[04/15 13:57:57 d2.evaluation.evaluator]: [0mInference done 1050/1257. 0.1254 s / img. ETA=0:00:44
[32m[04/15 13:58:02 d2.evaluation.evaluator]: [0mInference done 1077/1257. 0.1254 s / img. ETA=0:00:38
[32m[04/15 13:58:07 d2.evaluation.evaluator]: [0mInference done 1110/1257. 0.1253 s / img. ETA=0:00:31
[32m[04/15 13:58:12 d2.evaluation.evaluator]: [0mInference done 1141/1257. 0.1256 s / img. ETA=0:00:24
[32m[04/15 13:58:17 d2.evaluation.evaluator]: [0mInference done 1168/1257. 0.1260 s / img. ETA=0:00:18
[32m[04/15 13:58:22 d2.evaluation.evaluator]: [0mInference done 1204/1257. 0.1258 s / img. ETA=0:00:11
[32m[04/15 13:58:27 d2.evaluation.evaluator]: [0mInference done 1234/1257. 0.1257 s / img. ETA=0:00:04
[32m[04/15 13:58:31 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:17.716586 (0.205844 s / img per device, on 1 devices)
[32m[04/15 13:58:31 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:37 (0.125607 s / img per device, on 1 devices)
[32m[04/15 13:58:33 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 13:58:33 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 13:58:33 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=7.25s).
Accumulating evaluation results...
DONE (t=2.53s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.199
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.393
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.175
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.127
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.245
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.376
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.102
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.246
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.247
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.175
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.298
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.411
[32m[04/15 13:58:48 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 19.867 | 39.328 | 17.469 | 12.651 | 24.549 | 37.557 |
[32m[04/15 13:58:48 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 26.617 | bicycle       | 9.722 | car            | 43.127 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
============== The  27  *  2000  iterations ============
4 channel input
[32m[04/15 13:58:50 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 13:58:51 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.36 seconds.
[5m[31mWARNING[0m [32m[04/15 13:58:51 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 13:58:51 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 13:58:52 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 13:58:52 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 13:58:52 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 13:59:05 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 13:59:17 d2.utils.events]: [0m eta: 0:17:40  iter: 19  total_loss: 0.569  loss_cls: 0.190  loss_box_reg: 0.327  loss_rpn_cls: 0.018  loss_rpn_loc: 0.045  time: 0.5412  data_time: 0.0760  lr: 0.000100  max_mem: 3067M
[32m[04/15 13:59:38 d2.utils.events]: [0m eta: 0:17:29  iter: 39  total_loss: 0.507  loss_cls: 0.147  loss_box_reg: 0.320  loss_rpn_cls: 0.011  loss_rpn_loc: 0.032  time: 0.5337  data_time: 0.0077  lr: 0.000200  max_mem: 3067M
[32m[04/15 13:59:49 d2.utils.events]: [0m eta: 0:17:19  iter: 59  total_loss: 0.627  loss_cls: 0.192  loss_box_reg: 0.326  loss_rpn_cls: 0.017  loss_rpn_loc: 0.079  time: 0.5349  data_time: 0.0064  lr: 0.000300  max_mem: 3067M
[32m[04/15 14:00:00 d2.utils.events]: [0m eta: 0:17:16  iter: 79  total_loss: 0.586  loss_cls: 0.170  loss_box_reg: 0.336  loss_rpn_cls: 0.008  loss_rpn_loc: 0.046  time: 0.5414  data_time: 0.0061  lr: 0.000400  max_mem: 3067M
[32m[04/15 14:00:13 d2.utils.events]: [0m eta: 0:17:02  iter: 99  total_loss: 0.595  loss_cls: 0.199  loss_box_reg: 0.346  loss_rpn_cls: 0.017  loss_rpn_loc: 0.046  time: 0.5405  data_time: 0.0090  lr: 0.000500  max_mem: 3067M
[32m[04/15 14:00:24 d2.utils.events]: [0m eta: 0:16:44  iter: 119  total_loss: 0.542  loss_cls: 0.160  loss_box_reg: 0.327  loss_rpn_cls: 0.013  loss_rpn_loc: 0.049  time: 0.5367  data_time: 0.0124  lr: 0.000599  max_mem: 3067M
[32m[04/15 14:00:35 d2.utils.events]: [0m eta: 0:16:38  iter: 139  total_loss: 0.562  loss_cls: 0.182  loss_box_reg: 0.348  loss_rpn_cls: 0.013  loss_rpn_loc: 0.050  time: 0.5405  data_time: 0.0255  lr: 0.000699  max_mem: 3067M
[32m[04/15 14:00:47 d2.utils.events]: [0m eta: 0:16:32  iter: 159  total_loss: 0.529  loss_cls: 0.159  loss_box_reg: 0.303  loss_rpn_cls: 0.012  loss_rpn_loc: 0.045  time: 0.5477  data_time: 0.0199  lr: 0.000799  max_mem: 3067M
[32m[04/15 14:00:59 d2.utils.events]: [0m eta: 0:16:21  iter: 179  total_loss: 0.576  loss_cls: 0.179  loss_box_reg: 0.325  loss_rpn_cls: 0.014  loss_rpn_loc: 0.055  time: 0.5460  data_time: 0.0061  lr: 0.000899  max_mem: 3067M
[32m[04/15 14:01:11 d2.utils.events]: [0m eta: 0:16:13  iter: 199  total_loss: 0.650  loss_cls: 0.173  loss_box_reg: 0.366  loss_rpn_cls: 0.016  loss_rpn_loc: 0.050  time: 0.5465  data_time: 0.0102  lr: 0.000999  max_mem: 3067M
[32m[04/15 14:01:22 d2.utils.events]: [0m eta: 0:16:01  iter: 219  total_loss: 0.594  loss_cls: 0.173  loss_box_reg: 0.341  loss_rpn_cls: 0.011  loss_rpn_loc: 0.058  time: 0.5461  data_time: 0.0085  lr: 0.001099  max_mem: 3067M
[32m[04/15 14:01:33 d2.utils.events]: [0m eta: 0:15:52  iter: 239  total_loss: 0.593  loss_cls: 0.184  loss_box_reg: 0.310  loss_rpn_cls: 0.011  loss_rpn_loc: 0.048  time: 0.5491  data_time: 0.0083  lr: 0.001199  max_mem: 3067M
[32m[04/15 14:01:44 d2.utils.events]: [0m eta: 0:15:39  iter: 259  total_loss: 0.563  loss_cls: 0.187  loss_box_reg: 0.336  loss_rpn_cls: 0.012  loss_rpn_loc: 0.037  time: 0.5474  data_time: 0.0177  lr: 0.001299  max_mem: 3067M
[32m[04/15 14:01:56 d2.utils.events]: [0m eta: 0:15:26  iter: 279  total_loss: 0.624  loss_cls: 0.192  loss_box_reg: 0.328  loss_rpn_cls: 0.012  loss_rpn_loc: 0.061  time: 0.5448  data_time: 0.0068  lr: 0.001399  max_mem: 3067M
[32m[04/15 14:02:09 d2.utils.events]: [0m eta: 0:15:16  iter: 299  total_loss: 0.480  loss_cls: 0.149  loss_box_reg: 0.262  loss_rpn_cls: 0.009  loss_rpn_loc: 0.048  time: 0.5484  data_time: 0.0085  lr: 0.001499  max_mem: 3067M
[32m[04/15 14:02:20 d2.utils.events]: [0m eta: 0:15:06  iter: 319  total_loss: 0.519  loss_cls: 0.163  loss_box_reg: 0.296  loss_rpn_cls: 0.009  loss_rpn_loc: 0.031  time: 0.5474  data_time: 0.0224  lr: 0.001598  max_mem: 3067M
[32m[04/15 14:02:31 d2.utils.events]: [0m eta: 0:14:57  iter: 339  total_loss: 0.615  loss_cls: 0.196  loss_box_reg: 0.352  loss_rpn_cls: 0.010  loss_rpn_loc: 0.046  time: 0.5472  data_time: 0.0078  lr: 0.001698  max_mem: 3067M
[32m[04/15 14:02:43 d2.utils.events]: [0m eta: 0:14:47  iter: 359  total_loss: 0.498  loss_cls: 0.157  loss_box_reg: 0.296  loss_rpn_cls: 0.010  loss_rpn_loc: 0.045  time: 0.5488  data_time: 0.0270  lr: 0.001798  max_mem: 3067M
[32m[04/15 14:02:54 d2.utils.events]: [0m eta: 0:14:36  iter: 379  total_loss: 0.410  loss_cls: 0.126  loss_box_reg: 0.236  loss_rpn_cls: 0.010  loss_rpn_loc: 0.030  time: 0.5484  data_time: 0.0080  lr: 0.001898  max_mem: 3067M
[32m[04/15 14:03:05 d2.utils.events]: [0m eta: 0:14:24  iter: 399  total_loss: 0.503  loss_cls: 0.142  loss_box_reg: 0.305  loss_rpn_cls: 0.010  loss_rpn_loc: 0.034  time: 0.5472  data_time: 0.0089  lr: 0.001998  max_mem: 3067M
[32m[04/15 14:03:21 d2.utils.events]: [0m eta: 0:14:15  iter: 419  total_loss: 0.532  loss_cls: 0.157  loss_box_reg: 0.292  loss_rpn_cls: 0.015  loss_rpn_loc: 0.046  time: 0.5503  data_time: 0.0096  lr: 0.002098  max_mem: 3067M
[32m[04/15 14:03:33 d2.utils.events]: [0m eta: 0:14:04  iter: 439  total_loss: 0.575  loss_cls: 0.187  loss_box_reg: 0.328  loss_rpn_cls: 0.013  loss_rpn_loc: 0.047  time: 0.5516  data_time: 0.0615  lr: 0.002198  max_mem: 3067M
[32m[04/15 14:03:44 d2.utils.events]: [0m eta: 0:13:52  iter: 459  total_loss: 0.606  loss_cls: 0.183  loss_box_reg: 0.349  loss_rpn_cls: 0.013  loss_rpn_loc: 0.045  time: 0.5514  data_time: 0.0403  lr: 0.002298  max_mem: 3067M
[32m[04/15 14:03:55 d2.utils.events]: [0m eta: 0:13:40  iter: 479  total_loss: 0.483  loss_cls: 0.148  loss_box_reg: 0.288  loss_rpn_cls: 0.006  loss_rpn_loc: 0.041  time: 0.5512  data_time: 0.0065  lr: 0.002398  max_mem: 3067M
[32m[04/15 14:04:08 d2.utils.events]: [0m eta: 0:13:31  iter: 499  total_loss: 0.539  loss_cls: 0.176  loss_box_reg: 0.322  loss_rpn_cls: 0.010  loss_rpn_loc: 0.041  time: 0.5521  data_time: 0.0086  lr: 0.002498  max_mem: 3067M
[32m[04/15 14:04:21 d2.utils.events]: [0m eta: 0:13:20  iter: 519  total_loss: 0.599  loss_cls: 0.169  loss_box_reg: 0.356  loss_rpn_cls: 0.009  loss_rpn_loc: 0.060  time: 0.5517  data_time: 0.0161  lr: 0.002597  max_mem: 3067M
[32m[04/15 14:04:32 d2.utils.events]: [0m eta: 0:13:10  iter: 539  total_loss: 0.625  loss_cls: 0.182  loss_box_reg: 0.353  loss_rpn_cls: 0.007  loss_rpn_loc: 0.061  time: 0.5521  data_time: 0.0079  lr: 0.002697  max_mem: 3067M
[32m[04/15 14:04:45 d2.utils.events]: [0m eta: 0:13:01  iter: 559  total_loss: 0.644  loss_cls: 0.196  loss_box_reg: 0.362  loss_rpn_cls: 0.015  loss_rpn_loc: 0.065  time: 0.5544  data_time: 0.0132  lr: 0.002797  max_mem: 3067M
[32m[04/15 14:04:57 d2.utils.events]: [0m eta: 0:12:51  iter: 579  total_loss: 0.579  loss_cls: 0.183  loss_box_reg: 0.319  loss_rpn_cls: 0.009  loss_rpn_loc: 0.045  time: 0.5542  data_time: 0.0065  lr: 0.002897  max_mem: 3067M
[32m[04/15 14:05:08 d2.utils.events]: [0m eta: 0:12:39  iter: 599  total_loss: 0.508  loss_cls: 0.158  loss_box_reg: 0.288  loss_rpn_cls: 0.011  loss_rpn_loc: 0.064  time: 0.5534  data_time: 0.0141  lr: 0.002997  max_mem: 3067M
[32m[04/15 14:05:21 d2.utils.events]: [0m eta: 0:12:31  iter: 619  total_loss: 0.608  loss_cls: 0.185  loss_box_reg: 0.359  loss_rpn_cls: 0.007  loss_rpn_loc: 0.042  time: 0.5555  data_time: 0.0078  lr: 0.003097  max_mem: 3067M
[32m[04/15 14:05:32 d2.utils.events]: [0m eta: 0:12:20  iter: 639  total_loss: 0.598  loss_cls: 0.169  loss_box_reg: 0.336  loss_rpn_cls: 0.010  loss_rpn_loc: 0.053  time: 0.5553  data_time: 0.0091  lr: 0.003197  max_mem: 3067M
[32m[04/15 14:05:52 d2.utils.events]: [0m eta: 0:12:10  iter: 659  total_loss: 0.579  loss_cls: 0.184  loss_box_reg: 0.308  loss_rpn_cls: 0.015  loss_rpn_loc: 0.052  time: 0.5570  data_time: 0.0651  lr: 0.003297  max_mem: 3067M
[32m[04/15 14:06:04 d2.utils.events]: [0m eta: 0:11:58  iter: 679  total_loss: 0.557  loss_cls: 0.185  loss_box_reg: 0.303  loss_rpn_cls: 0.009  loss_rpn_loc: 0.050  time: 0.5570  data_time: 0.0767  lr: 0.003397  max_mem: 3067M
[32m[04/15 14:06:19 d2.utils.events]: [0m eta: 0:11:47  iter: 699  total_loss: 0.507  loss_cls: 0.144  loss_box_reg: 0.310  loss_rpn_cls: 0.011  loss_rpn_loc: 0.050  time: 0.5565  data_time: 0.0855  lr: 0.003497  max_mem: 3067M
[32m[04/15 14:06:37 d2.utils.events]: [0m eta: 0:11:37  iter: 719  total_loss: 0.673  loss_cls: 0.214  loss_box_reg: 0.375  loss_rpn_cls: 0.010  loss_rpn_loc: 0.058  time: 0.5579  data_time: 0.0211  lr: 0.003596  max_mem: 3067M
[32m[04/15 14:06:49 d2.utils.events]: [0m eta: 0:11:26  iter: 739  total_loss: 0.582  loss_cls: 0.204  loss_box_reg: 0.333  loss_rpn_cls: 0.008  loss_rpn_loc: 0.047  time: 0.5575  data_time: 0.0078  lr: 0.003696  max_mem: 3067M
[32m[04/15 14:07:18 d2.utils.events]: [0m eta: 0:11:17  iter: 759  total_loss: 0.595  loss_cls: 0.176  loss_box_reg: 0.337  loss_rpn_cls: 0.014  loss_rpn_loc: 0.067  time: 0.5587  data_time: 0.0079  lr: 0.003796  max_mem: 3067M
[32m[04/15 14:07:32 d2.utils.events]: [0m eta: 0:11:06  iter: 779  total_loss: 0.592  loss_cls: 0.192  loss_box_reg: 0.334  loss_rpn_cls: 0.011  loss_rpn_loc: 0.046  time: 0.5584  data_time: 0.0070  lr: 0.003896  max_mem: 3067M
[32m[04/15 14:07:42 d2.utils.events]: [0m eta: 0:10:54  iter: 799  total_loss: 0.491  loss_cls: 0.161  loss_box_reg: 0.281  loss_rpn_cls: 0.011  loss_rpn_loc: 0.031  time: 0.5571  data_time: 0.0066  lr: 0.003996  max_mem: 3067M
[32m[04/15 14:07:57 d2.utils.events]: [0m eta: 0:10:44  iter: 819  total_loss: 0.549  loss_cls: 0.157  loss_box_reg: 0.326  loss_rpn_cls: 0.014  loss_rpn_loc: 0.050  time: 0.5574  data_time: 0.0067  lr: 0.004096  max_mem: 3067M
[32m[04/15 14:08:13 d2.utils.events]: [0m eta: 0:10:33  iter: 839  total_loss: 0.501  loss_cls: 0.147  loss_box_reg: 0.294  loss_rpn_cls: 0.007  loss_rpn_loc: 0.045  time: 0.5618  data_time: 0.2300  lr: 0.004196  max_mem: 3067M
[32m[04/15 14:08:28 d2.utils.events]: [0m eta: 0:10:22  iter: 859  total_loss: 0.654  loss_cls: 0.193  loss_box_reg: 0.346  loss_rpn_cls: 0.008  loss_rpn_loc: 0.057  time: 0.5617  data_time: 0.0096  lr: 0.004296  max_mem: 3067M
[32m[04/15 14:08:44 d2.utils.events]: [0m eta: 0:10:12  iter: 879  total_loss: 0.637  loss_cls: 0.193  loss_box_reg: 0.353  loss_rpn_cls: 0.014  loss_rpn_loc: 0.058  time: 0.5623  data_time: 0.0154  lr: 0.004396  max_mem: 3067M
[32m[04/15 14:08:57 d2.utils.events]: [0m eta: 0:10:02  iter: 899  total_loss: 0.615  loss_cls: 0.184  loss_box_reg: 0.392  loss_rpn_cls: 0.015  loss_rpn_loc: 0.044  time: 0.5623  data_time: 0.0539  lr: 0.004496  max_mem: 3067M
[32m[04/15 14:09:10 d2.utils.events]: [0m eta: 0:09:49  iter: 919  total_loss: 0.675  loss_cls: 0.203  loss_box_reg: 0.365  loss_rpn_cls: 0.015  loss_rpn_loc: 0.057  time: 0.5614  data_time: 0.0179  lr: 0.004595  max_mem: 3067M
[32m[04/15 14:09:22 d2.utils.events]: [0m eta: 0:09:39  iter: 939  total_loss: 0.564  loss_cls: 0.176  loss_box_reg: 0.339  loss_rpn_cls: 0.011  loss_rpn_loc: 0.049  time: 0.5620  data_time: 0.0107  lr: 0.004695  max_mem: 3067M
[32m[04/15 14:09:34 d2.utils.events]: [0m eta: 0:09:27  iter: 959  total_loss: 0.580  loss_cls: 0.178  loss_box_reg: 0.343  loss_rpn_cls: 0.012  loss_rpn_loc: 0.044  time: 0.5614  data_time: 0.0175  lr: 0.004795  max_mem: 3067M
[32m[04/15 14:09:46 d2.utils.events]: [0m eta: 0:09:16  iter: 979  total_loss: 0.516  loss_cls: 0.158  loss_box_reg: 0.320  loss_rpn_cls: 0.011  loss_rpn_loc: 0.031  time: 0.5614  data_time: 0.0648  lr: 0.004895  max_mem: 3067M
[32m[04/15 14:10:00 d2.utils.events]: [0m eta: 0:09:06  iter: 999  total_loss: 0.513  loss_cls: 0.162  loss_box_reg: 0.296  loss_rpn_cls: 0.008  loss_rpn_loc: 0.043  time: 0.5624  data_time: 0.0128  lr: 0.004995  max_mem: 3067M
[32m[04/15 14:10:11 d2.utils.events]: [0m eta: 0:08:55  iter: 1019  total_loss: 0.465  loss_cls: 0.143  loss_box_reg: 0.279  loss_rpn_cls: 0.011  loss_rpn_loc: 0.040  time: 0.5617  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:10:22 d2.utils.events]: [0m eta: 0:08:44  iter: 1039  total_loss: 0.625  loss_cls: 0.210  loss_box_reg: 0.356  loss_rpn_cls: 0.014  loss_rpn_loc: 0.048  time: 0.5606  data_time: 0.0166  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:10:50 d2.utils.events]: [0m eta: 0:08:33  iter: 1059  total_loss: 0.621  loss_cls: 0.195  loss_box_reg: 0.358  loss_rpn_cls: 0.013  loss_rpn_loc: 0.056  time: 0.5602  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:11:03 d2.utils.events]: [0m eta: 0:08:21  iter: 1079  total_loss: 0.631  loss_cls: 0.172  loss_box_reg: 0.376  loss_rpn_cls: 0.013  loss_rpn_loc: 0.067  time: 0.5595  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:11:16 d2.utils.events]: [0m eta: 0:08:11  iter: 1099  total_loss: 0.589  loss_cls: 0.211  loss_box_reg: 0.328  loss_rpn_cls: 0.015  loss_rpn_loc: 0.059  time: 0.5602  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:11:29 d2.utils.events]: [0m eta: 0:08:00  iter: 1119  total_loss: 0.641  loss_cls: 0.185  loss_box_reg: 0.369  loss_rpn_cls: 0.012  loss_rpn_loc: 0.064  time: 0.5593  data_time: 0.0042  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:11:42 d2.utils.events]: [0m eta: 0:07:49  iter: 1139  total_loss: 0.576  loss_cls: 0.188  loss_box_reg: 0.310  loss_rpn_cls: 0.013  loss_rpn_loc: 0.046  time: 0.5599  data_time: 0.0052  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:11:53 d2.utils.events]: [0m eta: 0:07:37  iter: 1159  total_loss: 0.660  loss_cls: 0.213  loss_box_reg: 0.383  loss_rpn_cls: 0.022  loss_rpn_loc: 0.059  time: 0.5595  data_time: 0.0059  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:12:05 d2.utils.events]: [0m eta: 0:07:26  iter: 1179  total_loss: 0.537  loss_cls: 0.158  loss_box_reg: 0.327  loss_rpn_cls: 0.012  loss_rpn_loc: 0.050  time: 0.5589  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:12:16 d2.utils.events]: [0m eta: 0:07:15  iter: 1199  total_loss: 0.566  loss_cls: 0.158  loss_box_reg: 0.293  loss_rpn_cls: 0.009  loss_rpn_loc: 0.050  time: 0.5586  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:12:30 d2.utils.events]: [0m eta: 0:07:05  iter: 1219  total_loss: 0.573  loss_cls: 0.186  loss_box_reg: 0.335  loss_rpn_cls: 0.017  loss_rpn_loc: 0.043  time: 0.5587  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:12:41 d2.utils.events]: [0m eta: 0:06:53  iter: 1239  total_loss: 0.635  loss_cls: 0.188  loss_box_reg: 0.351  loss_rpn_cls: 0.012  loss_rpn_loc: 0.050  time: 0.5582  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:12:52 d2.utils.events]: [0m eta: 0:06:43  iter: 1259  total_loss: 0.644  loss_cls: 0.210  loss_box_reg: 0.355  loss_rpn_cls: 0.012  loss_rpn_loc: 0.057  time: 0.5577  data_time: 0.0060  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:13:05 d2.utils.events]: [0m eta: 0:06:32  iter: 1279  total_loss: 0.595  loss_cls: 0.203  loss_box_reg: 0.342  loss_rpn_cls: 0.014  loss_rpn_loc: 0.043  time: 0.5588  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:13:17 d2.utils.events]: [0m eta: 0:06:21  iter: 1299  total_loss: 0.561  loss_cls: 0.169  loss_box_reg: 0.310  loss_rpn_cls: 0.009  loss_rpn_loc: 0.047  time: 0.5584  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:13:28 d2.utils.events]: [0m eta: 0:06:10  iter: 1319  total_loss: 0.640  loss_cls: 0.193  loss_box_reg: 0.374  loss_rpn_cls: 0.012  loss_rpn_loc: 0.053  time: 0.5584  data_time: 0.0457  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:13:40 d2.utils.events]: [0m eta: 0:06:00  iter: 1339  total_loss: 0.643  loss_cls: 0.188  loss_box_reg: 0.381  loss_rpn_cls: 0.015  loss_rpn_loc: 0.055  time: 0.5589  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:13:53 d2.utils.events]: [0m eta: 0:05:49  iter: 1359  total_loss: 0.487  loss_cls: 0.160  loss_box_reg: 0.274  loss_rpn_cls: 0.010  loss_rpn_loc: 0.035  time: 0.5589  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:14:04 d2.utils.events]: [0m eta: 0:05:38  iter: 1379  total_loss: 0.519  loss_cls: 0.154  loss_box_reg: 0.290  loss_rpn_cls: 0.011  loss_rpn_loc: 0.052  time: 0.5585  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:14:16 d2.utils.events]: [0m eta: 0:05:27  iter: 1399  total_loss: 0.592  loss_cls: 0.202  loss_box_reg: 0.317  loss_rpn_cls: 0.011  loss_rpn_loc: 0.047  time: 0.5588  data_time: 0.0061  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:14:28 d2.utils.events]: [0m eta: 0:05:16  iter: 1419  total_loss: 0.627  loss_cls: 0.201  loss_box_reg: 0.365  loss_rpn_cls: 0.010  loss_rpn_loc: 0.054  time: 0.5588  data_time: 0.0091  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:14:39 d2.utils.events]: [0m eta: 0:05:05  iter: 1439  total_loss: 0.657  loss_cls: 0.187  loss_box_reg: 0.330  loss_rpn_cls: 0.014  loss_rpn_loc: 0.060  time: 0.5584  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:14:51 d2.utils.events]: [0m eta: 0:04:54  iter: 1459  total_loss: 0.625  loss_cls: 0.196  loss_box_reg: 0.375  loss_rpn_cls: 0.018  loss_rpn_loc: 0.058  time: 0.5586  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:15:03 d2.utils.events]: [0m eta: 0:04:43  iter: 1479  total_loss: 0.581  loss_cls: 0.206  loss_box_reg: 0.320  loss_rpn_cls: 0.017  loss_rpn_loc: 0.048  time: 0.5587  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:15:14 d2.utils.events]: [0m eta: 0:04:32  iter: 1499  total_loss: 0.583  loss_cls: 0.199  loss_box_reg: 0.311  loss_rpn_cls: 0.017  loss_rpn_loc: 0.054  time: 0.5583  data_time: 0.0103  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:15:25 d2.utils.events]: [0m eta: 0:04:21  iter: 1519  total_loss: 0.671  loss_cls: 0.199  loss_box_reg: 0.367  loss_rpn_cls: 0.017  loss_rpn_loc: 0.070  time: 0.5579  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:15:38 d2.utils.events]: [0m eta: 0:04:11  iter: 1539  total_loss: 0.563  loss_cls: 0.168  loss_box_reg: 0.325  loss_rpn_cls: 0.013  loss_rpn_loc: 0.049  time: 0.5590  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:15:50 d2.utils.events]: [0m eta: 0:03:59  iter: 1559  total_loss: 0.601  loss_cls: 0.181  loss_box_reg: 0.347  loss_rpn_cls: 0.010  loss_rpn_loc: 0.054  time: 0.5586  data_time: 0.0187  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:16:02 d2.utils.events]: [0m eta: 0:03:48  iter: 1579  total_loss: 0.700  loss_cls: 0.210  loss_box_reg: 0.389  loss_rpn_cls: 0.014  loss_rpn_loc: 0.075  time: 0.5584  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:16:15 d2.utils.events]: [0m eta: 0:03:38  iter: 1599  total_loss: 0.610  loss_cls: 0.184  loss_box_reg: 0.333  loss_rpn_cls: 0.009  loss_rpn_loc: 0.048  time: 0.5593  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:16:26 d2.utils.events]: [0m eta: 0:03:27  iter: 1619  total_loss: 0.495  loss_cls: 0.165  loss_box_reg: 0.279  loss_rpn_cls: 0.008  loss_rpn_loc: 0.040  time: 0.5589  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:16:38 d2.utils.events]: [0m eta: 0:03:16  iter: 1639  total_loss: 0.657  loss_cls: 0.210  loss_box_reg: 0.353  loss_rpn_cls: 0.015  loss_rpn_loc: 0.049  time: 0.5586  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:16:51 d2.utils.events]: [0m eta: 0:03:05  iter: 1659  total_loss: 0.513  loss_cls: 0.149  loss_box_reg: 0.308  loss_rpn_cls: 0.012  loss_rpn_loc: 0.060  time: 0.5596  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:17:01 d2.utils.events]: [0m eta: 0:02:54  iter: 1679  total_loss: 0.717  loss_cls: 0.226  loss_box_reg: 0.394  loss_rpn_cls: 0.019  loss_rpn_loc: 0.057  time: 0.5593  data_time: 0.0104  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:17:12 d2.utils.events]: [0m eta: 0:02:43  iter: 1699  total_loss: 0.696  loss_cls: 0.216  loss_box_reg: 0.376  loss_rpn_cls: 0.016  loss_rpn_loc: 0.059  time: 0.5589  data_time: 0.0180  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:17:25 d2.utils.events]: [0m eta: 0:02:32  iter: 1719  total_loss: 0.551  loss_cls: 0.176  loss_box_reg: 0.301  loss_rpn_cls: 0.012  loss_rpn_loc: 0.044  time: 0.5593  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:17:37 d2.utils.events]: [0m eta: 0:02:21  iter: 1739  total_loss: 0.593  loss_cls: 0.178  loss_box_reg: 0.339  loss_rpn_cls: 0.014  loss_rpn_loc: 0.053  time: 0.5591  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:17:48 d2.utils.events]: [0m eta: 0:02:10  iter: 1759  total_loss: 0.726  loss_cls: 0.229  loss_box_reg: 0.367  loss_rpn_cls: 0.030  loss_rpn_loc: 0.076  time: 0.5588  data_time: 0.0096  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:18:01 d2.utils.events]: [0m eta: 0:02:00  iter: 1779  total_loss: 0.498  loss_cls: 0.172  loss_box_reg: 0.288  loss_rpn_cls: 0.012  loss_rpn_loc: 0.048  time: 0.5590  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:18:13 d2.utils.events]: [0m eta: 0:01:49  iter: 1799  total_loss: 0.474  loss_cls: 0.151  loss_box_reg: 0.271  loss_rpn_cls: 0.010  loss_rpn_loc: 0.038  time: 0.5593  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:18:24 d2.utils.events]: [0m eta: 0:01:38  iter: 1819  total_loss: 0.628  loss_cls: 0.194  loss_box_reg: 0.355  loss_rpn_cls: 0.018  loss_rpn_loc: 0.063  time: 0.5590  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:18:35 d2.utils.events]: [0m eta: 0:01:27  iter: 1839  total_loss: 0.536  loss_cls: 0.168  loss_box_reg: 0.323  loss_rpn_cls: 0.010  loss_rpn_loc: 0.044  time: 0.5587  data_time: 0.0242  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:18:48 d2.utils.events]: [0m eta: 0:01:16  iter: 1859  total_loss: 0.666  loss_cls: 0.198  loss_box_reg: 0.366  loss_rpn_cls: 0.016  loss_rpn_loc: 0.065  time: 0.5593  data_time: 0.0104  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:19:00 d2.utils.events]: [0m eta: 0:01:05  iter: 1879  total_loss: 0.623  loss_cls: 0.193  loss_box_reg: 0.359  loss_rpn_cls: 0.013  loss_rpn_loc: 0.061  time: 0.5590  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:19:11 d2.utils.events]: [0m eta: 0:00:54  iter: 1899  total_loss: 0.619  loss_cls: 0.175  loss_box_reg: 0.327  loss_rpn_cls: 0.021  loss_rpn_loc: 0.057  time: 0.5587  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:19:47 d2.utils.events]: [0m eta: 0:00:43  iter: 1919  total_loss: 0.574  loss_cls: 0.194  loss_box_reg: 0.328  loss_rpn_cls: 0.010  loss_rpn_loc: 0.044  time: 0.5584  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:20:13 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.627  loss_cls: 0.192  loss_box_reg: 0.371  loss_rpn_cls: 0.011  loss_rpn_loc: 0.040  time: 0.5581  data_time: 0.0087  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:20:24 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.653  loss_cls: 0.201  loss_box_reg: 0.370  loss_rpn_cls: 0.023  loss_rpn_loc: 0.045  time: 0.5576  data_time: 0.0045  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:20:34 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.654  loss_cls: 0.211  loss_box_reg: 0.370  loss_rpn_cls: 0.011  loss_rpn_loc: 0.054  time: 0.5570  data_time: 0.0045  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:20:55 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json takes 1.58 seconds.
[5m[31mWARNING[0m [32m[04/15 14:20:55 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 14:20:55 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 14:20:56 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 14:20:56 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.718  loss_cls: 0.203  loss_box_reg: 0.407  loss_rpn_cls: 0.012  loss_rpn_loc: 0.056  time: 0.5567  data_time: 0.0042  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:21:07 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:32 (0.5570 s / it)
[32m[04/15 14:21:07 d2.engine.hooks]: [0mTotal training time: 0:21:58 (0:03:26 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 14:21:17 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 14:21:17 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 14:21:17 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 14:21:21 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1206 s / img. ETA=0:03:31
[32m[04/15 14:21:26 d2.evaluation.evaluator]: [0mInference done 39/1257. 0.1213 s / img. ETA=0:03:43
[32m[04/15 14:21:31 d2.evaluation.evaluator]: [0mInference done 61/1257. 0.1208 s / img. ETA=0:04:05
[32m[04/15 14:21:37 d2.evaluation.evaluator]: [0mInference done 82/1257. 0.1210 s / img. ETA=0:04:16
[32m[04/15 14:21:42 d2.evaluation.evaluator]: [0mInference done 113/1257. 0.1228 s / img. ETA=0:03:52
[32m[04/15 14:21:47 d2.evaluation.evaluator]: [0mInference done 139/1257. 0.1223 s / img. ETA=0:03:45
[32m[04/15 14:21:52 d2.evaluation.evaluator]: [0mInference done 160/1257. 0.1256 s / img. ETA=0:03:47
[32m[04/15 14:21:57 d2.evaluation.evaluator]: [0mInference done 182/1257. 0.1261 s / img. ETA=0:03:45
[32m[04/15 14:22:02 d2.evaluation.evaluator]: [0mInference done 215/1257. 0.1253 s / img. ETA=0:03:29
[32m[04/15 14:22:07 d2.evaluation.evaluator]: [0mInference done 243/1257. 0.1246 s / img. ETA=0:03:21
[32m[04/15 14:22:12 d2.evaluation.evaluator]: [0mInference done 272/1257. 0.1240 s / img. ETA=0:03:13
[32m[04/15 14:22:17 d2.evaluation.evaluator]: [0mInference done 299/1257. 0.1238 s / img. ETA=0:03:06
[32m[04/15 14:22:22 d2.evaluation.evaluator]: [0mInference done 329/1257. 0.1235 s / img. ETA=0:02:58
[32m[04/15 14:22:27 d2.evaluation.evaluator]: [0mInference done 359/1257. 0.1248 s / img. ETA=0:02:51
[32m[04/15 14:22:32 d2.evaluation.evaluator]: [0mInference done 391/1257. 0.1261 s / img. ETA=0:02:42
[32m[04/15 14:22:38 d2.evaluation.evaluator]: [0mInference done 416/1257. 0.1258 s / img. ETA=0:02:39
[32m[04/15 14:22:43 d2.evaluation.evaluator]: [0mInference done 438/1257. 0.1254 s / img. ETA=0:02:37
[32m[04/15 14:22:48 d2.evaluation.evaluator]: [0mInference done 467/1257. 0.1251 s / img. ETA=0:02:30
[32m[04/15 14:22:53 d2.evaluation.evaluator]: [0mInference done 489/1257. 0.1248 s / img. ETA=0:02:28
[32m[04/15 14:22:58 d2.evaluation.evaluator]: [0mInference done 503/1257. 0.1246 s / img. ETA=0:02:29
[32m[04/15 14:23:03 d2.evaluation.evaluator]: [0mInference done 532/1257. 0.1249 s / img. ETA=0:02:22
[32m[04/15 14:23:09 d2.evaluation.evaluator]: [0mInference done 556/1257. 0.1260 s / img. ETA=0:02:18
[32m[04/15 14:23:14 d2.evaluation.evaluator]: [0mInference done 585/1257. 0.1257 s / img. ETA=0:02:12
[32m[04/15 14:23:19 d2.evaluation.evaluator]: [0mInference done 606/1257. 0.1255 s / img. ETA=0:02:09
[32m[04/15 14:23:24 d2.evaluation.evaluator]: [0mInference done 628/1257. 0.1253 s / img. ETA=0:02:05
[32m[04/15 14:23:29 d2.evaluation.evaluator]: [0mInference done 649/1257. 0.1251 s / img. ETA=0:02:02
[32m[04/15 14:23:34 d2.evaluation.evaluator]: [0mInference done 673/1257. 0.1250 s / img. ETA=0:01:57
[32m[04/15 14:23:39 d2.evaluation.evaluator]: [0mInference done 695/1257. 0.1249 s / img. ETA=0:01:53
[32m[04/15 14:23:44 d2.evaluation.evaluator]: [0mInference done 711/1257. 0.1253 s / img. ETA=0:01:51
[32m[04/15 14:23:49 d2.evaluation.evaluator]: [0mInference done 739/1257. 0.1251 s / img. ETA=0:01:45
[32m[04/15 14:23:54 d2.evaluation.evaluator]: [0mInference done 765/1257. 0.1249 s / img. ETA=0:01:40
[32m[04/15 14:24:00 d2.evaluation.evaluator]: [0mInference done 795/1257. 0.1247 s / img. ETA=0:01:33
[32m[04/15 14:24:05 d2.evaluation.evaluator]: [0mInference done 824/1257. 0.1245 s / img. ETA=0:01:27
[32m[04/15 14:24:10 d2.evaluation.evaluator]: [0mInference done 859/1257. 0.1243 s / img. ETA=0:01:19
[32m[04/15 14:24:15 d2.evaluation.evaluator]: [0mInference done 883/1257. 0.1243 s / img. ETA=0:01:14
[32m[04/15 14:24:20 d2.evaluation.evaluator]: [0mInference done 906/1257. 0.1246 s / img. ETA=0:01:10
[32m[04/15 14:24:26 d2.evaluation.evaluator]: [0mInference done 933/1257. 0.1251 s / img. ETA=0:01:04
[32m[04/15 14:24:31 d2.evaluation.evaluator]: [0mInference done 958/1257. 0.1249 s / img. ETA=0:00:59
[32m[04/15 14:24:36 d2.evaluation.evaluator]: [0mInference done 981/1257. 0.1249 s / img. ETA=0:00:55
[32m[04/15 14:24:41 d2.evaluation.evaluator]: [0mInference done 1007/1257. 0.1248 s / img. ETA=0:00:50
[32m[04/15 14:24:46 d2.evaluation.evaluator]: [0mInference done 1041/1257. 0.1247 s / img. ETA=0:00:43
[32m[04/15 14:24:51 d2.evaluation.evaluator]: [0mInference done 1066/1257. 0.1245 s / img. ETA=0:00:38
[32m[04/15 14:24:57 d2.evaluation.evaluator]: [0mInference done 1085/1257. 0.1248 s / img. ETA=0:00:34
[32m[04/15 14:25:02 d2.evaluation.evaluator]: [0mInference done 1114/1257. 0.1254 s / img. ETA=0:00:28
[32m[04/15 14:25:07 d2.evaluation.evaluator]: [0mInference done 1149/1257. 0.1255 s / img. ETA=0:00:21
[32m[04/15 14:25:12 d2.evaluation.evaluator]: [0mInference done 1171/1257. 0.1254 s / img. ETA=0:00:17
[32m[04/15 14:25:17 d2.evaluation.evaluator]: [0mInference done 1194/1257. 0.1253 s / img. ETA=0:00:12
[32m[04/15 14:25:22 d2.evaluation.evaluator]: [0mInference done 1224/1257. 0.1252 s / img. ETA=0:00:06
[32m[04/15 14:25:28 d2.evaluation.evaluator]: [0mInference done 1247/1257. 0.1251 s / img. ETA=0:00:01
[32m[04/15 14:25:29 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:09.109127 (0.198969 s / img per device, on 1 devices)
[32m[04/15 14:25:29 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:36 (0.125080 s / img per device, on 1 devices)
[32m[04/15 14:25:30 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 14:25:30 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 14:25:31 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.52s).
Accumulating evaluation results...
DONE (t=1.13s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.423
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.183
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.135
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.259
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.458
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.105
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.193
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.310
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501
[32m[04/15 14:25:41 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 21.040 | 42.305 | 18.343 | 13.506 | 25.906 | 45.840 |
[32m[04/15 14:25:41 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP     | category       | AP     |
|:--------------|:-------|:--------------|:-------|:---------------|:-------|
| person        | 29.661 | bicycle       | 10.829 | car            | 43.671 |
| motorcycle    | nan    | airplane      | nan    | bus            | nan    |
| train         | nan    | truck         | nan    | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan    | stop sign      | nan    |
| parking meter | nan    | bench         | nan    | bird           | nan    |
| cat           | nan    | dog           | 0.000  | horse          | nan    |
| sheep         | nan    | cow           | nan    | elephant       | nan    |
| bear          | nan    | zebra         | nan    | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan    | handbag        | nan    |
| tie           | nan    | suitcase      | nan    | frisbee        | nan    |
| skis          | nan    | snowboard     | nan    | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan    | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan    | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan    | cup            | nan    |
| fork          | nan    | knife         | nan    | spoon          | nan    |
| bowl          | nan    | banana        | nan    | apple          | nan    |
| sandwich      | nan    | orange        | nan    | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan    | pizza          | nan    |
| donut         | nan    | cake          | nan    | chair          | nan    |
| couch         | nan    | potted plant  | nan    | bed            | nan    |
| dining table  | nan    | toilet        | nan    | tv             | nan    |
| laptop        | nan    | mouse         | nan    | remote         | nan    |
| keyboard      | nan    | cell phone    | nan    | microwave      | nan    |
| oven          | nan    | bike          | nan    | hydrant        | nan    |
| motor         | nan    | rider         | nan    | light          | nan    |
| sign          | nan    | motor vehicle | nan    | human face     | nan    |
| hair drier    | nan    | license plate | nan    |                |        |
============== The  28  *  2000  iterations ============
4 channel input
[32m[04/15 14:25:43 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 14:25:44 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.34 seconds.
[5m[31mWARNING[0m [32m[04/15 14:25:44 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 14:25:44 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 14:25:45 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 14:25:45 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 14:25:45 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 14:25:57 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 14:26:09 d2.utils.events]: [0m eta: 0:18:01  iter: 19  total_loss: 0.545  loss_cls: 0.163  loss_box_reg: 0.286  loss_rpn_cls: 0.016  loss_rpn_loc: 0.056  time: 0.5485  data_time: 0.0677  lr: 0.000100  max_mem: 3067M
[32m[04/15 14:26:22 d2.utils.events]: [0m eta: 0:18:30  iter: 39  total_loss: 0.546  loss_cls: 0.165  loss_box_reg: 0.289  loss_rpn_cls: 0.012  loss_rpn_loc: 0.043  time: 0.5703  data_time: 0.0066  lr: 0.000200  max_mem: 3067M
[32m[04/15 14:26:34 d2.utils.events]: [0m eta: 0:18:19  iter: 59  total_loss: 0.661  loss_cls: 0.195  loss_box_reg: 0.354  loss_rpn_cls: 0.012  loss_rpn_loc: 0.050  time: 0.5751  data_time: 0.0737  lr: 0.000300  max_mem: 3067M
[32m[04/15 14:26:46 d2.utils.events]: [0m eta: 0:18:08  iter: 79  total_loss: 0.424  loss_cls: 0.139  loss_box_reg: 0.236  loss_rpn_cls: 0.012  loss_rpn_loc: 0.041  time: 0.5741  data_time: 0.0063  lr: 0.000400  max_mem: 3067M
[32m[04/15 14:27:24 d2.utils.events]: [0m eta: 0:17:41  iter: 99  total_loss: 0.549  loss_cls: 0.173  loss_box_reg: 0.302  loss_rpn_cls: 0.011  loss_rpn_loc: 0.047  time: 0.5717  data_time: 0.0082  lr: 0.000500  max_mem: 3067M
[32m[04/15 14:27:36 d2.utils.events]: [0m eta: 0:17:39  iter: 119  total_loss: 0.663  loss_cls: 0.195  loss_box_reg: 0.385  loss_rpn_cls: 0.014  loss_rpn_loc: 0.054  time: 0.5766  data_time: 0.0069  lr: 0.000599  max_mem: 3067M
[32m[04/15 14:27:50 d2.utils.events]: [0m eta: 0:17:18  iter: 139  total_loss: 0.485  loss_cls: 0.149  loss_box_reg: 0.288  loss_rpn_cls: 0.015  loss_rpn_loc: 0.036  time: 0.5686  data_time: 0.0066  lr: 0.000699  max_mem: 3067M
[32m[04/15 14:28:06 d2.utils.events]: [0m eta: 0:17:08  iter: 159  total_loss: 0.517  loss_cls: 0.156  loss_box_reg: 0.298  loss_rpn_cls: 0.011  loss_rpn_loc: 0.043  time: 0.5702  data_time: 0.0093  lr: 0.000799  max_mem: 3067M
[32m[04/15 14:28:24 d2.utils.events]: [0m eta: 0:16:57  iter: 179  total_loss: 0.510  loss_cls: 0.166  loss_box_reg: 0.300  loss_rpn_cls: 0.015  loss_rpn_loc: 0.033  time: 0.5665  data_time: 0.0062  lr: 0.000899  max_mem: 3067M
[32m[04/15 14:28:38 d2.utils.events]: [0m eta: 0:16:42  iter: 199  total_loss: 0.451  loss_cls: 0.123  loss_box_reg: 0.275  loss_rpn_cls: 0.012  loss_rpn_loc: 0.043  time: 0.5649  data_time: 0.0084  lr: 0.000999  max_mem: 3067M
[32m[04/15 14:28:50 d2.utils.events]: [0m eta: 0:16:34  iter: 219  total_loss: 0.595  loss_cls: 0.199  loss_box_reg: 0.345  loss_rpn_cls: 0.010  loss_rpn_loc: 0.049  time: 0.5689  data_time: 0.0080  lr: 0.001099  max_mem: 3067M
[32m[04/15 14:29:05 d2.utils.events]: [0m eta: 0:16:22  iter: 239  total_loss: 0.602  loss_cls: 0.180  loss_box_reg: 0.342  loss_rpn_cls: 0.011  loss_rpn_loc: 0.062  time: 0.5667  data_time: 0.0084  lr: 0.001199  max_mem: 3067M
[32m[04/15 14:29:17 d2.utils.events]: [0m eta: 0:16:11  iter: 259  total_loss: 0.561  loss_cls: 0.166  loss_box_reg: 0.329  loss_rpn_cls: 0.007  loss_rpn_loc: 0.046  time: 0.5670  data_time: 0.0091  lr: 0.001299  max_mem: 3067M
[32m[04/15 14:29:29 d2.utils.events]: [0m eta: 0:16:00  iter: 279  total_loss: 0.501  loss_cls: 0.147  loss_box_reg: 0.288  loss_rpn_cls: 0.012  loss_rpn_loc: 0.053  time: 0.5684  data_time: 0.0094  lr: 0.001399  max_mem: 3067M
[32m[04/15 14:29:39 d2.utils.events]: [0m eta: 0:15:45  iter: 299  total_loss: 0.509  loss_cls: 0.141  loss_box_reg: 0.317  loss_rpn_cls: 0.017  loss_rpn_loc: 0.046  time: 0.5653  data_time: 0.0323  lr: 0.001499  max_mem: 3067M
[32m[04/15 14:30:37 d2.utils.events]: [0m eta: 0:15:34  iter: 319  total_loss: 0.571  loss_cls: 0.154  loss_box_reg: 0.328  loss_rpn_cls: 0.011  loss_rpn_loc: 0.055  time: 0.5661  data_time: 0.0085  lr: 0.001598  max_mem: 3067M
[32m[04/15 14:30:49 d2.utils.events]: [0m eta: 0:15:23  iter: 339  total_loss: 0.482  loss_cls: 0.142  loss_box_reg: 0.281  loss_rpn_cls: 0.009  loss_rpn_loc: 0.031  time: 0.5668  data_time: 0.0643  lr: 0.001698  max_mem: 3067M
[32m[04/15 14:31:01 d2.utils.events]: [0m eta: 0:15:11  iter: 359  total_loss: 0.534  loss_cls: 0.153  loss_box_reg: 0.319  loss_rpn_cls: 0.010  loss_rpn_loc: 0.036  time: 0.5666  data_time: 0.0070  lr: 0.001798  max_mem: 3067M
[32m[04/15 14:31:28 d2.utils.events]: [0m eta: 0:14:59  iter: 379  total_loss: 0.453  loss_cls: 0.140  loss_box_reg: 0.275  loss_rpn_cls: 0.011  loss_rpn_loc: 0.046  time: 0.5652  data_time: 0.0070  lr: 0.001898  max_mem: 3067M
[32m[04/15 14:31:40 d2.utils.events]: [0m eta: 0:14:48  iter: 399  total_loss: 0.580  loss_cls: 0.174  loss_box_reg: 0.348  loss_rpn_cls: 0.011  loss_rpn_loc: 0.045  time: 0.5652  data_time: 0.0082  lr: 0.001998  max_mem: 3067M
[32m[04/15 14:31:51 d2.utils.events]: [0m eta: 0:14:35  iter: 419  total_loss: 0.604  loss_cls: 0.190  loss_box_reg: 0.326  loss_rpn_cls: 0.016  loss_rpn_loc: 0.044  time: 0.5646  data_time: 0.0090  lr: 0.002098  max_mem: 3067M
[32m[04/15 14:32:06 d2.utils.events]: [0m eta: 0:14:23  iter: 439  total_loss: 0.555  loss_cls: 0.178  loss_box_reg: 0.340  loss_rpn_cls: 0.010  loss_rpn_loc: 0.041  time: 0.5632  data_time: 0.0071  lr: 0.002198  max_mem: 3067M
[32m[04/15 14:32:18 d2.utils.events]: [0m eta: 0:14:12  iter: 459  total_loss: 0.539  loss_cls: 0.171  loss_box_reg: 0.331  loss_rpn_cls: 0.013  loss_rpn_loc: 0.042  time: 0.5634  data_time: 0.0099  lr: 0.002298  max_mem: 3067M
[32m[04/15 14:32:30 d2.utils.events]: [0m eta: 0:14:01  iter: 479  total_loss: 0.568  loss_cls: 0.180  loss_box_reg: 0.319  loss_rpn_cls: 0.012  loss_rpn_loc: 0.048  time: 0.5643  data_time: 0.0076  lr: 0.002398  max_mem: 3067M
[32m[04/15 14:32:41 d2.utils.events]: [0m eta: 0:13:50  iter: 499  total_loss: 0.490  loss_cls: 0.150  loss_box_reg: 0.270  loss_rpn_cls: 0.011  loss_rpn_loc: 0.045  time: 0.5632  data_time: 0.0206  lr: 0.002498  max_mem: 3067M
[32m[04/15 14:32:53 d2.utils.events]: [0m eta: 0:13:37  iter: 519  total_loss: 0.492  loss_cls: 0.145  loss_box_reg: 0.282  loss_rpn_cls: 0.010  loss_rpn_loc: 0.040  time: 0.5625  data_time: 0.0074  lr: 0.002597  max_mem: 3067M
[32m[04/15 14:33:04 d2.utils.events]: [0m eta: 0:13:26  iter: 539  total_loss: 0.499  loss_cls: 0.168  loss_box_reg: 0.301  loss_rpn_cls: 0.012  loss_rpn_loc: 0.041  time: 0.5626  data_time: 0.0067  lr: 0.002697  max_mem: 3067M
[32m[04/15 14:33:18 d2.utils.events]: [0m eta: 0:13:13  iter: 559  total_loss: 0.451  loss_cls: 0.139  loss_box_reg: 0.264  loss_rpn_cls: 0.008  loss_rpn_loc: 0.043  time: 0.5608  data_time: 0.0081  lr: 0.002797  max_mem: 3067M
[32m[04/15 14:33:29 d2.utils.events]: [0m eta: 0:13:02  iter: 579  total_loss: 0.563  loss_cls: 0.176  loss_box_reg: 0.292  loss_rpn_cls: 0.011  loss_rpn_loc: 0.041  time: 0.5608  data_time: 0.0061  lr: 0.002897  max_mem: 3067M
[32m[04/15 14:33:40 d2.utils.events]: [0m eta: 0:12:51  iter: 599  total_loss: 0.601  loss_cls: 0.196  loss_box_reg: 0.336  loss_rpn_cls: 0.012  loss_rpn_loc: 0.051  time: 0.5596  data_time: 0.0171  lr: 0.002997  max_mem: 3067M
[32m[04/15 14:33:52 d2.utils.events]: [0m eta: 0:12:40  iter: 619  total_loss: 0.555  loss_cls: 0.186  loss_box_reg: 0.290  loss_rpn_cls: 0.009  loss_rpn_loc: 0.054  time: 0.5603  data_time: 0.0091  lr: 0.003097  max_mem: 3067M
[32m[04/15 14:34:03 d2.utils.events]: [0m eta: 0:12:29  iter: 639  total_loss: 0.599  loss_cls: 0.181  loss_box_reg: 0.353  loss_rpn_cls: 0.010  loss_rpn_loc: 0.052  time: 0.5594  data_time: 0.0297  lr: 0.003197  max_mem: 3067M
[32m[04/15 14:34:14 d2.utils.events]: [0m eta: 0:12:18  iter: 659  total_loss: 0.521  loss_cls: 0.171  loss_box_reg: 0.291  loss_rpn_cls: 0.015  loss_rpn_loc: 0.043  time: 0.5594  data_time: 0.0065  lr: 0.003297  max_mem: 3067M
[32m[04/15 14:34:27 d2.utils.events]: [0m eta: 0:12:07  iter: 679  total_loss: 0.666  loss_cls: 0.202  loss_box_reg: 0.356  loss_rpn_cls: 0.012  loss_rpn_loc: 0.052  time: 0.5607  data_time: 0.0075  lr: 0.003397  max_mem: 3067M
[32m[04/15 14:34:39 d2.utils.events]: [0m eta: 0:11:57  iter: 699  total_loss: 0.615  loss_cls: 0.178  loss_box_reg: 0.369  loss_rpn_cls: 0.012  loss_rpn_loc: 0.050  time: 0.5606  data_time: 0.0099  lr: 0.003497  max_mem: 3067M
[32m[04/15 14:34:50 d2.utils.events]: [0m eta: 0:11:46  iter: 719  total_loss: 0.515  loss_cls: 0.169  loss_box_reg: 0.298  loss_rpn_cls: 0.008  loss_rpn_loc: 0.039  time: 0.5604  data_time: 0.0072  lr: 0.003596  max_mem: 3067M
[32m[04/15 14:35:04 d2.utils.events]: [0m eta: 0:11:36  iter: 739  total_loss: 0.679  loss_cls: 0.226  loss_box_reg: 0.370  loss_rpn_cls: 0.016  loss_rpn_loc: 0.056  time: 0.5614  data_time: 0.0073  lr: 0.003696  max_mem: 3067M
[32m[04/15 14:35:15 d2.utils.events]: [0m eta: 0:11:25  iter: 759  total_loss: 0.533  loss_cls: 0.158  loss_box_reg: 0.331  loss_rpn_cls: 0.009  loss_rpn_loc: 0.043  time: 0.5609  data_time: 0.0095  lr: 0.003796  max_mem: 3067M
[32m[04/15 14:35:27 d2.utils.events]: [0m eta: 0:11:14  iter: 779  total_loss: 0.584  loss_cls: 0.194  loss_box_reg: 0.331  loss_rpn_cls: 0.010  loss_rpn_loc: 0.042  time: 0.5611  data_time: 0.0075  lr: 0.003896  max_mem: 3067M
[32m[04/15 14:35:42 d2.utils.events]: [0m eta: 0:11:04  iter: 799  total_loss: 0.608  loss_cls: 0.183  loss_box_reg: 0.356  loss_rpn_cls: 0.018  loss_rpn_loc: 0.063  time: 0.5621  data_time: 0.0069  lr: 0.003996  max_mem: 3067M
[32m[04/15 14:35:53 d2.utils.events]: [0m eta: 0:10:52  iter: 819  total_loss: 0.533  loss_cls: 0.156  loss_box_reg: 0.328  loss_rpn_cls: 0.007  loss_rpn_loc: 0.046  time: 0.5615  data_time: 0.0184  lr: 0.004096  max_mem: 3067M
[32m[04/15 14:36:05 d2.utils.events]: [0m eta: 0:10:41  iter: 839  total_loss: 0.498  loss_cls: 0.156  loss_box_reg: 0.302  loss_rpn_cls: 0.010  loss_rpn_loc: 0.037  time: 0.5617  data_time: 0.0206  lr: 0.004196  max_mem: 3067M
[32m[04/15 14:36:16 d2.utils.events]: [0m eta: 0:10:29  iter: 859  total_loss: 0.632  loss_cls: 0.188  loss_box_reg: 0.381  loss_rpn_cls: 0.013  loss_rpn_loc: 0.066  time: 0.5611  data_time: 0.0070  lr: 0.004296  max_mem: 3067M
[32m[04/15 14:36:28 d2.utils.events]: [0m eta: 0:10:19  iter: 879  total_loss: 0.612  loss_cls: 0.199  loss_box_reg: 0.354  loss_rpn_cls: 0.009  loss_rpn_loc: 0.048  time: 0.5622  data_time: 0.0087  lr: 0.004396  max_mem: 3067M
[32m[04/15 14:36:39 d2.utils.events]: [0m eta: 0:10:08  iter: 899  total_loss: 0.649  loss_cls: 0.204  loss_box_reg: 0.381  loss_rpn_cls: 0.011  loss_rpn_loc: 0.054  time: 0.5621  data_time: 0.0228  lr: 0.004496  max_mem: 3067M
[32m[04/15 14:36:50 d2.utils.events]: [0m eta: 0:09:57  iter: 919  total_loss: 0.563  loss_cls: 0.181  loss_box_reg: 0.346  loss_rpn_cls: 0.011  loss_rpn_loc: 0.044  time: 0.5618  data_time: 0.0066  lr: 0.004595  max_mem: 3067M
[32m[04/15 14:37:02 d2.utils.events]: [0m eta: 0:09:46  iter: 939  total_loss: 0.542  loss_cls: 0.176  loss_box_reg: 0.315  loss_rpn_cls: 0.012  loss_rpn_loc: 0.036  time: 0.5624  data_time: 0.0070  lr: 0.004695  max_mem: 3067M
[32m[04/15 14:37:13 d2.utils.events]: [0m eta: 0:09:34  iter: 959  total_loss: 0.516  loss_cls: 0.149  loss_box_reg: 0.287  loss_rpn_cls: 0.010  loss_rpn_loc: 0.052  time: 0.5616  data_time: 0.0157  lr: 0.004795  max_mem: 3067M
[32m[04/15 14:37:26 d2.utils.events]: [0m eta: 0:09:23  iter: 979  total_loss: 0.640  loss_cls: 0.202  loss_box_reg: 0.375  loss_rpn_cls: 0.010  loss_rpn_loc: 0.052  time: 0.5632  data_time: 0.1238  lr: 0.004895  max_mem: 3067M
[32m[04/15 14:37:42 d2.utils.events]: [0m eta: 0:09:13  iter: 999  total_loss: 0.637  loss_cls: 0.200  loss_box_reg: 0.344  loss_rpn_cls: 0.014  loss_rpn_loc: 0.065  time: 0.5684  data_time: 0.2794  lr: 0.004995  max_mem: 3067M
[32m[04/15 14:37:54 d2.utils.events]: [0m eta: 0:09:02  iter: 1019  total_loss: 0.614  loss_cls: 0.177  loss_box_reg: 0.358  loss_rpn_cls: 0.011  loss_rpn_loc: 0.050  time: 0.5681  data_time: 0.0265  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:38:06 d2.utils.events]: [0m eta: 0:08:51  iter: 1039  total_loss: 0.519  loss_cls: 0.174  loss_box_reg: 0.287  loss_rpn_cls: 0.010  loss_rpn_loc: 0.046  time: 0.5693  data_time: 0.0080  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:38:17 d2.utils.events]: [0m eta: 0:08:39  iter: 1059  total_loss: 0.519  loss_cls: 0.164  loss_box_reg: 0.277  loss_rpn_cls: 0.008  loss_rpn_loc: 0.057  time: 0.5684  data_time: 0.0330  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:38:31 d2.utils.events]: [0m eta: 0:08:29  iter: 1079  total_loss: 0.637  loss_cls: 0.209  loss_box_reg: 0.336  loss_rpn_cls: 0.016  loss_rpn_loc: 0.069  time: 0.5693  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:38:51 d2.utils.events]: [0m eta: 0:08:18  iter: 1099  total_loss: 0.561  loss_cls: 0.189  loss_box_reg: 0.309  loss_rpn_cls: 0.009  loss_rpn_loc: 0.046  time: 0.5688  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:39:04 d2.utils.events]: [0m eta: 0:08:07  iter: 1119  total_loss: 0.582  loss_cls: 0.195  loss_box_reg: 0.335  loss_rpn_cls: 0.020  loss_rpn_loc: 0.046  time: 0.5694  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:39:15 d2.utils.events]: [0m eta: 0:07:56  iter: 1139  total_loss: 0.654  loss_cls: 0.196  loss_box_reg: 0.353  loss_rpn_cls: 0.017  loss_rpn_loc: 0.049  time: 0.5691  data_time: 0.0182  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:39:26 d2.utils.events]: [0m eta: 0:07:44  iter: 1159  total_loss: 0.732  loss_cls: 0.248  loss_box_reg: 0.431  loss_rpn_cls: 0.015  loss_rpn_loc: 0.068  time: 0.5690  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:39:37 d2.utils.events]: [0m eta: 0:07:33  iter: 1179  total_loss: 0.638  loss_cls: 0.205  loss_box_reg: 0.351  loss_rpn_cls: 0.018  loss_rpn_loc: 0.057  time: 0.5688  data_time: 0.0296  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:39:50 d2.utils.events]: [0m eta: 0:07:22  iter: 1199  total_loss: 0.632  loss_cls: 0.171  loss_box_reg: 0.370  loss_rpn_cls: 0.015  loss_rpn_loc: 0.055  time: 0.5693  data_time: 0.0781  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:40:01 d2.utils.events]: [0m eta: 0:07:11  iter: 1219  total_loss: 0.487  loss_cls: 0.170  loss_box_reg: 0.266  loss_rpn_cls: 0.010  loss_rpn_loc: 0.047  time: 0.5692  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:40:12 d2.utils.events]: [0m eta: 0:07:00  iter: 1239  total_loss: 0.725  loss_cls: 0.206  loss_box_reg: 0.396  loss_rpn_cls: 0.016  loss_rpn_loc: 0.054  time: 0.5687  data_time: 0.0213  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:40:24 d2.utils.events]: [0m eta: 0:06:49  iter: 1259  total_loss: 0.612  loss_cls: 0.195  loss_box_reg: 0.330  loss_rpn_cls: 0.008  loss_rpn_loc: 0.043  time: 0.5690  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:40:35 d2.utils.events]: [0m eta: 0:06:38  iter: 1279  total_loss: 0.670  loss_cls: 0.204  loss_box_reg: 0.373  loss_rpn_cls: 0.010  loss_rpn_loc: 0.061  time: 0.5687  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:40:46 d2.utils.events]: [0m eta: 0:06:27  iter: 1299  total_loss: 0.657  loss_cls: 0.190  loss_box_reg: 0.383  loss_rpn_cls: 0.016  loss_rpn_loc: 0.062  time: 0.5684  data_time: 0.0069  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:40:58 d2.utils.events]: [0m eta: 0:06:16  iter: 1319  total_loss: 0.500  loss_cls: 0.177  loss_box_reg: 0.280  loss_rpn_cls: 0.012  loss_rpn_loc: 0.037  time: 0.5687  data_time: 0.0218  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:41:09 d2.utils.events]: [0m eta: 0:06:04  iter: 1339  total_loss: 0.532  loss_cls: 0.176  loss_box_reg: 0.314  loss_rpn_cls: 0.010  loss_rpn_loc: 0.053  time: 0.5683  data_time: 0.0205  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:41:20 d2.utils.events]: [0m eta: 0:05:54  iter: 1359  total_loss: 0.531  loss_cls: 0.167  loss_box_reg: 0.293  loss_rpn_cls: 0.015  loss_rpn_loc: 0.061  time: 0.5681  data_time: 0.0057  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:41:32 d2.utils.events]: [0m eta: 0:05:42  iter: 1379  total_loss: 0.642  loss_cls: 0.202  loss_box_reg: 0.376  loss_rpn_cls: 0.012  loss_rpn_loc: 0.051  time: 0.5681  data_time: 0.0149  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:41:44 d2.utils.events]: [0m eta: 0:05:32  iter: 1399  total_loss: 0.577  loss_cls: 0.192  loss_box_reg: 0.330  loss_rpn_cls: 0.009  loss_rpn_loc: 0.039  time: 0.5683  data_time: 0.0084  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:41:55 d2.utils.events]: [0m eta: 0:05:21  iter: 1419  total_loss: 0.594  loss_cls: 0.185  loss_box_reg: 0.334  loss_rpn_cls: 0.013  loss_rpn_loc: 0.049  time: 0.5681  data_time: 0.0088  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:42:05 d2.utils.events]: [0m eta: 0:05:09  iter: 1439  total_loss: 0.542  loss_cls: 0.172  loss_box_reg: 0.314  loss_rpn_cls: 0.014  loss_rpn_loc: 0.037  time: 0.5672  data_time: 0.0172  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:42:17 d2.utils.events]: [0m eta: 0:04:59  iter: 1459  total_loss: 0.708  loss_cls: 0.231  loss_box_reg: 0.385  loss_rpn_cls: 0.025  loss_rpn_loc: 0.068  time: 0.5680  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:42:28 d2.utils.events]: [0m eta: 0:04:47  iter: 1479  total_loss: 0.609  loss_cls: 0.191  loss_box_reg: 0.352  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5673  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:42:39 d2.utils.events]: [0m eta: 0:04:37  iter: 1499  total_loss: 0.496  loss_cls: 0.150  loss_box_reg: 0.287  loss_rpn_cls: 0.012  loss_rpn_loc: 0.046  time: 0.5672  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:42:51 d2.utils.events]: [0m eta: 0:04:26  iter: 1519  total_loss: 0.686  loss_cls: 0.203  loss_box_reg: 0.388  loss_rpn_cls: 0.014  loss_rpn_loc: 0.068  time: 0.5671  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:43:02 d2.utils.events]: [0m eta: 0:04:15  iter: 1539  total_loss: 0.537  loss_cls: 0.176  loss_box_reg: 0.302  loss_rpn_cls: 0.010  loss_rpn_loc: 0.046  time: 0.5670  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:43:14 d2.utils.events]: [0m eta: 0:04:04  iter: 1559  total_loss: 0.617  loss_cls: 0.158  loss_box_reg: 0.339  loss_rpn_cls: 0.023  loss_rpn_loc: 0.054  time: 0.5667  data_time: 0.0060  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:43:26 d2.utils.events]: [0m eta: 0:03:53  iter: 1579  total_loss: 0.701  loss_cls: 0.203  loss_box_reg: 0.363  loss_rpn_cls: 0.018  loss_rpn_loc: 0.064  time: 0.5667  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:43:37 d2.utils.events]: [0m eta: 0:03:42  iter: 1599  total_loss: 0.594  loss_cls: 0.183  loss_box_reg: 0.332  loss_rpn_cls: 0.016  loss_rpn_loc: 0.058  time: 0.5669  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:43:48 d2.utils.events]: [0m eta: 0:03:31  iter: 1619  total_loss: 0.630  loss_cls: 0.202  loss_box_reg: 0.355  loss_rpn_cls: 0.011  loss_rpn_loc: 0.050  time: 0.5666  data_time: 0.0085  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:43:59 d2.utils.events]: [0m eta: 0:03:19  iter: 1639  total_loss: 0.616  loss_cls: 0.209  loss_box_reg: 0.343  loss_rpn_cls: 0.011  loss_rpn_loc: 0.045  time: 0.5664  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:44:11 d2.utils.events]: [0m eta: 0:03:08  iter: 1659  total_loss: 0.587  loss_cls: 0.179  loss_box_reg: 0.325  loss_rpn_cls: 0.018  loss_rpn_loc: 0.046  time: 0.5667  data_time: 0.0106  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:44:22 d2.utils.events]: [0m eta: 0:02:57  iter: 1679  total_loss: 0.605  loss_cls: 0.189  loss_box_reg: 0.347  loss_rpn_cls: 0.010  loss_rpn_loc: 0.052  time: 0.5664  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:44:33 d2.utils.events]: [0m eta: 0:02:46  iter: 1699  total_loss: 0.594  loss_cls: 0.193  loss_box_reg: 0.361  loss_rpn_cls: 0.013  loss_rpn_loc: 0.045  time: 0.5662  data_time: 0.0152  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:44:45 d2.utils.events]: [0m eta: 0:02:35  iter: 1719  total_loss: 0.652  loss_cls: 0.217  loss_box_reg: 0.385  loss_rpn_cls: 0.012  loss_rpn_loc: 0.054  time: 0.5663  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:44:57 d2.utils.events]: [0m eta: 0:02:24  iter: 1739  total_loss: 0.622  loss_cls: 0.193  loss_box_reg: 0.356  loss_rpn_cls: 0.011  loss_rpn_loc: 0.056  time: 0.5662  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:45:08 d2.utils.events]: [0m eta: 0:02:13  iter: 1759  total_loss: 0.575  loss_cls: 0.180  loss_box_reg: 0.313  loss_rpn_cls: 0.015  loss_rpn_loc: 0.055  time: 0.5661  data_time: 0.0105  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:45:19 d2.utils.events]: [0m eta: 0:02:01  iter: 1779  total_loss: 0.608  loss_cls: 0.208  loss_box_reg: 0.344  loss_rpn_cls: 0.017  loss_rpn_loc: 0.058  time: 0.5655  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:45:30 d2.utils.events]: [0m eta: 0:01:50  iter: 1799  total_loss: 0.680  loss_cls: 0.215  loss_box_reg: 0.383  loss_rpn_cls: 0.020  loss_rpn_loc: 0.073  time: 0.5656  data_time: 0.0141  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:45:42 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.553  loss_cls: 0.183  loss_box_reg: 0.323  loss_rpn_cls: 0.013  loss_rpn_loc: 0.044  time: 0.5658  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:45:53 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.581  loss_cls: 0.188  loss_box_reg: 0.325  loss_rpn_cls: 0.012  loss_rpn_loc: 0.042  time: 0.5655  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:46:04 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.605  loss_cls: 0.182  loss_box_reg: 0.342  loss_rpn_cls: 0.010  loss_rpn_loc: 0.049  time: 0.5655  data_time: 0.0062  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:46:16 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.678  loss_cls: 0.216  loss_box_reg: 0.379  loss_rpn_cls: 0.011  loss_rpn_loc: 0.059  time: 0.5657  data_time: 0.0103  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:46:27 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.561  loss_cls: 0.191  loss_box_reg: 0.304  loss_rpn_cls: 0.012  loss_rpn_loc: 0.041  time: 0.5657  data_time: 0.0078  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:46:39 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.576  loss_cls: 0.199  loss_box_reg: 0.328  loss_rpn_cls: 0.010  loss_rpn_loc: 0.050  time: 0.5657  data_time: 0.0065  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:46:50 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.610  loss_cls: 0.186  loss_box_reg: 0.346  loss_rpn_cls: 0.010  loss_rpn_loc: 0.052  time: 0.5657  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:47:02 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.550  loss_cls: 0.163  loss_box_reg: 0.310  loss_rpn_cls: 0.011  loss_rpn_loc: 0.047  time: 0.5659  data_time: 0.0083  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:47:13 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.519  loss_cls: 0.164  loss_box_reg: 0.316  loss_rpn_cls: 0.012  loss_rpn_loc: 0.042  time: 0.5656  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 14:47:37 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 14:47:37 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 14:47:37 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 14:47:37 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.582  loss_cls: 0.180  loss_box_reg: 0.336  loss_rpn_cls: 0.012  loss_rpn_loc: 0.053  time: 0.5659  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 14:47:41 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:50 (0.5662 s / it)
[32m[04/15 14:47:41 d2.engine.hooks]: [0mTotal training time: 0:21:41 (0:02:50 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 14:47:49 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 14:47:49 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 14:47:49 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 14:47:54 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1260 s / img. ETA=0:03:06
[32m[04/15 14:48:00 d2.evaluation.evaluator]: [0mInference done 44/1257. 0.1236 s / img. ETA=0:03:06
[32m[04/15 14:48:05 d2.evaluation.evaluator]: [0mInference done 73/1257. 0.1280 s / img. ETA=0:03:11
[32m[04/15 14:48:10 d2.evaluation.evaluator]: [0mInference done 106/1257. 0.1315 s / img. ETA=0:03:02
[32m[04/15 14:48:15 d2.evaluation.evaluator]: [0mInference done 141/1257. 0.1283 s / img. ETA=0:02:54
[32m[04/15 14:48:20 d2.evaluation.evaluator]: [0mInference done 172/1257. 0.1267 s / img. ETA=0:02:50
[32m[04/15 14:48:25 d2.evaluation.evaluator]: [0mInference done 210/1257. 0.1254 s / img. ETA=0:02:39
[32m[04/15 14:48:30 d2.evaluation.evaluator]: [0mInference done 239/1257. 0.1278 s / img. ETA=0:02:38
[32m[04/15 14:48:35 d2.evaluation.evaluator]: [0mInference done 276/1257. 0.1279 s / img. ETA=0:02:30
[32m[04/15 14:48:40 d2.evaluation.evaluator]: [0mInference done 316/1257. 0.1269 s / img. ETA=0:02:20
[32m[04/15 14:48:45 d2.evaluation.evaluator]: [0mInference done 352/1257. 0.1279 s / img. ETA=0:02:14
[32m[04/15 14:48:50 d2.evaluation.evaluator]: [0mInference done 390/1257. 0.1272 s / img. ETA=0:02:07
[32m[04/15 14:48:55 d2.evaluation.evaluator]: [0mInference done 425/1257. 0.1267 s / img. ETA=0:02:02
[32m[04/15 14:49:01 d2.evaluation.evaluator]: [0mInference done 459/1257. 0.1260 s / img. ETA=0:01:57
[32m[04/15 14:49:06 d2.evaluation.evaluator]: [0mInference done 493/1257. 0.1256 s / img. ETA=0:01:52
[32m[04/15 14:49:11 d2.evaluation.evaluator]: [0mInference done 522/1257. 0.1270 s / img. ETA=0:01:49
[32m[04/15 14:49:16 d2.evaluation.evaluator]: [0mInference done 556/1257. 0.1270 s / img. ETA=0:01:44
[32m[04/15 14:49:21 d2.evaluation.evaluator]: [0mInference done 585/1257. 0.1267 s / img. ETA=0:01:41
[32m[04/15 14:49:26 d2.evaluation.evaluator]: [0mInference done 616/1257. 0.1270 s / img. ETA=0:01:36
[32m[04/15 14:49:31 d2.evaluation.evaluator]: [0mInference done 647/1257. 0.1266 s / img. ETA=0:01:32
[32m[04/15 14:49:36 d2.evaluation.evaluator]: [0mInference done 678/1257. 0.1264 s / img. ETA=0:01:28
[32m[04/15 14:49:41 d2.evaluation.evaluator]: [0mInference done 714/1257. 0.1262 s / img. ETA=0:01:22
[32m[04/15 14:49:46 d2.evaluation.evaluator]: [0mInference done 749/1257. 0.1262 s / img. ETA=0:01:17
[32m[04/15 14:49:52 d2.evaluation.evaluator]: [0mInference done 784/1257. 0.1269 s / img. ETA=0:01:11
[32m[04/15 14:49:57 d2.evaluation.evaluator]: [0mInference done 821/1257. 0.1271 s / img. ETA=0:01:05
[32m[04/15 14:50:02 d2.evaluation.evaluator]: [0mInference done 861/1257. 0.1268 s / img. ETA=0:00:59
[32m[04/15 14:50:07 d2.evaluation.evaluator]: [0mInference done 898/1257. 0.1270 s / img. ETA=0:00:53
[32m[04/15 14:50:12 d2.evaluation.evaluator]: [0mInference done 936/1257. 0.1270 s / img. ETA=0:00:47
[32m[04/15 14:50:17 d2.evaluation.evaluator]: [0mInference done 972/1257. 0.1268 s / img. ETA=0:00:42
[32m[04/15 14:50:22 d2.evaluation.evaluator]: [0mInference done 1007/1257. 0.1266 s / img. ETA=0:00:36
[32m[04/15 14:50:27 d2.evaluation.evaluator]: [0mInference done 1045/1257. 0.1268 s / img. ETA=0:00:31
[32m[04/15 14:50:32 d2.evaluation.evaluator]: [0mInference done 1077/1257. 0.1271 s / img. ETA=0:00:26
[32m[04/15 14:50:37 d2.evaluation.evaluator]: [0mInference done 1107/1257. 0.1269 s / img. ETA=0:00:22
[32m[04/15 14:50:42 d2.evaluation.evaluator]: [0mInference done 1133/1257. 0.1268 s / img. ETA=0:00:18
[32m[04/15 14:50:48 d2.evaluation.evaluator]: [0mInference done 1161/1257. 0.1266 s / img. ETA=0:00:14
[32m[04/15 14:50:53 d2.evaluation.evaluator]: [0mInference done 1192/1257. 0.1264 s / img. ETA=0:00:09
[32m[04/15 14:50:58 d2.evaluation.evaluator]: [0mInference done 1221/1257. 0.1263 s / img. ETA=0:00:05
[32m[04/15 14:51:03 d2.evaluation.evaluator]: [0mInference done 1255/1257. 0.1264 s / img. ETA=0:00:00
[32m[04/15 14:51:03 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:09.407380 (0.151284 s / img per device, on 1 devices)
[32m[04/15 14:51:03 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:38 (0.126433 s / img per device, on 1 devices)
[32m[04/15 14:51:04 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 14:51:04 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 14:51:04 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.05s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.05s).
Accumulating evaluation results...
DONE (t=1.32s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.419
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.175
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.132
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.432
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.102
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.193
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.466
[32m[04/15 14:51:11 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 20.823 | 41.888 | 17.479 | 13.222 | 25.712 | 43.163 |
[32m[04/15 14:51:11 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP     | category       | AP     |
|:--------------|:-------|:--------------|:-------|:---------------|:-------|
| person        | 28.155 | bicycle       | 10.630 | car            | 44.508 |
| motorcycle    | nan    | airplane      | nan    | bus            | nan    |
| train         | nan    | truck         | nan    | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan    | stop sign      | nan    |
| parking meter | nan    | bench         | nan    | bird           | nan    |
| cat           | nan    | dog           | 0.000  | horse          | nan    |
| sheep         | nan    | cow           | nan    | elephant       | nan    |
| bear          | nan    | zebra         | nan    | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan    | handbag        | nan    |
| tie           | nan    | suitcase      | nan    | frisbee        | nan    |
| skis          | nan    | snowboard     | nan    | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan    | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan    | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan    | cup            | nan    |
| fork          | nan    | knife         | nan    | spoon          | nan    |
| bowl          | nan    | banana        | nan    | apple          | nan    |
| sandwich      | nan    | orange        | nan    | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan    | pizza          | nan    |
| donut         | nan    | cake          | nan    | chair          | nan    |
| couch         | nan    | potted plant  | nan    | bed            | nan    |
| dining table  | nan    | toilet        | nan    | tv             | nan    |
| laptop        | nan    | mouse         | nan    | remote         | nan    |
| keyboard      | nan    | cell phone    | nan    | microwave      | nan    |
| oven          | nan    | bike          | nan    | hydrant        | nan    |
| motor         | nan    | rider         | nan    | light          | nan    |
| sign          | nan    | motor vehicle | nan    | human face     | nan    |
| hair drier    | nan    | license plate | nan    |                |        |
============== The  29  *  2000  iterations ============
4 channel input
[32m[04/15 14:51:13 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
  )
)
[32m[04/15 14:51:14 d2.data.datasets.coco]: [0mLoading ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json takes 1.25 seconds.
[5m[31mWARNING[0m [32m[04/15 14:51:14 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 14:51:14 d2.data.datasets.coco]: [0mLoaded 8355 images in COCO format from ../../../Datasets/FLIR/train/thermal_annotations_4_channel.json
[32m[04/15 14:51:15 d2.data.build]: [0mRemoved 889 images with no usable annotations. 7466 images left.
[32m[04/15 14:51:15 d2.data.detection_utils]: [0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[04/15 14:51:15 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[04/15 14:51:18 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[04/15 14:51:31 d2.utils.events]: [0m eta: 0:18:53  iter: 19  total_loss: 0.625  loss_cls: 0.199  loss_box_reg: 0.360  loss_rpn_cls: 0.012  loss_rpn_loc: 0.044  time: 0.6123  data_time: 0.0521  lr: 0.000100  max_mem: 3067M
[32m[04/15 14:51:42 d2.utils.events]: [0m eta: 0:18:10  iter: 39  total_loss: 0.677  loss_cls: 0.198  loss_box_reg: 0.329  loss_rpn_cls: 0.015  loss_rpn_loc: 0.066  time: 0.5737  data_time: 0.0061  lr: 0.000200  max_mem: 3067M
[32m[04/15 14:51:54 d2.utils.events]: [0m eta: 0:18:03  iter: 59  total_loss: 0.530  loss_cls: 0.172  loss_box_reg: 0.286  loss_rpn_cls: 0.013  loss_rpn_loc: 0.053  time: 0.5814  data_time: 0.0073  lr: 0.000300  max_mem: 3067M
[32m[04/15 14:52:05 d2.utils.events]: [0m eta: 0:17:52  iter: 79  total_loss: 0.524  loss_cls: 0.165  loss_box_reg: 0.296  loss_rpn_cls: 0.007  loss_rpn_loc: 0.044  time: 0.5776  data_time: 0.0071  lr: 0.000400  max_mem: 3067M
[32m[04/15 14:52:16 d2.utils.events]: [0m eta: 0:17:33  iter: 99  total_loss: 0.512  loss_cls: 0.168  loss_box_reg: 0.290  loss_rpn_cls: 0.009  loss_rpn_loc: 0.050  time: 0.5674  data_time: 0.0073  lr: 0.000500  max_mem: 3067M
[32m[04/15 14:52:27 d2.utils.events]: [0m eta: 0:17:22  iter: 119  total_loss: 0.574  loss_cls: 0.183  loss_box_reg: 0.330  loss_rpn_cls: 0.010  loss_rpn_loc: 0.051  time: 0.5658  data_time: 0.0083  lr: 0.000599  max_mem: 3067M
[32m[04/15 14:52:39 d2.utils.events]: [0m eta: 0:17:11  iter: 139  total_loss: 0.595  loss_cls: 0.185  loss_box_reg: 0.352  loss_rpn_cls: 0.009  loss_rpn_loc: 0.042  time: 0.5672  data_time: 0.0061  lr: 0.000699  max_mem: 3067M
[32m[04/15 14:52:50 d2.utils.events]: [0m eta: 0:16:56  iter: 159  total_loss: 0.523  loss_cls: 0.164  loss_box_reg: 0.282  loss_rpn_cls: 0.008  loss_rpn_loc: 0.038  time: 0.5639  data_time: 0.0071  lr: 0.000799  max_mem: 3067M
[32m[04/15 14:53:00 d2.utils.events]: [0m eta: 0:16:27  iter: 179  total_loss: 0.515  loss_cls: 0.164  loss_box_reg: 0.302  loss_rpn_cls: 0.010  loss_rpn_loc: 0.040  time: 0.5577  data_time: 0.0068  lr: 0.000899  max_mem: 3067M
[32m[04/15 14:53:12 d2.utils.events]: [0m eta: 0:16:25  iter: 199  total_loss: 0.537  loss_cls: 0.159  loss_box_reg: 0.332  loss_rpn_cls: 0.014  loss_rpn_loc: 0.047  time: 0.5610  data_time: 0.0070  lr: 0.000999  max_mem: 3067M
[32m[04/15 14:53:23 d2.utils.events]: [0m eta: 0:16:11  iter: 219  total_loss: 0.575  loss_cls: 0.170  loss_box_reg: 0.309  loss_rpn_cls: 0.009  loss_rpn_loc: 0.046  time: 0.5601  data_time: 0.0106  lr: 0.001099  max_mem: 3067M
[32m[04/15 14:53:38 d2.utils.events]: [0m eta: 0:16:02  iter: 239  total_loss: 0.471  loss_cls: 0.152  loss_box_reg: 0.242  loss_rpn_cls: 0.012  loss_rpn_loc: 0.042  time: 0.5723  data_time: 0.1963  lr: 0.001199  max_mem: 3067M
[32m[04/15 14:53:49 d2.utils.events]: [0m eta: 0:15:51  iter: 259  total_loss: 0.606  loss_cls: 0.189  loss_box_reg: 0.320  loss_rpn_cls: 0.013  loss_rpn_loc: 0.052  time: 0.5711  data_time: 0.0078  lr: 0.001299  max_mem: 3067M
[32m[04/15 14:54:01 d2.utils.events]: [0m eta: 0:15:47  iter: 279  total_loss: 0.577  loss_cls: 0.195  loss_box_reg: 0.333  loss_rpn_cls: 0.011  loss_rpn_loc: 0.055  time: 0.5735  data_time: 0.0084  lr: 0.001399  max_mem: 3067M
[32m[04/15 14:54:14 d2.utils.events]: [0m eta: 0:15:39  iter: 299  total_loss: 0.594  loss_cls: 0.189  loss_box_reg: 0.348  loss_rpn_cls: 0.014  loss_rpn_loc: 0.038  time: 0.5755  data_time: 0.0076  lr: 0.001499  max_mem: 3067M
[32m[04/15 14:54:24 d2.utils.events]: [0m eta: 0:15:24  iter: 319  total_loss: 0.645  loss_cls: 0.196  loss_box_reg: 0.362  loss_rpn_cls: 0.010  loss_rpn_loc: 0.043  time: 0.5726  data_time: 0.0093  lr: 0.001598  max_mem: 3067M
[32m[04/15 14:54:35 d2.utils.events]: [0m eta: 0:15:12  iter: 339  total_loss: 0.528  loss_cls: 0.154  loss_box_reg: 0.284  loss_rpn_cls: 0.008  loss_rpn_loc: 0.051  time: 0.5705  data_time: 0.0085  lr: 0.001698  max_mem: 3067M
[32m[04/15 14:54:47 d2.utils.events]: [0m eta: 0:15:01  iter: 359  total_loss: 0.466  loss_cls: 0.144  loss_box_reg: 0.267  loss_rpn_cls: 0.004  loss_rpn_loc: 0.040  time: 0.5691  data_time: 0.0076  lr: 0.001798  max_mem: 3067M
[32m[04/15 14:54:59 d2.utils.events]: [0m eta: 0:14:48  iter: 379  total_loss: 0.451  loss_cls: 0.133  loss_box_reg: 0.280  loss_rpn_cls: 0.008  loss_rpn_loc: 0.043  time: 0.5687  data_time: 0.0072  lr: 0.001898  max_mem: 3067M
[32m[04/15 14:55:10 d2.utils.events]: [0m eta: 0:14:37  iter: 399  total_loss: 0.625  loss_cls: 0.188  loss_box_reg: 0.327  loss_rpn_cls: 0.009  loss_rpn_loc: 0.054  time: 0.5672  data_time: 0.0072  lr: 0.001998  max_mem: 3067M
[32m[04/15 14:55:22 d2.utils.events]: [0m eta: 0:14:27  iter: 419  total_loss: 0.470  loss_cls: 0.154  loss_box_reg: 0.269  loss_rpn_cls: 0.011  loss_rpn_loc: 0.037  time: 0.5691  data_time: 0.0102  lr: 0.002098  max_mem: 3067M
[32m[04/15 14:55:33 d2.utils.events]: [0m eta: 0:14:16  iter: 439  total_loss: 0.527  loss_cls: 0.142  loss_box_reg: 0.277  loss_rpn_cls: 0.011  loss_rpn_loc: 0.055  time: 0.5692  data_time: 0.0083  lr: 0.002198  max_mem: 3067M
[32m[04/15 14:55:44 d2.utils.events]: [0m eta: 0:14:04  iter: 459  total_loss: 0.551  loss_cls: 0.170  loss_box_reg: 0.320  loss_rpn_cls: 0.007  loss_rpn_loc: 0.043  time: 0.5678  data_time: 0.0070  lr: 0.002298  max_mem: 3067M
[32m[04/15 14:55:56 d2.utils.events]: [0m eta: 0:13:55  iter: 479  total_loss: 0.506  loss_cls: 0.154  loss_box_reg: 0.304  loss_rpn_cls: 0.006  loss_rpn_loc: 0.040  time: 0.5689  data_time: 0.0061  lr: 0.002398  max_mem: 3067M
[32m[04/15 14:56:07 d2.utils.events]: [0m eta: 0:13:42  iter: 499  total_loss: 0.568  loss_cls: 0.174  loss_box_reg: 0.320  loss_rpn_cls: 0.008  loss_rpn_loc: 0.055  time: 0.5678  data_time: 0.0063  lr: 0.002498  max_mem: 3067M
[32m[04/15 14:56:18 d2.utils.events]: [0m eta: 0:13:30  iter: 519  total_loss: 0.549  loss_cls: 0.168  loss_box_reg: 0.288  loss_rpn_cls: 0.012  loss_rpn_loc: 0.063  time: 0.5667  data_time: 0.0064  lr: 0.002597  max_mem: 3067M
[32m[04/15 14:56:29 d2.utils.events]: [0m eta: 0:13:19  iter: 539  total_loss: 0.485  loss_cls: 0.166  loss_box_reg: 0.286  loss_rpn_cls: 0.008  loss_rpn_loc: 0.046  time: 0.5656  data_time: 0.0060  lr: 0.002697  max_mem: 3067M
[32m[04/15 14:56:41 d2.utils.events]: [0m eta: 0:13:09  iter: 559  total_loss: 0.547  loss_cls: 0.168  loss_box_reg: 0.334  loss_rpn_cls: 0.010  loss_rpn_loc: 0.048  time: 0.5665  data_time: 0.0085  lr: 0.002797  max_mem: 3067M
[32m[04/15 14:56:52 d2.utils.events]: [0m eta: 0:12:58  iter: 579  total_loss: 0.687  loss_cls: 0.194  loss_box_reg: 0.390  loss_rpn_cls: 0.009  loss_rpn_loc: 0.056  time: 0.5661  data_time: 0.0070  lr: 0.002897  max_mem: 3067M
[32m[04/15 14:57:03 d2.utils.events]: [0m eta: 0:12:46  iter: 599  total_loss: 0.547  loss_cls: 0.179  loss_box_reg: 0.318  loss_rpn_cls: 0.014  loss_rpn_loc: 0.058  time: 0.5649  data_time: 0.0060  lr: 0.002997  max_mem: 3067M
[32m[04/15 14:57:15 d2.utils.events]: [0m eta: 0:12:37  iter: 619  total_loss: 0.586  loss_cls: 0.167  loss_box_reg: 0.318  loss_rpn_cls: 0.009  loss_rpn_loc: 0.056  time: 0.5664  data_time: 0.0106  lr: 0.003097  max_mem: 3067M
[32m[04/15 14:57:27 d2.utils.events]: [0m eta: 0:12:27  iter: 639  total_loss: 0.537  loss_cls: 0.181  loss_box_reg: 0.311  loss_rpn_cls: 0.011  loss_rpn_loc: 0.050  time: 0.5662  data_time: 0.0070  lr: 0.003197  max_mem: 3067M
[32m[04/15 14:57:38 d2.utils.events]: [0m eta: 0:12:16  iter: 659  total_loss: 0.585  loss_cls: 0.187  loss_box_reg: 0.319  loss_rpn_cls: 0.011  loss_rpn_loc: 0.046  time: 0.5654  data_time: 0.0085  lr: 0.003297  max_mem: 3067M
[32m[04/15 14:57:50 d2.utils.events]: [0m eta: 0:12:08  iter: 679  total_loss: 0.628  loss_cls: 0.200  loss_box_reg: 0.347  loss_rpn_cls: 0.015  loss_rpn_loc: 0.052  time: 0.5666  data_time: 0.0068  lr: 0.003397  max_mem: 3067M
[32m[04/15 14:58:02 d2.utils.events]: [0m eta: 0:11:59  iter: 699  total_loss: 0.604  loss_cls: 0.181  loss_box_reg: 0.331  loss_rpn_cls: 0.007  loss_rpn_loc: 0.041  time: 0.5674  data_time: 0.0071  lr: 0.003497  max_mem: 3067M
[32m[04/15 14:58:13 d2.utils.events]: [0m eta: 0:11:46  iter: 719  total_loss: 0.625  loss_cls: 0.193  loss_box_reg: 0.357  loss_rpn_cls: 0.008  loss_rpn_loc: 0.050  time: 0.5664  data_time: 0.0067  lr: 0.003596  max_mem: 3067M
[32m[04/15 14:58:23 d2.utils.events]: [0m eta: 0:11:32  iter: 739  total_loss: 0.593  loss_cls: 0.169  loss_box_reg: 0.328  loss_rpn_cls: 0.011  loss_rpn_loc: 0.066  time: 0.5652  data_time: 0.0056  lr: 0.003696  max_mem: 3067M
[32m[04/15 14:58:35 d2.utils.events]: [0m eta: 0:11:21  iter: 759  total_loss: 0.562  loss_cls: 0.176  loss_box_reg: 0.312  loss_rpn_cls: 0.012  loss_rpn_loc: 0.049  time: 0.5652  data_time: 0.0064  lr: 0.003796  max_mem: 3067M
[32m[04/15 14:58:46 d2.utils.events]: [0m eta: 0:11:11  iter: 779  total_loss: 0.581  loss_cls: 0.171  loss_box_reg: 0.358  loss_rpn_cls: 0.008  loss_rpn_loc: 0.039  time: 0.5657  data_time: 0.0063  lr: 0.003896  max_mem: 3067M
[32m[04/15 14:58:57 d2.utils.events]: [0m eta: 0:10:58  iter: 799  total_loss: 0.547  loss_cls: 0.170  loss_box_reg: 0.298  loss_rpn_cls: 0.017  loss_rpn_loc: 0.043  time: 0.5646  data_time: 0.0098  lr: 0.003996  max_mem: 3067M
[32m[04/15 14:59:09 d2.utils.events]: [0m eta: 0:10:48  iter: 819  total_loss: 0.619  loss_cls: 0.202  loss_box_reg: 0.329  loss_rpn_cls: 0.009  loss_rpn_loc: 0.044  time: 0.5651  data_time: 0.0074  lr: 0.004096  max_mem: 3067M
[32m[04/15 14:59:20 d2.utils.events]: [0m eta: 0:10:38  iter: 839  total_loss: 0.602  loss_cls: 0.174  loss_box_reg: 0.367  loss_rpn_cls: 0.011  loss_rpn_loc: 0.058  time: 0.5653  data_time: 0.0073  lr: 0.004196  max_mem: 3067M
[32m[04/15 14:59:31 d2.utils.events]: [0m eta: 0:10:26  iter: 859  total_loss: 0.518  loss_cls: 0.177  loss_box_reg: 0.285  loss_rpn_cls: 0.012  loss_rpn_loc: 0.048  time: 0.5645  data_time: 0.0058  lr: 0.004296  max_mem: 3067M
[32m[04/15 14:59:42 d2.utils.events]: [0m eta: 0:10:15  iter: 879  total_loss: 0.451  loss_cls: 0.144  loss_box_reg: 0.266  loss_rpn_cls: 0.010  loss_rpn_loc: 0.032  time: 0.5643  data_time: 0.0066  lr: 0.004396  max_mem: 3067M
[32m[04/15 14:59:55 d2.utils.events]: [0m eta: 0:10:05  iter: 899  total_loss: 0.585  loss_cls: 0.177  loss_box_reg: 0.340  loss_rpn_cls: 0.012  loss_rpn_loc: 0.052  time: 0.5654  data_time: 0.0069  lr: 0.004496  max_mem: 3067M
[32m[04/15 15:00:06 d2.utils.events]: [0m eta: 0:09:54  iter: 919  total_loss: 0.602  loss_cls: 0.182  loss_box_reg: 0.348  loss_rpn_cls: 0.017  loss_rpn_loc: 0.046  time: 0.5646  data_time: 0.0093  lr: 0.004595  max_mem: 3067M
[32m[04/15 15:00:17 d2.utils.events]: [0m eta: 0:09:43  iter: 939  total_loss: 0.562  loss_cls: 0.180  loss_box_reg: 0.312  loss_rpn_cls: 0.009  loss_rpn_loc: 0.030  time: 0.5643  data_time: 0.0070  lr: 0.004695  max_mem: 3067M
[32m[04/15 15:00:30 d2.utils.events]: [0m eta: 0:09:33  iter: 959  total_loss: 0.652  loss_cls: 0.206  loss_box_reg: 0.359  loss_rpn_cls: 0.010  loss_rpn_loc: 0.056  time: 0.5653  data_time: 0.0072  lr: 0.004795  max_mem: 3067M
[32m[04/15 15:00:41 d2.utils.events]: [0m eta: 0:09:23  iter: 979  total_loss: 0.595  loss_cls: 0.181  loss_box_reg: 0.337  loss_rpn_cls: 0.011  loss_rpn_loc: 0.050  time: 0.5651  data_time: 0.0070  lr: 0.004895  max_mem: 3067M
[32m[04/15 15:00:51 d2.utils.events]: [0m eta: 0:09:11  iter: 999  total_loss: 0.563  loss_cls: 0.184  loss_box_reg: 0.316  loss_rpn_cls: 0.016  loss_rpn_loc: 0.065  time: 0.5643  data_time: 0.0069  lr: 0.004995  max_mem: 3067M
[32m[04/15 15:01:03 d2.utils.events]: [0m eta: 0:08:59  iter: 1019  total_loss: 0.526  loss_cls: 0.174  loss_box_reg: 0.297  loss_rpn_cls: 0.010  loss_rpn_loc: 0.056  time: 0.5644  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:01:14 d2.utils.events]: [0m eta: 0:08:48  iter: 1039  total_loss: 0.502  loss_cls: 0.156  loss_box_reg: 0.281  loss_rpn_cls: 0.013  loss_rpn_loc: 0.043  time: 0.5644  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:01:26 d2.utils.events]: [0m eta: 0:08:37  iter: 1059  total_loss: 0.547  loss_cls: 0.179  loss_box_reg: 0.284  loss_rpn_cls: 0.012  loss_rpn_loc: 0.047  time: 0.5639  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:01:37 d2.utils.events]: [0m eta: 0:08:25  iter: 1079  total_loss: 0.608  loss_cls: 0.187  loss_box_reg: 0.343  loss_rpn_cls: 0.012  loss_rpn_loc: 0.068  time: 0.5631  data_time: 0.0067  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:01:49 d2.utils.events]: [0m eta: 0:08:15  iter: 1099  total_loss: 0.630  loss_cls: 0.184  loss_box_reg: 0.362  loss_rpn_cls: 0.013  loss_rpn_loc: 0.053  time: 0.5642  data_time: 0.0090  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:02:00 d2.utils.events]: [0m eta: 0:08:04  iter: 1119  total_loss: 0.571  loss_cls: 0.186  loss_box_reg: 0.333  loss_rpn_cls: 0.009  loss_rpn_loc: 0.057  time: 0.5640  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:02:11 d2.utils.events]: [0m eta: 0:07:52  iter: 1139  total_loss: 0.557  loss_cls: 0.163  loss_box_reg: 0.303  loss_rpn_cls: 0.010  loss_rpn_loc: 0.054  time: 0.5633  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:02:24 d2.utils.events]: [0m eta: 0:07:43  iter: 1159  total_loss: 0.574  loss_cls: 0.182  loss_box_reg: 0.339  loss_rpn_cls: 0.010  loss_rpn_loc: 0.056  time: 0.5641  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:02:35 d2.utils.events]: [0m eta: 0:07:32  iter: 1179  total_loss: 0.580  loss_cls: 0.185  loss_box_reg: 0.329  loss_rpn_cls: 0.013  loss_rpn_loc: 0.050  time: 0.5640  data_time: 0.0093  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:02:45 d2.utils.events]: [0m eta: 0:07:21  iter: 1199  total_loss: 0.682  loss_cls: 0.206  loss_box_reg: 0.382  loss_rpn_cls: 0.021  loss_rpn_loc: 0.069  time: 0.5632  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:02:57 d2.utils.events]: [0m eta: 0:07:10  iter: 1219  total_loss: 0.580  loss_cls: 0.177  loss_box_reg: 0.339  loss_rpn_cls: 0.014  loss_rpn_loc: 0.060  time: 0.5633  data_time: 0.0233  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:03:09 d2.utils.events]: [0m eta: 0:06:59  iter: 1239  total_loss: 0.613  loss_cls: 0.198  loss_box_reg: 0.334  loss_rpn_cls: 0.012  loss_rpn_loc: 0.052  time: 0.5637  data_time: 0.0089  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:03:19 d2.utils.events]: [0m eta: 0:06:48  iter: 1259  total_loss: 0.517  loss_cls: 0.155  loss_box_reg: 0.301  loss_rpn_cls: 0.010  loss_rpn_loc: 0.042  time: 0.5631  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:03:30 d2.utils.events]: [0m eta: 0:06:35  iter: 1279  total_loss: 0.532  loss_cls: 0.159  loss_box_reg: 0.297  loss_rpn_cls: 0.007  loss_rpn_loc: 0.045  time: 0.5623  data_time: 0.0058  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:03:43 d2.utils.events]: [0m eta: 0:06:24  iter: 1299  total_loss: 0.689  loss_cls: 0.202  loss_box_reg: 0.404  loss_rpn_cls: 0.014  loss_rpn_loc: 0.067  time: 0.5631  data_time: 0.0109  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:03:53 d2.utils.events]: [0m eta: 0:06:13  iter: 1319  total_loss: 0.563  loss_cls: 0.171  loss_box_reg: 0.335  loss_rpn_cls: 0.018  loss_rpn_loc: 0.057  time: 0.5625  data_time: 0.0103  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:04:04 d2.utils.events]: [0m eta: 0:06:02  iter: 1339  total_loss: 0.600  loss_cls: 0.193  loss_box_reg: 0.348  loss_rpn_cls: 0.012  loss_rpn_loc: 0.066  time: 0.5622  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:04:16 d2.utils.events]: [0m eta: 0:05:52  iter: 1359  total_loss: 0.584  loss_cls: 0.187  loss_box_reg: 0.340  loss_rpn_cls: 0.010  loss_rpn_loc: 0.052  time: 0.5625  data_time: 0.0063  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:04:28 d2.utils.events]: [0m eta: 0:05:41  iter: 1379  total_loss: 0.549  loss_cls: 0.164  loss_box_reg: 0.315  loss_rpn_cls: 0.007  loss_rpn_loc: 0.045  time: 0.5628  data_time: 0.0079  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:04:39 d2.utils.events]: [0m eta: 0:05:29  iter: 1399  total_loss: 0.570  loss_cls: 0.170  loss_box_reg: 0.319  loss_rpn_cls: 0.010  loss_rpn_loc: 0.059  time: 0.5623  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:04:50 d2.utils.events]: [0m eta: 0:05:18  iter: 1419  total_loss: 0.698  loss_cls: 0.210  loss_box_reg: 0.392  loss_rpn_cls: 0.018  loss_rpn_loc: 0.073  time: 0.5624  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:05:02 d2.utils.events]: [0m eta: 0:05:08  iter: 1439  total_loss: 0.631  loss_cls: 0.189  loss_box_reg: 0.380  loss_rpn_cls: 0.019  loss_rpn_loc: 0.049  time: 0.5629  data_time: 0.0071  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:05:12 d2.utils.events]: [0m eta: 0:04:56  iter: 1459  total_loss: 0.552  loss_cls: 0.166  loss_box_reg: 0.298  loss_rpn_cls: 0.011  loss_rpn_loc: 0.062  time: 0.5622  data_time: 0.0064  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:05:23 d2.utils.events]: [0m eta: 0:04:45  iter: 1479  total_loss: 0.629  loss_cls: 0.175  loss_box_reg: 0.366  loss_rpn_cls: 0.012  loss_rpn_loc: 0.052  time: 0.5618  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:05:36 d2.utils.events]: [0m eta: 0:04:35  iter: 1499  total_loss: 0.503  loss_cls: 0.146  loss_box_reg: 0.261  loss_rpn_cls: 0.015  loss_rpn_loc: 0.047  time: 0.5626  data_time: 0.0076  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:05:47 d2.utils.events]: [0m eta: 0:04:24  iter: 1519  total_loss: 0.639  loss_cls: 0.204  loss_box_reg: 0.359  loss_rpn_cls: 0.017  loss_rpn_loc: 0.053  time: 0.5622  data_time: 0.0150  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:05:58 d2.utils.events]: [0m eta: 0:04:12  iter: 1539  total_loss: 0.564  loss_cls: 0.186  loss_box_reg: 0.309  loss_rpn_cls: 0.014  loss_rpn_loc: 0.063  time: 0.5616  data_time: 0.0058  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:06:10 d2.utils.events]: [0m eta: 0:04:01  iter: 1559  total_loss: 0.582  loss_cls: 0.171  loss_box_reg: 0.351  loss_rpn_cls: 0.009  loss_rpn_loc: 0.045  time: 0.5623  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:06:21 d2.utils.events]: [0m eta: 0:03:50  iter: 1579  total_loss: 0.560  loss_cls: 0.187  loss_box_reg: 0.332  loss_rpn_cls: 0.012  loss_rpn_loc: 0.047  time: 0.5623  data_time: 0.0092  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:06:33 d2.utils.events]: [0m eta: 0:03:40  iter: 1599  total_loss: 0.612  loss_cls: 0.188  loss_box_reg: 0.299  loss_rpn_cls: 0.014  loss_rpn_loc: 0.049  time: 0.5618  data_time: 0.0066  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:06:44 d2.utils.events]: [0m eta: 0:03:28  iter: 1619  total_loss: 0.626  loss_cls: 0.183  loss_box_reg: 0.339  loss_rpn_cls: 0.014  loss_rpn_loc: 0.058  time: 0.5618  data_time: 0.0074  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:06:55 d2.utils.events]: [0m eta: 0:03:17  iter: 1639  total_loss: 0.586  loss_cls: 0.191  loss_box_reg: 0.307  loss_rpn_cls: 0.016  loss_rpn_loc: 0.048  time: 0.5617  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:07:06 d2.utils.events]: [0m eta: 0:03:06  iter: 1659  total_loss: 0.621  loss_cls: 0.185  loss_box_reg: 0.344  loss_rpn_cls: 0.018  loss_rpn_loc: 0.051  time: 0.5614  data_time: 0.0062  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:07:18 d2.utils.events]: [0m eta: 0:02:55  iter: 1679  total_loss: 0.631  loss_cls: 0.196  loss_box_reg: 0.348  loss_rpn_cls: 0.016  loss_rpn_loc: 0.043  time: 0.5614  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:07:30 d2.utils.events]: [0m eta: 0:02:44  iter: 1699  total_loss: 0.606  loss_cls: 0.181  loss_box_reg: 0.342  loss_rpn_cls: 0.012  loss_rpn_loc: 0.063  time: 0.5619  data_time: 0.0081  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:07:41 d2.utils.events]: [0m eta: 0:02:33  iter: 1719  total_loss: 0.575  loss_cls: 0.190  loss_box_reg: 0.319  loss_rpn_cls: 0.009  loss_rpn_loc: 0.051  time: 0.5616  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:07:52 d2.utils.events]: [0m eta: 0:02:22  iter: 1739  total_loss: 0.556  loss_cls: 0.201  loss_box_reg: 0.315  loss_rpn_cls: 0.011  loss_rpn_loc: 0.052  time: 0.5614  data_time: 0.0082  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:08:04 d2.utils.events]: [0m eta: 0:02:11  iter: 1759  total_loss: 0.609  loss_cls: 0.176  loss_box_reg: 0.334  loss_rpn_cls: 0.010  loss_rpn_loc: 0.046  time: 0.5622  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:08:15 d2.utils.events]: [0m eta: 0:02:00  iter: 1779  total_loss: 0.590  loss_cls: 0.178  loss_box_reg: 0.342  loss_rpn_cls: 0.014  loss_rpn_loc: 0.049  time: 0.5620  data_time: 0.0086  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:08:26 d2.utils.events]: [0m eta: 0:01:50  iter: 1799  total_loss: 0.581  loss_cls: 0.177  loss_box_reg: 0.328  loss_rpn_cls: 0.015  loss_rpn_loc: 0.043  time: 0.5616  data_time: 0.0059  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:08:38 d2.utils.events]: [0m eta: 0:01:39  iter: 1819  total_loss: 0.699  loss_cls: 0.226  loss_box_reg: 0.396  loss_rpn_cls: 0.024  loss_rpn_loc: 0.051  time: 0.5617  data_time: 0.0072  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:08:50 d2.utils.events]: [0m eta: 0:01:28  iter: 1839  total_loss: 0.530  loss_cls: 0.170  loss_box_reg: 0.286  loss_rpn_cls: 0.012  loss_rpn_loc: 0.041  time: 0.5620  data_time: 0.0075  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:09:02 d2.utils.events]: [0m eta: 0:01:17  iter: 1859  total_loss: 0.668  loss_cls: 0.212  loss_box_reg: 0.377  loss_rpn_cls: 0.017  loss_rpn_loc: 0.061  time: 0.5617  data_time: 0.0068  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:09:31 d2.utils.events]: [0m eta: 0:01:06  iter: 1879  total_loss: 0.670  loss_cls: 0.200  loss_box_reg: 0.333  loss_rpn_cls: 0.022  loss_rpn_loc: 0.064  time: 0.5638  data_time: 0.2828  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:09:42 d2.utils.events]: [0m eta: 0:00:55  iter: 1899  total_loss: 0.572  loss_cls: 0.196  loss_box_reg: 0.333  loss_rpn_cls: 0.012  loss_rpn_loc: 0.049  time: 0.5638  data_time: 0.0115  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:09:53 d2.utils.events]: [0m eta: 0:00:44  iter: 1919  total_loss: 0.649  loss_cls: 0.202  loss_box_reg: 0.352  loss_rpn_cls: 0.015  loss_rpn_loc: 0.053  time: 0.5637  data_time: 0.0110  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:10:12 d2.utils.events]: [0m eta: 0:00:33  iter: 1939  total_loss: 0.564  loss_cls: 0.179  loss_box_reg: 0.322  loss_rpn_cls: 0.013  loss_rpn_loc: 0.054  time: 0.5633  data_time: 0.0162  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:10:24 d2.utils.events]: [0m eta: 0:00:22  iter: 1959  total_loss: 0.659  loss_cls: 0.203  loss_box_reg: 0.372  loss_rpn_cls: 0.028  loss_rpn_loc: 0.067  time: 0.5629  data_time: 0.0073  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:10:35 d2.utils.events]: [0m eta: 0:00:11  iter: 1979  total_loss: 0.563  loss_cls: 0.176  loss_box_reg: 0.336  loss_rpn_cls: 0.011  loss_rpn_loc: 0.045  time: 0.5628  data_time: 0.0070  lr: 0.005000  max_mem: 3067M
[5m[31mWARNING[0m [32m[04/15 15:11:35 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 15:11:35 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[5m[31mWARNING[0m [32m[04/15 15:11:35 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[04/15 15:11:35 d2.utils.events]: [0m eta: 0:00:00  iter: 1999  total_loss: 0.584  loss_cls: 0.184  loss_box_reg: 0.331  loss_rpn_cls: 0.014  loss_rpn_loc: 0.040  time: 0.5632  data_time: 0.0077  lr: 0.005000  max_mem: 3067M
[32m[04/15 15:11:42 d2.engine.hooks]: [0mOverall training speed: 1997 iterations in 0:18:45 (0.5635 s / it)
[32m[04/15 15:11:42 d2.engine.hooks]: [0mTotal training time: 0:20:22 (0:01:37 on hooks)
4 channel input
[5m[31mWARNING[0m [32m[04/15 15:11:52 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[04/15 15:11:52 d2.data.datasets.coco]: [0mLoaded 1257 images in COCO format from ../../../Datasets/FLIR/val/thermal_annotations_4_channel.json
[32m[04/15 15:11:52 d2.evaluation.evaluator]: [0mStart inference on 1257 images
[32m[04/15 15:11:56 d2.evaluation.evaluator]: [0mInference done 11/1257. 0.1219 s / img. ETA=0:04:29
[32m[04/15 15:12:01 d2.evaluation.evaluator]: [0mInference done 42/1257. 0.1198 s / img. ETA=0:03:32
[32m[04/15 15:12:06 d2.evaluation.evaluator]: [0mInference done 72/1257. 0.1201 s / img. ETA=0:03:22
[32m[04/15 15:12:11 d2.evaluation.evaluator]: [0mInference done 100/1257. 0.1211 s / img. ETA=0:03:22
[32m[04/15 15:12:16 d2.evaluation.evaluator]: [0mInference done 128/1257. 0.1208 s / img. ETA=0:03:19
[32m[04/15 15:12:21 d2.evaluation.evaluator]: [0mInference done 150/1257. 0.1215 s / img. ETA=0:03:24
[32m[04/15 15:12:27 d2.evaluation.evaluator]: [0mInference done 183/1257. 0.1268 s / img. ETA=0:03:12
[32m[04/15 15:12:32 d2.evaluation.evaluator]: [0mInference done 214/1257. 0.1279 s / img. ETA=0:03:04
[32m[04/15 15:12:37 d2.evaluation.evaluator]: [0mInference done 235/1257. 0.1271 s / img. ETA=0:03:06
[32m[04/15 15:12:42 d2.evaluation.evaluator]: [0mInference done 256/1257. 0.1265 s / img. ETA=0:03:07
[32m[04/15 15:12:47 d2.evaluation.evaluator]: [0mInference done 278/1257. 0.1260 s / img. ETA=0:03:07
[32m[04/15 15:12:52 d2.evaluation.evaluator]: [0mInference done 297/1257. 0.1256 s / img. ETA=0:03:08
[32m[04/15 15:12:58 d2.evaluation.evaluator]: [0mInference done 319/1257. 0.1256 s / img. ETA=0:03:08
[32m[04/15 15:13:03 d2.evaluation.evaluator]: [0mInference done 346/1257. 0.1266 s / img. ETA=0:03:03
[32m[04/15 15:13:08 d2.evaluation.evaluator]: [0mInference done 368/1257. 0.1261 s / img. ETA=0:03:00
[32m[04/15 15:13:13 d2.evaluation.evaluator]: [0mInference done 403/1257. 0.1253 s / img. ETA=0:02:48
[32m[04/15 15:13:18 d2.evaluation.evaluator]: [0mInference done 436/1257. 0.1247 s / img. ETA=0:02:39
[32m[04/15 15:13:23 d2.evaluation.evaluator]: [0mInference done 472/1257. 0.1241 s / img. ETA=0:02:29
[32m[04/15 15:13:29 d2.evaluation.evaluator]: [0mInference done 506/1257. 0.1237 s / img. ETA=0:02:20
[32m[04/15 15:13:34 d2.evaluation.evaluator]: [0mInference done 539/1257. 0.1252 s / img. ETA=0:02:12
[32m[04/15 15:13:39 d2.evaluation.evaluator]: [0mInference done 574/1257. 0.1252 s / img. ETA=0:02:04
[32m[04/15 15:13:44 d2.evaluation.evaluator]: [0mInference done 616/1257. 0.1245 s / img. ETA=0:01:54
[32m[04/15 15:13:49 d2.evaluation.evaluator]: [0mInference done 644/1257. 0.1241 s / img. ETA=0:01:49
[32m[04/15 15:13:54 d2.evaluation.evaluator]: [0mInference done 670/1257. 0.1242 s / img. ETA=0:01:45
[32m[04/15 15:13:59 d2.evaluation.evaluator]: [0mInference done 691/1257. 0.1241 s / img. ETA=0:01:42
[32m[04/15 15:14:04 d2.evaluation.evaluator]: [0mInference done 720/1257. 0.1240 s / img. ETA=0:01:37
[32m[04/15 15:14:09 d2.evaluation.evaluator]: [0mInference done 754/1257. 0.1239 s / img. ETA=0:01:30
[32m[04/15 15:14:14 d2.evaluation.evaluator]: [0mInference done 788/1257. 0.1237 s / img. ETA=0:01:23
[32m[04/15 15:14:19 d2.evaluation.evaluator]: [0mInference done 808/1257. 0.1236 s / img. ETA=0:01:20
[32m[04/15 15:14:24 d2.evaluation.evaluator]: [0mInference done 833/1257. 0.1236 s / img. ETA=0:01:16
[32m[04/15 15:14:29 d2.evaluation.evaluator]: [0mInference done 857/1257. 0.1236 s / img. ETA=0:01:12
[32m[04/15 15:14:34 d2.evaluation.evaluator]: [0mInference done 886/1257. 0.1240 s / img. ETA=0:01:07
[32m[04/15 15:14:40 d2.evaluation.evaluator]: [0mInference done 915/1257. 0.1247 s / img. ETA=0:01:01
[32m[04/15 15:14:45 d2.evaluation.evaluator]: [0mInference done 942/1257. 0.1247 s / img. ETA=0:00:57
[32m[04/15 15:14:50 d2.evaluation.evaluator]: [0mInference done 962/1257. 0.1246 s / img. ETA=0:00:53
[32m[04/15 15:14:55 d2.evaluation.evaluator]: [0mInference done 986/1257. 0.1244 s / img. ETA=0:00:49
[32m[04/15 15:15:00 d2.evaluation.evaluator]: [0mInference done 1021/1257. 0.1243 s / img. ETA=0:00:43
[32m[04/15 15:15:05 d2.evaluation.evaluator]: [0mInference done 1046/1257. 0.1242 s / img. ETA=0:00:38
[32m[04/15 15:15:10 d2.evaluation.evaluator]: [0mInference done 1076/1257. 0.1242 s / img. ETA=0:00:33
[32m[04/15 15:15:15 d2.evaluation.evaluator]: [0mInference done 1103/1257. 0.1246 s / img. ETA=0:00:28
[32m[04/15 15:15:20 d2.evaluation.evaluator]: [0mInference done 1130/1257. 0.1248 s / img. ETA=0:00:23
[32m[04/15 15:15:25 d2.evaluation.evaluator]: [0mInference done 1160/1257. 0.1247 s / img. ETA=0:00:17
[32m[04/15 15:15:31 d2.evaluation.evaluator]: [0mInference done 1198/1257. 0.1245 s / img. ETA=0:00:10
[32m[04/15 15:15:36 d2.evaluation.evaluator]: [0mInference done 1220/1257. 0.1245 s / img. ETA=0:00:06
[32m[04/15 15:15:41 d2.evaluation.evaluator]: [0mInference done 1238/1257. 0.1244 s / img. ETA=0:00:03
[32m[04/15 15:15:45 d2.evaluation.evaluator]: [0mTotal inference time: 0:03:49.827442 (0.183568 s / img per device, on 1 devices)
[32m[04/15 15:15:45 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:02:35 (0.124308 s / img per device, on 1 devices)
[32m[04/15 15:15:49 d2.evaluation.FLIR_evaluation]: [0mPreparing results for COCO format ...
[32m[04/15 15:15:49 d2.evaluation.FLIR_evaluation]: [0mSaving results to output_val/coco_instances_results.json
[32m[04/15 15:15:51 d2.evaluation.FLIR_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.05s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.72s).
Accumulating evaluation results...
DONE (t=1.43s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.202
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.406
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.172
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.123
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.250
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.416
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.099
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.247
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.175
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.294
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.457
[32m[04/15 15:15:59 d2.evaluation.FLIR_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 20.188 | 40.587 | 17.226 | 12.348 | 24.970 | 41.603 |
[32m[04/15 15:15:59 d2.evaluation.FLIR_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category      | AP    | category       | AP     |
|:--------------|:-------|:--------------|:------|:---------------|:-------|
| person        | 27.065 | bicycle       | 9.971 | car            | 43.717 |
| motorcycle    | nan    | airplane      | nan   | bus            | nan    |
| train         | nan    | truck         | nan   | boat           | nan    |
| traffic light | nan    | fire hydrant  | nan   | stop sign      | nan    |
| parking meter | nan    | bench         | nan   | bird           | nan    |
| cat           | nan    | dog           | 0.000 | horse          | nan    |
| sheep         | nan    | cow           | nan   | elephant       | nan    |
| bear          | nan    | zebra         | nan   | giraffe        | nan    |
| backpack      | nan    | umbrella      | nan   | handbag        | nan    |
| tie           | nan    | suitcase      | nan   | frisbee        | nan    |
| skis          | nan    | snowboard     | nan   | sports ball    | nan    |
| kite          | nan    | baseball bat  | nan   | baseball glove | nan    |
| skateboard    | nan    | surfboard     | nan   | tennis racket  | nan    |
| bottle        | nan    | wine glass    | nan   | cup            | nan    |
| fork          | nan    | knife         | nan   | spoon          | nan    |
| bowl          | nan    | banana        | nan   | apple          | nan    |
| sandwich      | nan    | orange        | nan   | broccoli       | nan    |
| carrot        | nan    | hot dog       | nan   | pizza          | nan    |
| donut         | nan    | cake          | nan   | chair          | nan    |
| couch         | nan    | potted plant  | nan   | bed            | nan    |
| dining table  | nan    | toilet        | nan   | tv             | nan    |
| laptop        | nan    | mouse         | nan   | remote         | nan    |
| keyboard      | nan    | cell phone    | nan   | microwave      | nan    |
| oven          | nan    | bike          | nan   | hydrant        | nan    |
| motor         | nan    | rider         | nan   | light          | nan    |
| sign          | nan    | motor vehicle | nan   | human face     | nan    |
| hair drier    | nan    | license plate | nan   |                |        |
4 channel input
[5m[31mWARNING[0m [32m[04/15 15:16:00 d2.evaluation.FLIR_evaluation]: [0mjson_file was not found in MetaDataCatalog for 'FLIR'
[32m[04/15 15:16:00 d2.data.datasets.coco]: [0mConverting dataset annotations in 'FLIR' to COCO format ...)
